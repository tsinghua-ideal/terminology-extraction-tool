@inproceedings{10.1145/3676642.3729205,
author = {Heiser, Gernot},
title = {Will We Ever have Truly Secure Operating Systems?},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3729205},
doi = {10.1145/3676642.3729205},
abstract = {Half a century after PSOS, the first attempts to prove an operating system (OS) secure, OS faults remain a major threat to computer systems security. A major step forward was the verification of the seL4 microkernel, the first proof of implementation correctness of an OS kernel. Over the next 4 years this proof was extended to the binary code, proofs of security enforcement, and sound and complete worst-case execution-time analysis. The proofs now cover 4 ISAs. Yet, 15 years later, there is still no provably secure OS. While seL4 has been successfully deployed in defence and civilian security- and safety-critical systems, it is a microkernel that mostly guarantees process isolation without providing the application-oriented services expected from an OS. This not only makes seL4 difficult to deploy, but also means that there is limited assurance that a system built on top is secure in any real sense. Why has seL4 not been leveraged into a secure OS? In this talk I explore some of the reasons behind this disappointing state of affairs, and what can be done about it. Specifically, I discuss our current work on LionsOS, a new seL4-based OS targeting the embedded/cyberphysical domain, which is easy to use and designed to be verifiable. We aim to achieve our goals by a principled design driven by simplicity, modularity, strict separation of concerns, and highly use-case specific policies, rather than the standard approach of trying to generalise policies. I also discuss more speculative, early-stage work towards a provably secure, general-purpose OS},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {1–2},
numpages = {2},
keywords = {formal verification, operating systems, reliability, safety, security},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3729206,
author = {Maas, Martin},
title = {Has Machine Learning for Systems Reached an Inflection Point?},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3729206},
doi = {10.1145/3676642.3729206},
abstract = {A wide range of research areas - from natural language processing to computer vision and software engineering - have been (or are being) revolutionized by machine learning and artificial intelligence. Each of these areas went through an inflection point where they transitioned from ML as one of many approaches to ML becoming a predominant approach of the field. No example symbolizes this better than the AlexNet paper from 2012, which fundamentally transformed the field of computer vision. Computer systems remain a notable exception. In this talk, I will discuss emerging trends in the ML for Systems domain, how systems differ from these other areas, and what an ''AlexNet Moment'' for systems might look like. Along the way, I will describe a framework for categorizing work in the field and discuss emerging research problems and opportunities.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {3–4},
numpages = {2},
keywords = {cluster scheduling, code optimization, data centers, large language models, machine learning for systems, memory allocation, storage systems},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736399,
author = {Moffitt, Michael D. and Fegade, Pratik},
title = {The ASPLOS 2025 / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736399},
doi = {10.1145/3676642.3736399},
abstract = {A chief enabler of large-scale deep learning is the distribution of computation across multiple interconnected hardware accelerators. In order to unlock the maximum possible performance, a compiler must first select a reasonable strategy to parallelize a model's operations. Since neural network architectures admit multiple flavors of parallelism, determining the proper strategy for each instruction is a critical (albeit non-trivial) task. To solicit new ideas toward solving this challenging combinatorial optimization problem, we organized the ASPLOS 2025 / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning, a multi-month competition focused on advancing the state-of-the-art for model partitioning algorithms. In this paper, we offer a retrospective of this event, including the basic problem formulation, key challenges \&amp; opportunities, our new benchmark suite, and the quality of submissions received.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {5–17},
numpages = {13},
keywords = {combinatorial optimization, distributed deep learning, hardware acceleration, ml compilers},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736117,
author = {Liu, Fangxin and Li, Haomin and Zhu, Bowen and Wang, Zongwu and Song, Zhuoran and Guan, Haibing and Jiang, Li},
title = {ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736117},
doi = {10.1145/3676642.3736117},
abstract = {Neural Radiance Fields (NeRF) offer significant promise for generating photorealistic images and videos. However, existing mainstream neural rendering models often fall short in meeting the demands for immediacy and power efficiency in practical applications. Specifically, these models frequently exhibit irregular access patterns and substantial computational overhead, leading to undesirable inference latency and high power consumption. Computing-in-memory (CIM), an emerging computational paradigm, has the potential to address these access bottlenecks and reduce the power consumption associated with model execution. To bridge the gap between model performance and real-world scene requirements, we propose an algorithm-architecture co-design approach, abbreviated as ASDR, a CIM-based accelerator supporting efficient neural rendering. At the algorithmic level, we propose two rendering optimization schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of different pixels, thus reducing access memory and computational overhead. (2) Reducing MLP overhead by decoupling and approximating the volume rendering of color and density. At the architecture level, we design an efficient ReRAM-based CIM architecture with efficient data mapping and reuse microarchitecture. Experiments demonstrate that our design can achieve up to 9.55\texttimes{} and 69.75\texttimes{} speedup over state-of-the-art NeRF accelerators and Xavier NX GPU in graphics rendering tasks with only 0.1 PSNR loss.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {18–33},
numpages = {16},
keywords = {computing-in-memory (cim), hardware accelerator, neural networks, neural radiance field (nerf)},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736125,
author = {Demicco, David B. and Cole, Matthew and Yuksek, Gokturk and Gollapudi, RaviTheja and Prakash, Aravind and Ghose, Kanad and Umrigar, Zerksis},
title = {COGENT: Adaptable Compiler Toolchain for Tagging RISC-V Binaries},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736125},
doi = {10.1145/3676642.3736125},
abstract = {Tags, or metadata, enrich software with domain-specific information that is consumed by hardware to enforce security and testing policies during runtime. However, given a target architecture, developing custom compilers that encode tags can be tedious and time-consuming. We present COGENT, a highly flexible and feature-rich compiler toolchain for instruction tag generation on the RISC-V architecture. Central to this effort is a LLVM-based compiler that is supplemented with a tag-aware disassembler and a tag integrity checker. COGENT is capable of: (a) generating tags at one or more of varying granularity (per function, per basic block, or per instruction), and (b) associating variable-width tags (1-32 bits) to instructions, and arbitrary-width tags to each function or basic block. Additionally, COGENT is capable of emitting control-flow labels, which are crucial in asserting control-flow integrity (CFI), a runtime property that aids in detecting bugs and exploits that violate control flow. We evaluate the correctness of tags generated by COGENT's compiler and the associated performance penalties, along with how well COGENT preserves IR-level tags at the lower level. We provide three exemplar applications-Control Flow Integrity, Adaptive Tracing, and Hardware-Level Function Tracing that can leverage COGENT. The tagged code incurs an average cycle count overhead from 5.24\% to 0.94\% in the worst and best cases, respectively, making it ideal for debugging and testing applications, including fuzzing.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {34–47},
numpages = {14},
keywords = {compilers, instruction tagging, risc-v},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736127,
author = {Zhao, Qidong and Wu, Hao and Hao, Yueming and Ye, Zilingfeng and Li, Jiajia and Liu, Xu and Zhou, Keren},
title = {DeepContext: A Context-aware, Cross-platform, and Cross-framework Tool for Performance Profiling and Analysis of Deep Learning Workloads},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736127},
doi = {10.1145/3676642.3736127},
abstract = {Effective performance optimization of deep learning models requires comprehensive profiling across heterogeneous computing environments, yet existing tools fail to bridge the semantic gap between high-level operations and low-level execution. This paper presents DeepContext, a novel profiling system that correlates program contexts across Python code, deep learning frameworks, C/C++ libraries, and GPU execution. DeepContext features a framework-agnostic shim layer that seamlessly correlates the behavior of the deep learning framework with hardware performance metrics. Furthermore, DeepContext provides an automated performance analyzer that offers actionable optimization guidance based on its holistic view of the entire software stack of deep learning applications. DeepContext works for mainstream deep learning frameworks and runs on modern CPU+GPU architectures with low overhead. Our evaluation demonstrates that DeepContext uncovers previously hidden performance bottlenecks in real-world deep-learning applications. Guided by DeepContext, we are able to fix multiple performance issues, achieving speed-ups between 1.06\texttimes{} and 1.66\texttimes{}.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {48–63},
numpages = {16},
keywords = {automated analysis, context-aware profiling, deep learning, heterogeneous computing, performance tuning, profiling tool},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736115,
author = {Xu, Jinyan and Zhou, Yangye and Zhang, Xingzhi and Li, Yinshuai and Tan, Qinhan and Zhang, Yinqian and Zhou, Yajin and Chang, Rui and Shen, Wenbo},
title = {DejaVuzz: Disclosing Transient Execution Bugs with Dynamic Swappable Memory and Differential Information Flow Tracking Assisted Processor Fuzzing},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736115},
doi = {10.1145/3676642.3736115},
abstract = {Transient execution vulnerabilities have emerged as a critical threat to modern processors. Hardware fuzzing testing techniques have recently shown promising results in discovering transient execution bugs in large-scale out-of-order processor designs. However, their poor microarchitectural controllability and observability prevent them from effectively and efficiently detecting transient execution vulnerabilities.This paper proposes DejaVuzz, a novel pre-silicon stage processor transient execution bug fuzzer. DejaVuzz utilizes two innovative operating primitives: dynamic swappable memory and differential information flow tracking, enabling more effective and efficient transient execution vulnerability detection. The dynamic swappable memory enables the isolation of different instruction streams within the same address space. Leveraging this capability, DejaVuzz generates targeted training for arbitrary transient windows and eliminates ineffective training, enabling efficient triggering of diverse transient windows. The differential information flow tracking aids in observing the propagation of sensitive data across the microarchitecture. Based on taints, DejaVuzz designs the taint coverage matrix to guide mutation and uses taint liveness annotations to identify exploitable leakages. Our evaluation shows that DejaVuzz outperforms the state-of-the-art fuzzer SpecDoctor, triggering more comprehensive transient windows with lower training overhead and achieving a 4.7x coverage improvement. And DejaVuzz also mitigates control flow over-tainting with acceptable overhead and identifies 5 previously undiscovered transient execution vulnerabilities (with 6 CVEs assigned) on BOOM and XiangShan.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {64–80},
numpages = {17},
keywords = {hardware fuzzing, information flow tracking, processor fuzzing, transient execution bug},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736124,
author = {Zhang, Chao and Xu, Tao and Liu, Junming and Liu, Pai and Xu, Zhilang and Yan, Kang and Shi, Shuo and Li, Jintao and Li, Jinhu and Liang, Chen and Shu, Wenhui and Fan, FeiFei and Shen, Yibin and Yang, Hang and Song, Jianming and Zheng, Xudong and Wu, Jiesheng and Li, Jian},
title = {Fault Escaping: Improving Robustness of DPU Enhanced Platform with Mutual Assisted VM Recovery},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736124},
doi = {10.1145/3676642.3736124},
abstract = {Modern cloud servers achieve significant performance improvements by exploiting data processing units (DPUs) to offload virtualization overhead. Unlike traditional monolithic hypervisors, this offloading approach splits VM state and distributes hypervisor functions across the DPU and the Host, transforming the cloud server from a single system into a sophisticated orchestration of multiple self-managed processing units. However, the failure rate of these systems has significantly increased, as errors from either the DPU or the Host can crash the entire machine. All these changes necessitate a comprehensive revamp of current VM fault tolerance and isolation mechanisms.This paper explores a novel approach to VM fault tolerance by treating the split hypervisors on the DPU SoC and the Host as redundant peers. We propose a fault-escaping scheme that enables VMs to escape from a failing SoC or Host. With hybrid synchronization, the entire VM state becomes fully accessible on either the SoC or the Host with minimum synchronization overhead (in kilobytes). Instead of recovering the failed hypervisor, this scheme migrates the verified VM state, allowing the VM to escape from the failure and resume on a new machine. The evaluation demonstrates that the escape-based mechanism introduces no runtime performance overhead, achieves performance levels comparable to VM live migration, and reduces the VM recovery time objective (RTO) by 4x to 14x in one of the largest cloud production environments.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {81–95},
numpages = {15},
keywords = {cloud computing, data processing unit, fault recovery, fault tolerance, robustness, virtualization},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736126,
author = {Du, Xingran and Emer, Joel S. and Sanchez, Daniel},
title = {Hopps: Leveraging Sparsity to Accelerate Automata Processing},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736126},
doi = {10.1145/3676642.3736126},
abstract = {Automata processing (AP) is a key kernel in data analytics and scientific computing. AP workloads process a stream of symbols with many automata (FSMs) in parallel, e.g., pattern-matching network traffic against many malicious strings. The need for high-performance AP has sparked the design of specialized accelerators. But prior AP accelerators are inefficient: AP workloads have substantial sparsity, but accelerators exploit no or limited sparsity. Specifically, each AP workload can be expressed as the concurrent traversal of all automata, which are encoded as graphs. But state-of-the-art accelerators store these graphs uncompressed, using bitsets. This allows the use of specialized memory crossbars that provide high parallelism and efficiency when graphs are dense. But many graphs are highly sparse, making crossbar-based accelerators inefficient. We present Hopps, the first automata processing accelerator that exploits sparse data representations. Hopps combines two types of processing units: one represents data uncompressed, which achieves high throughput but is space-inefficient, while the other uses a compressed-sparse representation, which achieves high space efficiency but lower and more variable throughput. To use Hopps well, we present a novel automata mapping algorithm that maps most work to high-throughput units, while keeping a large fraction of state in space-efficient units. Hopps's hybrid design relaxes several constraints in crossbar-based designs, allowing for more efficient high-throughput units (e.g., by using a large number of smaller crossbars). Thus, by making the uncommon case cheap, Hopps makes the common case even faster. We evaluate Hopps on AutomataZoo benchmarks. Hopps outperforms prior state-of-the-art accelerators Impala and SpAP by gmean 2.5x and 2.2x when using equal area.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {96–111},
numpages = {16},
keywords = {accelerators, automata processing, sparsity},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736119,
author = {Song, Kevin and Yang, Jiacheng and Wang, Zixuan and Zhao, Jishen and Liu, Sihang and Pekhimenko, Gennady},
title = {HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736119},
doi = {10.1145/3676642.3736119},
abstract = {Modern workloads are demanding increasingly larger memory capacity. Compute Express Link (CXL)-based memory tiering has emerged as a promising solution for addressing this problem by utilizing traditional DRAM alongside slow-tier CXL memory devices. We analyze prior tiering systems and observe two challenges for high-performance memory tiering: adapting to skewed but dynamically varying data hotness distributions while minimizing memory and cache overhead due to tiering. To address these challenges, we propose HybridTier, an adaptive and lightweight tiering system for CXL memory. HybridTier tracks both long-term data access frequency and short-term access momentum simultaneously to accurately capture and adapt to shifting hotness distributions. HybridTier reduces the metadata memory overhead by tracking data accesses probabilistically, obtaining higher memory efficiency by trading off a small amount of tracking inaccuracy that has a negligible impact on application performance. To reduce cache overhead, HybridTier uses lightweight data structures that optimize for data locality to track data hotness. Our evaluations show that HybridTier outperforms prior systems by up to 91\% (19\% geomean), incurring 2.0-7.8x less memory overhead and 1.7-3.5x less cache misses.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {112–128},
numpages = {17},
keywords = {compute express link, tiered memory},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736129,
author = {Liu, Xuting and Pavlatos, Spyros and Liu, Yuhao and Liu, Vincent},
title = {λ-trim: Optimizing Function Initialization in Serverless Applications With Cost-driven Debloating},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736129},
doi = {10.1145/3676642.3736129},
abstract = {In this paper, we focus on an often-overlooked component of serverless application cold starts: monetary costs and Function Initialization. Traditionally considered the user's responsibility, Function Initialization is billable and accounts for more than 50\% of the monetary cost associated with cold starts in real-world machine-learning applications. We introduce λ-trim a system that optimizes Python serverless applications by eliminating redundant code while maintaining correctness. To maximize cost savings, λ-trim leverages the typical serverless pricing model to prioritize modules that significantly impact latency and memory usage. λ-trim features an automated pipeline comprising a static analyzer, a profiler specialized for the serverless pricing model, and a debloater. The optimized application can be directly deployed on serverless platforms, leading to substantial reductions in both latency and cost for cold starts.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {129–146},
numpages = {18},
keywords = {cold starts, debloating, serverless computing},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736114,
author = {Wang, Tuowei and Fan, Ruwen and Huang, Minxing and Hao, Zixu and Li, Kun and Cao, Ting and Lu, Youyou and Zhang, Yaoxue and Ren, Ju},
title = {Neuralink: Fast on-Device LLM Inference with Neuron Co-Activation Linking},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736114},
doi = {10.1145/3676642.3736114},
abstract = {Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.In this paper, we propose Neuralink, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Neuralink leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize I/O efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Neuralink achieves on average 1.49x improvements in end-to-end latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Neuralink explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design for LLM inference.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {147–162},
numpages = {16},
keywords = {large language model, mobile computing, model sparsity, parameter storage},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736128,
author = {Ruan, Jixuan and Fang, Xiang and Zhang, Hezi and Li, Ang and Humble, Travis and Ding, Yufei},
title = {PowerMove: Optimizing Compilation for Neutral Atom Quantum Computers with Zoned Architecture},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736128},
doi = {10.1145/3676642.3736128},
abstract = {Neutral atom quantum computers (NAQCs) have emerged as promising candidates for scalable quantum computing, thanks to their advanced hardware capabilities, particularly qubit movement and the Zoned Architecture (ZA). However, fully harnessing these features presents significant compilation challenges, requiring careful coordination across gate scheduling, qubit positioning, atom movement, and inter-zone communication. In this paper, we propose PowerMove, an efficient compiler for NAQCs that unlocks new optimization opportunities, significantly improving qubit movement strategies while seamlessly integrating ZA. Our evaluation demonstrates orders-of-magnitude fidelity improvements over state-of-the-art methods, with execution time reduced by up to 3.76\texttimes{} and compilation time accelerated by up to 216.9\texttimes{} across various NISQ applications. Furthermore, PowerMove extends naturally to the fault-tolerant quantum computing (FTQC) setting, where physical qubits are replaced by logical qubits encoded in QEC codes, achieving a 4.78\texttimes{} reduction in execution time. These results highlight PowerMove's impact on both near-term NISQ applications and long-term FTQC implementations. We have open-sourced our codes at https://github.com/Scarlett0815/PowerMove to facilitate further research and collaboration within the community.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {163–178},
numpages = {16},
keywords = {compiler, neutral atom quantum computing (naqc), zoned architecture (za)},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736120,
author = {Zhao, Yilong and Gao, Mingyu and Zhang, Huanchen and Liu, Fangxin and Chen, Gongye and Xian, He and Guan, Haibing and Jiang, Li},
title = {PUSHtap: PIM-based In-Memory HTAP with Unified Data Storage Format},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736120},
doi = {10.1145/3676642.3736120},
abstract = {Hybrid transaction/analytical processing (HTAP) is an emerging database paradigm that supports both online transaction processing (OLTP) and online analytical processing (OLAP) workloads. Computing-intensive OLTP operations, involving row-wise data manipulation, are suitable for row-store format. In contrast, memory-intensive OLAP operations, which are column-centric, benefit from column-store format. This data-format dilemma prevents HTAP systems from concurrently achieving three design goals: performance isolation, data freshness, and workload-specific optimization. Another background technology is Processing-in-Memory (PIM), which integrates computing units (PIM units) inside DRAM memory devices to accelerate memory-intensive workloads, including OLAP. Our key insight is to combine the interleaved CPU access and localized PIM unit access to provide two-dimensional access to address the data format contradictions inherent in HTAP. First, we propose a unified data storage format with novel data alignment and placement techniques to optimize the effective bandwidth of CPUs and PIM units and exploit the PIM's parallelism. Second, we implement the multi-version concurrency control (MVCC) essential for single-instance HTAP. Third, we extend the commercial PIM architecture to support the OLAP operations and concurrent access from PIM and CPU. Experiments show that PUSHtap can achieve 3.4X/4.4X OLAP/OLTP throughput improvement compared to multi-instance PIM-based design.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {179–194},
numpages = {16},
keywords = {dram, hybrid transactional/analytical processing (htap), processing-in-memory (pim), unified data format},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736123,
author = {Ryan, Kaki and Sturton, Cynthia},
title = {SYLQ-SV: Scaling Symbolic Execution of Hardware Designs with Query Caching},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736123},
doi = {10.1145/3676642.3736123},
abstract = {Symbolic execution of hardware designs is a path-based analysis that can deliver high quality coverage and security verification results. Unfortunately, the technique has historically struggled with the path explosion problem and, despite recent advances, remains expensive. We present SylQ-SV, a dedicated SystemVerilog symbolic execution engine that uses SMT query caching to improve execution times, reducing run time by up to 17\% over the state of the art. SylQ-SV provides language support for all necessary SystemVerilog constructs, including SystemVerilog Assertions, to provide end-to-end verification workflows of the open-source designs most commonly appearing in the literature. We evaluate SYLQ-SV on the OR1200 CPU, the OpenTitan SoC with Ibex core, and two SoC designs from the HACK@DAC competitions (with PULPissimo core and CVA6 core, respectively). We make the SylQ-SV source code and all data used in the evaluation publicly available.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {195–211},
numpages = {17},
keywords = {hardware security, query caching, symbolic execution},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736118,
author = {Zhuo, Yongqi and Su, Zhengyuan and Zhao, Chenggang and Gao, Mingyu},
title = {Syno: Structured Synthesis for Neural Operators},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736118},
doi = {10.1145/3676642.3736118},
abstract = {The desires for better prediction accuracy and higher execution performance in neural networks never end. Neural architecture search (NAS) and tensor compilers are two popular techniques to optimize these two goals, but they are both limited to composing or optimizing existing manually designed operators rather than coming up with completely new designs. In this work, we explore the less studied direction of neural operator synthesis, which aims to automatically and efficiently discover novel neural operators with better accuracy and/or speed. We develop an end-to-end framework Syno, to realize practical neural operator synthesis. Syno makes use of a novel set of fine-grained primitives defined on tensor dimensions, which ensure various desired properties to ease model training, and also enable expression canonicalization techniques to avoid redundant candidates during search. Syno further adopts a novel guided synthesis flow to obtain valid operators matched with the specified input/output dimension sizes, and leverages efficient stochastic tree search algorithms to quickly explore the design space. We demonstrate that Syno discovers better operators with average speedups of 1.37\texttimes{} to 2.06\texttimes{} on various hardware and compiler choices, while keeping less than 1\% accuracy loss even on NAS-optimized models.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {212–229},
numpages = {18},
keywords = {neural architecture search, program synthesis},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736116,
author = {Zhao, Jin and Wang, Qian and He, Ligang and Zhang, Yu and Di, Sheng and He, Bingsheng and Wang, Xinlei and Yu, Hui and Qi, Hao and Lin, Longlong and Yu, Linchen and Liao, Xiaofei and Jin, Hai},
title = {TempGraph: An Efficient Chain-driven Temporal Graph Computing Framework on the GPU},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736116},
doi = {10.1145/3676642.3736116},
abstract = {Tackling temporal path problems in temporal graphs is essential for time-sensitive applications. Although many solutions have been proposed to handle temporal path problems, due to the intrinsic time constraints, these solutions require the vertices of the temporal graph to be sequentially handled along the time-dependent chains (i.e., the temporal dependencies between these vertices) to form the temporal path. This sequential temporal nature poses the challenges of poor parallelism and slow convergence speed, preventing existing solutions from fully leveraging the massive parallelism and high internal bandwidth of GPU to handle temporal path problems. To overcome these challenges, this paper proposes TempGraph, an efficient chain-driven GPU-based temporal graph computing framework. Specifically, it transforms the temporal graph into a set of disjoint time-dependent chains that can elegantly expose the temporal dependency between the vertices while facilitating the fast path exploration along these chains over GPU. Furthermore, TempGraph employs a novel Generate-Activate-Compute execution model to decouple the temporal dependency between different chains through maintaining a set of shortcuts for them, which enables multiple chains to be concurrently handled by massive GPU threads, achieving fast convergence speed and high parallelism on the GPU. Experiments on an A100 GPU show that TempGraph outperforms the state-of-the-art GPU-based solutions by 3.0-16.2\texttimes{}. Besides, TempGraph on an A100 GPU gains 33.9-368.9\texttimes{} speedups compared to the cutting-edge CPU-based system TeGraph on a 128-core CPU machine.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {230–246},
numpages = {17},
keywords = {convergence speed, data parallelism, gpu, temporal graph computing},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736121,
author = {Zhou, Yangjie and Shen, Wenting and Leng, Jingwen and Lu, Shuwen and Liu, Zihan and Cui, Weihao and Zhang, Zhendong and Xiao, Wencong and Ai, Baole and Li, Yong and Lin, Wei and Zeng, Deze and Liang, Yun and Chen, Quan and Liu, Ning and Guo, Minyi},
title = {Voyager: Input-Adaptive Algebraic Transformations for High-Performance Graph Neural Networks},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736121},
doi = {10.1145/3676642.3736121},
abstract = {Graph neural networks (GNNs) are gaining popularity in diverse application domains and growing in complexity. As a result, it is crucial to achieve high-performance GNN execution. Among various techniques, algebraic transformations, including operator reordering and operator fusion, have been successfully applied to improve the computation and memory access efficiencies of DNN models. However, directly applying these methods to GNNs results in suboptimal performance. This is primarily due to the diversity of input graph structures, making it challenging to predict performance benefits. Additionally, the complex dependencies introduced by these input graph structures limit the effectiveness of existing optimization techniques.To address these challenges, we propose Voyager, an input-adaptive compiler designed specifically for GNNs. Unlike existing methods that rely on static optimization rules, Voyager dynamically generates and evaluates a wide range of algebraic transformation candidates, including novel reordering and fusion strategies tailored to graph characteristics. By leveraging symbolic graph attributes, Voyager efficiently estimates tensor shapes and prunes low-performance candidates. A runtime selection module further enhances performance by choosing the optimal transformation based on real-time input features. Our evaluation shows that Voyager achieves an average 3.5 \texttimes{} speedup on GPUs over the state-of-the-art works on representative GNNs and datasets.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {247–263},
numpages = {17},
keywords = {compilation optimization, graph neural networks, high-performance computing},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3676642.3736113,
author = {Humphries, Jack Tigar and Natu, Neel and Kaffes, Kostis and Novakovi\'{c}, Stanko and Turner, Paul and Levy, Henry M. and Culler, David and Kozyrakis, Christos},
title = {Wave: Offloading Resource Management to SmartNIC Cores},
year = {2025},
isbn = {9798400710803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676642.3736113},
doi = {10.1145/3676642.3736113},
abstract = {SmartNICs are increasingly deployed in datacenters to offload tasks from server CPUs, improving the efficiency and flexibility of datacenter security, networking and storage. Optimizing cloud server efficiency in this way is critically important to ensure that virtually all server resources are available to paying customers. Userspace system software, specifically, decision-making tasks performed by various operating system subsystems, is particularly well suited for execution on mid-tier SmartNIC ARM cores. To this end, we introduce Wave, a framework for offloading userspace system software to processes/agents running on the SmartNIC. Wave uses Linux userspace systems to better align system functionality with SmartNIC capabilities. It also introduces a new host-SmartNIC communication API that enables offloading of even μs-scale system software. To evaluate Wave, we offloaded preexisting userspace system software including kernel thread scheduling, memory management, and an RPC stack to SmartNIC ARM cores, which showed a performance degradation of 1.1\%-7.4\% in an apples-to-apples comparison with on-host implementations. Wave recovered host resources consumed by on-host system software for memory management (saving 16 host cores), RPCs (saving 8 host cores), and virtual machines (an 11.2\% performance improvement). Wave highlights the potential for rethinking system software placement in modern datacenters, unlocking new opportunities for efficiency and scalability.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {264–281},
numpages = {18},
keywords = {operating systems, smartnics},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

