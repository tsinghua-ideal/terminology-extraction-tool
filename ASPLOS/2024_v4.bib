@inproceedings{10.1145/3622781.3674183,
author = {Williams, Harrison and Hicks, Matthew},
title = {A Software Caching Runtime for Embedded NVRAM Systems},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674183},
doi = {10.1145/3622781.3674183},
abstract = {Increasingly sophisticated low-power microcontrollers are at the heart of millions of IoT and edge computing deployments, with developers pushing large-scale data collection, processing, and inference to end nodes. Advanced workloads on resource-constrained systems depend on emerging technologies to meet performance and lifetime demands. High-performance Non-Volatile RAMs (NVRAMs) are one such technology enabling a new class of systems previously made impossible by memory limitations, including ultra-low-power designs using program state non-volatility and sensing systems storing and processing large blocks of data.Unfortunately, existing NVRAM significantly underperforms SRAM's access latency/energy cost and flash's read performance---condemning systems dependent on NVRAM to pay a steep energy and time penalty for software execution. We observe that this performance penalty stems predominately from instruction fetches into NVRAM, which represent &gt;75\% of memory accesses in typical embedded software. To eliminate this performance bottleneck, we propose SwapRAM, a new operating model for NVRAM-based platforms which repurposes underutilized SRAM as an instruction cache, maximizing the proportion of accesses directed towards higher-performance SRAM. SwapRAM consists of a set of compile-time code transformations and a runtime management system that transparently and dynamically copies code into SRAM throughout execution, with an extensible logic to delay eviction of hot code. Across nine embedded benchmarks running on a real FRAM platform, SwapRAM's software-based design increases execution speed by up to 46\% (average 26\%) and reduces energy consumption by up to 36\% (average 24\%) compared to a baseline system using the existing hardware cache.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {1–16},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674176,
author = {G\'{o}mez-Hern\'{a}ndez, Eduardo Jos\'{e} and Cebrian, Juan M. and Kaxiras, Stefanos and Ros, Alberto},
title = {Bounding Speculative Execution of Atomic Regions to a Single Retry},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674176},
doi = {10.1145/3622781.3674176},
abstract = {Mutual exclusion has long served as a fundamental construct in parallel programs. Despite a long history of optimizing the lower-level lock and unlock operations used to enforce mutual exclusion, such operations largely dictate performance in parallel programs. Speculative Lock Elision, and more generally Hardware Transactional Memory, allow executing atomic regions (ARs) concurrently and speculatively, and ensure correctness by using conflict detection. However, practical implementations of these ideas are best-effort and, in case of conflicts, the execution of ARs is retried a predetermined number of times before falling back to mutual exclusion.This work explores the opportunities of using cacheline locking to bound the number of retries of speculative solutions. Our key insight is that ARs that access exactly the same set of addresses when re-executing can learn that set in the first execution and execute non-speculatively in the next one by performing an ordered cacheline locking. This way the speculative execution is bounded to a single retry.We first establish the conditions for ARs to be able to re-execute under a cacheline-locked mode. Based on these conditions, we propose cleAR, cacheline-locked executed AR, a novel technique that on the first abort, forces the re-execution to use cacheline locking. The detection and conversion to cacheline-locking mode is transparent to software.Using gem5 running data-structure benchmarks and the STAMP benchmark suite, we show that the average number of ARs that succeed on the first retry grows from 35.4\% in our baseline to 64.4\% with cleAR, reducing the percentage of fallback (coarse-grain mutual exclusion) execution from 37.2\% to 15.4\%. These improvements reduce average execution time by 35.0\% over a baseline configuration and by 23.3\% over more elaborated approaches like PowerTM.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {17–30},
numpages = {14},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674189,
author = {Khan, Asif Ali and Farzaneh, Hamid and Friebel, Karl Friedrich Alexander and Fournier, Cl\'{e}ment and Chelini, Lorenzo and Castrillon, Jeronimo},
title = {CINM (Cinnamon): A Compilation Infrastructure for Heterogeneous Compute In-Memory and Compute Near-Memory Paradigms},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674189},
doi = {10.1145/3622781.3674189},
abstract = {The rise of data-intensive applications exposed the limitations of conventional processor-centric von-Neumann architectures that struggle to meet the off-chip memory bandwidth demand. Therefore, recent innovations in computer architecture advocate compute-in-memory (CIM) and compute-near-memory (CNM), non-von-Neumann paradigms achieving orders-of-magnitude improvements in performance and energy consumption. Despite significant technological breakthroughs in the last few years, the programmability of these systems is still a serious challenge. Their programming models are too low-level and specific to particular system implementations. Since such future architectures are predicted to be highly heterogeneous, developing novel compiler abstractions and frameworks becomes necessary. To this end, we present CINM (Cinnamon), a first end-to-end compilation flow that leverages the hierarchical abstractions to generalize over different CIM and CNM devices and enable device-agnostic and device-aware optimizations. Cinnamon progressively lowers input programs and performs optimizations at each level in the lowering pipeline. To show its efficacy, we evaluate CINM on a set of benchmarks for a real CNM system (UPMEM) and the memristors-based CIM accelerators. We show that Cinnamon, supporting multiple hardware targets, generates high-performance code comparable to or better than state-of-the-art implementations.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {31–46},
numpages = {16},
keywords = {hardware emerging architectures, hardware emerging tools and methodologies, hardware emerging languages and compilers, computing methodologies parallel computing methodologies},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674178,
author = {Seifert, Lennart Maximilian and Dangwal, Siddharth and Chong, Frederic T. and Ravi, Gokul Subramanian},
title = {Clapton: Clifford Assisted Problem Transformation for Error Mitigation in Variational Quantum Algorithms},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674178},
doi = {10.1145/3622781.3674178},
abstract = {Variational quantum algorithms (VQAs) show potential for quantum advantage in the near term of quantum computing, but demand a level of accuracy that surpasses the current capabilities of NISQ devices. To systematically mitigate the impact of quantum device error on VQAs, we propose Clapton: Clifford-Assisted Problem Transformation for Error Mitigation in Variational Quantum Algorithms. Clapton leverages classically estimated good quantum states for a given VQA problem, classical simulable models of device noise, and the variational principle for VQAs. It applies transformations on the VQA problem's Hamiltonian to lower the energy estimates of known good VQA states in the presence of the modeled device noise. The Clapton hypothesis is that as long as the known good states of the VQA problem are close to the problem's ideal ground state and the device noise modeling is reasonably accurate (both of which are generally true), then the Clapton transformation substantially decreases the impact of device noise on the ground state of the VQA problem, thereby increasing the accuracy of the VQA solution. Clapton is built as an end-to-end application-to-device framework and achieves mean VQA initialization improvements of 1.7x to 3.7x, and up to a maximum of 13.3x, over the state-of-the-art baseline when evaluated for a variety of scientific applications from physics and chemistry on noise models and real quantum devices.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {47–62},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674170,
author = {Sisco, Zachary D. and Alex, Andrew David and Ma, Zechen and Aghamohammadi, Yeganeh and Kong, Boming and Darnell, Benjamin and Sherwood, Timothy and Hardekopf, Ben and Balkind, Jonathan},
title = {Control Logic Synthesis: Drawing the Rest of the OWL},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674170},
doi = {10.1145/3622781.3674170},
abstract = {System-on-chip (SoC) design requires complex reasoning about the interactions between an architectural specification, the microarchitectural datapath (e.g., functional units), and the control logic (which coordinates the datapath) to facilitate the critical computing tasks on which we all depend. Hardware specialization is now the expectation rather than the exception, meaning we need new hardware design tools to bring ideas to reality with both agility and correctness.We introduce a new technique, "control logic synthesis", which automatically generates control logic given a datapath description and an architectural specification. This enables an entirely new hardware design process where the designer only needs to write a datapath sketch, leaving the control logic as "holes." Then, guided by an architectural specification, we adapt program synthesis techniques to automatically generate a correct hardware implementation of the control logic, filling the holes and completing the design.We evaluate control logic synthesis over two classes of control (state machines and instruction decoders) and different architectures (embedded-class RISC-V cores and hardware accelerators for cryptography). We demonstrate how agile-oriented SoC developers can iterate over designs without writing control logic by hand yet still retain formal assurances with only minimal microarchitectural information.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {63–78},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674184,
author = {Wang, Haoyuan and Nijssen, Thomas and Beamer, Scott},
title = {Don't Repeat Yourself! Coarse-Grained Circuit Deduplication to Accelerate RTL Simulation},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674184},
doi = {10.1145/3622781.3674184},
abstract = {Designing a digital integrated circuit requires many register transfer level (RTL) simulations for design, debugging, and especially verification. To cope with the slow speed of RTL simulation, industry frequently uses private server farms to run many simulations in parallel. Surprisingly, the implications of parallel runs of different RTL simulations have not been extensively explored. Moreover, in modern digital hardware, there is a growing trend to replicate components to scale out. However, the potential for circuit deduplication has been mostly overlooked.In this work, we pinpoint the shared last-level cache as the primary bottleneck impacting the throughput of RTL simulation. To address this issue, we propose a coarse-grained circuit deduplication strategy integrated into an RTL simulator. Our method involves identifying multiple instances of a single module within a digital circuit and creating shared code that can be applied to all of these instances. Our approach reduces the cache footprint by increasing code reuse, which consequently benefits processor components such as caches and branch predictors. Our experiments demonstrate that deduplication can bring up to 1.95\texttimes{} speedup in a single simulation, and achieve up to 2.09\texttimes{} overall RTL simulation throughput.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {79–93},
numpages = {15},
keywords = {RTL simulation, simulation throughput, deduplication},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674167,
author = {Zhu, Zeyu and Wang, Peisong and Hu, Qinghao and Li, Gang and Liang, Xiaoyao and Cheng, Jian},
title = {FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674167},
doi = {10.1145/3622781.3674167},
abstract = {Graph Neural Networks (GNNs) have shown great superiority on non-Euclidean graph data, achieving ground-breaking performance on various graph-related tasks. As a practical solution to train GNN on large graphs with billions of nodes and edges, the sampling-based training is widely adopted by existing training frameworks. However, through an in-depth analysis, we observe that the efficiency of existing sampling-based training frameworks is still limited due to the key bottlenecks lying in all three phases of sampling-based training, i.e., subgraph sample, memory IO, and computation. To this end, we propose FastGL, a GPU-efficient Framework for accelerating sampling-based training of GNN at Large scale by simultaneously optimizing all above three phases, taking into account both GPU characteristics and graph structure. Specifically, by exploiting the inherent overlap within graph structures, FastGL develops the Match-Reorder strategy to reduce the data traffic, which accelerates the memory IO without incurring any GPU memory overhead. Additionally, FastGL leverages a Memory-Aware computation method, harnessing the GPU memory's hierarchical nature to mitigate irregular data access during computation. FastGL further incorporates the Fused-Map approach aimed at diminishing the synchronization overhead during sampling. Extensive experiments demonstrate that FastGL can achieve an average speedup of 11.8\texttimes{}, 2.2\texttimes{} and 1.5\texttimes{} over the state-of-the-art frameworks PyG, DGL, and GNNLab, respectively. Our code is available at https://github.com/a1bc2def6g/fastgl-ae.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {94–110},
numpages = {17},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674185,
author = {Li, Yingheng and Pawar, Aditya and Mo, Zewei and Zhang, Youtao and Yang, Jun and Tang, Xulong},
title = {FMCC: Flexible Measurement-based Quantum Computation over Cluster State},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674185},
doi = {10.1145/3622781.3674185},
abstract = {Measurement-based quantum computing (MBQC) is a promising quantum computing paradigm that performs computation through "one-way" measurements on entangled quantum qubits. It is widely used in photonic quantum computing (PQC), where the computation is carried out on photonic cluster states (i.e., a 2-D mesh of entangled photons). In MBQC-based PQC, the cluster state depth (i.e., the length of one-way measurements) plays an important role in the overall execution time and circuit error. In this paper, we propose FMCC, a compilation framework that employs dynamic programming with heuristics to efficiently minimize the cluster state depth. Experimental results on six quantum applications show that FMCC achieves 51.7\%, 57.4\%, and 56.8\% average depth reductions in small, medium, and large qubit counts compared to the state-of-the-art MBQC compilations.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {111–126},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674186,
author = {Gerami, Armin and Asgari, Bahar},
title = {GUST: Graph Edge-Coloring Utilization for Accelerating Sparse Matrix Vector Multiplication},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674186},
doi = {10.1145/3622781.3674186},
abstract = {Sparse matrix-vector multiplication (SpMV) plays a vital role in various scientific and engineering fields, from scientific computing to machine learning. Traditional general-purpose processors often fall short of their peak performance with sparse data, leading to the development of domain-specific architectures to enhance SpMV. Yet, these specialized approaches, whether tailored explicitly for SpMV or adapted from matrix-matrix multiplication accelerators, still face challenges in fully utilizing hardware resources as a result of sparsity. To tackle this problem, we introduce GUST, a hardware/software co-design, the key insight of which lies in separating multipliers and adders in the hardware, thereby enabling resource sharing across multiple rows and columns, leading to efficient hardware utilization and ameliorating negative performance impacts from sparsity. Resource sharing, however, can lead to collisions, a problem we address through a specially devised edge-coloring scheduling algorithm. Our comparisons with various prior domain specific architectures using real-world datasets shows the effectiveness of GUST, with an average hardware utilization of 33.67\%. We further evaluate GUST by comparing SpMV execution time and energy consumption of length-256 and -87 GUST with length-256 1-dimensional systolic array (1D), achieving an average speedup of 411\texttimes{} and 108\texttimes{}, and energy efficiency improvement of 137\texttimes{} and 148\texttimes{}, respectively. To asses the implementation aspect, we compare resource consumption of GUST with 1D as a baseline through FPGA synthesis. Length-256 GUST uses the same number of arithmetic units as length-256 1D, while length-87 GUST uses considerably less. We also compare GUST with Serpens, a state-of-the-art FPGA-based SpMV accelerator, with GUST achieving lower execution time on seven out of nine matrices and lower energy consumption on four.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {127–141},
numpages = {15},
keywords = {SpMV, SpMV accelerator, edge coloring, FPGA},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3698899,
author = {Zhang, Ziqing and Weng, Weijie and Li, Yaning and Cai, Lijia and Wang, Haoyu and Boland, David and Bao, Yungang and Shi, Kan},
title = {Hassert: Hardware Assertion-Based Verification Framework with FPGA Acceleration},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3698899},
doi = {10.1145/3622781.3698899},
abstract = {Hardware verification is typically the bottleneck of the chip development cycle, mainly due to the time-consuming simulation and debugging process using software simulators. Assertion-Based Verification (ABV) has been widely adopted to provide better visibility into microarchitecture details and automatically detect unexpected behaviors. While ABV significantly improves verification efficiency, checking assertions using software simulators requires extremely long times for large benchmarks. Prototyping designs on an FPGA is a potential alternative to verify hardware, but it lacks fine-grained debugging capabilities for when errors occur.To address these challenges, we present Hassert, an efficient ABV framework that combines high-performance verification on FPGAs with fine-grained debugging in software. Hassert automates the scheduling and mapping of SystemVerilog Assertions (SVAs) to the available FPGA fabric with the design-under-test (DUT), allowing for extensive hardware testing. Hassert also enables dynamic switching between different assertions, either user-specified or based on SVA coverage satisfaction, by partially reconfiguring the FPGA at runtime, eliminating the need to recompile the DUT.To further improve debugging efficiency, we also propose a microarchitecture-guided hardware snapshot scheme. If any assertion is fired, Hassert automatically generates snapshots of the current status of the entire FPGA hardware. These snapshots are then transferred to an external simulator, where the operation is reconstructed in software for further debugging. We demonstrate that these contributions can improve significant verification efficiency over traditional software simulation-based approaches for various hardware benchmarks and RISC-V processor designs whilst maintaining full visibility and debugging capabilities at the cost of only a small area overhead.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {142–154},
numpages = {13},
keywords = {FPGA, debugging, assertion, verification, emulation, acceleration},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674181,
author = {Pei, Qi and Wang, Yipeng and Shin, Seunghee},
title = {Litmus: Fair Pricing for Serverless Computing},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674181},
doi = {10.1145/3622781.3674181},
abstract = {Serverless computing has emerged as a market-dominant paradigm in modern cloud computing, benefiting both cloud providers and tenants. While service providers can optimize their machine utilization, tenants only need to pay for the resources they use. To maximize resource utilization, these serverless systems co-run numerous short-lived functions, bearing frequent system condition shifts. When the system gets overcrowded, a tenant's function may suffer from disturbing slowdowns. Ironically, tenants also incur higher costs during these slowdowns, as commercial serverless platforms determine costs proportional to their execution times.This paper argues that cloud providers should compensate tenants for losses incurred when the server is over-provisioned. However, estimating tenants' losses is challenging without pre-profiled information about their functions. Prior studies have indicated that assessing tenant losses leads to heavy overheads. As a solution, this paper introduces a new pricing model that offers discounts based on the machine's state while presuming the tenant's loss under that state. To monitor the machine state accurately, Litmus pricing frequently conducts Litmus tests, an effective and lightweight solution for measuring system congestion. Our experiments show that Litmus pricing can accurately gauge the impact of system congestion and offer nearly ideal prices, with only a 0.2\% price difference on average, in a heavily congested system.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {155–169},
numpages = {15},
keywords = {serverless computing, congestion estimation, resource sharing, online pricing},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674177,
author = {Ye, Chengfeng and Cai, Yuandao and Zhou, Anshunkang and Huang, Heqing and Ling, Hao and Zhang, Charles},
title = {Manta: Hybrid-Sensitive Type Inference Toward Type-Assisted Bug Detection for Stripped Binaries},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674177},
doi = {10.1145/3622781.3674177},
abstract = {Static binary bug detection has been a prominent approach for ensuring the security of binaries used in our daily lives. However, the type information lost in binaries prevents the improvement opportunity for a static analyzer to utilize type information to prune away infeasible facts and increase analysis precision. To make binary bug detection more practical with higher precision, in this work, we propose the first hybrid-sensitive type inference, Manta, that combines data-flow analysis with different sensitivities to complement each other and infer precise types for many variables. The inferred types are then used to assist with bug detection by pruning infeasible indirect call targets and data dependencies. Our experiments indicate Manta outperforms prior work by inferring types with 78.7\% precision and 97.2\% recall. Based on the inferred types, we can prune away 63.9\% more infeasible indirect-call targets compared to existing type analysis techniques and perform program slicing on binaries with 61.1\% similarity to that on source code. Moreover, Manta has led to 86 new developer-confirmed vulnerabilities in many popular IoT firmware, with 64 CVE/PSV IDs assigned.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {170–187},
numpages = {18},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674172,
author = {Fan, Ruwen and Xie, Minhui and Jiang, Haodi and Lu, Youyou},
title = {MaxEmbed: Maximizing SSD bandwidth utilization for huge embedding models serving},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674172},
doi = {10.1145/3622781.3674172},
abstract = {Deep learning recommendation models (DLRMs) have gained widespread application across search, advertising, and e-commerce. Still, DLRMs present notable challenges as they depend heavily on large embedding tables to represent sparse features in recommendation systems. This raises concerns about both memory capacity and cost. Solid-state drives (SSDs) offer a cost-effective solution with a significantly larger capacity, but they introduce read amplification issues because of the mismatch between embedding size and SSD read granularity. Prior SSD embedding storage systems aim to tackle these challenges by employing hypergraph partitioning to co-locate co-appearing embeddings onto the same SSD page, alleviating read amplification. However, this approach has a drawback as it divides embeddings into completely disjoint clusters, limiting potential combinations between embeddings.In response to this limitation, we introduce MaxEmbed. Capitalizing on the extensive storage capacity of SSDs, Max-Embed effectively mines relationships between storage combinations of embeddings with replication, thereby enhancing the effective bandwidth of SSDs. Additionally, MaxEmbed incorporates a corresponding online service module for embedding query request handling, leveraging two key optimizations to reduce the overhead brought by replication. Our evaluations demonstrate that MaxEmbed boosts SSD embedding serving throughput by up to 18.7\% under various settings.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {188–202},
numpages = {15},
keywords = {embedding models, SSD, data placement, data replication},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674175,
author = {Wang, Yongqin and Rajat, Rachit and Annavaram, Murali},
title = {MPC-Pipe: an Efficient Pipeline Scheme for Semi-honest MPC Machine Learning},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674175},
doi = {10.1145/3622781.3674175},
abstract = {Multi-party computing (MPC) has been gaining popularity as a secure computing model over the past few years. However, prior works have demonstrated that MPC protocols still pay substantial performance penalties compared to plaintext, particularly when applied to ML algorithms. The overhead is due to added computation and communication costs. Prior studies, as well as our own analysis, found that most MPC protocols today sequentially perform communication and computation. The participating parties must compute on their shares first and then perform data communication to allow the distribution of new secret shares before proceeding to the next computation step. In this work, we show that serialization is unnecessary, particularly in the context of ML computations (both in Convolutional neural networks and in Transformer-based models). We demonstrate that it is possible to carefully orchestrate the computation and communication steps to overlap.We propose MPC-Pipe, an efficient MPC system for both training and inference of ML workloads, which pipelines computations and communications in an MPC protocol during the online phase. MPC-Pipe proposes three pipeline schemes to optimize the online phase of ML in the semi-honest majority adversary setting. The three pipeline schemes are 1) inter-linear pipeline, 2) inner-layer pipeline, and 3) inter-batch pipeline. Inter-linear pipeline focuses on linear layers; inner-layer pipeline focuses on non-linear layers; inter-batch pipeline focuses on communication and computation overlaps in different input batches. We implement MPC-Pipe by augmenting a modified version of CrypTen, which separates online and offline phases. We evaluate the end-to-end system performance benefits of the online phase of MPC using deep neural networks (VGG16, ResNet50) and Transformers using different network settings. We show that MPC-Pipe can improve the throughput and latency of ML workloads.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {203–219},
numpages = {17},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674182,
author = {Ma, Jiacheng and Ganaiem, Majd and Burbage, Madeline and Gregersen, Theo and McAmis, Rachel and Gabbay, Freddy and Kasikci, Baris},
title = {Proactive Runtime Detection of Aging-Related Silent Data Corruptions: A Bottom-Up Approach},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674182},
doi = {10.1145/3622781.3674182},
abstract = {Recent advancements in semiconductor process technologies have unveiled the susceptibility of hardware circuits to reliability issues, especially those related to transistor aging. Transistor aging gradually degrades gate performance, eventually causing hardware to behave incorrectly. Such misbehaving hardware can result in silent data corruptions (SDCs) in software---a type of failure that comes without logs or exceptions, but causes miscomputing instructions, bitflips, and broken cache coherency. Alas, while design efforts can be made to mitigate transistor aging, complete elimination of this problem during design and fabrication cannot be guaranteed. This emerging challenge calls for a mechanism that not only detects potentially aged hardware in the field, but also triggers software mitigations at application runtime.We propose Vega, a novel workflow that allows efficient detection of aging-related failures at software runtime. Vega leverages the well-studied gate-level modeling of aging effects to identify susceptible signal propagation paths that could fail due to transistor aging. It then utilizes formal verification techniques to generate short test cases that activate these paths and detect any failure within them. Vega integrates the test cases into a user application by directly fusing them together, or by packaging the test cases into a library that the application can invoke. We demonstrate our proposed techniques on the arithmetic logic unit and floating-point unit of a RISC-V CPU. We show that Vega generates effective test cases and integrates them into applications with an average of 0.8\% performance overhead.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {220–235},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674179,
author = {Pawar, Aditya and Li, Yingheng and Mo, Zewei and Guo, Yanan and Tang, Xulong and Zhang, Youtao and Yang, Jun},
title = {QRCC: Evaluating Large Quantum Circuits on Small Quantum Computers through Integrated Qubit Reuse and Circuit Cutting},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674179},
doi = {10.1145/3622781.3674179},
abstract = {Quantum computing has recently emerged as a promising computing paradigm for many application domains. However, the size of quantum circuits that can be run with high fidelity is constrained by the limited quantity and quality of physical qubits. Recently proposed schemes, such as wire cutting and qubit reuse, mitigate the problem but produce sub-optimal results as they address the problem individually. In addition, gate cutting, an alternative circuit-cutting strategy that is suitable for circuits computing expectation values, has not been fully explored in the field.In this paper, we propose QRCC, an integrated approach that exploits qubit reuse and circuit-cutting (including wire cutting and gate cutting) to run large circuits on small quantum computers. Circuit-cutting techniques introduce non-negligible post-processing overhead, which increases exponentially with the number of cuts. QRCC exploits qubit reuse to find better cutting solutions to minimize the cut numbers and thus the post-processing overhead. Our evaluation results show that on average we reduce the number of cuts by 29\% and additional reduction when considering gate cuts.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {236–251},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674169,
author = {Zou, Yu and Li, Yiran and Wang, Sheng and Su, Le and Gu, Zhen and Lu, Yanheng and Guan, Yijin and Niu, Dimin and Gao, Mingyu and Xie, Yuan and Li, Feifei},
title = {Salus: A Practical Trusted Execution Environment for CPU-FPGA Heterogeneous Cloud Platforms},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674169},
doi = {10.1145/3622781.3674169},
abstract = {CPU-FPGA heterogeneous architectures have become increasingly popular in cloud environments for accelerating compute-intensive tasks. Ensuring the protection of sensitive data processed by these architectures requires the presence of a trusted execution environment (TEE). This work highlights the requirements for designing an FPGA TEE, the challenges faced in deploying existing solutions on commercial-off-the-shelf (COTS) cloud FPGA services, and the limitations of previous works that primarily focus on standalone FPGA TEEs. In response to these challenges, Salus introduces an innovative approach by leveraging an enclave running on the host with a TEE-enabled CPU. This approach aims to protect and attest the bitstream loaded on the FPGA side. By repurposing COTS FPGA bitstream utilities in a novel manner and adopting a proposed security-enhanced FPGA IP, Salus presents a practical design for an FPGA TEE, with minor efforts required.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {252–266},
numpages = {15},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674190,
author = {Castes, Charly and Baumann, Andrew},
title = {Sharing is leaking: blocking transient-execution attacks with core-gapped confidential VMs},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674190},
doi = {10.1145/3622781.3674190},
abstract = {Confidential VMs on platforms such as Intel TDX, AMD SEV and Arm CCA promise greater security for cloud users against even a hypervisor-level attacker, but this promise has been shattered by repeated transient-execution vulnerabilities and CPU bugs. At the root of this problem lies the need to multiplex CPU cores with all their complex microarchitectural state among distrusting entities, with an untrusted hypervisor in control of the multiplexing.We propose core-gapped confidential VMs, a set of software-only modifications that ensure that no distrusting code shares a core, thus removing all same-core side-channels and transient-execution vulnerabilities from the guest's TCB. We present an Arm-based prototype along with a performance evaluation showing that, not only does core-gapping offer performance competitive with non-confidential VMs, the greater locality achieved by avoiding shared cores can even improve performance for CPU-intensive workloads.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {267–281},
numpages = {15},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674168,
author = {Han, Husheng and Zheng, Xinyao and Wen, Yuanbo and Hao, Yifan and Feng, Erhu and Liang, Ling and Mu, Jianan and Li, Xiaqing and Ma, Tianyun and Jin, Pengwei and Song, Xinkai and Du, Zidong and Guo, Qi and Hu, Xing},
title = {TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674168},
doi = {10.1145/3622781.3674168},
abstract = {Heterogeneous collaborative computing with NPU and CPU has received widespread attention due to its substantial performance benefits. To ensure data confidentiality and integrity during computing, Trusted Execution Environments (TEE) is considered a promising solution because of its comparatively lower overhead. However, existing heterogeneous TEE designs are inefficient for collaborative computing due to fine and different memory granularities between CPU and NPU. 1) The cacheline granularity of CPU TEE intensifies memory pressure due to its extra memory access, and 2) the cacheline granularity MAC of NPU escalates the pressure on the limited memory storage. 3) Data transfer across heterogeneous enclaves relies on the transit of non-secure regions, resulting in cumbersome re-encryption and scheduling.To address these issues, we propose TensorTEE, a unified tensor-granularity heterogeneous TEE for efficient secure collaborative tensor computing. First, we virtually support tensor granularity in CPU TEE to eliminate the off-chip metadata access by detecting and maintaining tensor structures on-chip. Second, we propose tensor-granularity MAC management with predictive execution to avoid computational stalls while eliminating off-chip MAC storage and access. Moreover, based on the unified granularity, we enable direct data transfer without re-encryption and scheduling dilemmas. Our evaluation is built on enhanced Gem5 and a cycle-accurate NPU simulator. The results show that TensorTEE improves the performance of Large Language Model (LLM) training workloads by 4.0x compared to existing work and incurs only 2.1\% overhead compared to non-secure training, offering a practical security assurance for LLM training.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {282–297},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674171,
author = {Ou, Xianfei and Li, Cong and Jiang, Yanyan and Xu, Chang},
title = {The Mutators Reloaded: Fuzzing Compilers with Large Language Model Generated Mutation Operators},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674171},
doi = {10.1145/3622781.3674171},
abstract = {Crafting high-quality mutators-the core of mutation-based fuzzing that shapes the search space-is challenging. It requires human expertise and creativity, and their implementation demands knowledge of compiler internals. This paper presents MetaMut framework for developing new, useful mutators for compiler fuzzing. It integrates our compiler-domain knowledge into prompts and processes that can best harness the capabilities of a large language model. With MetaMut, we have successfully created 118 semantic-aware mutators at approximately $0.5 each, with only moderate human effort. With these mutators, our fuzzer uncovered 131 bugs in GCC and Clang, 129 of which were confirmed or fixed. The success of MetaMut suggests that the integration of AI into software and system engineering tasks traditionally thought to require expert human intervention could be a promising research direction.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {298–312},
numpages = {15},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674180,
author = {Dong, Juechu and Rosenblum, Jonah and Narayanasamy, Satish},
title = {Toleo: Scaling Freshness to Tera-scale Memory Using CXL and PIM},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674180},
doi = {10.1145/3622781.3674180},
abstract = {Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half.Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {313–328},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674174,
author = {Tong, Haining and Gavrilenko, Natalia and Ponce de Leon, Hernan and Heljanko, Keijo},
title = {Towards Unified Analysis of GPU Consistency},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674174},
doi = {10.1145/3622781.3674174},
abstract = {After more than 30 years of research, there is a solid understanding of the consistency guarantees given by CPU systems. Unfortunately, the same is not yet true for GPUs. The growing popularity of general purpose GPU programming has been a call for action which industry players like Nvidia and Khronos have answered by formalizing their Ptx and Vulkan consistency models. These models give precise answers to questions about program's correctness. However, interpreting them still requires a level of expertise that escapes most developers, and the current tool support is insufficient.To remedy this, we translated and integrated the Ptx and Vulkan models into the Dartagnan verification tool. This makes Dartagnan the first analysis tool for multiple GPU consistency models that can analyze real GPU code. During the validation of the translated models, we discovered two bugs in the original Ptx and Vulkan consistency models.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {329–344},
numpages = {16},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674188,
author = {Xie, Zifan and Wen, Ming and Qiu, Shiyu and Jin, Hai},
title = {Validating JVM Compilers via Maximizing Optimization Interactions},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674188},
doi = {10.1145/3622781.3674188},
abstract = {This paper introduces the concept of optimization interaction, which refers to the practice in modern compilers where multiple optimization phases, such as inlining, loop unrolling, and dead code elimination, are not completed in a one-off sequential order while being interacted instead. Therefore, while optimizing a certain phase, the compiler needs to ensure that the results of other optimization phases will not be disrupted, as this could lead to compiler crashes or unpredictable results. To verify whether compilers can correctly handle the optimization process across various phases, we propose MopFuzzer, which aims at maximizing runtime optimization interactions during fuzzing. Specifically, it encourages the JVM to perform multi-stage optimizations and verifies the correctness of the compiler's optimized code through differential testing. Currently, MopFuzzer has implemented 13 mutators, and each is intended to trigger a certain optimization behavior. Such mutators are applied iteratively to the same program point, aiming to maximize optimization interactions. Subsequently, the testing process is guided by a novel method based on profile data, which records the optimization behaviors performed by the compiler. The guidance enables MopFuzzer to generate mutants that are able to maximize optimization behaviors and their interactions. Our evaluation has led to 59 bug reports for widely used production JVMs, OpenJDK and OpenJ9.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {345–360},
numpages = {16},
keywords = {JVM, JIT compiler, optimization},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3622781.3674173,
author = {Xie, Weiyu and Zhang, MingXing and Liao, Xia and Chen, Kang and Jiang, Jinlei and Wu, YongWei},
title = {VertexSurge: Variable Length Graph Pattern Match on Billion-edge Graphs},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674173},
doi = {10.1145/3622781.3674173},
abstract = {Variable-Length Graph Pattern Matching (VLGPM) is a critical functionality in graph databases, pivotal for identifying patterns where the number of connecting edges between two matched vertices is variable. This function plays a vital role in analyzing complex and dynamic networks such as social networks or bank transfers networks, where relationships can vary extensively in both length and structure. However, despite its importance, current graph databases, optimized primarily for single-hop subgraph matching, struggle with VLGPM over large graphs.To bridge this gap between essential user requirements and the lack of efficient support in existing systems, we introduce VertexSurge. Central to VertexSurge is an innovative variable-length expand (VExpand) operator, which incorporates several microarchitecture-friendly optimizations to efficiently compute the reachability matrix between two sets of vertices. These optimizations enable VertexSurge to handle the surge of vertex count due to variable length with high performance. Additionally, VertexSurge combines VExpand with effective multi-set intersection for pattern matching, ruled-based planning, and disk offloading for large datasets, to implement a full-fledged VLGPM engine. Our evaluations with real-world graph datasets and representative patterns demonstrate that VertexSurge significantly outperforms existing systems in VLGPM, validating its efficacy in handling large-scale graph pattern matching challenges.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {361–375},
numpages = {15},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

