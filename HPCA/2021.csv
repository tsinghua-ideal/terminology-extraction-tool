"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Common Counters: Compressed Encryption Counters for Secure GPU Memory","S. Na; S. Lee; Y. Kim; J. Park; J. Huh","School of Computing, KAIST; School of Computing, KAIST; School of Computing, KAIST; School of Computing, KAIST; School of Computing, KAIST",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","1","13","Hardware-based trusted execution has opened a promising new opportunity for enabling secure cloud computing. Nevertheless, the current trusted execution environments are limited to the traditional CPU-based computation, while more and more critical workloads such as machine learning require to use GPUs. For secure GPU computing, recent studies proposed to isolate critical GPU operations from the operating system by providing trusted execution environments for GPUs. However, a missing component in the prior GPU trusted execution studies is a hardware-based memory protection technique optimized for GPUs. Start-of-art memory protection techniques for CPUs use counter-mode encryption, where per-block counters are used as timestamps for generating one-time pads. The integrity of counters is protected by a counter tree. This study investigates the hardware memory encryption for GPUs and identifies counter cache misses as one of the key performance bottlenecks. To mitigate the overheads of counter cache misses, this paper proposes a novel technique, called common counters, for efficient counter-mode encryption and counter integrity protection. The proposed technique exploits unique characteristics of common GPU applications. In GPU applications, a large portion of application memory is written only once by the initial data transfer from the CPU memory, or the number of writes by the GPU applications to major data structures tends to be uniform. Exploiting the uniform write property, this study proposes to use a common counter representation to complement the existing per-block encryption counters. With the common counters, the proposed technique can almost eliminate the performance overheads caused by counter cache misses, reducing the degradation to 2.9%.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00011","National Research Foundation; Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407091","","Degradation;Cloud computing;Operating systems;Graphics processing units;Machine learning;Data transfer;Data structures","","23","","56","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Streamline Ring ORAM Accesses through Spatial and Temporal Optimization","D. Cao; M. Zhang; H. Lu; X. Ye; D. Fan; Y. Che; R. Wang","Tsinghua University, Beijing, China; State Key Laboratory of Computer Architecture, ICT, CAS, Beijing, China; State Key Laboratory of Computer Architecture, ICT, CAS, Beijing, China; State Key Laboratory of Computer Architecture, ICT, CAS, Beijing, China; State Key Laboratory of Computer Architecture, ICT, CAS, Beijing, China; Illinois Institute of Technology; Illinois Institute of Technology",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","14","25","Memory access patterns could leak temporal and spatial information in a sensitive program; therefore, obfuscated memory access patterns are desired from the security perspective. Oblivious RAM (ORAM) has been the favored candidate to eliminate the access pattern leakage through randomly remapping data blocks around the physical memory space. Meanwhile, accessing memory with ORAM protocols results in significant memory bandwidth overhead. For each memory request, after going through the ORAM obfuscation, the main memory needs to service tens of actual memory accesses, and only one real access out of them is useful for the program execution. Besides, to ensure the memory bus access patterns are indistinguishable, extra dummy blocks need to be stored and transmitted, which cause memory space waste and poor performance. In this work, we introduce a new framework, String ORAM, that accelerates the Ring ORAM accesses with Spatial and Temporal optimization schemes. First, we identify that dummy blocks could significantly waste memory space and propose a compact ORAM organization that leverages the real blocks in memory to obfuscate the memory access pattern. Then, we identify the inefficiency of current transaction-based Ring ORAM scheduling on DRAM devices and propose an effective scheduling technique that can overlap the time spent on row buffer misses while ensuring correctness and security. With a minimal modification on the hardware and software, and negligible impact on security, the framework reduces 30.05% execution time and up to 40% memory space overhead compared to the state-of-the-art bandwidth-efficient Ring ORAM.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00012","National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407129","Ring ORAM;Performance;Space Efficiency;Security","Protocols;Space debris;Memory management;Random access memory;Performance gain;Scheduling;Software","","9","","36","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Cheetah: Optimizing and Accelerating Homomorphic Encryption for Private Inference","B. Reagen; W. -S. Choi; Y. Ko; V. T. Lee; H. -H. S. Lee; G. -Y. Wei; D. Brooks",New York University; Seoul National University; Harvard University; Facebook Reality Labs Research; Facebook AI Research; Harvard University; Harvard University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","26","39","As the application of deep learning continues to grow, so does the amount of data used to make predictions. While traditionally big-data deep learning was constrained by computing performance and off-chip memory bandwidth, a new constraint has emerged: privacy. One solution is homomorphic encryption (HE). Applying HE to the client-cloud model allows cloud services to perform inferences directly on clients’ encrypted data. While HE can meet privacy constraints it introduces enormous computational challenges and remains impractically slow on current systems.This paper introduces Cheetah, a set of algorithmic and hardware optimizations for server-side HE DNN inference. Cheetah proposes HE-parameter tuning and operator scheduling optimizations, which together deliver up to $79 \times$ speedup over the state-of-the-art. However, HE inference still falls short of real-time inference speeds by nearly four orders of magnitude. Cheetah further proposes an accelerator architecture to understand the degree of speedup hardware can provide and whether it can bridge HE’s real-time performance gap. We evaluate several DNNs and find that privacy-preserving HE inference for ResNet50 can approach real-time speeds with a 587mm2 accelerator dissipating 30W in 5nm.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407118","","Deep learning;Privacy;Accelerator architectures;Seals;Real-time systems;Inference algorithms;Encryption","","97","","73","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"New Models for Understanding and Reasoning about Speculative Execution Attacks","Z. He; G. Hu; R. Lee",Princeton University; Princeton University; Princeton University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","40","53","Spectre and Meltdown attacks and their variants exploit hardware performance optimization features to cause security breaches. Secret information is accessed and leaked through covert or side channels. New attack variants keep appearing and we do not have a systematic way to capture the critical characteristics of these attacks and evaluate why they succeed or fail.In this paper, we provide a new attack-graph model for reasoning about speculative execution attacks. We model attacks as ordered dependency graphs, and prove that a race condition between two nodes can occur if there is a missing dependency edge between them. We define a new concept, “security dependency”, between a resource access and its prior authorization operation. We show that a missing security dependency is equivalent to a race condition between authorization and access, which is a root cause of speculative execution attacks. We show detailed examples of how our attack graph models the Spectre and Meltdown attacks, and is generalizable to all the attack variants published so far. This attack model is also very useful for identifying new attacks and for generalizing defense strategies. We identify several defense strategies with different performance-security tradeoffs. We show that the defenses proposed so far all fit under one of our defense strategies. We also explain how attack graphs can be constructed and point to this as promising future work for tool designers.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407076","Hardware security;speculative execution attacks;graph model;security dependency;cache;side channel;covert channel;delayed exceptions;prediction;race condition","Authorization;Analytical models;Computational modeling;Tools;Hardware;Cognition;Security","","12","","48","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"A Computational Stack for Cross-Domain Acceleration","S. Kinzer; J. K. Kim; S. Ghodrati; B. Yatham; A. Althoff; D. Mahajan; S. Lerner; H. Esmaeilzadeh","Alternative Computing Technologies (ACT) Lab, University of California San Diego; Alternative Computing Technologies (ACT) Lab, University of California San Diego; Alternative Computing Technologies (ACT) Lab, University of California San Diego; Alternative Computing Technologies (ACT) Lab, University of California San Diego; Tortuga Logic; Microsoft; Alternative Computing Technologies (ACT) Lab, University of California San Diego; Alternative Computing Technologies (ACT) Lab, University of California San Diego",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","54","70","Domain-specific accelerators obtain performance benefits by restricting their algorithmic domain. These accelerators utilize specialized languages constrained to particular hardware, thus trading off expressiveness for high performance. The pendulum has swung from one hardware for all domains (general-purpose processors) to one hardware per individual domain. The middle-ground on this spectrum–which provides a unified computational stack across multiple, but not all, domains– is an emerging and open research challenge. This paper sets out to explore this region and its associated tradeoff between expressiveness and performance by defining a cross-domain stack, dubbed PolyMath. This stack defines a high-level cross-domain language (CDL), called PMLang, that in a modular and reusable manner encapsulates mathematical properties to be expressive across multiple domains–Robotics, Graph Analytics, Digital Signal Processing, Deep Learning, and Data Analytics. PMLang is backed by a recursively-defined intermediate representation allowing simultaneous access to all levels of operation granularity, called sr DFG. Accelerator-specific or domain-specific IRs commonly capture operations in the granularity that best fits a set of Domain-Specific Architectures (DSAs). In contrast, the recursive nature of the sr DFG enables simultaneous access to all the granularities of computation for every operation, thus forming an ideal bridge for converting to various DSA-specific IRs across multiple domains. Our stack unlocks multi-acceleration for end-to-end applications that cross the boundary of multiple domains each comprising different data and compute patterns. Evaluations show that by using PolyMath it is possible to harness accelerators across the five domains to realize an average speedup of 3.3× over a Xeon CPU along with 18.1× reduction in energy. In comparison to Jetson Xavier and Titan XP, cross-domain acceleration offers 1.7× and 7.2× improvement in performance-per-watt, respectively. We measure the cross-domain expressiveness and performance tradeoff by comparing each benchmark against its hand-optimized implementation to achieve 83.9% and 76.8% of the optimal performance for single-domain algorithms and end-to-end applications. For the two case studies of end-to-end applications (comprising algorithms from multiple domains), results show that accelerating all kernels offers an additional 2.0× speedup over CPU, 6.1× improvement in performance-per-watt over Titan Xp, and 2.8× speedup over Jetson Xavier compared to only the one most effective single-domain kernel being accelerated. Finally, we examine the utility and expressiveness of PolyMath through a user study, which shows, on average, PolyMath requires 1.9× less time to implement algorithms from two different domains with 2.5× fewer lines of code relative to Python.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00015","Qualcomm; Google; Microsoft; Xilinx; Leidos; Air Force Office of Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407145","Accelerator design;Machine learning systems;Algorithms/System Co-design;Domain Specific Language","Deep learning;Bridges;Data analysis;Signal processing algorithms;Computer architecture;Hardware;Acceleration","","8","","73","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Heterogeneous Dataflow Accelerators for Multi-DNN Workloads","H. Kwon; L. Lai; M. Pellauer; T. Krishna; Y. -H. Chen; V. Chandra",Georgia Insitute of Technology; Facebook; NVIDIA; Georgia Insitute of Technology; Facebook; Facebook,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","71","83","Emerging AI-enabled applications such as augmented and virtual reality (AR/VR) leverage multiple deep neural network (DNN) models for various sub-tasks such as object detection, image segmentation, eye-tracking, speech recognition, and so on. Because of the diversity of the sub-tasks, the layers within and across the DNN models are highly heterogeneous in operation and shape. Diverse layer operations and shapes are major challenges for a fixed dataflow accelerator (FDA) that employs a fixed dataflow strategy on a single DNN accelerator substrate since each layer prefers different dataflows (computation order and parallelization) and tile sizes. Reconfigurable DNN accelerators (RDAs) have been proposed to adapt their dataflows to diverse layers to address the challenge. However, the dataflow flexibility in RDAs is enabled at the cost of expensive hardware structures (switches, interconnects, controller, etc.) and requires per-layer reconfiguration, which introduces considerable energy costs. Alternatively, this work proposes a new class of accelerators, heterogeneous dataflow accelerators (HDAs), which deploy multiple accelerator substrates (i.e., sub-accelerators), each supporting a different dataflow. HDAs enable coarser-grained dataflow flexibility than RDAs with higher energy efficiency and lower area cost comparable to FDAs. To exploit such benefits, hardware resource partitioning across sub-accelerators and layer execution schedule need to be carefully optimized. Therefore, we also present Herald, a framework for co-optimizing hardware partitioning and layer scheduling. Using Herald on a suite of AR/VR and MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which demonstrates 65.3% lower latency and 5.0% lower energy compared to the best fixed dataflow accelerators and 22.0% lower energy at the cost of 20.7% higher latency compared to a state-of-the-art reconfigurable DNN accelerator (RDA). The results suggest that HDA is an alternative class of Pareto-optimal accelerators to RDA with strength in energy, which can be a better choice than RDAs depending on the use cases.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407116","DNN Accelerator;Multi DNN Workloads","Schedules;Solid modeling;Systematics;Shape;Computer architecture;Virtual reality;Speech recognition","","101","","39","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"SPAGHETTI: Streaming Accelerators for Highly Sparse GEMM on FPGAs","R. Hojabr; A. Sedaghati; A. Sharifian; A. Khonsari; A. Shriraman","ECE Dept., University of Tehran; School of Computing Science, Simon Fraser University; Untether AI; ECE Dept., University of Tehran; School of Computing Science, Simon Fraser University",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","84","96","Generalized Sparse Matrix-Matrix Multiplication (Sparse GEMM) is widely used across multiple domains, but the computation’s regularity is dependent on the input sparsity pattern. The majority of sparse GEMM accelerators are based on the inner product method and propose new storage formats [5], [28], [31] to regularize computation. We find that these storage formats are more suited for denser matrices. Accelerators [26], [34] adopting the outer product algorithm are more suitable for highly sparse inputs $(\lt 1$% density), since they support CSC/CSR storage formats. The current state-of-the-art, SpArch [34], condenses inputs to improve output reuse, but then spoils input reuse. The condensing effectiveness varies across inputs leading to high variance in DRAM utilization and speedup across inputs. SpArch also requires a complex memory hierarchy (e.g., prefetch caches) to re-capture input reuse.We propose Spaghetti, an open-source Chisel generator for creating FPGA-optimized outer product accelerators. The key novelty in Spaghetti is a new pattern-aware software scheduler that analyzes the sparsity pattern and schedules row-col pairs of the inputs onto the fixed microarchitecture. Spaghetti takes advantage of our observation that the rows in the input matrix lead to mutually independent rows in the final output. Thus the scheduler can partition the input into tiles that maximize reuse and eliminate re-fetching the partial matrices from the DRAM. The microarchitecture template we create has the following key benefits: i) we can statically schedule the inputs in a streaming fashion and maximize DRAM utilization, ii) we can parallelize the merge phase and generate multiple rows of the output in parallel maximally using the output DRAM bandwidth, iii) we can adapt to the varying logic resources and bandwidth across various FPGA devices and attain maximal roofline performance (only limited by memory bandwidth). We auto-generate sparse GEMM accelerators on Amazon AWS FPGAs and demonstrate that we can achieve performance improvement over CPUs and GPUs between 1.1 – 34.5 x. Compared to SpArch [34], our design improves performance by an average of $2.6 \times$, and reduces DRAM accesses by an average of $4 \times$.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407115","","Performance evaluation;Schedules;Microarchitecture;Prefetching;Random access memory;Prototypes;Bandwidth","","32","","34","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning","H. Wang; Z. Zhang; S. Han","EECS, Massachusetts Institute of Technology, Cambridge, MA, US; EECS, Massachusetts Institute of Technology, Cambridge, MA, US; EECS, Massachusetts Institute of Technology, Cambridge, MA, US",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","97","110","The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction.Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0× with no accuracy loss, and achieves 1.6×, 3.0×, 162×, 347× speedup, and 1.4×, 3.2×, 1193×, 4059× energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407232","Natural Language Processing;Attention;Domain-Specific Accelerator;Algorithm-Architecture Co-design;Pruning;Quantization","Quantization (signal);Redundancy;Memory management;Random access memory;Throughput;Natural language processing;Space exploration","","260","","80","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"BBB: Simplifying Persistent Programming using Battery-Backed Buffers","M. Alshboul; P. Ramrakhyani; W. Wang; J. Tuck; Y. Solihin","ECE, North Carolina State University; Arm Research; Arm Research; ECE, North Carolina State University; Computer Science, University of Central Florida",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","111","124","Non-volatile memory (NVM) is poised to augment or replace DRAM as main memory. With the right abstraction and support, non-volatile main memory (NVMM) can provide an alternative to the storage system to host long-lasting persistent data. However, keeping persistent data in memory requires programs to be written such that data is crash consistent (i.e. it can be recovered after failure). Critical to supporting crash recovery is the guarantee of ordering of when stores become durable with respect to program order. Strict persistency, which requires persist order to coincide with program order of stores, is simple and intuitive but generally thought to be too slow. More relaxed persistency models are available but demand higher programming complexity, e.g. they require the programmer to insert persist barriers correctly in their program.We identify the source of strict persistency inefficiency as the gap between the point of visibility (PoV) which is the cache, and the point of persistency (PoP) which is the memory. In this paper, we propose a new approach to close the PoV/PoP gap which we refer to as Battery-Backed Buffer (BBB). The key idea of BBB is to provide a battery-backed persist buffer (bbPB) in each core next to the L1 data cache (L1D). A store value is allocated in the bbPB as it is written to cache, becoming part of the persistence domain. If a crash occurs, battery ensures bbPB can be fully drained to NVMM. BBB simplifies persistent programming as the programmer does not need to insert persist barriers or flushes. Furthermore, our BBB design achieves nearly identical results to eADR in terms of performance and number of NVMM writes, while requiring two orders of magnitude smaller energy and time to drain.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407087","Emerging Memory Technology;Memory Systems;Multicore and Parallel Architectures","Microarchitecture;Nonvolatile memory;Memory management;Random access memory;Programming;Computer crashes;Complexity theory","","28","","98","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"TSOPER: Efficient Coherence-Based Strict Persistency","P. Ekemark; Y. Yao; A. Ros; K. Sagonas; S. Kaxiras","Dept. of Information Technology, Uppsala University, Uppsala, Sweden; Dept. of Information Technology, Uppsala University, Uppsala, Sweden; Computer Engineering Dept., University of Murcia, Murcia, Spain; Dept. of Information Technology, Uppsala University, Uppsala, Sweden; Dept. of Information Technology, Uppsala University, Uppsala, Sweden",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","125","138","We propose a novel approach for hardware-based strict TSO persistency, called TSOPER. We allow a TSO persistency model to freely coalesce values in the caches, by forming atomic groups of cachelines to be persisted. A group persist is initiated for an atomic group if any of its newly written values are exposed to the outside world. A key difference with prior work is that our architecture is based on the concept of a TSO persist buffer, that sits in parallel to the shared LLC, and persists atomic groups directly from private caches to NVM, bypassing the coherence serialization of the LLC.To impose dependencies among atomic groups that are persisted from the private caches to the TSO persist buffer, we introduce a sharing-list coherence protocol that naturally captures the order of coherence operations in its sharing lists, and thus can reconstruct the dependencies among different atomic groups entirely at the private cache level without involving the shared LLC. The combination of the sharing-list coherence and the TSO persist buffer allows persist operations and writes to non-volatile memory to happen in the background and trail the coherence operations. Coherence runs ahead at full speed; persistency follows belatedly.Our evaluation shows that TSOPER provides the same level of reordering as a program-driven relaxed model, hence, approximately the same level of performance, albeit without needing the programmer or compiler to be concerned about false sharing, data-race-free semantics, etc., and guaranteeing all software that can run on top of TSO, automatically persists in TSO.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00021","European Research Council; Vetenskapsrådet; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407141","non-volatile memory;persistent memory;persistency;total store order;coherence","Protocols;Program processors;Nonvolatile memory;Computational modeling;Semantics;Coherence;Computer architecture","","7","","46","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems","M. Alwadi; V. R. Kommareddy; C. Hughes; S. D. Hammond; A. Awad",University of Central Florida; University of Central Florida; Sandia National Laboratories; Sandia National Laboratories; North Carolina State University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","139","152","Non-volatile memories (NVMs) have the characteristics of both traditional storage systems (persistent) and traditional memory systems (byte-addressable). However, they suffer from high write latency and have a limited write endurance. Researchers have proposed hybrid memory systems that combine DRAM and NVM, utilizing the lower latency of the DRAM to hide some of the shortcomings of the NVM – improving system’s performance by caching resident NVM data in the DRAM. However, this can nullify the persistency of the cached pages, leading to a question of trade-offs in terms of performance and reliability. In this paper, we propose Stealth-Persist, a novel architecture support feature that allows applications that need persistence to run in the DRAM while maintaining the persistency features provided by the NVM. Stealth-Persist creates the illusion of a persistent memory for the application to use, while utilizing the DRAM for performance optimizations. Our experimental results show that Stealth-Persist improves the performance by 42.02% for persistent applications.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407136","persistent applications;hybrid memories","Nonvolatile memory;System performance;Memory management;Random access memory;Performance gain;Parallel processing;Hardware","","6","","65","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"TILT: Achieving Higher Fidelity on a Trapped-Ion Linear-Tape Quantum Computing Architecture","X. -C. Wu; D. M. Debroy; Y. Ding; J. M. Baker; Y. Alexeev; K. R. Brown; F. T. Chong","Department of Computer Science, University of Chicago, Chicago, USA; Department of Physics, Duke University, Durham, USA; Department of Computer Science, University of Chicago, Chicago, USA; Department of Computer Science, University of Chicago, Chicago, USA; Computational Science Division Argonne National Laboratory, Lemont, USA; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Computer Science, University of Chicago, Chicago, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","153","166","Trapped-ion qubits are a leading technology for practical quantum computing. In this work, we present an architectural analysis of a linear-tape architecture for trapped ions. In order to realize our study, we develop and evaluate mapping and scheduling algorithms for this architecture. In particular, we introduce TILT, a linear “Turing-machinelike” architecture with a multilaser control “head,” where a linear chain of ions moves back and forth under the laser head. We find that TILT can substantially reduce communication as compared with comparable-sized Quantum Charge Coupled Device (QCCD) architectures. We also develop two important scheduling heuristics for TILT. The first heuristic reduces the number of swap operations by matching data traveling in opposite directions into an “opposing swap.”, and also avoids the maximum swap distance across the width of the head, as maximum swap distances make scheduling multiple swaps in one head position difficult. The second heuristic minimizes ion chain motion by scheduling the tape to the position with the maximal executable operations for every movement. We provide application performance results from our simulation, which suggest that TILT can outperform QCCD in a range of NISQ applications in terms of success rate (up to 4.35x and 1.95x on average). We also discuss using TILT as a building block to extend existing scalable trapped-ion quantum computing proposals.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00023","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407237","Quantum Computing;Trapped-ion Architecture;Circuit Optimization","Performance evaluation;Scheduling algorithms;Optimizing compilers;Architecture;Qubit;Computer architecture;Ions","","13","","92","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"QuCloud: A New Qubit Mapping Mechanism for Multi-programming Quantum Computing in Cloud Environment","L. Liu; X. Dou","Sys-Inventor Lab, SKLCA, ICT, CAS; Sys-Inventor Lab, SKLCA, ICT, CAS",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","167","178","For a specific quantum chip, multi-programming improves overall throughput and resource utilization. Previous studies on mapping multiple programs often lead to resource under-utilization, high error rate, and low fidelity. This paper proposes QuCloud, a new approach for mapping quantum programs in the cloud environment. We have three new designs in QuCloud. (1) We leverage the community detection technique to partition physical qubits among concurrent quantum programs, avoiding the waste of robust resources. (2) We design X-SWAP scheme that enables inter-program SWAPs and prioritizes SWAPs associated with critical gates to reduce the SWAP overheads. (3) We propose a compilation task scheduler that schedules concurrent quantum programs to be compiled and executed based on estimated fidelity for the best practice. We evaluate our work on publicly available quantum computer IBMQ16 and a simulated quantum chip IBMQ50. Our work outperforms the state-of-the-art work for multi-programming on fidelity and compilation overheads by 9.7% and 11.6%, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407180","","Schedules;Error analysis;Qubit;Computer architecture;Logic gates;Throughput;Market research","","33","","33","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Systematic Approaches for Precise and Approximate Quantum State Runtime Assertion","J. Liu; H. Zhou","North Carolina State University, Raleigh, United States; North Carolina State University, Raleigh, United States",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","179","193","With the rapid growth of quantum computing technology, programmers need new tools for debugging quantum programs. Recent works show that assertions are a promising way for debugging quantum programs. However, there are two main drawbacks with the existing schemes. First, the existing schemes, including both statistical and dynamic assertions are only capable of asserting limited types of states, namely classical, superposition, and specific entanglement states. Second, the use cases of these assertions are limited, since the programmer has to know the exact/precise state to assert.In this work, we propose two systematic approaches for dynamic quantum state assertion and they can assert a much broader range of quantum states including both pure states and mixed states. We also introduce the idea of approximate quantum state assertion for the cases where the programmers only have limited knowledge of the quantum states. Approximate assertion is capable of checking membership in a set of states $\{|\psi\rangle,\ |\phi\rangle,\ldots\}$. While precise quantum state assertion can check a specific quantum state, approximate assertion enables a way to check whether the qubits of interest are in a super-set of some expected states, which is analogous to the well-known Bloom filter for membership checking in classical computing. Our experiments demonstrate that our systematic approaches can assert many more quantum states and can be used in various assertion locations for qubit state checking.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407042","quantum computing;runtime assertion","Computers;Systematics;Runtime;Qubit;Debugging;Computer architecture;Quantum state","","18","","43","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Faster Schrödinger-style simulation of quantum circuits","A. Fatima; I. L. Markov","Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, United States; Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, United States",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","194","207","Recent demonstrations of superconducting quantum computers by Google and IBM and trapped-ion computers from IonQ fueled new research in quantum algorithms, compilation into quantum circuits, and empirical algorithmics. While online access to quantum hardware remains too limited to meet the demand, simulating quantum circuits on conventional computers satisfies many needs. We advance Schrödinger-style simulation of quantum circuits that is useful standalone and as a building block in layered simulation algorithms, both cases are illustrated in our results. Our algorithmic contributions show how to simulate multiple quantum gates at once, how to avoid floating-point multiplies, how to best use data-level and thread-level parallelism as well as CPU cache, and how to leverage these optimizations by reordering circuit gates. While not described previously, these techniques implemented by us supported published high-performance distributed simulations up to 64 qubits. To show additional impact, we benchmark our simulator against Microsoft, IBM and Google simulators on hard circuits from Google.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407212","quantum circuits;simulation;algorithms","Computers;Computational modeling;Quantum chemistry;Qubit;Clustering algorithms;Logic gates;Internet","","18","","29","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework","S. -E. Chang; Y. Li; M. Sun; R. Shi; H. K. . -H. So; X. Qian; Y. Wang; X. Lin",Northeastern University; Northeastern University; Northeastern University; The University of Hong Kong; The University of Hong Kong; University of Southern California; Northeastern University; Northeastern University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","208","220","Deep Neural Networks (DNNs) have achieved extraordinary performance in various application domains. To support diverse DNN models, efficient implementations of DNN inference on edge-computing platforms, e.g., ASICs, FPGAs, and embedded systems, are extensively investigated. Due to the huge model size and computation amount, model compression is a critical step to deploy DNN models on edge devices. This paper focuses on weight quantization, a hardware-friendly model compression approach that is complementary to weight pruning.Unlike existing methods that use the same quantization scheme for all weights, we propose the first solution that applies different quantization schemes for different rows of the weight matrix. It is motivated by (1) the distribution of the weights in the different rows are not the same; and (2) the potential of achieving better utilization of heterogeneous FPGA hardware resources. To achieve that, we first propose a hardware-friendly quantization scheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight distribution, in which the multiplication arithmetic can be replaced with logic shifter and adder, thereby enabling highly efficient implementations with the FPGA LUT resources. In contrast, the existing fixed-point quantization is suitable for Uniform-like weight distribution and can be implemented efficiently by DSP. Then to fully explore the resources, we propose an FPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed SP2 and the fixed-point schemes. Combining the two schemes can maintain, or even increase accuracy due to better matching with weight distributions.For the FPGA implementations, we develop a parameterized architecture with heterogeneous Generalized Matrix Multiplication (GEMM) cores—one using LUTs for computations with SP2 quantized weights and the other utilizing DSPs for fixed-point quantized weights. Given the partition ratio among the two schemes based on resource characterization, MSQ quantization training algorithm derives an optimally quantized model for the FPGA implementation. We evaluate our FPGA-centric quantization framework across multiple application domains. With optimal SP2/fixed-point ratios on two FPGA devices, i.e., Zynq XC7Z020 and XC7Z045, we achieve performance improvement of 2.1 × -4.1 × compared to solely exploiting DSPs for all multiplication operations. In addition, the CNN implementations with the proposed MSQ scheme can achieve higher accuracy and comparable hardware utilization efficiency compared to the state-of-the-art designs.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00027","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407035","deep neural network;quantization;FPGA;inference","Performance evaluation;Quantization (signal);Recurrent neural networks;Computational modeling;Hardware;Table lookup;Field programmable gate arrays","","93","","81","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Revisiting HyperDimensional Learning for FPGA and Low-Power Architectures","M. Imani; Z. Zou; S. Bosch; S. A. Rao; S. Salamat; V. Kumar; Y. Kim; T. Rosing","Department of Computer Science, University of California Irvine; Department of Computer Science, University of California Irvine; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology; Department of Computer Science, University of California Irvine; Department of Computer Science, University of California Irvine; Department of Computer Science, University of California Irvine; Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology; Department of Computer Science, University of California Irvine",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","221","234","Today’s applications are using machine learning algorithms to analyze the data collected from a swarm of devices on the Internet of Things (IoT). However, most existing learning algorithms are overcomplex to enable real-time learning on IoT devices with limited resources and computing power. Recently, Hyperdimensional computing (HDC) is introduced as an alternative computing paradigm for enabling efficient and robust learning. HDC emulates the cognitive task by representing the values as patterns of neural activity in high-dimensional space. HDC first encodes all data points to high-dimensional vectors. It then efficiently performs the learning task using a well-defined set of operations. Existing HDC solutions have two main issues that hinder their deployments on low-power embedded devices: (i) the encoding module is costly, dominating 80% of the entire training performance, (ii) the HDC model size and the computation cost grow significantly with the number of classes in online inference.In this paper, we proposed a novel architecture, LookHD, which enables real-time HDC learning on low-power edge devices. LookHD exploits computation reuse to memorize the encoding module and simplify its computation with single memory access. LookHD also address the inference scalability by exploiting HDC governing mathematics that compresses the HDC trained model into a single hypervector. We present how the proposed architecture can be implemented on the existing low power architectures: ARM processor and FPGA design. We evaluate the efficiency of the proposed approach on a wide range of practical classification problems such as activity recognition, face recognition, and speech recognition. Our evaluations show that LookHD can achieve, on average, $ 28.3\times$ faster and $ 97.4\times$ more energy-efficient training as compared to the state-of-the-art HDC implemented on the FPGA. Similarly, in the inference, LookHD is $ 2.2\times$ faster, $ 4.1\times$ more energy-efficient, and has $ 6.3\times$ smaller model size than the same state-of-the-art algorithms.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00028","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407181","HyprDimensional computing;Brain-inspired computing;Machine learning;Real-time learning;FPGA","Training;Computational modeling;Scalability;Computer architecture;Encoding;Energy efficiency;Real-time systems","","66","","64","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training","Y. Kwon; Y. Lee; M. Rhu","School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","235","248","Personalized recommendations are one of the most widely deployed machine learning (ML) workload serviced from cloud datacenters. As such, architectural solutions for high-performance recommendation inference have recently been the target of several prior literatures. Unfortunately, little have been explored and understood regarding the training side of this emerging ML workload. In this paper, we first perform a detailed workload characterization study on training recommendations, root-causing sparse embedding layer training as one of the most significant performance bottlenecks. We then propose our algorithm-architecture co-design called Tensor Casting, which enables the development of a generic accelerator architecture for tensor gather-scatter that encompasses all the key primitives of training embedding layers. When prototyped on a real CPUGPU system, Tensor Casting provides $1.9 - 21 \times$ improvements in training throughput compared to state-of-the-art approaches.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00029","Samsung; KAIST; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407139","computer architecture;memory-centric;machine learning;deep learning;recommendation system","Training;Casting;Analytical models;Tensors;Statistical analysis;Computational modeling;Focusing","","27","","57","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent","H. Kim; H. Park; T. Kim; K. Cho; E. Lee; S. Ryu; H. -J. Lee; K. Choi; J. Lee","ECE, Seoul National University; Samsung Electronics, SAIT; ECE, Seoul National University; CS, Yonsei University; Memory Business, Samsung Electronics; ECE, Seoul National University; ECE, Seoul National University; ECE, Seoul National University; CS, Yonsei University",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","249","262","In this paper, we present GradPIM, a processingin-memory architecture which accelerates parameter updates of deep neural networks training. As one of processing-in-memory techniques that could be realized in the near future, we propose an incremental, simple architectural design that does not invade the existing memory protocol. Extending DDR4 SDRAM to utilize bank-group parallelism makes our operation designs in processing-in-memory (PIM) module efficient in terms of hardware cost and performance. Our experimental results show that the proposed architecture can improve the performance of DNN training and greatly reduce memory bandwidth requirement while posing only a minimal amount of overhead to the protocol and DRAM area.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00030","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407195","","Training;Protocols;Neural networks;Memory management;Bandwidth;Performance gain;Parallel processing","","37","","107","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"SynCron: Efficient Synchronization Support for Near-Data-Processing Architectures","C. Giannoula; N. Vijaykumar; N. Papadopoulou; V. Karakostas; I. Fernandez; J. Gómez-Luna; L. Orosa; N. Koziris; G. Goumas; O. Mutlu","National Technical University, Athens; University of Toronto; National Technical University, Athens; National Technical University, Athens; University of Malaga; ETH, Zürich; ETH, Zürich; National Technical University, Athens; National Technical University, Athens; ETH, Zürich",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","263","276","Near-Data-Processing (NDP) architectures present a promising way to alleviate data movement costs and can provide significant performance and energy benefits to parallel applications. Typically, NDP architectures support several NDP units, each including multiple simple cores placed close to memory. To fully leverage the benefits of NDP and achieve high performance for parallel workloads, efficient synchronization among the NDP cores of a system is necessary. However, supporting synchronization in many NDP systems is challenging because they lack shared caches and hardware cache coherence support, which are commonly used for synchronization in multicore systems, and communication across different NDP units can be expensive.This paper comprehensively examines the synchronization problem in NDP systems, and proposes SynCron, an end-to-end synchronization solution for NDP systems. SynCron adds low-cost hardware support near memory for synchronization acceleration, and avoids the need for hardware cache coherence support. SynCron has three components: 1) a specialized cache memory structure to avoid memory accesses for synchronization and minimize latency overheads, 2) a hierarchical message-passing communication protocol to minimize expensive communication across NDP units of the system, and 3) a hardware-only overflow management scheme to avoid performance degradation when hardware resources for synchronization tracking are exceeded.We evaluate SynCron using a variety of parallel workloads, covering various contention scenarios. SynCron improves performance by $1.27\times$ on average (up to $1.78\times$) under high-contention scenarios, and by $ 1.35\times$ on average (up to $2.29\times)$ under low-contention real applications, compared to state-of-the-art approaches. SynCron reduces system energy consumption by $ 2.08\times$ on average (up to $4.25\times$).","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407213","Near Data Processing;Processing Near Memory;Synchronization;Accelerators;Irregular Applications;Parallel Applications;Multithreaded Applications;Programmability","Degradation;Energy consumption;Protocols;Multicore processing;Memory management;Cache memory;Coherence","","47","","158","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Zero Directory Eviction Victim: Unbounded Coherence Directory and Core Cache Isolation","M. Chaudhuri","Department of CSE, Indian Institute of Technology, Kanpur",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","277","290","A directory structure is traditionally employed for tracking coherence information of the privately cached blocks in a cache-coherent chip-multiprocessor (CMP). The eviction of a directory entry necessarily invalidates the privately cached copies of the block that the evicted entry was tracking. These forced directory eviction victims pose two major challenges. First, with decreasing directory size, the volume of these victim blocks increases significantly causing performance degradation. As a result, sizing the directory remains an important challenge. Second, the tight coupling between the directory evictions and the private cache contents can be exploited to launch timing-based side-channel attacks, as has been demonstrated recently. The existing solutions to the first problem allow reducing the directory capacity only up to a certain extent before the performance starts degrading. The existing mitigation technique for the security vulnerability avoids generation of only a certain specific subset of directory victims. In this paper, we present the Zero Directory Eviction Victim (Ze-roDEV) coherence protocol and accompanying novel mechanisms that guarantee freedom from invalidations arising from directory victims, thereby completely isolating the private core caches from the coherence directory evictions. This is the first fully hardwired design proposal that enables a practically unbounded coherence directory which, to the core caches in a CMP, appears to never evict a live entry. Unlike the prior proposals that have completely eliminated the directory and the coherence information eviction victims in a multi-/many-core CMP, our proposal does not require any operating system or application software changes. Our proposal, instead, repurposes the on-die last-level cache (LLC) space for holding the evicted directory entries and engineers a novel mechanism to handle directory entry eviction from the LLC without generating any invalidation to the private core caches. The ZeroDEV protocol evaluated on multi-threaded and multi-programmed workloads for inclusive and two popular non-inclusive CMP cache hierarchy designs performs within 1-2% of a well-provisioned traditional baseline. Importantly, as an additional benefit of eliminating directory eviction victims and utilizing the large on-die LLC for caching directory entries, we show that our proposal does not need any dedicated directory structure at all for certain classes of CMP cache hierarchy designs while maintaining the performance level and continuing to guarantee complete isolation of the core caches from directory entry eviction.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407149","chip-multiprocessor;cache coherence;sparse directory;directory eviction victim.","Degradation;Couplings;Protocols;Operating systems;Coherence;Side-channel attacks;Computer architecture","","7","","54","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Designing a Cost-Effective Cache Replacement Policy using Machine Learning","S. Sethumurugan; J. Yin; J. Sartori","Electrical and Computer Engineering, University of Minnesota, Twin Cities Minneapolis, MN; Electrical and Computer Engineering, Lehigh University, Bethlehem, PA; Electrical and Computer Engineering, University of Minnesota, Twin Cities Minneapolis, MN",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","291","303","Extensive research has been carried out to improve cache replacement policies, yet designing an efficient cache replacement policy that incurs low hardware overhead remains a challenging and time-consuming task. Given the surging interest in applying machine learning (ML) to challenging computer architecture design problems, we use ML as an offline tool to design a cost-effective cache replacement policy. We demonstrate that ML is capable of guiding and expediting the generation of a cache replacement policy that is competitive with state-of-the-art hand-crafted policies. In this work, we use Reinforcement Learning (RL) to learn a cache replacement policy. After analyzing the learned model, we are able to focus on a few critical features that might impact system performance. Using the insights provided by RL, we successfully derive a new cache replacement policy – Reinforcement Learned Replacement (RLR). Compared to the state-of-the-art policies, RLR has low hardware overhead, and it can be implemented without needing to modify the processor’s control and data path to propagate information such as program counter. On average, RLR improves single-core and four-core system performance by 3.25% and 4.86% over LRU, with an overhead of 16.75KB for 2MB last-level cache (LLC) and 67KB for 8MB LLC.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407137","","Training;System performance;Neural networks;Process control;Computer architecture;Reinforcement learning;Tools","","57","","36","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"WiDir: A Wireless-Enabled Directory Cache Coherence Protocol","A. Franques; A. Kokolis; S. Abadal; V. Fernando; S. Misailovic; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Universitat Politécnica de Catalunya; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","304","317","As the core count in shared-memory manycores keeps increasing, it is becoming increasingly harder to design cache-coherence protocols that deliver high performance without an inordinate increase in complexity and cost. In particular, sharing patterns where a group of cores frequently reads and writes a shared variable are hard to support efficiently. Hence, programmers end up tuning their applications to avoid these patterns, hurting the programmability of shared memory. To address this problem, this paper uses the recently-proposed on-chip wireless network technology to augment a conventional invalidation-based directory cache coherence protocol. We call the resulting protocol WiDir. WiDir seamlessly transitions between wired and wireless coherence transactions for a given line based on the access patterns in a programmer-transparent manner. In this paper, we describe the protocol transitions in detail. Further, an evaluation using SPLASH and PARSEC applications shows that WiDir substantially reduces the memory stall time of applications. As a result, for 64-core runs, WiDir reduces the execution time of applications by an average of 22% compared to a conventional directory protocol. Moreover, WiDir is more scalable. These benefits are obtained with a very modest power cost.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407196","Multicore;Wireless Network on chip;Directory cache coherence protocol","Protocols;Multicore processing;Wireless networks;Coherence;Computer architecture;System-on-chip;Complexity theory","","10","","84","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Heat Behind the Meter: A Hidden Threat of Thermal Attacks in Edge Colocation Data Centers","Z. Shao; M. A. Islam; S. Ren","University of California, Riverside; University of Texas at Arlington; University of California, Riverside",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","318","331","The widespread adoption of Internet of Things and latency-critical applications has fueled the burgeoning development of edge colocation data centers (a.k. a., edge colocation) — small-scale data centers in distributed locations. In an edge colocation, multiple entities/tenants house their own physical servers together, sharing the power and cooling infrastructures for cost efficiency and scalability. In this paper, we discover that the sharing of cooling systems also exposes edge colocations’ potential vulnerabilities to cooling load injection attacks (called thermal attacks) by an attacker which, if left at large, may create thermal emergencies and even trigger system outages. Importantly, thermal attacks can be launched by leveraging the emerging architecture of built-in batteries integrated with servers that can conceal the attacker’s actual server power (or cooling load). We consider both one-shot attacks (which aim at creating system outages) and repeated attacks (which aim at causing frequent thermal emergencies). For repeated attacks, we present a foresighted attack strategy which, using reinforcement learning, learns on the fly a good timing for attacks based on the battery state and benign tenants’ load. We also combine prototype experiments with simulations to validate our attacks and show that, for a small 8kW edge colocation, an attacker can potentially cause significant losses. Finally, we suggest effective countermeasures to the potential threat of thermal attacks.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407172","","Data centers;Cooling;Scalability;Prototypes;Reinforcement learning;Computer architecture;Batteries","","8","","74","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Trident: A Hybrid Correlation-Collision GPU Cache Timing Attack for AES Key Recovery","J. Ahn; C. Jin; J. Kim; M. Rhu; Y. Fei; D. Kaeli; J. Kim","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; KAIST, Daejeon, Republic of Korea",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","332","344","Given the parallel processing capabilities of Graphics Processing Units (GPUs), many applications are exploiting GPUs and cryptographic systems have also begun to leverage GPUs to accelerate encryption/decryption. Recent work has identified how microarchitectural side-channel attacks can be carried out on AES (Advanced Encryption Standard) by exploiting the SIMT characteristics and memory coalescing of GPUs. In this work, we first show that previously proposed correlation-based side-channel attacks are not feasible on modern GPUs that support narrower data-cache accesses via a sectored-cache microarchitecture–resulting in memory accesses from different levels of the memory hierarchy. In comparison, we identify how negative timing correlation can occur in modern GPUs when data is fetched from different levels of the cache hierarchy. We then propose Trident - a hybrid cache-collision timing attack on GPUs that can fully recover all AES key bytes on modern GPUs. Cache collisions in GPUs present challenges due to the large number of threads and the number of samples required. To address these challenges, Trident consists of three different components - negative timing correlation, cache-collision attack, and chosen plaintext attack. We leverage the negative timing correlation to recover earlier key bytes of AES while exploiting cache-collision attacks for the latter AES key bytes. To enable GPU cache collision attacks, we exploit memory coalescing to control the number of memory accesses through chosen-plaintext attacks to significantly reduce the number of timing samples needed. Our proposed Trident attack results in over 10× reduction in the number of samples needed to recover the key bytes compared with prior work, while still being successful in full AES key recovery in modern GPUs. We also propose TridentShield - a latency-based countermeasure to the Trident attack that minimizes throughput degradation in GPUs.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407157","GPU;AES;side-channel attack;sectored-cache","Degradation;Correlation;Microarchitecture;Graphics processing units;Side-channel attacks;Parallel processing;Throughput","","8","","49","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"BlockHammer: Preventing RowHammer at Low Cost by Blacklisting Rapidly-Accessed DRAM Rows","A. G. Yağlikçi; M. Patel; J. S. Kim; R. Azizi; A. Olgun; L. Orosa; H. Hassan; J. Park; K. Kanellopoulos; T. Shahroodi; S. Ghose; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; University of Illinois at Urbana–Champaign; ETH Zürich,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","345","358","Aggressive memory density scaling causes modern DRAM devices to suffer from RowHammer, a phenomenon where rapidly activating (i.e., hammering) a DRAM row can cause bit-flips in physically-nearby rows. Recent studies demonstrate that modern DDR4/LPDDR4 DRAM chips, including chips previously marketed as RowHammer-safe, are even more vulnerable to RowHammer than older DDR3 DRAM chips. Many works show that attackers can exploit RowHammer bit-flips to reliably mount system-level attacks to escalate privilege and leak private data. Therefore, it is critical to ensure RowHammersafe operation on all DRAM-based systems as they become increasingly more vulnerable to RowHammer. Unfortunately, state-of-the-art RowHammer mitigation mechanisms face two major challenges. First, they incur increasingly higher performance and/or area overheads when applied to more vulnerable DRAM chips. Second, they require either closely-guarded proprietary information about the DRAM chips’ physical circuit layouts or modifications to the DRAM chip design.In this paper, we show that it is possible to efficiently and scalably prevent RowHammer bit-flips without knowledge of or modification to DRAM internals. To this end, we introduce BlockHammer, a low-cost, effective, and easy-to-adopt RowHammer mitigation mechanism that prevents all RowHammer bit-flips while overcoming the two key challenges. BlockHammer selectively throttles memory accesses that could otherwise potentially cause RowHammer bit-flips. The key idea of BlockHammer is to (1) track row activation rates using area-efficient Bloom filters, and (2) use the tracking data to ensure that no row is ever activated rapidly enough to induce RowHammer bit-flips. By guaranteeing that no DRAM row ever experiences a RowHammer-unsafe activation rate, BlockHammer (1) makes it impossible for a RowHammer bit-flip to occur and (2) greatly reduces a RowHammer attack’s impact on the performance of co-running benign applications. Our evaluations across a comprehensive range of 280 workloads show that, compared to the best of six state-of-the-art RowHammer mitigation mechanisms (all of which require knowledge of or modification to DRAM internals), BlockHammer provides (1) competitive performance and energy when the system is not under a RowHammer attack and (2) significantly better performance and energy when the system is under a RowHammer attack.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407238","","Energy consumption;Layout;Computer architecture;DRAM chips;Reliability;Blacklisting;Faces","","71","","164","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"A Write-Friendly and Fast-Recovery Scheme for Security Metadata in Non-Volatile Memories","J. Huang; Y. Hua","Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology; Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","359","370","Non-Volatile Memories (NVMs) require security mechanisms, e.g., counter mode encryption and integrity tree verification, which are important to protect systems in terms of encryption and data integrity. These security mechanisms heavily rely on extra security metadata that need to be efficiently and accurately recovered after system crashes or power off. Established SGX integrity tree (SIT) becomes efficient to protect system integrity and however fails to be restored from leaves, since the computations of SIT nodes need their parent nodes as inputs. To recover the security metadata with low write overhead and short recovery time, we propose an efficient and instantaneous persistence scheme, called STAR, which instantly persists the modifications of security metadata without extra memory writes. STAR is motivated by our observation that the parent nodes in cache are modified due to persisting their child nodes. STAR stores the modifications of parent nodes in their child nodes and persists them just using one atomic memory write. To eliminate the overhead of persisting the modifications, STAR coalesces the modifications and MACs in the evicted metadata. For fast recovery and verification of the metadata, STAR uses bitmap lines in asynchronous DRAM refresh (ADR) to indicate the locations of stale metadata, and constructs a cached merkle tree to verify the correctness of the recovery process. Our evaluation results show that compared with state-of-the-art work, our proposed STAR delivers high performance, low write traffic, low energy consumption and short recovery time.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00038","National Key Research and Development Program of China; National Natural Science Foundation of China; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407203","NVM;integrity tree","Energy consumption;Nonvolatile memory;System performance;Memory management;Random access memory;Metadata;Computer crashes","","12","","46","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"DepGraph: A Dependency-Driven Accelerator for Efficient Iterative Graph Processing","Y. Zhang; X. Liao; H. Jin; L. He; B. He; H. Liu; L. Gu","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; Department of Computer Science, University of Warwick, United Kingdom; National University of Singapore, Singapore; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","371","384","Many graph processing systems have been recently developed for many-core processors. However, for iterative graph processing, due to the dependencies between vertices’ states, the propagations of new states of vertices are inherently conducted along graph paths sequentially and are also dependent on each other. Despite the years’ research effort, existing solutions still severely underutilize many-core processors to quickly propagate the new states of vertices, suffering from slow convergence speed. In this paper, we propose a dependency-driven programmable accelerator, DepGraph, which couples with the core architecture of the many-core processor and can fundamentally alleviate the challenge of dependencies for faster state propagation. Specifically, we propose an effective dependency-driven asynchronous execution approach into novel microarchitecture designs for faster state propagations. DepGraph prefetches the vertices for the core on-the-fly along the dependency chains between their states and the active vertices’ new states, aiming to effectively accelerate the propagations of the active vertices’ new states and also ensure better data locality. Through transforming the dependency chains along the frequently-used paths into direct ones at runtime and maintaining these calculated direct dependencies as a set of fast shortcuts, called hub index, DepGraph further accelerates most state propagations. Also, many propagations do not need to wait for the completion of other propagations, which enables more propagations to be effectively conducted along the paths with higher degree of parallelism. The experimental results show that for iterative graph processing on a simulated 64-core processor, a cutting-edge software graph processing system can achieve 5.0–22.7 times speedup after integrating with our DepGraph while incurring only 0.6% area cost. In comparison with three state-of-the-art hardware solutions, i.e., HATS, Minnow, and PHI, DepGraph improves the performance by up to 3.0–14.2, 2.2–5.8, and 2.4–10.1 times, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00039","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407071","Accelerator;graph processing;many core;domain specialized","Program processors;Prefetching;Computer architecture;Parallel processing;Software;Acceleration;Indexes","","27","","77","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"QEI: Query Acceleration Can be Generic and Efficient in the Cloud","Y. Yuan; Y. Wang; R. Wang; R. B. R. Chowhury; C. Tai; N. S. Kim",UIUC; Intel; Intel; Intel; Intel; UIUC,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","385","398","Data query operations of different data structures are ubiquitous and critical in today’s data center infrastructures and applications. However, query operations are not always performance-optimal to be executed on general-purpose CPU cores. These operations exhibit insufficient memory-level parallelism and frontend bottlenecks due to unstructured control flow. Furthermore, the data access patterns are not cache- or prefetch-friendly. Based on our performance analysis on a commodity server, query operations can consume a large percentage of the CPU cycles in various modern cloud workloads. Existing accelerator solutions for query operations do not strike a balance between their generality, scalability, latency, and hardware complexity. In this paper, we propose QEI, a generic, integrated, and efficient acceleration solution for various data structure queries. We first abstract the query operations to a few regular steps and map them to a simple and hardware-friendly configurable finite automaton model. Based on this model, we develop the QEI architecture that allows multiple query operations to execute in parallel to maximize throughput. We also propose a novel way to integrate the accelerator into the CPU that balances performance, latency, and hardware cost. QEI keeps the main control logic near the L2 cache to leverage existing hardware resources in the core while distributing the data-intensive comparison logic to each last-level cache slice for higher parallelism. Our results with five representative data center workloads show that QEI can achieve $6. 5 \times \sim 11. 2 \times $ performance improvement in various scenarios with low overhead.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00040","National Science Foundation; Intel Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407097","data query;on-chip accelerator;near-cache processing","Data centers;Scalability;Prefetching;Parallel processing;Data structures;Throughput;Hardware","","4","","83","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"EXMA: A Genomics Accelerator for Exact-Matching","L. Jiang; F. Zokaee","Indiana University, Bloomington; Indiana University, Bloomington",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","399","411","Genomics is the foundation of precision medicine, global food security and virus surveillance. Exact-match is one of the most essential operations widely used in almost every step of genomics such as alignment, assembly, annotation, and compression. Modern genomics adopts Ferragina-Manzini Index (FMIndex) augmenting space-efficient Burrows-Wheeler transform (BWT) with additional data structures to permit ultra-fast exact-match operations. However, FM-Index is notorious for its poor spatial locality and random memory access pattern. Prior works create GPU-, FPGA-, ASIC- and even process-in-memory (PIM)based accelerators to boost FM-Index search throughput. Though they achieve the state-of-the-art FM-Index search throughput, the same as all prior conventional accelerators, FM-Index PIMs process only one DNA symbol after each DRAM row activation, thereby suffering from poor memory bandwidth utilization. In this paper, we propose a hardware accelerator, EXMA, to enhance FM-Index search throughput. We first create a novel EXMA table with a multi-task-learning (MTL)-based index to process multiple DNA symbols with each DRAM row activation. We then build an accelerator to search over an EXMA table. We propose 2-stage scheduling to increase the cache hit rate of our accelerator. We introduce dynamic page policy to improve the row buffer hit rate of DRAM main memory. We also present CHAIN compression to reduce the data structure size of EXMA tables. Compared to state-of-the-art FM-Index PIMs, EXMA improves search throughput by $4.9 \times$, and enhances search throughput per Watt by $4.8 \times$.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00041","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407120","Domain-Specific Hardware Accelerator;Genomics;Exact-Matching","Genomics;Random access memory;DNA;Transforms;Throughput;Data structures;Hardware","","8","","62","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Ultra-Elastic CGRAs for Irregular Loop Specialization","C. Torng; P. Pan; Y. Ou; C. Tan; C. Batten","Stanford University, Stanford, CA; Cornell University, Ithaca, NY; Cornell University, Ithaca, NY; Cornell University, Ithaca, NY; Cornell University, Ithaca, NY",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","412","425","Reconfigurable accelerator fabrics, including coarse-grain reconfigurable arrays (CGRAs), have experienced a resurgence in interest because they allow fast-paced software algorithm development to continue evolving post-fabrication. CGRAs traditionally target regular workloads with data-level parallelism (e.g., neural networks, image processing), but once integrated into an SoC they remain idle and unused for irregular workloads. An emerging trend towards repurposing these idle resources raises important questions for how to efficiently map and execute general-purpose loops which may have irregular memory accesses, irregular control flow, and inter-iteration loop dependencies. Recent work has increasingly leveraged elasticity in CGRAs to mitigate the first two challenges, but elasticity alone does not address inter-iteration loop dependencies which can easily bottleneck overall performance. In this paper, we address all three challenges for irregular loop specialization and propose ultra-elastic CGRAs (UE-CGRAs), a novel elastic CGRA that accelerates true-dependency bottlenecks and saves energy in irregular loops by overcoming traditional VLSI challenges. UE-CGRAs allow configurable fine-grain dynamic voltage and frequency scaling (DVFS) for each of potentially hundreds of tiny processing elements (PEs) in the CGRA, enabling chains of connected PEs to “rest” at lower voltages and frequencies to save energy, while other chains of connected PEs can “sprint” at higher voltages and frequencies to accelerate through true-dependency bottlenecks. UE-CGRAs rely on a novel ratiochronous clocking scheme carefully overlaid on the inter-PE elastic interconnect to enable low-latency crossings while remaining fully verifiable with commercial static timing analysis tools. We present the UE-CGRA analytical model, compiler, architectural template, and VLSI circuitry, and we demonstrate how UE-CGRAs can specialize for irregular loops and improve performance ($ 1.42-1.50\times$) or energy efficiency $(1.24-2.32\times)$ with reasonable area overhead compared to traditional inelastic and elastic CGRAs, while also improving performance ($ 1.35-3.38\times$) or energy efficiency (up to $1.53\times$) compared to a RISC-V core.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00042","Xilinx; Semiconductor Research Corporation; Synopsys; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407079","","Software algorithms;Neural networks;Very large scale integration;Tools;Elasticity;Parallel processing;Energy efficiency","","56","","62","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"GSSA: A Resource Allocation Scheme Customized for 3D NAND SSDs","C. -Y. Liu; Y. Lee; W. Choi; M. Jung; M. T. Kandemir; C. Das","Computer Science and Engineering, The Pennsylvania State University State College, PA, USA; Computer Science and Engineering, The Pennsylvania State University State College, PA, USA; Computer Science and Engineering, The Pennsylvania State University State College, PA, USA; CAMEL, Electrical Engineering, KAIST, Daejeon, South Korea; Computer Science and Engineering, The Pennsylvania State University State College, PA, USA; Computer Science and Engineering, The Pennsylvania State University State College, PA, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","426","439","The high density of 3D NAND-based SSDs comes with longer write latencies due to the increasing program complexity. To address this write performance degradation issue, NAND flash manufacturers implement a 3D NAND-specific full-sequence program (FSP) operation. The FSP can program multiple-bit information into a cell simultaneously with the same latency as the baseline program operation, thereby dramatically boosting the write performance. However, directly adopting the (large granularity) FSP operation in SSD firmware can result in a lifetime degradation problem, where small writes are amplified to large granularities with a significant fraction of empty data. This problem cannot completely be mitigated by the DRAM buffer in the SSDs since the “sync” commands from the host prevent the DRAM buffer from accumulating enough written data. To solve this FSP-induced performance/lifetime dilemma, in this work, we propose and evaluate GSSA (Generalized and Specialized Scramble Allocation), a novel written-data allocation scheme in SSD firmware, which considers both various 3D NAND program operations and the internal 3D NAND flash architecture. By adopting GSSA, SSDs can enjoy the performance benefits brought by the FSP without excessively consuming the lifetime. Our experimental evaluations reveal that GSSA can achieve the throughput and the spent-lifetime of the best-performance and best-lifetime single granularity schemes, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407188","SSD management;page allocation;3D NAND flash;scramble allocation","Degradation;Three-dimensional displays;Microprocessors;Random access memory;Computer architecture;Throughput;Resource management","","13","","49","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Memristive Data Ranking","A. K. Prasad; M. Rezaalipour; M. Dehyadegari; M. N. Bojnordi",University of Utah; K.N. Toosi University of Technology; K.N. Toosi University of Technology; University of Utah,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","440","452","Sorting is a fundamental operation in many large-scale data processing applications. In big data computing, sorting imposes a massive requirement on the available memory bandwidth because of its natural demand for pairwise comparison. This high bandwidth requirement often leads to a significant degradation in performance and energy-efficiency. Processing-in-memory has been examined as an effective solution to the memory bandwidth problem for SIMD and data-parallel operations, which does not necessarily solve the bandwidth problem for pairwise comparison. This paper proposes a viable hardware/software mechanism for performing large-scale data ranking in memory with a bandwidth complexity of O(1). Large-scale comparison that forms the core computation of sorting algorithms is reformulated in terms of novel bit-level operations within the physical memory arrays for in-situ ranking, thereby eliminating the need for any pairwise comparison outside the memory arrays. The proposed mechanism, called RIME, provides an API library granting the user application sufficient control over the fundamental operations for in-situ ranking, sorting, and merging. Our simulation results on a set of high-performance parallel sorting kernels indicate 12.4–50.7× throughput gains for RIME. When used for ranking and sorting in a set of database applications, graph analytics, and network processing, RIME achieves more than 90% energy reduction and 2.3–43.6 × performance improvements.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00045","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407200","","Simulation;Merging;Memory management;Memory architecture;Bandwidth;Throughput;Data processing","","19","","91","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"DeACT: Architecture-Aware Virtual Memory Support for Fabric Attached Memory Systems","V. R. Kommareddy; C. Hughes; S. D. Hammond; A. Awad","University of Central Florida, Orlando, Florida, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; North Carolina State University, Raleigh, North Carolina, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","453","466","1 The exponential growth of data has driven technology providers to develop new protocols, such as cache coherent interconnects and memory semantic fabrics, to help users and facilities leverage advances in memory technologies to satisfy these growing memory and storage demands. Using these new protocols, fabric-attached memories (FAM) can be directly attached to a system interconnect and be easily integrated with a variety of processing elements (PEs). Moreover, systems that support FAM can be smoothly upgraded and allow multiple PEs to share the FAM memory pools using well-defined protocols. The sharing of FAM between PEs allows efficient data sharing, improves memory utilization, reduces cost by allowing flexible integration of different PEs and memory modules from several vendors, and makes it easier to upgrade the system. One promising use-case for FAMs is in High-Performance Compute (HPC) systems, where the underutilization of memory is a major challenge. However, adopting FAMs in HPC systems brings new challenges. In addition to cost, flexibility, and efficiency, one particular problem that requires rethinking is virtual memory support for security and performance. To address these challenges, this paper presents decoupled access control and address translation (DeACT), a novel virtual memory implementation that supports HPC systems equipped with FAM. Compared to the state-of-the-art two-level translation approach, DeACT achieves speedup of up to 4.59x (1.8x on average) without compromising security.1Part of this work was done when Vamsee was working under the supervision of Amro Awad at UCF. Amro Awad is now with the ECE Department at NC State.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00046","Sandia National Laboratories; U.S. Department of Energy; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407130","disaggregated fabric attached memory systems;virtual memory;security","Access control;Protocols;Scalability;Memory management;Semantics;Memory modules;Fabrics","","7","","50","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Analyzing and Leveraging Decoupled L1 Caches in GPUs","M. A. Ibrahim; O. Kayiran; Y. Eckert; G. H. Loh; A. Jog","William & Mary; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; William & Mary",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","467","478","Graphics Processing Units (GPUs) use caches to provide on-chip bandwidth as a way to address the memory wall. However, they are not always efficiently utilized for optimal GPU performance. We find that the main source of this inefficiency stems from the tightly-coupled design of cores with L1 caches. First, such a design assumes a per-core private local L1 cache in which each core independently caches the required data. This allows the same cache line to get replicated across cores, which wastes precious cache capacity. Second, due to the many-to-few traffic pattern, the tightly-coupled design leads to low per-core L1 bandwidth utilization while L2/memory is heavily utilized.To address these inefficiencies, we renovate the conventional GPU cache hierarchy by proposing a new DC-L1 (DeCoupled-L1) cache – an L1 cache separated from the GPU core. We show how decoupling the L1 cache from the GPU core provides opportunities to reduce data replication across the L1s and increase their bandwidth utilization. Specifically, we investigate how to aggregate the DC-L1s; how to manage data placement across the aggregated DC-L1s; and how to efficiently connect the DC-L1s to the GPU cores and the L2/memory partitions. Our evaluation shows that our new cache design boosts the useful L1 cache bandwidth and achieves significant improvement in performance and energy efficiency across a wide set of GPGPU applications while reducing the overall NoC area footprint.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00047","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407080","Bandwidth;GPUs;Locality;Network-on-Chip","Aggregates;Graphics processing units;Bandwidth;Organizations;Computer architecture;Traffic control;Energy efficiency","","19","","47","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Deadline-Aware Offloading for High-Throughput Accelerators","T. T. Yeh; M. D. Sinclair; B. M. Beckmann; T. G. Rogers",Purdue University; University of Wisconsin; AMD Research; Purdue University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","479","492","Contemporary GPUs are widely used for throughput-oriented data-parallel workloads and increasingly are being considered for latency-sensitive applications in datacenters. Examples include recurrent neural network (RNN) inference, network packet processing, and intelligent personal assistants. These data parallel applications have both high throughput demands and real-time deadlines (40μs-7ms). Moreover, the kernels in these applications have relatively few threads that do not fully utilize the device unless a large batch size is used. However, batching forces jobs to wait, which increases their latency, especially when realistic job arrival times are considered.Previously, programmers have managed the tradeoffs associated with concurrent, latency-sensitive jobs by using a combination of GPU streams and advanced scheduling algorithms running on the CPU host. Although GPU streams allow the accelerator to execute multiple jobs concurrently, prior state-of-the-art solutions use the relatively distant CPU host to prioritize the latency-sensitive GPU tasks. Thus, these approaches are forced to operate at a coarse granularity and cannot quickly adapt to rapidly changing program behavior.We observe that fine-grain, device-integrated kernel schedulers efficiently meet the deadlines of concurrent, latency-sensitive GPU jobs. To overcome the limitations of software-only, CPU-side approaches, we extend the GPU queue scheduler to manage real-time deadlines. We propose a novel laxity-aware scheduler (LAX) that uses information collected within the GPU to dynamically vary job priority based on how much laxity jobs have before their deadline. Compared to contemporary GPUs, 3 state-of-the-art CPU-side schedulers and 6 other advanced GPU-side schedulers, LAX meets the deadlines of 1.7X – 5.0X more jobs and provides better energy-efficiency, throughput, and 99-percentile tail latency.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407162","GPGPU;job scheduling;laxity","Recurrent neural networks;Scheduling algorithms;Instruction sets;Graphics processing units;Throughput;Real-time systems;Kernel","","14","","94","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Lazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference","Y. Choi; Y. Kim; M. Rhu","School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","493","506","In cloud ML inference systems, batching is an essential technique to increase throughput which helps optimize total-cost-of-ownership. Prior graph batching combines the individual DNN graphs into a single one, allowing multiple inputs to be concurrently executed in parallel. We observe that the coarse-grained graph batching becomes suboptimal in effectively handling the dynamic inference request traffic, leaving significant performance left on the table. This paper proposes LazyBatching, an SLA-aware batching system that considers both scheduling and batching in the granularity of individual graph nodes, rather than the entire graph for flexible batching. We show that LazyBatching can intelligently determine the set of nodes that can be efficiently batched together, achieving an average $15\times, 1.5\times$, and $5.5\times$ improvement than graph batching in terms of average response time, throughput, and SLA satisfaction, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00049","National Research Foundation; KAIST; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407131","computer architecture;machine learning;deep learning;inference server;batching","Training;Quality of service;Machine learning;Computer architecture;Throughput;Time factors","","40","","82","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Dead Page and Dead Block Predictors: Cleaning TLBs and Caches Together","C. Mazumdar; P. Mitra; A. Basu","Department of Computer Science and Automation, Indian Institute of Science; Department of Computer Science and Automation, Indian Institute of Science; Department of Computer Science and Automation, Indian Institute of Science",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","507","519","The last level TLB (LLT) and the last level cache (LLC) play a critical role in the overall performance of memory-intensive applications. While management of LLC content has received significant attention, the same may not be true for LLT. In this work, we first explore the well-known concept of dead blocks in caches for TLBs. We find that dead pages are fairly common in the LLT. Different from dead blocks in LLCs, dead pages in LLTs are most often dead-on-arrival, i.e., they produce zero hits in the TLB. We design a storage-efficient dead page predictor that works with a fraction of storage compared to typical dead block predictors. This is important since an LLT itself requires only a few KBs of storage compared to MBs in LLC. We then leverage the dead page information to guide a simple dead block predictor in LLC. This is driven by the observation that dead blocks are often concentrated within dead pages. In effect, we designed a dead page predictor and a correlating dead block predictor with a total storage overhead of only 11KB to bypass predicted dead pages and dead blocks in LLTs and LLCs, respectively. Together, these predictors help improve the IPC of a set of 14 memory-intensive workloads by 8.3%, on average.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00050","VMware; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407144","TLB;virtual memory;last-level cache;replacement algorithm","Correlation;Computer architecture;Prediction algorithms;Cleaning","","11","","51","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"ParaDox: Eliminating Voltage Margins via Heterogeneous Fault Tolerance","S. Ainsworth; L. Zoubritzky; A. Mycroft; T. M. Jones","University of Edinburgh, Edinburgh, UK; École Normale Supéerieure – PSL, Paris, France; University of Cambridge, Cambridge, UK; University of Cambridge, Cambridge, UK",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","520","532","Providing reliability is becoming a challenge for chip manufacturers, faced with simultaneously trying to improve miniaturization, performance and energy efficiency. This leads to very large margins on voltage and frequency, designed to avoid errors even in the worst case, along with significant hardware expenditure on eliminating voltage spikes and other forms of transient error, causing considerable inefficiency in power consumption and performance. We flip traditional ideas about reliability and performance around, by exploring the use of error resilience for power and performance gains. ParaMedic is a recent architecture that provides a solution for reliability with low overheads via automatic hardware error recovery. It works by splitting up checking onto many small cores in a heterogeneous multicore system with hardware logging support. However, its design is based on the idea that errors are exceptional. We transform ParaMedic into ParaDox, which shows high performance in both error-intensive and scarce-error scenarios, thus allowing correct execution even when undervolted and overclocked. Evaluation within error-intensive simulation environments confirms the error resilience of ParaDox and the low associated recovery cost. We estimate that compared to a non-resilient system with margins, ParaDox can reduce energy-delay product by 15% through undervolting, while completely recovering from any induced errors.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00051","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407209","fault tolerance;microarchitecture;error detection;voltage margins","Fault tolerance;Multicore processing;Fault tolerant systems;Computer architecture;Transforms;Reliability engineering;Hardware","","5","","70","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"CARE: Coordinated Augmentation for Elastic Resilience on DRAM Errors in Data Centers","J. Chen; X. Jiang; Y. Zhang; L. Liu; H. Xu; Q. Liu","Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","533","544","As the computation density and memory capacity continues to grow, DRAM errors have become the leading cause of server crashes and/or system failures in modern data centers. While myriads of techniques have been proposed to mitigate their impact on system reliability, these solutions either incur significant overhead on performance, power and memory capacity or require modifying multiple system components; hence, they are impractical to implement or deploy. This paper proposes CARE, a novel error tolerance framework for efficient and elastic resilience on DRAM errors. It introduces a cache-like structure in the memory controller for dynamic error tracking and proactive resilience enhancement to achieve high error tolerance economically and practically. Experiment results show that with around 58KB area overhead in the memory controller, CARE achieves near Chipkill reliability without any memory capacity penalty and incurs negligible performance overhead compared with the baseline SEC-DED systems. CARE provides an attractive alternative to enhance the reliability in data centers.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407105","DRAM errors;memory controller;page retirement;data centers","Data centers;Cloud computing;Correlation;Memory management;Random access memory;Computer crashes;Reliability","","10","","29","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Automatic Microprocessor Performance Bug Detection","E. C. Barboza; S. Jacob; M. Ketkar; M. Kishinevsky; P. Gratz; J. Hu",Texas A&M University; Texas A&M University; Intel Corportation; Intel Corportation; Texas A&M University; Texas A&M University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","545","556","Processor design validation and debug is a difficult and complex task, which consumes the lion’s share of the design process. Design bugs that affect processor performance rather than its functionality are especially difficult to catch, particularly in new microarchitectures. This is because, unlike functional bugs, the correct processor performance of new microarchitectures on complex, long-running benchmarks is typically not deterministically known. Thus, when performance benchmarking new microarchitectures, performance teams may assume that the design is correct when the performance of the new microarchitecture exceeds that of the previous generation, despite significant performance regressions existing in the design. In this work we present a two-stage, machine learning-based methodology that is able to detect the existence of performance bugs in microprocessors. Our results show that our best technique detects 91.5% of microprocessor core performance bugs whose average IPC impact across the studied applications is greater than 1% versus a bug-free design with zero false positives. When evaluated on memory system bugs, our technique achieves 100% detection with zero false positives. Moreover, the detection is automatic, requiring very little performance engineer time.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00053","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407243","Performance;debugging;Machine Learning;Modeling;bugs;microprocessor;core;automatic","Microarchitecture;Microprocessors;Computer bugs;Computer architecture;Benchmark testing;Task analysis","","5","","55","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"CAPE: A Content-Addressable Processing Engine","H. Caminal; K. Yang; S. Srinivasa; A. K. Ramanathan; K. Al-Hawaj; T. Wu; V. Narayanan; C. Batten; J. F. Martínez",Cornell University; Cornell University; Intel Labs; Pennsylvania State University; Cornell University; Cornell University; Pennsylvania State University; Cornell University; Cornell University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","557","569","Processing-in-memory (PIM) architectures attempt to overcome the von Neumann bottleneck by combining computation and storage logic into a single component. The content-addressable parallel processing paradigm (CAPP) from the seventies is an in-situ PIM architecture that leverages content-addressable memories to realize bit-serial arithmetic and logic operations, via sequences of search and update operations over multiple memory rows in parallel. In this paper, we set out to investigate whether the concepts behind classic CAPP can be used successfully to build an entirely CMOS-based, general-purpose microarchitecture that can deliver manyfold speedups while remaining highly programmable. We conduct a full-stack design of a Content-Addressable Processing Engine (CAPE), built out of dense push-rule 6T SRAM arrays. CAPE is programmable using the RISC-V ISA with standard vector extensions. Our experiments show that CAPE achieves an average speedup of 14 (up to 254) over an area-equivalent (slightly under 9 mm2 at 7 nm) out-of-order processor core with three levels of caches.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407239","Associative processing;associative memory;vector processors","Out of order;Microarchitecture;Random access memory;Computer architecture;Parallel processing;Associative processing;Engines","","21","","51","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"SpaceA: Sparse Matrix Vector Multiplication on Processing-in-Memory Accelerator","X. Xie; Z. Liang; P. Gu; A. Basak; L. Deng; L. Liang; X. Hu; Y. Xie","Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA; Department of Electrical and Computer Engineering, UCSB, Santa Barbara, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","570","583","Sparse matrix-vector multiplication (SpMV) is an important primitive across a wide range of application domains such as scientific computing and graph analytics. Due to its intrinsic memory-bound characteristics, the performance of SpMV on throughput-oriented architectures such as GPU is bounded by the limited bandwidth between processors and memory. Processing-in-memory (PIM) architectures, made feasible by advances in 3D stacking, provide new opportunities to utilize ultra-high bandwidth by integrating compute-logic into memory.In this paper, we develop an SpMV accelerator, named as SpaceA, based on PIM architectures. SpaceA integrates compute logic near memory banks to exploit bank-level bandwidth. SpaceA contains both hardware and data-mapping design features to alleviate irregular memory access patterns which hinder full utilization of high memory bandwidth. In terms of hardware design features, SpaceA consists of two unique features: (1) it utilizes the capability of outstanding memory requests to hide the memory access latency to data located in non-local memory banks; (2) it integrates Content Addressable Memory (CAM) at the bank level to exploit data reuse of the input vectors. In addition, we develop a mapping scheme that partitions the sparse matrix into different memory banks, to maximize the data locality of the input vector and to achieve workload balance among processing elements (PEs) near each bank. Overall, SpaceA together with the proposed mapping method achieves 13.54x speedup and 87.49% energy saving on average over the GPU baseline on SpMV computation. In addition to SpMV primitives, we conduct a case study on graph analytics to demonstrate the benefits of SpaceA for applications built on SpMV. Compared to Tesseract and GraphP, state-of-the-art graph accelerators, SpaceA obtains better performance due to its higher effective bandwidth provided by near-bank integration.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407163","SpMV;Accelerator;Processing-in-memory","Associative memory;Three-dimensional displays;Scientific computing;Stacking;Memory architecture;Graphics processing units;Bandwidth","","66","","79","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Layerweaver: Maximizing Resource Utilization of Neural Processing Units via Layer-Wise Scheduling","Y. H. Oh; S. Kim; Y. Jin; S. Son; J. Bae; J. Lee; Y. Park; D. U. Kim; T. J. Ham; J. W. Lee","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Neural Processing Research Center (NPRC), Seoul National University, Seoul, Korea",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","584","597","To meet surging demands for deep learning inference services, many cloud computing vendors employ high-performance specialized accelerators, called neural processing units (NPUs). One important challenge for effective use of NPUs is to achieve high resource utilization over a wide spectrum of deep neural network (DNN) models with diverse arithmetic intensities. There is often an intrinsic mismatch between the compute-to-memory bandwidth ratio of an NPU and the arithmetic intensity of the model it executes, leading to under-utilization of either compute resources or memory bandwidth. Ideally, we want to saturate both compute TOP/s and DRAM bandwidth to achieve high system throughput. Thus, we propose Layerweaver, an inference serving system with a novel multi-model time-multiplexing scheduler for NPUs. Layerweaver reduces the temporal waste of computation resources by interweaving layer execution of multiple different models with opposing characteristics: compute-intensive and memory-intensive. Layerweaver hides the memory time of a memory-intensive model by overlapping it with the relatively long computation time of a compute-intensive model, thereby minimizing the idle time of the computation units waiting for off-chip data transfers. For a two-model serving scenario of batch 1 with 16 different pairs of compute- and memory-intensive models, Layerweaver improves the temporal utilization of computation units and memory channels by 44.0% and 28.7%, respectively, to increase the system throughput by 60.1% on average, over the baseline executing one model at a time.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00056","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407236","Layer-wise Scheduling;Systems for Machine Learning;Inference Serving System;Neural Networks;Accelerator Systems;Multi-tasking","Schedules;Scheduling algorithms;Computational modeling;Neural networks;Memory management;Random access memory;Bandwidth","","30","","67","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Sentinel: Efficient Tensor Migration and Allocation on Heterogeneous Memory Systems for Deep Learning","J. Ren; J. Luo; K. Wu; M. Zhang; H. Jeon; D. Li","University of California, Merced; University of California, Merced; University of California, Merced; Microsoft; University of California, Merced; University of California, Merced",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","598","611","Memory capacity is a major bottleneck for training deep neural networks (DNN). Heterogeneous memory (HM) combining fast and slow memories provides a promising direction to increase memory capacity. However, HM imposes challenges on tensor migration and allocation for high performance DNN training. Prior work heavily relies on DNN domain knowledge, unnecessarily causes tensor migration due to page-level false sharing, and wastes fast memory space. We present Sentinel, a software runtime system that automatically optimizes tensor management on HM. Sentinel uses dynamic profiling, and coordinates operating system (OS) and runtime-level profiling to bridge the semantic gap between OS and applications, which enables tensor-level profiling. This profiling enables co-allocating tensors with similar lifetime and memory access frequency into the same pages. Such fine-grained profiling and tensor collocation avoids unnecessary data movement, improves tensor movement efficiency, and enables larger batch training because of saving in fast memory space. Sentinel reduces fast memory consumption by 80% while retaining comparable performance to fast memory-only system; Sentinel consistently outperforms a state-of-the-art solution on CPU by 37% and two state-of-the-art solutions on GPU by 2x and 21% respectively in training throughput.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00057","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407112","heterogeneous memory;deep neural network training;memory management","Training;Tensors;Runtime;Operating systems;Memory management;Neural networks;Semantics","","47","","48","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"CSCNN: Algorithm-hardware Co-design for CNN Accelerators using Centrosymmetric Filters","J. Li; A. Louri; A. Karanth; R. Bunescu","Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, North Carolina",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","612","625","Convolutional neural networks (CNNs) are at the core of many state-of-the-art deep learning models in computer vision, speech, and text processing. Training and deploying such CNN-based architectures usually require a significant amount of computational resources. Sparsity has emerged as an effective compression approach for reducing the amount of data and computation for CNNs. However, sparsity often results in computational irregularity, which prevents accelerators from fully taking advantage of its benefits for performance and energy improvement. In this paper, we propose CSCNN, an algorithm/hardware co-design framework for CNN compression and acceleration that mitigates the effects of computational irregularity and provides better performance and energy efficiency. On the algorithmic side, CSCNN uses centrosymmetric matrices as convolutional filters. In doing so, it reduces the number of required weights by nearly 50% and enables structured computational reuse without compromising regularity and accuracy. Additionally, complementary pruning techniques are leveraged to further reduce computation by a factor of $2.8-7.2\times $ with a marginal accuracy loss. On the hardware side, we propose a CSCNN accelerator that effectively exploits the structured computational reuse enabled by centrosymmetric filters, and further eliminates zero computations for increased performance and energy efficiency. Compared against a dense accelerator, SCNN and SparTen, the proposed accelerator performs $3.7\times $, $1.6\times $ and $1.3\times $ better, and improves the EDP (Energy Delay Product) by $8.9\times $, $2.8\times $ and $2.0\times $, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407182","Convolutional Neural Networks;Domain-specific Accelerators","Training;Redundancy;Computer architecture;Filtering algorithms;Energy efficiency;Computational efficiency;Convolutional neural networks","","7","","98","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Improving GPU Multi-tenancy with Page Walk Stealing","B. Pratheek; N. Jawalkar; A. Basu","Department of Computer Science and Automation, Indian Institute of Science; Department of Computer Science and Automation, Indian Institute of Science; Department of Computer Science and Automation, Indian Institute of Science",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","626","639","GPU (Graphics Processing Unit) architecture has evolved to accelerate parts of a single application at a time. Consequently, several aspects of its architecture, particularly the virtual memory, have embraced a shared-mostly design. This implicitly assumes that a single application and, thus, one address space is resident in the GPU at a time. However, recent trends, e.g., deployment of GPUs in the cloud, necessitate efficient multi-tenancy. Multi-tenancy is needed for sharing the physical resources of a large server-class GPU across multiple concurrent tenants (applications) for resource consolidation while ensuring fairness among the tenants.We first quantify how different components of GPU’s virtual memory can impede multi-tenancy. We show that shared page walkers are a key bottleneck under multi-tenancy. We, therefore, propose dynamic page walk stealing that enables soft partitioning of the shared pool of walkers– reducing destructive interference between the tenants while also aggregating resources where possible. Over today’s design, we improve throughput by 37%, and weighted IPC by 15%, on average, over 45 workloads.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407125","graphics processing units;virtual memory;address translation;page table walkers;multi-programming","Cloud computing;Graphics processing units;Process control;Computer architecture;Interference;Throughput;Market research","","18","","50","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Stream Floating: Enabling Proactive and Decentralized Cache Optimizations","Z. Wang; J. Weng; J. Lowe-Power; J. Gaur; T. Nowatzki",UCLA; UCLA; UC Davis; Intel; UCLA,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","640","653","As multicore systems continue to grow in scale and on-chip memory capacity, the on-chip network bandwidth and latency become problematic bottlenecks. Because of this, overheads in data transfer, the coherence protocol and replacement policies become increasingly important. Unfortunately, even in well-structured programs, many natural optimizations are difficult to implement because of the reactive and centralized nature of traditional cache hierarchies, where all requests are initiated by the core for short, cache line granularity accesses. For example, long-lasting access patterns could be streamed from shared caches without requests from the core. Indirect memory access can be performed by chaining requests made from within the cache, rather than constantly returning to the core. Our primary insight is that if programs can embed information about long-term memory stream behavior in their ISAs, then these streams can be floated to the appropriate level of the memory hierarchy. This decentralized approach to address generation and cache requests can lead to better cache policies and lower request and data traffic by proactively sending data before the cores even request it. To evaluate the opportunities of stream floating, we enhance a tiled multicore cache hierarchy with stream engines to process stream requests in last-level cache banks. We develop several novel optimizations that are facilitated by stream exposure in the ISA, and subsequent exposure to caches. We evaluate using a cycle-level execution-driven gem5-based simulator, using 10 data-processing workloads from Rodinia and 2 streaming kernels written in OpenMP. We find that stream floating enables 52% and 39% speedup over an inorder and OOO core with state of art prefetcher design respectively, with 64% and 49% energy efficiency advantage.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407107","Network on Chip;Stream;Specialization;Prefetching;Microarchitecture;ISA;Memory Specialization;Decoupled Access Execute;Proactive Cache","Out of order;Technological innovation;Microarchitecture;Protocols;Multicore processing;Prefetching;Coherence","","20","","65","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Prodigy: Improving the Memory Latency of Data-Indirect Irregular Workloads Using Hardware-Software Co-Design","N. Talati; K. May; A. Behroozi; Y. Yang; K. Kaszyk; C. Vasiladiotis; T. Verma; L. Li; B. Nguyen; J. Sun; J. M. Morton; A. Ahmadi; T. Austin; M. O’Boyle; S. Mahlke; T. Mudge; R. Dreslinski",University of Michigan; University of Michigan; University of Michigan; University of Michigan; University of Edinburgh; University of Edinburgh; University of Michigan; University of Edinburgh; University of Michigan; University of Edinburgh; University of Edinburgh; University of Michigan; University of Michigan; University of Edinburgh; University of Michigan; University of Michigan; University of Michigan,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","654","667","Irregular workloads are typically bottlenecked by the memory system. These workloads often use sparse data representations, e.g., compressed sparse row/column (CSR/CSC), to conserve space at the cost of complicated, irregular traversals. Such traversals access large volumes of data and offer little locality for caches and conventional prefetchers to exploit. This paper presents Prodigy, a low-cost hardware-software codesign solution for intelligent prefetching to improve the memory latency of several important irregular workloads. Prodigy targets irregular workloads including graph analytics, sparse linear algebra, and fluid mechanics that exhibit two specific types of data-dependent memory access patterns. Prodigy adopts a “best of both worlds” approach by using static program information from software, and dynamic run-time information from hardware. The core of the system is the Data Indirection Graph (DIG)—a proposed compact representation used to express program semantics such as the layout and memory access patterns of key data structures. The DIG representation is agnostic to a particular data structure format and is demonstrated to work with several sparse formats including CSR and CSC. Program semantics are automatically captured with a compiler pass, encoded as a DIG, and inserted into the application binary. The DIG is then used to program a low-cost hardware prefetcher to fetch data according to an irregular algorithm’s data structure traversal pattern. We equip the prefetcher with a flexible prefetching algorithm that maintains timeliness by dynamically adapting its prefetch distance to an application’s execution pace. We evaluate the performance, energy consumption, and transistor cost of Prodigy using a variety of algorithms from the GAP, HPCG, and NAS benchmark suites. We compare the performance of Prodigy against a non-prefetching baseline as well as state-of-the-art prefetchers. We show that by using just 0.8KB of storage, Prodigy outperforms a non-prefetching baseline by $2.6 \times$ and saves energy by $1.6 \times$, on average. Prodigy also outperforms modern data prefetchers by $1.5- 2.3 \times$.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00061","Defense Advanced Research Projects Agency; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407222","DRAM stalls;irregular workloads;graph processing;hardware-software co-design;programming model;programmer annotations;compiler;and hardware prefetching.","Prefetching;Heuristic algorithms;Software algorithms;Semantics;Layout;Random access memory;Data structures","","43","","104","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"P-OPT: Practical Optimal Cache Replacement for Graph Analytics","V. Balaji; N. Crago; A. Jaleel; B. Lucia",Carnegie Mellon University; NVIDIA; NVIDIA; Carnegie Mellon University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","668","681","Graph analytics is an important workload that achieves suboptimal performance due to poor cache locality. State-of-the-art cache replacement policies fail to capture the highly dynamic and input-specific reuse patterns of graph application data. The main insight of this work is that for graph applications, the transpose of a graph succinctly represents the next references of all vertices in a graph execution; enabling an efficient emulation of Belady’s MIN replacement policy. In this work, we propose P-OPT, an architecture solution that uses a specialized compressed representation of a transpose’s next reference information to enable a practical implementation of Belady’s MIN replacement policy. Our evaluations across multiple applications and inputs reveal that P-OPT improves cache locality for graph applications providing an average performance improvement of 33% (56% maximum) over LRU replacement.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00062","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407090","Graph analytics;Cache Replacement;Locality Optimization","Quantization (signal);Emulation;Computer architecture;Optimization","","30","","59","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Pitstop: Enabling a Virtual Network Free Network-on-Chip","H. Farrokhbakht; H. Kao; K. Hasan; P. V. Gratz; T. Krishna; J. San Miguel; N. E. Jerger",University of Toronto; University of Toronto; University of Toronto; Texas A & M; Georgia Institute of Technology; University of Wisconsin-Madison; University of Toronto,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","682","695","Maintaining correctness is of paramount importance in the design of a computer system. Within a multiprocessor interconnection network, correctness is guaranteed by having deadlock-free communication at both the protocol and network levels. Modern network-on-chip (NoC) designs use multiple virtual networks to maintain protocol-level deadlock freedom, at the expense of high power and area overheads. Other techniques involve complex detection and recovery mechanisms, or use misrouting which incurs additional packet latency. Considering that the probability of deadlocks occurring is low, the additional resources needed to avoid/resolve deadlocks should also be low. To this end, we propose Pitstop, a low-cost technique that guarantees correctness by resolving both protocol and network-level deadlocks without the use of virtual networks, complex hardware, or misrouting. Pitstop transfers blocked packets to the network interface (NI) creating a bubble (empty buffer slot) which breaks deadlock. The blocked packet can make forward progress through NI to NI traversals using low complexity bypassing mechanisms. This scheme performs better due to higher utilization of virtual channels and works on arbitrary irregular topologies without any virtual networks. Compared to state-of-the-art solutions, Pitstop can improve performance up to 11% and reduce power and area up to 41% and 40%.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00063","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407176","","Protocols;Network topology;Multiprocessor interconnection networks;Network-on-chip;Computer architecture;System recovery;Hardware","","20","","66","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"BoomGate: Deadlock Avoidance in Non-Minimal Routing for High-Radix Networks","G. Kwauk; S. Kang; H. Kasan; H. Son; J. Kim","KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","696","708","Avoiding routing deadlock is an important component of an interconnection network. For large-scale systems with high-radix topologies that leverage non-minimal adaptive routing, virtual channels (VCs) are commonly used to prevent routing deadlock. However, VCs in large-scale networks can be costly because of deep buffers and restrict VC usage. In this work, we propose BoomGATE for deadlock avoidance in large-scale networks. In particular, BoomGATE consists of two components – Restricted Intermediate-node Non-minimal Routing (RINR) algorithm and opportunistic flow control (OFC) which both exploit the low-diameter characteristics of high-radix networks while maximizing path diversity within the topology. We identify how routing deadlock in fully-connected topologies are caused by non-minimal routes and propose to restrict the non-minimal routing to ensure deadlock freedom without additional virtual channels. We also propose an algorithm that ensures path diversity is load-balanced across all nodes in the system. However, since path diversity is restricted with the RINR algorithm, complement RINR algorithm with opportunistic flow control (OFC) where “illegal routes” are allowed if and only if sufficient buffer can be guaranteed to ensure cyclical dependency does not occur. We propose both a static and dynamic OFC implementation. We evaluate the performance of BoomGATE and demonstrate there is minimal performance loss compared to global adaptive routing, while reducing the amount of buffers required by 50%.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407148","High-radix networks;deadlock;global-adaptive routing","Adaptive systems;Network topology;Multiprocessor interconnection;Heuristic algorithms;System recovery;Routing;Topology","","6","","55","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"CHOPIN: Scalable Graphics Rendering in Multi-GPU Systems via Parallel Image Composition","X. Ren; M. Lis",The University of British Columbia; The University of British Columbia,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","709","722","The appetite for higher and higher 3D graphics quality continues to drive GPU computing requirements. To satisfy these demands, GPU vendors are moving towards new architectures, such as MCM-GPU and multi-GPUs, that connect multiple chip modules or GPUs with high-speed links (e.g., NVLink and XGMI) to provide higher computing capability. Unfortunately, it is not clear how to adequately parallelize the rendering pipeline to take advantage of these resources while maintaining low rendering latencies. Current implementations of Split Frame Rendering (SFR) are bottlenecked by redundant computations and sequential inter-GPU synchronization, and fail to scale as the GPU count increases. In this paper, we propose CHOPIN, a novel SFR scheme for multi-GPU systems that exploits the parallelism available in image composition to eliminate the bottlenecks inherent to existing solutions. CHOPIN composes opaque sub-images out-of order, and leverages the associativity of image composition to compose adjacent sub-images of transparent objects asynchronously. To mitigate load imbalance across GPUs and avoid inter-GPU network congestion, CHOPIN includes two new scheduling mechanisms: a draw-command scheduler and an image composition scheduler. Detailed cycle-level simulations on eight real-world game traces show that, in an 8-GPU system, CHOPIN offers speedups of up to $ 1.56\times$ ($ 1.25\times$ gmean) compared to the best prior SFR implementation.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00065","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407075","GPU;Graphics Rendering;Sort Last Rendering;Image Composition;Inter GPU Synchronization","Three-dimensional displays;Pipelines;Graphics processing units;Computer architecture;Parallel processing;Rendering (computer graphics);Synchronization","","10","","68","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Adapt-NoC: A Flexible Network-on-Chip Design for Heterogeneous Manycore Architectures","H. Zheng; K. Wang; A. Louri","Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","723","735","The increased computational capability in heterogeneous manycore architectures facilitates the concurrent execution of many applications. This requires, among other things, a flexible, high-performance, and energy-efficient communication fabric capable of handling a variety of traffic patterns needed for running multiple applications at the same time. Such stringent requirements are posing a major challenge for current Network-on-Chips (NoCs) design. In this paper, we propose Adapt-NoC, a flexible NoC architecture, along with a reinforcement learning (RL)-based control policy, that can provide efficient communication support for concurrent application execution. Adapt-NoC can dynamically allocate several disjoint regions of the NoC, called subNoCs, with different sizes and locations for the concurrently running applications. Each of the dynamically-allocated subNoCs is capable of adapting to a given topology such as a mesh, cmesh, torus, or tree thus tailoring the topology to satisfy application’s needs in terms of performance and power consumption. Moreover, we explore the use of RL to design an efficient control policy which optimizes the subNoC topology selection for a given application. As such, Adapt-NoC can not only provide several topology choices for concurrently running applications, but can also optimize the selection of the most suitable topology for a given application with the aim of improving performance and energy efficiency. We evaluate Adapt-NoC using both GPU and CPU benchmark suites. Simulation results show that the proposed Adapt-NoC can achieve up to 34% latency reduction, 10% overall execution time reduction and 53% NoC energy-efficiency improvement when compared to prior work.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407201","Network-on-Chips (NoCs);Reinforcement Learning;Reconfigurable Topology;Flexible NoC Designs","Power demand;Simulation;Graphics processing units;Computer architecture;Reinforcement learning;Traffic control;Benchmark testing","","47","","69","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Hardware-Based Address-Centric Acceleration of Key-Value Store","C. Ye; Y. Xu; X. Shen; X. Liao; H. Jin; Y. Solihin","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; North Carolina State University, Raleigh, North Carolina, USA; North Carolina State University, Raleigh, North Carolina, USA; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Computer Science, University of Central Florida, Florida, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","736","748","Efficiently retrieving data is essential for key-value store applications. A major part of the retrieving time is on data addressing, that is, finding the location of the value in memory that corresponds to a key. This paper introduces an address-centric approach to speed up the addressing by creating a shortcut for the translation of a key to the physical address of the value. The new technique is materialized with a novel in-memory table, STLT, a virtual-physical address buffer, and two new instructions. It creates a fast path for data addressing and meanwhile opens up opportunities for the use of simpler and faster hash tables to strike a better tradeoff between hashing conflicts and hashing overhead. Together, the new technique brings up to 1.4× speedups on key-value store application Redis and up to 13× speedups on some widely used indexing data structures, consistently outperforming prior solutions significantly.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00067","National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407194","key value store;address translation;hardware acceleration","Production;Data structures;Acceleration;Indexing","","6","","61","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"BRIM: Bistable Resistively-Coupled Ising Machine","R. Afoakwa; Y. Zhang; U. K. R. Vengalam; Z. Ignjatovic; M. Huang","Department of Electrical and Computer Engineering, University of Rochester; Department of Electrical and Computer Engineering, University of Rochester; Department of Electrical and Computer Engineering, University of Rochester; Department of Electrical and Computer Engineering, University of Rochester; Department of Electrical and Computer Engineering, University of Rochester",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","749","760","Physical Ising machines rely on nature to guide a dynamical system towards an optimal state which can be read out as a heuristical solution to a combinatorial optimization problem. Such designs that use nature as a computing mechanism can lead to higher performance and/or lower operation costs. Quantum annealers are a prominent example of such efforts. However, existing Ising machines are generally bulky and energy intensive. Such disadvantages may be acceptable if these designs provide some significant intrinsic advantages at a much larger scale in the future, which remains to be seen. But for now, integrated electronic designs of Ising machines allow more immediate applications. We propose one such design that uses bistable nodes, coupled with programmable and variable strengths. The design is fully CMOS compatible for on-chip applications and demonstrates competitive solution quality and significantly superior execution time and energy.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00068","University of Rochester; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407038","Ising machine;optimization;CMOS accelerators;nature-based computing;quantum annealing","Annealing;Uncertainty;Prototypes;Simulated annealing;Computer architecture;System-on-chip;Optical bistability","","40","","40","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"An Analog Preconditioner for Solving Linear Systems","B. Feinberg; R. Wong; T. P. Xiao; C. H. Bennett; J. N. Rohan; E. G. Boman; M. J. Marinella; S. Agarwal; E. Ipek",Sandia National Laboratories; University of Rochester; Sandia National Laboratories; Sandia National Laboratories; University of Texas at Austin; Sandia National Laboratories; Sandia National Laboratories; Sandia National Laboratories; Qualcomm Inc,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","761","774","Over the past decade as Moore’s Law has slowed, the need for new forms of computation that can provide sustainable performance improvements has risen. A new method, called in situ computing, has shown great potential to accelerate matrix vector multiplication (MVM), an important kernel for a diverse range of applications from neural networks to scientific computing. Existing in situ accelerators for scientific computing, however, have a significant limitation: these accelerators provide no acceleration for preconditioning–a key bottleneck in linear solvers and in scientific computing workflows. This paper enables in situ acceleration for state-of-the-art linear solvers by demonstrating how to use a new in situ matrix inversion accelerator for analog preconditioning. As existing techniques that enable high precision and scalability for in situ MVM are inapplicable to in situ matrix inversion, new techniques to compensate for circuit non-idealities are proposed. Additionally, a new approach to bit slicing that enables splitting operands across multiple devices without external digital logic is proposed. For scalability, this paper demonstrates how in situ matrix inversion kernels can work in tandem with existing domain decomposition techniques to accelerate the solutions of arbitrarily large linear systems. The analog kernel can be directly integrated into existing preconditioning workflows, leveraging several well-optimized numerical linear algebra tools to improve the behavior of the circuit. The result is an analog preconditioner that is more effective (up to 50% fewer iterations) than the widely used incomplete LU factorization preconditioner, ILU(0), while also reducing the energy and execution time of each approximate solve operation by 1025x and 105x respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00069","National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407108","Accelerator architectures;Analog computers;Linear systems","Linear systems;Runtime;Scientific computing;Scalability;Linear algebra;Tools;Acceleration","","19","","40","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"GCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks","J. Li; A. Louri; A. Karanth; R. Bunescu","Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; Department of Electrical and Computer Engineering, George Washington University, Washington, D.C.; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, North Carolina",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","775","788","Graph convolutional neural networks (GCNs) have emerged as an effective approach to extend deep learning for graph data analytics. Given that graphs are usually irregular, as nodes in a graph may have a varying number of neighbors, processing GCNs efficiently pose a significant challenge on the underlying hardware. Although specialized GCN accelerators have been proposed to deliver better performance over generic processors, prior accelerators not only under-utilize the compute engine, but also impose redundant data accesses that reduce throughput and energy efficiency. Therefore, optimizing the overall flow of data between compute engines and memory, i.e., the GCN dataflow, which maximizes utilization and minimizes data movement is crucial for achieving efficient GCN processing.In this paper, we propose a flexible and optimized dataflow for GCNs that simultaneously improves resource utilization and reduces data movement. This is realized by fully exploring the design space of GCN dataflows and evaluating the number of execution cycles and DRAM accesses through an analysis framework. Unlike prior GCN dataflows, which employ rigid loop orders and loop fusion strategies, the proposed dataflow can reconFigure the loop order and loop fusion strategy to adapt to different GCN configurations, which results in much improved efficiency. We then introduce a novel accelerator architecture called GCNAX, which tailors the compute engine, buffer structure and size based on the proposed dataflow. Evaluated on five real-world graph datasets, our simulation results show that GCNAX reduces DRAM accesses by a factor of $8.1 \times$ and $2.4 \times$, while achieving $8.9 \times, 1.6 \times$ speedup and $9.5 \times$, $2.3 \times$ energy savings on average over HyGCN and AWB-GCN, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407104","Graph Convolutional Neural Networks;Dataflow Accelerators;Domain-specific Accelerators","Program processors;Simulation;Random access memory;Accelerator architectures;Throughput;Energy efficiency;Space exploration","","121","","65","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry Track Paper","H. Liao; J. Tu; J. Xia; H. Liu; X. Zhou; H. Yuan; Y. Hu","Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","789","801","Deep neural networks (DNNs) have been successfully applied to a great variety of applications, ranging from small IoT devices to large scale services in a data center. In order to improve the efficiency of processing these DNN models, dedicated hardware accelerators are required for all these scenarios. Theoretically, there exists an optimized acceleration architecture for each application. However, considering the cost of chip design and corresponding tool-chain development, researchers need to trade off between efficiency and generality. In this work, we demonstrate that it is practical to use a unified architecture, called Ascend, to support those applications, ranging from IoT devices to data-center services. We provide a lot of design details to explain that the success of Ascend relies on contributions from different levels. First, heterogeneous computing units are employed to support various DNN models. And the datapath is adapted according to the requirement of computing and data access. Second, when scaling the Ascend architecture from a single core to a cluster containing thousands of cores, it involves design efforts, such as memory hierarchy and system level integration. Third, a multi-tier compiler, which provides flexible choices for developers, is the last critical piece. Experimental results show that using accelerators based on the Ascend architecture can achieve comparable or even better performance in different applications. In addition, various chips based on the Ascend architecture have been successfully commercialized. More than 100 million chips have been used in real products.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407221","","Training;Neural networks;Memory management;Computer architecture;Software;Heterogeneous networks;Hardware","","51","","42","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Understanding Training Efficiency of Deep Learning Recommendation Models at Scale","B. Acun; M. Murphy; X. Wang; J. Nie; C. -J. Wu; K. Hazelwood","Facebook AI Research, Menlo Park, USA; Facebook, Menlo Park, USA; Facebook, Menlo Park, USA; Facebook, Menlo Park, USA; Facebook AI Research, Menlo Park, USA; Facebook AI Research, Menlo Park, USA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","802","814","The use of GPUs has proliferated for machine learning workflows and is now considered mainstream for many deep learning models. Meanwhile, when training state-of-the-art personal recommendation models, which consume the highest number of compute cycles at our large-scale datacenters, the use of GPUs came with various challenges due to having both compute-intensive and memory-intensive components. GPU performance and efficiency of these recommendation models are largely affected by model architecture configurations such as dense and sparse features, MLP dimensions. Furthermore, these models often contain large embedding tables that do not fit into limited GPU memory. The goal of this paper is to explain the intricacies of using GPUs for training recommendation models, factors affecting hardware efficiency at scale, and learnings from a new scale-up GPU server design, Zion.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407135","Recommendation models;GPUs;training efficiency;deep learning","Training;Deep learning;Computational modeling;Memory management;Graphics processing units;Production;Throughput","","54","","60","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management","Y. Zhang; J. Chen; X. Jiang; Q. Liu; I. M. Steiner; A. J. Herdrich; K. Shu; R. Das; L. Cui; L. Jiang","Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Intel Corporation Hangzhou, China; Intel Corporation Hangzhou, China; Intel Corporation Hangzhou, China; Intel Corporation Hangzhou, China; Intel Corporation Hangzhou, China; Intel Corporation Hangzhou, China",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","815","826","Modern Cloud Service Providers (CSP) heavily co-schedule tasks with different priorities on the same computing node to increase server utilization. To ensure the performance of high priority jobs, CSPs usually employ Quality-of-Service (QoS) mechanisms to manage or regulate the usage of shared hardware resources. Among the critical shared hardware resources, there has been very limited analysis on effective sharing of memory bandwidth among co-scheduled jobs, mainly for two reasons: (1) The correlation between application performance and its memory bandwidth allocation is complicated. (2) An effective hardware throttling mechanism for precise memory bandwidth control is unavailable. These limitations drive CSPs to design conservative policies to ensure the performance of the high priority tasks, which significantly degrades the throughput of batch jobs and reduces the overall benefits of workload co-scheduling. This paper proposes LIBRA, a holistic framework for dynamic memory bandwidth management in production data centers. LIBRA incorporates a novel hardware throttling mechanism, Dynamic Resource Control, to support self-adaptive memory bandwidth regulation. It also employs a lightweight control policy to further enhance the bandwidth scalability for the throttled tasks. Our evaluation results on a cluster demonstrate that LIBRA is capable of increasing the performance of batch jobs by up to 52.8% compared to existing QoS schemes.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407101","data center;QoS;memory bandwidth;MBA;DRC","Scalability;Memory management;Bandwidth;Quality of service;Production;Throughput;Hardware","","11","","36","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Eudoxus: Characterizing and Accelerating Localization in Autonomous Machines Industry Track Paper","Y. Gan; Y. Bo; B. Tian; L. Xu; W. Hu; S. Liu; Q. Liu; Y. Zhang; J. Tang; Y. Zhu",University of Rochester; PerceptIn; University of Rochester; PerceptIn; PerceptIn; PerceptIn; Tianjin University; Beijing Institute of Technology; SCUT; University of Rochester,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","827","840","We develop and commercialize autonomous machines, such as logistic robots and self-driving cars, around the globe. A critical challenge to our—and any—autonomous machine is accurate and efficient localization under resource constraints, which has fueled specialized localization accelerators recently. Prior acceleration efforts are point solutions in that they each specialize for a specific localization algorithm. In real-world commercial deployments, however, autonomous machines routinely operate under different environments and no single localization algorithm fits all the environments. Simply stacking together point solutions not only leads to cost and power budget overrun, but also results in an overly complicated software stack. This paper demonstrates our new software-hardware co-designed framework for autonomous machine localization, which adapts to different operating scenarios by fusing fundamental algorithmic primitives. Through characterizing the software framework, we identify ideal acceleration candidates that contribute significantly to the end-to-end latency and/or latency variation. We show how to co-design a hardware accelerator to systematically exploit the parallelisms, locality, and common building blocks inherent in the localization framework. We build, deploy, and evaluate an FPGA prototype on our next-generation self-driving cars. To demonstrate the flexibility of our framework, we also instantiate another FPGA prototype targeting drones, which represent mobile autonomous machines. We achieve about $2 \times$ speedup and $4 \times$ energy reduction compared to widely-deployed, optimized implementations on general-purpose platforms.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407103","","Location awareness;Target tracking;Software algorithms;Stacking;Prototypes;Software;Autonomous automobiles","","22","","89","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"NeuroMeter: An Integrated Power, Area, and Timing Modeling Framework for Machine Learning Accelerators Industry Track Paper","T. Tang; S. Li; L. Nai; N. Jouppi; Y. Xie","UC Santa Barbara, Santa Barbara, CA, US; Google, Mountain View, CA, US; Google, Mountain View, CA, US; Google, Mountain View, CA, US; UC Santa Barbara, Santa Barbara, CA, US",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","841","853","As Machine Learning (ML) becomes pervasive in the era of artificial intelligence, ML specific tools and frameworks are required for architectural research. This paper introduces NeuroMeter, an integrated power, area, and timing modeling framework for ML accelerators. NeuroMeter models the detailed architecture of ML accelerators and generates a fast and accurate estimation on power, area, and chip timing. Meanwhile, it also enables the runtime analysis of system-level performance and efficiency when the runtime activity factors are provided. NeuroMeter’s micro-architecture model includes fundamental components of ML accelerators, including systolic array based tensor units (TU), reduction trees (RT), and 1D vector units (VU). NeuroMeter has accurate modeling results, with the average power and area estimation errors below 10% and 17% respectively when validated against TPU-v1, TPU-v2, and Eyeriss.Leveraging the NeuroMeter’s new capabilities on architecting manycore ML accelerators, this paper presents the first in-depth study on the design space and tradeoffs of “Brawny and Wimpy” inference accelerators in datacenter scenarios with the insights that are otherwise difficult to discover without NeuroMeter. Our study shows that brawny designs with 64x64 systolic arrays are the most performant and efficient for inference tasks in the 28nm datacenter architectural space with a 500mm2 die area budget. Our study also reveals important tradeoffs between performance and efficiency. For datacenter accelerators with low batch inference, a small $(\sim 16$%) sacrifice of system performance (in achieved Tera OPerations per Second, aka TOPS) can lead to more than a 2x efficiency improvement (in achieved TOPS/TCO). To showcase NeuroMeter’s capability to model a wide range of diverse ML accelerator architectures, we also conduct a followon mini-case study on implications of sparsity on different ML accelerators, demonstrating wimpier accelerator architectures benefit more readily from sparsity processing despite their lower achievable raw energy efficiency.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407039","accelerator;hardware modeling;deep learning","Analytical models;Runtime;Accelerator architectures;Tools;Energy efficiency;Timing;Task analysis","","9","","58","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Chasing Carbon: The Elusive Environmental Footprint of Computing","U. Gupta; Y. G. Kim; S. Lee; J. Tse; H. -H. S. Lee; G. -Y. Wei; D. Brooks; C. -J. Wu",Harvard University; Arizona State University; Facebook Inc; Facebook Inc; Facebook Inc; Harvard University; Harvard University; Facebook Inc,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","854","867","Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407142","Data center;mobile;energy;carbon footprint","Technological innovation;Energy consumption;Software algorithms;Carbon dioxide;Computer architecture;Hardware;Software","","117","","119","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator","O. Villa; D. Lustig; Z. Yan; E. Bolotin; Y. Fu; N. Chatterjee; N. Jiang; D. Nellans",NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","868","880","The demands of high-performance computing (HPC) and machine learning (ML) workloads have resulted in the rapid architectural evolution of GPUs over the last decade. The growing memory footprint and diversity of data types in these workloads has required GPUs to embrace micro-architectural heterogeneity and increased memory system sophistication to scale performance. Effective simulation of new architectural features early in the design cycle enables quick and effective exploration of design trade-offs across this increasingly diverse set of workloads. This work provides a retrospective on the design and development of NVArchSim (NVAS), an architectural simulator used within NVIDIA to design and evaluate features that are difficult to appraise using other methodologies due to workload type, size, complexity, or lack of modeling flexibility. We argue that overly precise and/or overly slow architectural models hamper an architect’s ability to evaluate new features within a reasonable time frame, hurting productivity. Because of its speed, NVAS is being used to trace and evaluate hundreds of HPC and state-of-the-art ML workloads on single-GPU or multi-GPU systems. By adding component fidelity only when necessary to improve system-level modeling accuracy, NVAS delivers simulation speed orders of magnitude higher than most publicly available GPU simulators while retaining high levels of accuracy and simulation flexibility. Building trustworthy high-level simulation platforms is a difficult exercise in balance and compromise; we share our experiences to help and encourage those in academia who take on the challenge of building GPU simulation platforms.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407154","GPU;Simulation;Performance Modeling;System Simulation","Productivity;Computational modeling;Buildings;Memory management;Graphics processing units;Machine learning;Tools","","28","","63","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"Operating Liquid-Cooled Large-Scale Systems: Long-Term Monitoring, Reliability Analysis, and Efficiency Measures","R. B. Roy; T. Patel; R. Kettimuthu; W. Allcock; P. Rich; A. Scovel; D. Tiwari",Northeastern University; Northeastern University; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; Northeastern University,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","881","893","The past decade has seen a rise in the use of liquid cooling due to its energy efficiency. While many previous works have helped make progress toward improving data center cooling, a vast majority of them perform studies on a small system over a short span. The computer systems and HPC community lacks a long-term study highlighting the challenges and solutions in operating a liquid-cooled large-scale data center. We conduct the first detailed characterization of a petascale supercomputer, Mira, over a span of six years. The study is enabled by systematic monitoring of the environmental metrics, and discusses new research avenues, including coolant monitor failures.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407082","","Measurement;Data centers;Production systems;Systematics;Supercomputers;Reliability;Monitoring","","1","","77","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"FuseKNA: Fused Kernel Convolution based Accelerator for Deep Neural Networks","J. Yang; Z. Zhang; Z. Liu; J. Zhou; L. Liu; S. Wei; S. Yin","Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Beijing Innovation Center for Future Chips, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","894","907","Bit-serial computation has been a prevailing convolution method to accelerate varying-precision DNNs by slicing a multi-bit data into multiple 1-bit data and transforming a multiplication into multiple additions, where additions of zero bits are ineffectual, while additions of non-zero bits are repetitive since multiple kernels are quite possible to possess non-zero bits at the same kernel positions. Previous bit-serial accelerators only remove ineffectual additions by skipping computation of zero bits, however, repetitive additions are unable to be eliminated since they compute convolution of each kernel independently. In this work, we propose fused kernel convolution algorithm to eliminate both ineffectual and repetitive additions in bit-serial computation by exploiting bit repetition and bit sparsity in weights, for both convolutional and fully-connected layers. It unifies convolutions of multiple kernels into convolution of one fused kernel by firstly grouping additions into different patterns and secondly reconstructing convolution results, minimizing addition count. Meantime, the memory accesses of activations and partial sums are decreased due to less convolution count. Then a fused kernel convolution based accelerator, FuseKNA, is designed with compact compute logic, which fully exploits value sparsity of activations and bit sparsity of weights. Benchmarked with a set of mainstream DNNs, FuseKNA improves performance by $4.47 \times$, $2.31 \times$ and $1.81 \times$, energy efficiency by $4.13 \times$, $3.06 \times$ and $2.53 \times$ over state-of-the-art Stripes, Pragmatic and Bit-Tactical.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407121","DNN acceleration;bit-serial computation;sparsity;redundant computation","Convolution;Neural networks;Memory management;Benchmark testing;Energy efficiency;Computational efficiency;Acceleration","","14","","40","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction","B. Asgari; R. Hadidi; J. Cao; D. E. Shim; S. -K. Lim; H. Kim","Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","908","920","Memory-bound sparse gathering, caused by irregular random memory accesses, has become an obstacle in several on-demand applications such as embedding lookup in recommendation systems. To reduce the amount of data movement, and thereby better utilize memory bandwidth, previous studies have proposed near-data processing (NDP) solutions. The issue of prior work, however, is that they either minimize data movement effectively at the cost of limited memory parallelism or try to improve memory parallelism (up to a certain degree) but cannot successfully decrease data movement, as prior proposals rely on spatial locality (an optimistic assumption) to utilize NDP. More importantly, neither approach proposes a solution for gathering data from random memory addresses; rather they just offload operations to NDP. We propose an effective solution for sparse gathering, an efficient near-memory intelligent reduction (Fafnir) tree, the leaves of which are all the ranks in a memory system, and the nodes gradually apply reduction operations while data is gathered from any rank. By using such an overall tree, Fafnir does not rely on spatial locality; therefore, it minimizes data movement by performing entire operations at NDP and fully benefits from parallel memory accesses in parallel processing at NDP. Further, Fafnir offers other advantages such as using fewer connections (because of the tree topology), eliminating redundant memory accesses without using costly and less effective caching mechanisms, and being applicable to other domains of sparse problems such as scientific computing and graph analytics. To evaluate Fafnir, we implement it on an XCVU9P Xilinx FPGA and in 7 nm ASAP ASIC. Fafnir looks up the embedding tables up to 21.3× more quickly than the state-of-the-art NDP proposal. Furthermore, the generic architecture of Fafnir allows running classic sparse problems using the same 1.2 mm2 hardware up to 4.6× more quickly than the state of the art.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00080","Booz Allen Hamilton; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407100","Recommendation Systems;Embedding Lookup;Sparse Gathering;Near Data Processing","Scientific computing;Memory management;Bandwidth;Parallel processing;Hardware;Topology;Proposals","","74","","42","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
"VIA: A Smart Scratchpad for Vector Units with Application to Sparse Matrix Computations","J. Pavon; I. V. Valdivieso; A. Barredo; J. Marimon; M. Moreto; F. Moll; O. Unsal; M. Valero; A. Cristal","Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Universitat Politecnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center",2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"22 Apr 2021","2021","","","921","934","Sparse matrix operations are critical kernels in multiple application domains such as High Performance Computing, artificial intelligence and big data. Vector processing is widely used to improve performance on mathematical kernels with dense matrices. Unfortunately, existing vector architectures do not cope well with sparse matrix computations, achieving much lower performance in comparison with their dense counterparts.To overcome this limitation, we present the Vector Indexed Architecture (VIA), a novel hardware vector architecture that accelerates applications with irregular memory access patterns such as sparse matrix computations. There are two main bottlenecks when computing with sparse matrices: irregular memory accesses and index matching. VIA addresses these two bottlenecks with a smart scratchpad that is tightly coupled to the Vector Functional Units within the core.Thanks to this structure, VIA improves locality for sparse-dense computations and improves the index matching search process for sparse computations. As a result, VIA achieves significant performance speedup over highly optimized state-of-the-art C++ algebra libraries. On average, VIA outperforms sparse matrix vector multiplication, sparse matrix addition and sparse matrix matrix multiplication kernels by 4.22 ×, 6.14 × and 6.00 ×, respectively, when evaluated over a thousand sparse matrices that arise in real applications. In addition, we prove the generality of VIA by showing that it can accelerate histogram and stencil applications by 4.5 × and 3.5 ×, respectively.","2378-203X","978-1-6654-2235-2","10.1109/HPCA51647.2021.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407226","Sparse Algebra;Vector Computing;Scratchpad Memory","Histograms;High performance computing;Memory management;Linear algebra;Hardware;Libraries;Sparse matrices","","16","","72","IEEE","22 Apr 2021","","","IEEE","IEEE Conferences"
