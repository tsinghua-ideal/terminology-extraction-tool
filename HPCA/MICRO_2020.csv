"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Graphene: Strong yet Lightweight Row Hammer Protection","Y. Park; W. Kwon; E. Lee; T. J. Ham; J. Ho Ahn; J. W. Lee",Seoul National University; Seoul National University; Seoul National University; Seoul National University; Seoul National University; Seoul National University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1","13","Row Hammer is a serious security threat to modern computing systems using DRAM as main memory. It causes charge loss in DRAM cells adjacent to a frequently activated aggressor row and eventually leads to data bit flips in those cells. Even with countermeasures from hardware vendors for years, many latest DDR4 DRAM-based systems are still vulnerable to Row Hammer. Furthermore, technology scaling continues to reduce the Row Hammer threshold, hence posing even greater challenges than before. Although many architectural solutions for Row Hammer have been proposed in both industry and academia, they still incur substantial overhead in terms of chip area, energy, and performance, fail to provide a sufficient level of protection, or both. Thus, we propose Graphene, a low-cost Row Hammer prevention technique based on a space-efficient algorithm that identifies frequent elements from an incoming data stream. Graphene is provably secure without false negatives and with tightly bounded false positives. Furthermore, Graphene has an order of magnitude smaller area overhead compared to a state-of-the-art counter-based scheme. This makes Graphene a scalable solution to Row Hammer attacks for the memory systems of today and the future. Our evaluation shows that Graphene features nearly zero performance and energy overhead when running realistic workloads. Even for the most adversarial memory access patterns, Graphene increases refresh energy only by 0.34%.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251863","","Industries;Microarchitecture;Graphene;Random access memory;Hardware;Security","","73","","56","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Persist Level Parallelism: Streamlining Integrity Tree Updates for Secure Persistent Memory","A. Freij; S. Yuan; H. Zhou; Y. Solihin","Dept. of Electrical & Computer Engineering, North Carolina State University; Dept. of Electrical & Computer Engineering, North Carolina State University; Dept. of Electrical & Computer Engineering, North Carolina State University; Dept. of Computer Science, University of Central Florida",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","14","27","Emerging non-volatile main memory (NVMM) is rapidly being integrated into computer systems. However, NVMM is vulnerable to potential data remanence and replay attacks. Memory encryption and integrity verification have been introduced to protect against such data integrity attacks. However, they are not compatible with a growing use of NVMM for providing crash recoverable persistent memory. Recent works on secure NVMM pointed out the need for data and its metadata, including the counter, the message authentication code (MAC), and the Bonsai Merkle Tree (BMT) to be persisted atomically. However, memory persistency models have been overlooked for secure NVMM, which is essential for crash recoverability.In this work, we analyze the invariants that need to be ensured in order to support crash recovery for secure NVMM. We highlight that by not adhering to these invariants, prior research has substantially under-estimated the cost of BMT persistence. We propose several optimization techniques to reduce the overhead of atomically persisting updates to BMTs. The optimizations proposed explore the use of pipelining, out-of-order updates, and update coalescing while conforming to strict or epoch persistency models, respectively. We evaluate our work and show that our proposed optimizations significantly reduce the performance overhead of secure crash-recoverable NVMM from 720% to just 20%.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251985","persistency;security;integrity tree update;persist-level parallelism","Out of order;Nonvolatile memory;Computational modeling;Metadata;Computer crashes;Encryption;Optimization","","20","","68","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"PThammer: Cross-User-Kernel-Boundary Rowhammer through Implicit Accesses","Z. Zhang; Y. Cheng; D. Liu; S. Nepal; Z. Wang; Y. Yarom","University of New South Wales, Australia; Baidu Security; Data61, CSIRO, Australia; Data61, CSIRO, Australia; Florida State University, America; Data61, CSIRO, Australia",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","28","41","Rowhammer is a hardware vulnerability in DRAM memory, where repeated access to memory can induce bit flips in neighboring memory locations. Being a hardware vulnerability, rowhammer bypasses all of the system memory protection, allowing adversaries to compromise the integrity and confidentiality of data. Rowhammer attacks have shown to enable privilege escalation, sandbox escape, and cryptographic key disclosures.Recently, several proposals suggest exploiting the spatial proximity between the accessed memory location and the location of the bit flip for a defense against rowhammer. These all aim to deny the attacker's permission to access memory locations near sensitive data.In this paper, we question the core assumption underlying these defenses. We present PThammer, a confused-deputy attack that causes accesses to memory locations that the attacker is not allowed to access. Specifically, PThammer exploits the address translation process of modern processors, inducing the processor to generate frequent accesses to protected memory locations. We implement PThammer, demonstrating that it is a viable attack, resulting in a system compromise (e.g., kernel privilege escalation). We further evaluate the effectiveness of proposed software-only defenses showing that PThammer can overcome those.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00016","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251982","Rowhammer;Confused-deputy Attack;Address Translation;Privilege Escalation","Program processors;Microarchitecture;Random access memory;Hardware;Proposals;Cryptography;Kernel","","47","","60","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Draco: Architectural and Operating System Support for System Call Security","D. Skarlatos; Q. Chen; J. Chen; T. Xu; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","42","57","System call checking is extensively used to protect the operating system kernel from user attacks. However, existing solutions such as Seccomp execute lengthy rule-based checking programs against system calls and their arguments, leading to substantial execution overhead.To minimize checking overhead, this paper proposes Draco, a new architecture that caches system call IDs and argument values after they have been checked and validated. System calls are first looked-up in a special cache and, on a hit, skip all checks. We present both a software and a hardware implementation of Draco. The latter introduces a System Call Lookaside Buffer (SLB) to keep recently-validated system calls, and a System Call Target Buffer to preload the SLB in advance. In our evaluation, we find that the average execution time of macro and micro benchmarks with conventional Seccomp checking is 1.14× and 1.25× higher, respectively, than on an insecure baseline that performs no security checks. With our software Draco, the average execution time reduces to 1.10× and 1.18× higher, respectively, than on the insecure baseline. With our hardware Draco, the execution time is within 1% of the insecure baseline.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251949","System call checking;Security;Operating system;Containers;Virtualization;Microarchitecture","Microarchitecture;Computer architecture;Benchmark testing;Software;Hardware;Security;Kernel","","10","","115","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"SuperNPU: An Extremely Fast Neural Processing Unit Using Superconducting Logic Devices","K. Ishida; I. Byun; I. Nagaoka; K. Fukumitsu; M. Tanaka; S. Kawakami; T. Tanimoto; T. Ono; J. Kim; K. Inoue","Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University; Department of Electrical and Computer Engineering, Seoul National University; Department of Electronics, Nagoya University; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University; Department of Electronics, Nagoya University; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University; Department of Electrical and Computer Engineering, Seoul National University; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","58","72","Superconductor single-flux-quantum (SFQ) logic family has been recognized as a highly promising solution for the post-Moore's era, thanks to its ultra-fast and low-power switching characteristics. Therefore, researchers have made a tremendous amount of effort in various aspects to promote the technology and automate its circuit design process (e.g., low-cost fabrication, design tool development). However, there has been no progress in designing a convincing SFQ-based architectural unit due to the architects' lack of understanding of the technology's potentials and limitations at the architecture level. In this paper, we present how to architect an SFQ-based architectural unit by providing design principles with an extreme-performance neural processing unit (NPU). To achieve the goal, we first implement an architecture-level simulator to model an SFQ-based NPU accurately. We validate this model using our die-level prototypes, design tools, and logic cell library. This simulator accurately measures the NPU's performance, power consumption, area, and cooling overheads. Next, driven by the modeling, we identify key architectural challenges for designing a performance-effective SFQ-based NPU (e.g., expensive on-chip data movements and buffering). Lastly, we present SuperNPU, our example SFQ-based NPU architecture, which effectively resolves the challenges. Our evaluation shows that the proposed design outperforms a conventional state-of-the-art NPU by 23 times. With free cooling provided as done in quantum computing, the performance per chip power increases up to 490 times. Our methodology can also be applied to other architecture designs with SFQ-friendly characteristics.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00018","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251979","Single flux quantum (SFQ);Cryogenic computing;Modeling;Simulation","Superconducting devices;Quantum computing;Superconducting logic circuits;Cooling;Computer architecture;Tools;Design tools","","47","","71","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Printed Machine Learning Classifiers","M. H. Mubarik; D. D. Weller; N. Bleier; M. Tomei; J. Aghassi-Hagmann; M. B. Tahoori; R. Kumar",University of Illinois Urbana-Champaign; Karlsruhe Institute of Technology; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Applied Sciences Offenburg; Karlsruhe Institute of Technology; University of Illinois Urbana-Champaign,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","73","87","A large number of application domains have requirements on cost, conformity, and non-toxicity that silicon-based computing systems cannot meet, but that may be met by printed electronics. For several of these domains, a typical computational task to be performed is classification. In this work, we explore the hardware cost of inference engines for popular classification algorithms (Multi-Layer Perceptrons, Support Vector Machines (SVMs), Logistic Regression, Random Forests and Binary Decision Trees) in EGT and CNT-TFT printed technologies and determine that Decision Trees and SVMs provide a good balance between accuracy and cost. We evaluate conventional Decision Tree and SVM architectures in these technologies and conclude that their area and power overhead must be reduced. We explore, through SPICE and gate-level hardware simulations and multiple working prototypes, several classifier architectures that exploit the unique cost and implementation tradeoffs in printed technologies - a) Bespoke printed classifers that are customized to a model generated for a given application using specific training datasets, b) Lookup-based printed classifiers where key hardware computations are replaced by lookup tables, and c) Analog printed classifiers where some classifier components are replaced by their analog equivalents. Our evaluations show that bespoke implementation of EGT printed Decision Trees has 48.9× lower area (average) and 75.6× lower power (average) than their conventional equivalents; corresponding benefits for bespoke SVMs are 12.8× and Decision outperform 12.7× respectively. Lookup-based Trees their non-lookup bespoke equivalents by 38% and 70%; lookup-based SVMs are better by 8% and 0.6%. Analog printed Decision Trees provide 437× area and 27× power benefits over digital bespoke counterparts; analog SVMs yield 490× area and 12× power improvements. Our results and prototypes demonstrate feasibility of fabricating and deploying battery and self-powered printed classifiers in the application domains of interest.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251954","printed electronics;machine learning","Support vector machines;Prototypes;Computer architecture;SPICE;Hardware;Decision trees;Task analysis","","30","","80","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Look-Up Table based Energy Efficient Processing in Cache Support for Neural Network Acceleration","A. K. Ramanathan; G. S. Kalsi; S. Srinivasa; T. M. Chandran; K. R. Pillai; O. J. Omer; V. Narayanan; S. Subramoney","The Pennsylvania State University; Processor Architecture Research Lab, Intel labs; The Pennsylvania State University; The Pennsylvania State University; Processor Architecture Research Lab, Intel labs; Processor Architecture Research Lab, Intel labs; The Pennsylvania State University; Processor Architecture Research Lab, Intel labs",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","88","101","This paper presents a Look-Up Table (LUT) based Processing-In-Memory (PIM) technique with the potential for running Neural Network inference tasks. We implement a bitline computing free technique to avoid frequent bitline accesses to the cache sub-arrays and thereby considerably reducing the memory access energy overhead. LUT in conjunction with the compute engines enables sub-array level parallelism while executing complex operations through data lookup which otherwise requires multiple cycles. Sub-array level parallelism and systolic input data flow ensure data movement to be confined to the SRAM slice.Our proposed LUT based PIM methodology exploits substantial parallelism using look-up tables, which does not alter the memory structure/organization, that is, preserving the bit-cell and peripherals of the existing SRAM monolithic arrays. Our solution achieves 1.72× higher performance and 3.14x lower energy as compared to a state-of-the-art processing-in-cache solution. Sub-array level design modifications to incorporate LUT along with the compute engines will increase the overall cache area by 5.6%. We achieve 3.97x speedup w.r.t neural network systolic accelerator with a similar area. The re-configurable nature of the compute engines enables various neural network operations and thereby supporting sequential networks (RNNs) and transformer models. Our quantitative analysis demonstrates 101×, 3× faster execution and 91×, 11× energy efficient than CPU and GPU respectively while running the transformer model, BERT-Base.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251975","Processing-in-memory;SRAM;Look-up table;Neural networks","Computational modeling;Neural networks;Parallel processing;Energy efficiency;Table lookup;Task analysis;Engines","","29","","58","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"FReaC Cache: Folded-logic Reconfigurable Computing in the Last Level Cache","A. Dhar; X. Wang; H. Franke; J. Xiong; J. Huang; W. -m. Hwu; N. S. Kim; D. Chen","University of Illinois, Urbana, Champaign; University of Illinois, Urbana, Champaign; IBM Research; IBM Research; University of Illinois, Urbana, Champaign; University of Illinois, Urbana, Champaign; University of Illinois, Urbana, Champaign; University of Illinois, Urbana, Champaign",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","102","117","The need for higher energy efficiency has resulted in the proliferation of accelerators across platforms, with custom and reconfigurable accelerators adopted in both edge devices and cloud servers. However, existing solutions fall short in providing accelerators with low-latency, high-bandwidth access to the working set and suffer from the high latency and energy cost of data transfers. Such costs can severely limit the smallest granularity of the tasks that can be accelerated and thus the applicability of the accelerators. In this work, we present FReaC Cache, a novel architecture that natively supports reconfigurable computing in the last level cache (LLC), thereby giving energy-efficient accelerators low-latency, high-bandwidth access to the working set. By leveraging the cache's existing dense memory arrays, buses, and logic folding, we construct a reconfigurable fabric in the LLC with minimal changes to the system, processor, cache, and memory architecture. FReaC Cache is a low-latency, low-cost, and low-power alternative to off-die/offchip accelerators, and a flexible, and low-cost alternative to fixed function accelerators. We demonstrate an average speedup of 3X and Perf/W improvements of 6.1X over an edge-class multi-core CPU, and add 3.5% to 15.3% area overhead per cache slice.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00021","Xilinx; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251953","Reconfigurable Computing;Near Memory Acceleration;In Cache Computing;Logic Folding","Performance evaluation;Microarchitecture;Multicore processing;Energy efficiency;Acceleration;Servers;Task analysis","","4","","73","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"BranchNet: A Convolutional Neural Network to Predict Hard-To-Predict Branches","S. Zangeneh; S. Pruett; S. Lym; Y. N. Patt",University of Texas at Austin; University of Texas at Austin; Nvidia; University of Texas at Austin,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","118","130","The state-of-the-art branch predictor, TAGE, remains inefficient at identifying correlated branches deep in a noisy global branch history. We argue this inefficiency is a fundamental limitation of runtime branch prediction and not a coincidental artifact due to the design of TAGE. To further improve branch prediction, we need to relax the constraint of runtime only training and adopt more sophisticated prediction mechanisms. To this end, Tarsa et al. proposed using convolutional neural networks (CNNs) that are trained at compile-time to accurately predict branches that TAGE cannot. Given enough profiling coverage, CNNs learn input-independent branch correlations that can accurately predict branches when running a program with unseen inputs. We build on their work and introduce BranchNet, a CNN with a practical on-chip inference engine tailored to the needs of branch prediction. At runtime, BranchNet predicts a few hard-to-predict branches, while TAGE-SC-L predicts the remaining branches. This hybrid approach reduces the MPKI of SPEC2017 Integer benchmarks by 7.6% (and up to 15.7%) when compared to a very large (impractical) MTAGE-SC baseline, demonstrating a fundamental advantage in the prediction capabilities of BranchNet compared to TAGE-like predictors. We also propose a practical resource-constrained variant of BranchNet that improves the MPKI by 9.6% (and up to 17.7%) compared to a 64KB TAGE-SC-L without increasing the prediction latency.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251928","","Training;Deep learning;Runtime;Correlation;History;Convolutional neural networks;Noise measurement","","36","","55","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"CHiRP: Control-Flow History Reuse Prediction","S. Mirbagher-Ajorpaz; E. Garza; G. Pokam; D. A. Jiménez","Computer Science and Engineering, Texas A&M University, College Station, USA; Computer Science and Engineering, Texas A&M University, College Station, USA; Intel Labs, Santa Clara, USA; Computer Science and Engineering, Texas A&M University, College Station, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","131","145","Translation Lookaside Buffers (TLBs) play a critical role in hardware-supported memory virtualization. To speed up address translation and reduce costly page table walks, TLBs cache a small number of recently-used virtual-to-physical address translations. TLBs must make the best use of their limited capacities. Thus, TLB entries with low potential for reuse should be replaced by more useful entries. This paper contributes to an aspect of TLB management that has received little attention in the literature: replacement policy. We show how predictive replacement policies can be tailored toward TLBs to reduce miss rates and improve overall performance. We begin by applying recently proposed predictive cache replacement policies to the TLB. We show these policies do not work well without considering specific TLB behavior. Next, we introduce a novel TLB-focused predictive policy, Control-flow History Reuse Prediction (CHIRP). This policy uses a history signature and replacement algorithm that correlates to known TLB behavior, outperforming other policies. For a 1024-entry 8-way set-associative L2 TLB with a 4KB page size, we show that CHiRP reduces misses per 1000 instructions (MPKI) by an average 28.21% over the least-recently-used (LRU) policy, outperforming Static Re-reference Interval Prediction (SRRIP) [1], Global History Reuse Policy (GHRP) [2] and SHiP [3], which reduce MPKI by an average of 10.36%, 9.03% and 0.88%, respectively.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251943","Translation Lookaside Buffers;Replacement Policies;Paging;Microarchitectures","Microarchitecture;Chirp;Prediction algorithms;History;Virtualization;Marine vehicles","","9","","110","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"I-SPY: Context-Driven Conditional Instruction Prefetching with Coalescing","T. A. Khan; A. Sriraman; J. Devietti; G. Pokam; H. Litz; B. Kasikci","University of Michigan, Santa Cruz; University of Michigan, Santa Cruz; University of Pennsylvania, Santa Cruz; Intel Corporation, Santa Cruz; University of California, Santa Cruz; University of Michigan, Santa Cruz",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","146","159","Modern data center applications have rapidly expanding instruction footprints that lead to frequent instruction cache misses, increasing cost and degrading data center performance and energy efficiency. Mitigating instruction cache misses is challenging since existing techniques (1) require significant hardware modifications, (2) expect impractical on-chip storage, or (3) prefetch instructions based on inaccurate understanding of program miss behavior. To overcome these limitations, we first investigate the challenges of effective instruction prefetching. We then use insights derived from our investigation to develop I-SPY, a novel profile-driven prefetching technique. I-SPY uses dynamic miss profiles to drive an offline analysis of I-cache miss behavior, which it uses to inform prefetching decisions. Two key techniques underlie I-SPY's design: (1) conditional prefetching, which only prefetches instructions if the program context is known to lead to misses, and (2) prefetch coalescing, which merges multiple prefetches of non-contiguous cache lines into a single prefetch instruction. I-SPY exposes these techniques via a family of light-weight hardware code prefetch instructions. We study I-SPY in the context of nine data center applications and show that it provides an average of 15.5% (up to 45.9%) speedup and 95.9% (and up to 98.4%) reduction in instruction cache misses, outperforming the state-of-the-art prefetching technique by 22.5%. We show that I-SPY achieves performance improvements that are on average 90.5% of the performance of an ideal cache with no misses.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00024","Intel Corporation; Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251862","Prefetching;frontend stalls;memory systems","Data centers;Microarchitecture;Prefetching;Hardware;Energy efficiency;System-on-chip","","22","","66","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Improving the Utilization of Micro-operation Caches in x86 Processors","J. B. Kotra; J. Kalamatianos","AMD Research, USA; AMD Research, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","160","172","Most modern processors employ variable length, Complex Instruction Set Computing (CISC) instructions to reduce instruction fetch energy cost and bandwidth requirements. High throughput decoding of CISC instructions requires energy hungry logic for instruction identification. Efficient CISC instruction execution motivated mapping them to fixed length micro-operations (also known as uops). To reduce costly decoder activity, commercial CISC processors employ a micro-operations cache (uop cache) that caches uop sequences, bypassing the decoder. Uop cache's benefits are: (1) shorter pipeline length for uops dispatched by the uop cache, (2) lower decoder energy consumption, and, (3) earlier detection of mispredicted branches.In this paper, we observe that a uop cache can be heavily fragmented under certain uop cache entry construction rules. Based on this observation, we propose two complementary optimizations to address fragmentation: Cache Line boundary AgnoStic uoP cache design (CLASP) and uop cache compaction. CLASP addresses the internal fragmentation caused by short, sequential uop sequences, terminated at the I-cache line boundary, by fusing them into a single uop cache entry. Compaction further lowers fragmentation by placing to the same uop cache entry temporally correlated, non-sequential uop sequences mapped to the same uop cache set. Our experiments on a x86 simulator using a wide variety of benchmarks show that CLASP improves performance up to 5.6% and lowers decoder power up to 19.63%. When CLASP is coupled with the most aggressive compaction variant, performance improves by up to 12.8% and decoder power savings are up to 31.53%.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251990","Micro-operations Cache;CPU front-end;CISC;X86;Micro-ops","Power demand;Pipelines;Bandwidth;Throughput;Decoding;Compaction;Optimization","","7","1","55","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Virtualized Logical Qubits: A 2.5D Architecture for Error-Corrected Quantum Computing","C. Duckering; J. M. Baker; D. I. Schuster; F. T. Chong",University of Chicago; University of Chicago; University of Chicago; University of Chicago,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","173","185","Current, near-term quantum devices have shown great progress in the last several years culminating recently with a demonstration of quantum supremacy. In the medium-term, however, quantum machines will need to transition to greater reliability through error correction, likely through promising techniques like surface codes which are well suited for near-term devices with limited qubit connectivity. We discover quantum memory, particularly resonant cavities with transmon qubits arranged in a 2.5D architecture, can efficiently implement surface codes with substantial hardware savings and performance/fidelity gains. Specifically, we virtualize logical qubits by storing them in layers of qubit memories connected to each transmon.Surprisingly, distributing each logical qubit across many memories has a minimal impact on fault tolerance and results in substantially more efficient operations. Our design permits fast transversal application of CNOT operations between logical qubits sharing the same physical address (same set of cavities) which are 6x faster than standard lattice surgery CNOTs. We develop a novel embedding which saves approximately 10x in transmons with another 2x savings from an additional optimization for compactness.Although qubit virtualization pays a 10x penalty in serialization, advantages in the transversal CNOT and in area efficiency result in fault-tolerance and performance comparable to conventional 2D transmon-only architectures. Our simulations show our system can achieve fault tolerance comparable to conventional two-dimensional grids while saving substantial hardware. Furthermore, our architecture can produce magic states at 1.22x the baseline rate given a fixed number of transmon qubits. This is a critical benchmark for future fault-tolerant quantum computers as magic states are essential and machines will spend the majority of their resources continuously producing them. This architecture substantially reduces the hardware requirements for fault-tolerant quantum computing and puts within reach a proof-of-concept experimental demonstration of around 10 logical qubits, requiring only 11 transmons and 9 attached cavities in total.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251988","quantum computing;quantum error correction;quantum memory","Fault tolerance;Quantum computing;Protocols;Qubit;Fault tolerant systems;Memory management;Wires;Hardware;Error correction;Error correction codes","","11","","23","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Optimized Quantum Compilation for Near-Term Algorithms with OpenPulse","P. Gokhale; A. Javadi-Abhari; N. Earnest; Y. Shi; F. T. Chong",University of Chicago; IBM; IBM; University of Chicago; University of Chicago,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","186","200","Quantum computers are traditionally operated by programmers at the granularity of a gate-based instruction set. However, the actual device-level control of a quantum computer is performed via analog pulses. We introduce a compiler that exploits direct control at this microarchitectural level to achieve significant improvements for quantum programs. Unlike quantum optimal control, our approach is bootstrapped from existing gate calibrations and the resulting pulses are simple. Our techniques are applicable to any quantum computer and realizable on current devices. We validate our techniques with millions of experimental shots on IBM quantum computers, controlled via the OpenPulse control interface. For representative benchmarks, our pulse control techniques achieve both 1.6x lower error rates and 2x faster execution time, relative to standard gate-based compilation. These improvements are critical in the near-term era of quantum computing, which is bottlenecked by error rates and qubit lifetimes.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251970","quantum computing;pulse control;OpenPulse","Computers;Quantum computing;Microarchitecture;Error analysis;Optimal control;Logic gates;Hardware","","55","","103","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Systematic Crosstalk Mitigation for Superconducting Qubits via Frequency-Aware Compilation","Y. Ding; P. Gokhale; S. F. Lin; R. Rines; T. Propson; F. T. Chong","Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","201","214","One of the key challenges in current Noisy Intermediate-Scale Quantum (NISQ) computers is to control a quantum system with high-fidelity quantum gates. There are many reasons a quantum gate can go wrong - for superconducting transmon qubits in particular, one major source of gate error is the unwanted crosstalk between neighboring qubits due to a phenomenon called frequency crowding. We motivate a systematic approach for understanding and mitigating the crosstalk noise when executing near-term quantum programs on superconducting NISQ computers. We present a general software solution to alleviate frequency crowding by systematically tuning qubit frequencies according to input programs, trading parallelism for higher gate fidelity when necessary. The net result is that our work dramatically improves the crosstalk resilience of tunable-qubit, fixed-coupler hardware, matching or surpassing other more complex architectural designs such as tunable-coupler systems. On NISQ benchmarks, we improve worst-case program success rate by 13.3x on average, compared to existing traditional serialization strategies.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251858","quantum computing;error mitigation;compiler optimization;superconducting qubit","Systematics;Qubit;Crosstalk;Computer architecture;Logic gates;Software;Tuning","","48","1","53","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Circuit Compilation Methodologies for Quantum Approximate Optimization Algorithm","M. Alam; A. Ash-Saki; S. Ghosh","Pennsylvania State University University Park, USA; Pennsylvania State University University Park, USA; Pennsylvania State University University Park, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","215","228","The quantum approximate optimization algorithm (QAOA) is a promising quantum-classical hybrid algorithm to solve hard combinatorial optimization problems. The multi-qubit CPHASE gates used in the quantum circuit for QAOA are commutative i.e., the order of the gates can be altered without changing the output state. This re-ordering leads to the execution of more gates in parallel and a smaller number of additional SWAP gates to compile the QAOA-circuit. Consequently, the circuit-depth and cumulative gate-count become lower which is beneficial for circuit execution time and noise resilience. A less number of gates indicates a lower accumulation of gate-errors, and a reduced circuit-depth means less decoherence time for the qubits. However, finding the best-ordered circuit is a difficult problem and does not scale well with circuit size. This paper presents four generic methodologies to optimize QAOA-circuits by exploiting gate re-ordering. We demonstrate a reduction in gate-count by ≈23.0% and circuit-depth by ≈53.0% on average over a conventional approach without incurring any compilation-time penalty. We also present a variation-aware compilation which enhances the compiled circuit success probability by ≈62.7% for the target hardware over the variation unaware approach. A new metric, Approximation Ratio Gap (ARG), is proposed to validate the quality of the compiled QAOA-circuit instances on actual devices. Hardware implementation of a number of QAOA instances shows ≈25.8% improvement in the proposed metric on average over the conventional approach on ibmq 16 melbourne.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00029","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251960","Quantum computing;QAOA;MaxCut;Compiler;Qubit mapping","Qubit;Logic gates;Approximation algorithms;Hardware;Quantum circuit;Optimization;Resilience","","42","","71","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Fast-BCNN: Massive Neuron Skipping in Bayesian Convolutional Neural Networks","Q. Wan; X. Fu","ECE Department, ECOMS Lab, University of Houston, Houston, USA; ECE Department, ECOMS Lab, University of Houston, Houston, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","229","240","Bayesian Convolutional Neural Networks (BCNNs) have emerged as a robust form of Convolutional Neural Networks (CNNs) with the capability of uncertainty estimation. A BCNN model is implemented by adding a dropout layer after each convolutional layer in the original CNN. By executing the stochastic inferences many times, BCNNs are able to provide an output distribution that reflects the uncertainty of the final prediction. Repeated inferences in this process lead to much longer execution time, which makes it challenging to apply Bayesian technique to CNNs in real-world applications. In this study, we propose Fast-BCNN, an FPGA-based hardware accelerator design that intelligently skips the redundant computations for two types of neurons during repeated BCNN inferences. Firstly, within a sample inference, we aim to skip the dropped neurons that predetermined by dropout masks. Secondly, by leveraging the information from the first inference and dropout masks, we predict the zero neurons and skip all their corresponding computations during the following sample inferences. Particularly, an optimization algorithm is employed to guarantee the accuracy of zero neuron prediction while achieving the maximal computation reduction. To support our neuron skipping strategy at hardware level, we explore an efficient parallelism for CNN convolution to gracefully skip the corresponding computations for both types of neurons, we then propose a novel PE architecture that accommodates the parallel operation of convolution and prediction with negligible overhead. Experimental results demonstrate that our Fast-BCNN achieves 2.1~8.2× speedup and 44%~84% energy reduction over the baseline CNN accelerator.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00030","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252005","Bayesian deep learning;uncertainty;Neural networks accelerator;Neuron skipping","Uncertainty;Convolution;Neurons;Parallel processing;Hardware;Bayes methods;Convolutional neural networks","","18","","34","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Ptolemy: Architecture Support for Robust Deep Learning","Y. Gan; Y. Qiu; J. Leng; M. Guo; Y. Zhu","University of Rochester; Shanghai Jiao Tong University; Shanghai Qi Zhi Institute, Shanghai Jiao Tong University; Shanghai Qi Zhi Institute, Shanghai Jiao Tong University; University of Rochester",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","241","255","Deep learning is vulnerable to adversarial attacks, where carefully-crafted input perturbations could mislead a well-trained Deep Neural Network (DNN) to produce incorrect results. Adversarial attacks jeopardize the safety, security, and privacy of DNN-enabled systems. Today's countermeasures to adversarial attacks either do not have the capability to detect adversarial samples at inference-time, or introduce prohibitively high overhead to be practical at inference-time.We propose Ptolemy, an algorithm-architecture co-designed system that detects adversarial attacks at inference time with low overhead and high accuracy. We exploit the synergies between DNN inference and imperative program execution: an input to a DNN uniquely activates a set of neurons that contribute significantly to the inference output, analogous to the sequence of basic blocks exercised by an input in a conventional program. Critically, we observe that adversarial samples tend to activate distinctive paths from those of benign inputs. Leveraging this insight, we propose an adversarial sample detection framework, which uses canary paths generated from offline profiling to detect adversarial samples at runtime. The Ptolemy compiler along with the co-designed hardware enable efficient execution by exploiting the unique algorithmic characteristics. Extensive evaluations show that Ptolemy achieves higher or similar adversarial sample detection accuracy than today's mechanisms with a much lower (as low as 2%) runtime overhead.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251936","DNN;Robustness;Deep learning;Adversarial Attack;Adversarial Samples;Defense","Deep learning;Runtime;Perturbation methods;Neurons;Software;Robustness;Safety","","17","","76","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Non-Blocking Simultaneous Multithreading: Embracing the Resiliency of Deep Neural Networks","G. Shomron; U. Weiser","Faculty of Electrical Engineering, Technion — Israel Institute of Technology; Faculty of Electrical Engineering, Technion — Israel Institute of Technology",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","256","269","Deep neural networks (DNNs) are known for their inability to utilize underlying hardware resources due to hard-ware susceptibility to sparse activations and weights. Even in finer granularities, many of the non-zero values hold a portion of zero-valued bits that may cause inefficiencies when executed on hard-ware. Inspired by conventional CPU simultaneous multithreading (SMT) that increases computer resource utilization by sharing them across several threads, we propose non-blocking SMT (NB-SMT) designated for DNN accelerators. Like conventional SMT, NB-SMT shares hardware resources among several execution flows. Yet, unlike SMT, NB-SMT is non-blocking, as it handles structural hazards by exploiting the algorithmic resiliency of DNNs. Instead of opportunistically dispatching instructions while they wait in a reservation station for available hardware, NB-SMT temporarily reduces the computation precision to accommodate all threads at once, enabling a non-blocking operation. We demonstrate NB-SMT applicability using SySMT, an NB-SMT-enabled output-stationary systolic array (OS-SA). Compared with a conventional OS-SA, a 2-threaded SySMT consumes 1.4× the area and delivers 2× speedup with 33% energy savings and less than 1% accuracy degradation of state-of-the-art CNNs with ImageNet. A 4-threaded SySMT consumes 2.5× the area and delivers, for example, 3.4× speedup and 39%×energy savings with 1% accuracy degradation of 40%-pruned ResNet-18.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251935","neural networks;deep learning;multithreading;accelerator","Degradation;Multithreading;Instruction sets;Neural networks;Hardware;Hazards;Resilience","","11","","70","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators","Y. He; P. Balaprakash; Y. Li","University of Chicago, Chicago, IL, USA; Argonne National Laboratory, Argonne, IL, USA; University of Chicago, Chicago, IL, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","270","281","We present a resilience analysis framework, called FIdelity, to accurately and quickly analyze the behavior of hardware errors in deep learning accelerators. Our framework enables resilience analysis starting from the very beginning of the design process to ensure that the reliability requirements are met, so that these accelerators can be safely deployed for a wide range of applications, including safety-critical applications such as self-driving cars.Existing resilience analysis techniques suffer from the following limitations: 1. general-purpose hardware techniques can achieve accurate results, but they require access to RTL to perform time-consuming RTL simulations, which is not feasible for early design exploration; 2. general-purpose software techniques can produce results quickly, but they are highly inaccurate; 3. techniques targeting deep learning accelerators only focus on memory errors.Our FIdelity framework overcomes these limitations. FIdelity only requires a minimal amount of high-level design information that can be obtained from architectural descriptions/block diagrams, or estimated and varied for sensitivity analysis. By leveraging unique architectural properties of deep learning accelerators, we are able to systematically model a major class of hardware errors – transient errors in logic components – in software with high fidelity. Therefore, FIdelity is both quick and accurate, and does not require access to RTL.We thoroughly validate our FIdelity framework using Nvidia’s open-source accelerator called NVDLA, which shows that the results are highly accurate – out of 60K fault injection experiments, the software fault models derived using FIdelity closely match the behaviors observed from RTL simulations. Using the validated FIdelity framework, we perform a large-scale resilience study on NVDLA, which consists of 46M fault injection experiments running various representative deep neural network applications. We report the key findings and architectural insights, which can be used to guide the design of future accelerators.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251852","","Deep learning;Sensitivity analysis;Reliability engineering;Hardware;Transient analysis;Open source software;Resilience","","53","","28","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions by Exploiting DRAM Data Retention Characteristics","M. Patel; J. S. Kim; T. Shahroodi; H. Hassan; O. Mutlu",ETH Zürich; Carnegie Mellon University; ETH Zürich; ETH Zürich; ETH Zürich,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","282","297","Increasing single-cell DRAM error rates have pushed DRAM manufacturers to adopt on-die error-correction coding (ECC), which operates entirely within a DRAM chip to improve factory yield. The on-die ECC function and its effects on DRAM reliability are considered trade secrets, so only the manufacturer knows precisely how on-die ECC alters the externally-visible reliability characteristics. Consequently, on-die ECC obstructs third-party DRAM customers (e.g., test engineers, experimental researchers), who typically design, test, and validate systems based on these characteristicsTo give third parties insight into precisely how on-die ECC transforms DRAM error patterns during error correction, we introduce Bit-Exact ECC Recovery (BEER), a new methodology for determining the full DRAM on-die ECC function (i.e., its parity-check matrix) without hardware tools, prerequisite knowledge about the DRAM chip or on-die ECC mechanism, or access to ECC metadata (e.g., error syndromes, parity information). BEER exploits the key insight that non-intrusively inducing data-retention errors with carefully-crafted test pat-terns reveals behavior that is unique to a specific ECC functionWe use BEER to identify the ECC functions of 80 real LPDDR4 DRAM chips with on-die ECC from three major DRAM manufacturers. We evaluate BEER's correctness in simulation and performance on a real system to show that BEER is effective and practical across a wide range of on-die ECC functions. To demonstrate BEER's value, we propose and discuss several ways that third parties can use BEER to improve their design and testing practices. As a concrete example, we introduce and evaluate BEEP, the first error profiling method-ology that uses the known on-die ECC function to recover the number and bit-exact locations of unobservable raw bit errors responsible for observable post-correction errors.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251987","DRAM;Error Correction Codes;ECC;Data Retention Errors;On-Die ECC;Reliability;Analysis;Simulation;Error Characterization;Testing;Memory","Transforms;Metadata;Tools;DRAM chips;Hardware;Error correction codes;Reliability","","31","","182","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"DStress: Automatic Synthesis of DRAM Reliability Stress Viruses using Genetic Algorithms","L. Mukhanov; D. S. Nikolopoulos; G. Karakonstantis","Queen’s University, Belfast, UK; Virginia Tech, USA; Queen’s University, Belfast, UK",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","298","312","Failures become inevitable in DRAM devices, which is a major obstacle for scaling down the density of cells in future DRAM technologies. These failures can be detected by specific DRAM tests that implement the data and memory access patterns having a strong impact on DRAM reliability. However, the design of such tests is very challenging, especially for testing DRAM devices in operation, due to an extremely large number of possible cell-to-cell interference effects and combinations of patterns inducing these effects.In this paper, we present a new framework for the synthesis of DRAM reliability stress viruses, DStress. This framework automatically searches for the data and memory access patterns that induce the worst-case DRAM error behavior regardless the internal DRAM design. The search engine of our framework is based on Genetic Algorithms (GA) and a programming tool that we use to specify the patterns examined by GA. To evaluate the effect of program viruses on DRAM reliability, we integrate DStress with an experimental server where 72 DRAM chips can operate under various operating parameters and temperatures.We present the results of our 7-month experimental study on the search of DRAM reliability stress viruses. We show that DStress finds the worst-case data pattern virus and the worst-case memory access virus with probabilities of 1 - 4 × 10-7 and 0.95, respectively. We demonstrate that the discovered patterns induce by at least 45% more errors than the traditional data pattern micro-benchmarks used in previous studies. We show that DStress enables us to detect the marginal DRAM operating parameters reducing the DRAM power by 17.7 % on average without compromising reliability. Overall, our framework facilitates the exploration of new data patterns and memory access scenarios increasing the probability of DRAM errors, which is essential for improving the state-of-the-art DRAM testing mechanisms.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251864","DRAM;reliability;program synthesis;viruses","Random access memory;Tools;Reliability;Stress;Viruses (medical);Genetic algorithms;Testing","","10","","110","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"FIGARO: Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching","Y. Wang; L. Orosa; X. Peng; Y. Guo; S. Ghose; M. Patel; J. S. Kim; J. G. Luna; M. Sadrosadati; N. M. Ghiasi; O. Mutlu",National University of Defense Technology; ETH Zürich; Chinese University of Hong Kong; National University of Defense Technology; University of Illinois at Urbana–Champaign; ETH Zürich; ETH Zürich; ETH Zürich; Institute of Research in Fundamental Sciences; ETH Zürich; ETH Zürich,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","313","328","Main memory, composed of DRAM, is a performance bottleneck for many applications, due to the high DRAM access latency. In-DRAM caches work to mitigate this latency by augmenting regular-latency DRAM with small-but-fast regions of DRAM that serve as a cache for the data held in the regular-latency (i.e., slow) region of DRAM. While an effective in-DRAM cache can allow a large fraction of memory requests to be served from a fast DRAM region, the latency savings are often hindered by inefficient mechanisms for migrating (i.e., relocating) copies of data into and out of the fast regions. Existing in-DRAM caches have two sources of inefficiency: (1) their data relocation granularity is an entire multi-kilobyte row of DRAM, even though much of the row may never be accessed due to poor data locality; and (2) because the relocation latency increases with the physical distance between the slow and fast regions, multiple fast regions are physically interleaved among slow regions to reduce the relocation latency, resulting in increased hardware area and manufacturing complexityWe propose a new substrate, FIGARO, that uses existing shared global buffers among subarrays within a DRAM bank to provide support for in-DRAM data relocation across subar-rays at the granularity of a single cache block. FIGARO has a distance-independent latency within a DRAM bank, and avoids complex modifications to DRAM (such as the interleaving of fast and slow regions). Using FIGARO, we design a fine-grained in-DRAM cache called FIGCache. The key idea of FIGCache is to cache only small, frequently-accessed portions of different DRAM rows in a designated region of DRAM. By caching only the parts of each row that are expected to be accessed in the near future, we can pack more of the frequently-accessed data into FIGCache, and can benefit from additional row hits in DRAM (i.e., accesses to an already-open row, which have a lower latency than accesses to an unopened row). FIGCache provides benefits for systems with both heterogeneous DRAM banks (i.e., banks with fast regions and slow regions) and conventional homogeneous DRAM banks (i.e., banks with only slow regions)Our evaluations across a wide variety of applications show that FIGCache improves the average performance of a system using DDR4 DRAM by 16.3% and reduces average DRAM energy consumption by 7.8% for 8-core workloads, over a conventional system without in-DRAM caching. We show that FIGCache outperforms state-of-the-art in-DRAM caching techniques, and that its performance gains are robust across many system and mechanism parameters.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00036","National University of Defense Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251865","","Energy consumption;System performance;Random access memory;Performance gain;Hardware;Manufacturing;Substrates","","30","","158","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"PerpLE: Improving the Speed and Effectiveness of Memory Consistency Testing","T. Melissaris; M. Markakis; K. Shaw; M. Martonosi",Princeton University; Princeton University; Williams College; Princeton University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","329","341","Even as most of today’s computer systems have turned to parallelism to improve performance, their documentation often remains informal, incomplete or even incorrect regarding their memory consistency models. This leads to programmer and designer confusion and to buggy concurrent systems. Existing tools for empirical memory consistency testing rely on large numbers of iterations of simple multi-threaded litmus tests to perform conformance testing. The current approach typically employs thread synchronization at every iteration, which imposes a significant overhead and can reduce testing performance and efficiency.This paper proposes new litmus test variants called perpetual litmus tests, which allow for consistency testing without periteration synchronization. Perpetual litmus tests use arithmetic sequences in store operations to reduce the required synchronization points. We present PerpLE, a software suite that includes tools for the generation, execution, and analysis of perpetual litmus tests. We introduce an algorithm for determining the outcomes of perpetual litmus tests as well as a scalable linear heuristic algorithm. Evaluating the performance, scalability and ability of our tool to find outcomes of interest on an x86 system, we observe a wider variety of outcomes than litmus7 while experiencing runtime speedups over all litmus7 synchronization modes (8.89x over the default user mode). Compared to the default litmus7 synchronization (user) mode, PerpLE offers over four orders-of-magnitude improvement in the rate with which we detect target outcomes.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00037","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251867","","Heuristic algorithms;Instruction sets;Software algorithms;Tools;Software;Synchronization;Testing","","","","43","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"CATCAM: Constant-time Alteration Ternary CAM with Scalable In-Memory Architecture","D. Chen; Z. Li; T. Xiong; Z. Liu; J. Yang; S. Yin; S. Wei; L. Liu",Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","342","355","TCAM (Ternary Content-Addressable Memory) is the essential component for high-speed packet classification in modern hardware switches. However, due to its relatively slow update process, recent advances in Software-Defined Network (SDN) regard them as the bottleneck to the agile deployment of network services. Rule installation in commodity switches suffers from non-deterministic delays, ranging from a few milliseconds to nearly half a second. The crux of the problem is that TCAM prioritizes rules based on physical addresses. Corresponding entries have to be reallocated according to the priority of an incoming rule, such that the insertion delay grows linearly with the number of existing rules in a TCAM. In this paper, we present Constant-time Alteration Ternary CAM (CATCAM) that can accomplish both lookup queries and update requests for packet classification in a few nanoseconds. The key to fast update is to decouple rule priorities from physical addresses. We propose a matrix-based priority encoding scheme that records the priority relation between rules and can be implemented in 8T SRAM arrays with the emerging Processing In-Memory (PIM) technique. CATCAM also comes with a hierarchical architecture to scale out, its interval-based scheduling scheme guarantees deterministic update performance in all scenarios. CATCAM is developed under full-custom design in the 28 nm process. Evaluation across benchmark workloads shows that CATCAM provides at least three orders of magnitude speedup over state-of-the-art TCAM update algorithms and offers equivalent search capability to conventional TCAM while incurring 0.3% power and 20% area overhead.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00038","National Natural Science Foundation of China; Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251963","packet classification;ternary content-addressable memory;processing in-memory;hardware accelerator","Microarchitecture;Scalability;Random access memory;Hardware;Encoding;Delays;Classification algorithms","","6","","60","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"DUAL: Acceleration of Clustering Algorithms using Digital-based Processing In-Memory","M. Imani; S. Pampana; S. Gupta; M. Zhou; Y. Kim; T. Rosing","Department of Computer Science, UC, Irvine; Department of Computer Science and Engineering, UC, San Diego; Department of Computer Science and Engineering, UC, San Diego; Department of Computer Science and Engineering, UC, San Diego; Department of Information and Communication Engineering, DGIST; Department of Computer Science and Engineering, UC, San Diego",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","356","371","Today's applications generate a large amount of data that need to be processed by learning algorithms. In practice, the majority of the data are not associated with any labels. Unsupervised learning, i.e., clustering methods, are the most commonly used algorithms for data analysis. However, running clustering algorithms on traditional cores results in high energy consumption and slow processing speed due to a large amount of data movement between memory and processing units. In this paper, we propose DUAL, a Digital-based Unsupervised learning AcceLeration, which supports a wide range of popular algorithms on conventional crossbar memory. Instead of working with the original data, DUAL maps all data points into high-dimensional space, replacing complex clustering operations with memory-friendly operations. We accordingly design a PIM-based architecture that supports all essential operations in a highly parallel and scalable way. DUAL supports a wide range of essential operations and enables in-place computations, allowing data points to remain in memory. We have evaluated DUAL on several popular clustering algorithms for a wide range of large-scale datasets. Our evaluation shows that DUAL provides a comparable quality to existing clustering algorithms while using a binary representation and a simplified distance metric. DUAL also provides 58.8× speedup and 251.2× energy efficiency improvement as compared to the state-of-the-art solution running on GPU.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251944","Processing in-memory;Unsupervised learning;Hyperdimensional computing;Algorithm-hardware co-design","Microarchitecture;Memory management;Clustering algorithms;Graphics processing units;Energy efficiency;Acceleration;Unsupervised learning","","78","","114","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning","M. He; C. Song; I. Kim; C. Jeong; S. Kim; I. Park; M. Thottethodi; T. N. Vijaykumar","Electrical and Computer Engineering, Purdue University, West Lafayette, IN, U.S.A; DRAM Design, SK Hynix, Icheon, South Korea; DRAM Design, SK Hynix, Icheon, South Korea; DRAM Design, SK Hynix, Icheon, South Korea; DRAM Design, SK Hynix, Icheon, South Korea; DRAM Design, SK Hynix, Icheon, South Korea; Electrical and Computer Engineering, Purdue University, West Lafayette, IN, U.S.A; Electrical and Computer Engineering, Purdue University, West Lafayette, IN, U.S.A",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","372","385","Advances in machine learning (ML) have ignited hardware innovations for efficient execution of the ML models many of which are memory-bound (e.g., long short-term memories, multi-level perceptrons, and recurrent neural networks). Specifically, inference using these ML models with small batches, as would be the case at the Cloud edge, has little reuse of the large filters and is deeply memory-bound. Simultaneously, processing-in or -near memory (PIM or PNM) is promising unprecedented high-bandwidth connection between compute and memory. Fortunately, the memory-bound ML models are a good fit for PIM. We focus on digital PIM which provides higher bandwidth than PNM and does not incur the reliability issues of analog PIM. Previous PIM and PNM approaches advocate full processor cores which do not conform to PIM’s severe area and power constraints. We describe Newton, a major DRAM maker’s upcoming accelerator-in-memory (AiM) product for machine learning, which makes the following contributions: (1) To satisfy PIM’s area constraints, Newton (a) places a minimal compute of only multiply-accumulate units and buffers in the DRAM which avoids the full-core area and power overheads of previous work and thus makes PIM feasible for the first time, and (b) employs a DRAM-like interface for the host to issue commands to the PIM compute. The PIM compute is rate-matched to the internal DRAM bandwidth and employs a non-intuitive, global input vector buffer shared by the entire channel to capture input reuse while amortizing buffer area cost. To the host, Newton’s interface is indistinguishable from regular DRAM without any offloading overheads and PIM/non-PIM mode switching, and with the same deterministic latencies even for floating-point commands. (2) To prevent the PIM-host interface from becoming a bottleneck, we include three optimizations: commands which gang multiple compute operations both within a bank and across banks; complex, multi-step compute commands – both of which save critical command bandwidth; and targeted reduction of tFAW overhead. (3) To capture output vector reuse with reasonable buffering, Newton employs an unusually-wide interleaved layout for the matrix. Our simulations running state-of-the-art neural networks show that building on a realistic HBM2E-like DRAM, Newton achieves 10x and 54x average speedup over a non-PIM system with infinite compute that perfectly uses the external DRAM bandwidth and a realistic GPU, respectively.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251855","processing-in-memory;fully-connected neural networks;DRAM","Computational modeling;Random access memory;Graphics processing units;Bandwidth;Machine learning;Acceleration;Optimization","","132","","45","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"AQUOMAN: An Analytic-Query Offloading Machine","S. Xu; T. Bourgeat; T. Huang; H. Kim; S. Lee; A. Arvind","MIT CSAIL, DGIST; MIT CSAIL, DGIST; MIT CSAIL, DGIST; MIT CSAIL, DGIST; MIT CSAIL, DGIST; MIT CSAIL, DGIST",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","386","399","Analytic workloads on terabyte data-sets are often run in the cloud, where application and storage servers are separate and connected via network. In order to saturate the storage bandwidth and to hide the long storage latency, such a solution requires an expensive server cluster with sufficient aggregate DRAM capacity and hardware threads. An alternative solution is to push the query computation into storage servers.In this paper we present an in-storage Analytics QUery Offloading MAchiNe (AQUOMAN) to ""offload"" most SQL operators, including multi-way joins, to SSDs. AQUOMAN executes Table Tasks, which apply a static dataflow graph of SQL operators to relational tables to produce an output table. Table Tasks use a streaming computation model, which allows AQUOMAN to process queries with a reasonable amount of DRAM for intermediate results. AQUOMAN is a general analytic query processor, which can be integrated in the database software stack transparently. We have built a prototype of AQUOMAN in FPGAs, and using TPC-H benchmarks on 1TB data sets, shown that a single instance of 1TB AQUOMAN disk, on average, can free up 70% CPU cycles and reduce DRAM usage by 60%. One way to visualize this saving is to think that if we run queries sequentially and ignore inter-query page cache reuse, MonetDB running on a 4-core, 16GB-DRAM machine with AQUOMAN augmented SSDs performs, on average, as well as a MonetDB running on a 32-core, 128GB-DRAM machine with standard SSDs.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00041","Samsung; National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251860","Accelerator;SQL analytics;Near-data computing;FPGA;Flash storage;Database","Microarchitecture;Random access memory;Prototypes;Servers;Task analysis;Standards;Field programmable gate arrays","","12","","55","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"MOUSE: Inference In Non-volatile Memory for Energy Harvesting Applications","S. Resch; S. K. Khatamifard; Z. I. Chowdhury; M. Zabihi; Z. Zhao; H. Cilasun; J. -P. Wang; S. S. Sapatnekar; U. R. Karpuzcu","University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","400","414","There is increasing demand to bring machine learning capabilities to low power devices. By integrating the computational power of machine learning with the deployment capabilities of low power devices, a number of new applications become possible. In some applications, such devices will not even have a battery, and must rely solely on energy harvesting techniques. This puts extreme constraints on the hardware, which must be energy efficient and capable of tolerating interruptions due to power outages. Here, we propose an in-memory machine learning accelerator utilizing non-volatile spintronic memory. The combination of processing-in-memory and non-volatility provides a key advantage in that progress is effectively saved after every operation. This enables instant shut down and restart capabilities with minimal overhead. Additionally, the operations are highly energy efficient leading to low power consumption.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251938","Intermittent computing;Processing-in-Memory","Nonvolatile memory;Machine learning;Energy efficiency;Mice;Power system reliability;Energy harvesting;Spintronics","","27","","97","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"More with Less – Deriving More Translation Rules with Less Training Data for DBTs Using Parameterization","J. Jiang; R. Dong; Z. Zhou; C. Song; W. Wang; P. -C. Yew; W. Zhang","School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University; Department of Computer Science, University of Georgia; Department of Computer Science and Engineering, University of Minnesota, Twin Cities; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","415","426","Dynamic binary translation (DBT) is widely used in system virtualization and many other important applications. To achieve a higher translation quality, a learning-based approach has been recently proposed to automatically learn semantically-equivalent translation rules. Because translation rules directly impact the quality and performance of the translated host codes, one of the key issues is to collect as many translation rules as possible through minimal training data set. The collected translation rules should also cover (i.e. apply to) as many guest binary instructions or code sequences as possible at the runtime. For those guest binary instructions that are not covered by the learned rules, emulation has to be used, which will incur additional runtime overhead. Prior learning-based DBT systems only achieve an average of about 69% dynamic code coverage for SPEC CINT 2006.In this paper, we propose a novel parameterization approach to take advantage of the regularity and the well-structured format in most modern ISAs. It allows us to extend the learned translation rules to include instructions or instruction sequences of similar structures or characteristics that are not covered in the training set. More translation rules can thus be harvested from the same training set. Experimental results on QEMU 4.1 show that using such a parameterization approach we can expand the learned 2,724 rules to 86,423 applicable rules for SPEC CINT 2006. Its code coverage can also be expanded from about 69.7% to about 95.5% with a 24% performance improvement compared to enhanced learning-based approach.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00043","National Natural Science Foundation of China; University of Georgia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251952","DBT;Rule based;Parameterization","Training;Runtime;Microarchitecture;Emulation;Training data;Prototypes;Virtualization","","8","","26","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data","J. Zhao; P. Di","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; Huawei Technologies Co. ,Ltd, Beijing, China",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","427","441","Optimizing compilers exploit the memory hierarchy using loop tiling and fusion, but these two transformations usually interfere with each other due to the oversight of transformations on data in memories. We present a novel composition of loop tiling and fusion in this paper. Unlike existing tiling-after-fusion algorithms that only transform computation spaces, our approach first applies rectangular/parallelogram tiling to live-out computation spaces for fitting the memory hierarchy, followed by the computation of the memory footprints required by each tile. The upwards exposed data extracted from the memory footprints are used to determine the tile shapes of intermediate computation spaces, allowing the construction of arbitrary tile shapes. Finally, our technique implements a post-tiling fusion strategy for maximizing data locality without losing tilability or parallelism of live-out computation spaces, thereby enabling storage reduction and reuse, and optimizing the memory hierarchy. We demonstrate that our approach can achieve superior performance on both CPU and GPU architectures over the state of the art by experimenting on 11 benchmarks extracted from numerous domains including neural networks, image processing, sparse matrix computation and linear algebra. Also, the results of the ResNet-50 model on an AI accelerator show that our approach can obtain 16% performance improvement.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00044","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251965","memory hierarchy;data locality;parallelism;polyhedral model;tiling;fusion","Shape;Tiles;Memory management;Transforms;Benchmark testing;Sparse matrices;Optimization","","19","","63","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates","A. Renda; Y. Chen; C. Mendis; M. Carbin",MIT CSAIL; MIT CSAIL; MIT CSAIL; MIT CSAIL,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","442","455","CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. In some cases, these parameters do not necessarily have a physical realization and are therefore fundamentally approximate, or even unmeasurable. In this paper we present DiffTune, a system for learning the parameters of x86 basic block CPU simulators from coarse-grained end-to-end measurements. Given a simulator, DiffTune learns its parameters by first replacing the original simulator with a differentiable surrogate, another function that approximates the original function; by making the surrogate differentiable, DiffTune is then able to apply gradient-based optimization techniques even when the original function is non-differentiable, such as is the case with CPU simulators. With this differentiable surrogate, DiffTune then applies gradient-based optimization to produce values of the simulator's parameters that minimize the simulator's error on a dataset of ground truth end-to-end performance measurements. Finally, the learned parameters are plugged back into the original simulator. DiffTune is able to automatically learn the entire set of microarchitecture-specific parameters within the Intel x86 simulation model of llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling model. DiffTune's learned parameters lead llvm-mca to an average error that not only matches but lowers that of its original, expert-provided parameter values.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00045","National Science Foundation; Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251993","Performance Modeling;Machine Learning","Microarchitecture;Tools;Complexity theory;Software measurement;Task analysis;Optimization;Tuning","","16","","41","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Predicting Execution Times With Partial Simulations in Virtual Memory Research: Why and How","M. Agbarya; I. Yaniv; J. Gandhi; D. Tsafrir",Technion – Israel Institute of Technology; Technion – Israel Institute of Technology; VMware Research; Technion & VMware Research,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","456","470","Computer architects frequently use cycle-accurate simulations, which incur heavy overheads. Recently, virtual memory studies increasingly employ a lighter-weight methodology that utilizes partial simulations-of only the memory subsystem- whose output is fed into a mathematical linear model that predicts execution runtimes. The latter methodology is much faster, but its accuracy is only assumed, never rigorously validated.We question the assumption and put it to the test by developing Mosalloc, the Mosaic Memory Allocator. Mosalloc backs the virtual memory of applications with arbitrary combinations of 4KB, 2MB, and 1GB pages (each combination forms a ""mosaic"" of pages). Previous studies used a single page size per execution (either 4KB or 2MB) to generate exactly two execution samples, which defined the aforementioned linear model. In contrast, Mosalloc can generate numerous samples, allowing us to test instead of assume the model's accuracy. We find that prediction errors of existing models can be as high as 25%-192%. We propose a new model that bounds the maximal error below 3%, making it more reliable and useful for exploring new ideas.""The phenomena surrounding computers are deep and obscure, requiring much experimentation to assess their nature."" (A. Newell and H. A. Simon).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251981","simulation;virtual memory;caches;TLB","Runtime;Microarchitecture;Computational modeling;Predictive models;Data models;Mathematical model;Reliability","","6","","74","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling","S. Rogers; J. Slycord; M. Baharani; H. Tabkhi","Dept. of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, North Carolina; Dept. of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, North Carolina; Dept. of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, North Carolina; Dept. of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, North Carolina",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","471","482","With the prevalence of hardware accelerators as an integral part of the modern systems on chip (SoCs), the ability to quickly and accurately model accelerators within the system it operates is critical. This paper presents gem5-SALAM as a novel system architecture for LLVM-based modeling and simulation of custom hardware accelerators integrated into the gem5 framework. gem5-SALAM overcomes the inherent limitations of state-of-the-art trace-based pre-register-transfer level (RTL) simulators by offering a truly ""execute-in-execute"" LLVM-based model. It enables scalable modeling of multiple dynamically interacting accelerators with full-system simulation support. To create sustainable long-term expansion compatible with the gem5 system framework, gem5-SALAM offers a general-purpose and modular communication interface and memory hierarchy integrated into the gem5 ecosystem which streamlines designing and modeling accelerators for new and emerging applications. Validation on the MachSuite [17] benchmarks present a timing estimation error of less than 1% against Vivado High-Level Synthesis (HLS) tool. Results also show less than a 4% area and power estimation error against Synopsys Design Compiler. Additionally, system validation against implementations on a Ultrascale+ ZCU102 shows an average end-to-end timing error of less than 2%. Lastly, this paper presents the capabilities of gem5-SALAM in cycle-level profiling and full system design space exploration of accelerator-rich systems.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251937","hardware accelerators;LLVM;system simulation;gem5","Estimation error;Ecosystems;Systems architecture;Hardware;Systems simulation;Timing;Space exploration","","21","","21","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Shaving Retries with Sentinels for Fast Read over High-Density 3D Flash","Q. Li; M. Ye; Y. Cui; L. Shi; X. Li; T. -W. Kuo; C. J. Xue","City University of Hong Kong; City University of Hong Kong; City University of Hong Kong; East China Normal University; YEESTOR Microelectronics Co., Ltd; City University of Hong Kong; City University of Hong Kong",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","483","495","High-density flash-memory chips are under tremendous demands with the exponential growth of data. At the same time, the slow read performance of these high-density flash-memory chips becomes a new challenge. In this work, we analyze the high raw bit error rates (RBER) issue by characterizing the error behaviours of 3D QLC flash-memory chips. A preferred read voltage to a QLC cell could vary among layers and might even change in a short period of time due to the temperature. A sentinel-cell approach is thus proposed to utilize the error characteristics among cells. We propose to infer the optimal read voltages of a wordline based on errors introduced on sentinel cells. An on-line calibration procedure is further presented to resolve the problem of possible non-uniform error distribution on some wordlines. With optimal voltages being inferred, the number of read retries will be significantly reduced. Experiments show that optimal read voltages can be instantly obtained in 94% cases on average over the evaluated QLC flash memory with at most 2 read retries, and with merely 0.2% space overheads for adopting sentinel cells. The number of read retries could be reduced by 82% on average, and the read performance can be improved by 74% on average through a series of extensive experiments over 3D TLC and QLC flash-memory chips.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251942","3D NAND flash;read retry;reliability","Industries;Solid modeling;Three-dimensional displays;Temperature;Microarchitecture;Memory management;Flash memories","","35","","44","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Characterizing and Modeling Non-Volatile Memory Systems","Z. Wang; X. Liu; J. Yang; T. Michailidis; S. Swanson; J. Zhao","University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","496","508","Scalable server-grade non-volatile RAM (NVRAM) DIMMs became commercially available with the release of Intel's Optane DIMM. Recent studies on Optane DIMM systems unveil discrepant performance characteristics, compared to what many researchers assumed before the product release. Most of these studies focus on system software design and performance analysis. To thoroughly analyze the source of this discrepancy and facilitate real-NVRAM-aware architecture design, we propose a framework that characterizes and models Optane DIMM's microarchitecture. Our framework consists of a Low-level profilEr for Non-volatile memory Systems (LENS) and a Validated cycle-Accurate NVRAM Simulator (VANS). LENS allows us to comprehensively analyze the performance attributes and reverse engineer NVRAM microarchitectures. Based on LENS characterization, we develop VANS, which models the sophisticated microarchitecture design of Optane DIMM, and is validated by comparing with the detailed performance characteristics of Optane-DIMM-attached Intel servers. VANS adopts a modular design that can be easily modified to extend to other NVRAM architecture designs; it can also be attached to full-system simulators, such as gem51. By using LENS and VANS, we develop two architectural optimizations on top of Optane DIMM, Lazy Cache and Pre-translation, which significantly improve cloud workload performance.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251957","nonvolatile memory;memory systems;simulation","Performance evaluation;Microarchitecture;Nonvolatile memory;Random access memory;Computer architecture;System software;Lenses","","61","","70","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"P-INSPECT: Architectural Support for Programmable Non-Volatile Memory Frameworks","A. Kokolis; T. Shull; J. Huang; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","509","524","The availability of user-friendly programming frameworks is key to the success of Non-Volatile Memory (NVM). Unfortunately, most current NVM frameworks rely heavily on user intervention to mark persistent objects and even persistent writes. This not only complicates NVM programming, but also introduces potential bugs. To address these issues, researchers have proposed Persistence by Reachability frameworks, which require minimal user intervention. However, these frameworks are slow because their runtimes have to perform checks at program load/store operations, and move data structures between DRAM and NVM during program execution. In this paper, we introduce P-inspect, a novel hardware architecture targeted to speeding up persistence by reachability NVM programming frameworks. P-inspect uses bloom-filter hardware to perform various checks in a transparent and efficient manner. It also provides hardware for low-overhead persistent writes. Our simulation-based evaluation running a state-of-the-art persistence by reachability framework shows that P-inspect retains programmability and eliminates most of the overhead. We use real-world applications to demonstrate that, on average, P-inspect reduces an application's number of executed instructions by 26% and the execution time by 16%-delivering similar performance to an ideal runtime that has no persistence by reachability overhead.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251857","Non-volatile memory;Programming frameworks;Hardware for programmability","Runtime;Microarchitecture;Nonvolatile memory;Random access memory;Programming;Data structures;Hardware","","","","79","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System","J. Jeong; J. Hong; S. Maeng; C. Jung; Y. Kwon","Department of Computer Science, Purdue University; School of Computing, KAIST; School of Computing, KAIST; Department of Computer Science, Purdue University; School of Computing, KAIST",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","525","538","Persistent memory programming requires failure atomicity. To achieve this in an efficient manner, recent proposals use hardware-based logging for atomic-durable updates and hardware transactional memory (HTM) for isolation. Although the unbounded HTMs are promising for both performance and programmability reasons, none of the previous studies satisfies the practical requirements. They either require unrealistic hard-ware overheads or do not allow transactions to exceed on-chip cache boundaries. Furthermore, it has never been possible to use both DRAM and NVM in HTM, though it is becoming a popular persistency model. To this end, this study proposes UHTM, unbounded hardware transactional memory for DRAM and NVM hybrid memory systems. UHTM combines the cache coherence protocol and address-signatures to detect conflicts in the entire memory space. This approach improves concurrency by significantly reducing the false-positive rates of previous studies. More importantly, UHTM allows both DRAM and NVM data to interact with each other in transactions without compromising the consistency guarantee. This is rendered possible by UHTM's hybrid version management that provides an undo-based log for DRAM and a redo-based log for NVM. The experimental results show that UHTM outperforms the state-of-the-art durable HTM, which is LLC-bounded, by 56% on average and up to 818%.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251977","Persistent Memory;Transaction;Conflict Detection;Version Management;Hybrid Memory;Hardware Transactional Memory","Protocols;Nonvolatile memory;Memory management;Random access memory;Programming;Hardware;System-on-chip","","25","","67","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"(Almost) Fence-less Persist Ordering","S. M. Shahri; S. Armin Vakil Ghahani; A. Kolli","Pennsylvania State University, State College, USA; Pennsylvania State University, State College, USA; Pennsylvania State University, State College, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","539","554","The semantics and implementation of a memory persistency model can significantly impact the performance achieved on persistent memory systems. The only commercially available and widely used x86 persistency model causes significant performance losses by requiring redundant, expensive fence operations for commonly used undo logging programming patterns. In this work, we propose light-weight extensions to the x86 persistency model to provide some ordering guarantees without an intervening fence operation. Our extension, Themis, eliminates over 91.7% of the fence operations in undo-logging PM programs and improves average performance by 45.8% while incurring only 1.2% increase in data cache size.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251971","Persistent Memory;Memory Persistency Model","Microarchitecture;Semantics;Programming;Data models;Hardware","","8","","87","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Speculative Enforcement of Store Atomicity","A. Ros; S. Kaxiras","Computer Engineering Department, University of Murcia, Murcia, Spain; Department of Information Technology, Uppsala University, Uppsala, Sweden",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","555","567","Various memory consistency model implementations (e.g., x86, SPARC) willfully allow a core to see its own stores while they are in limbo, i.e., executed (and perhaps retired) but not yet inserted in memory order. This is known as store-to-load forwarding and it is a necessity to safeguard the local thread's sequential program semantics while achieving high performance. However, this can lead to counter-intuitive behaviours, requiring fences to prevent such behaviours when needed.Other vendors (e.g., IBM 370 and the z/Architecture series) opt for enforcing what we call in this work store atomicity, that is, disallowing a core to see its own stores before they are written to memory, trading off performance for a more intuitive memory model. Ideally, we want a stricter model to ease programability at the same time that architects can provide high-performance solutions. We make a simple observation. What holds for any other rule in a consistency model, also holds for store atomicity: it is not a crime to break the rule, unless we get caught.In this work, we detail the different ways of detecting a store atomicity violation. This leads us to a new insight: a load performed by a forwarding from an in-limbo store is not speculative; younger loads performed after that forwarding are. Based on this insight we propose an effective and cheap speculative approach to dynamically enforce store atomicity only when the detection of its violation actually occurs. In practice, these cases are rare during the execution of a program. In all other cases (the bulk of the execution of a program) store-to-load forwarding can be done without violating store atomicity. The end result is that we provide the best of both worlds: a more intuitive store-atomic memory model, i.e., the 370 model, with the performance and cost approaching (at an average of just 2.5% and 2.7% overhead for parallel and sequential applications, respectively) that of a non-store-atomic model, i.e., the x86 model.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00053","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251951","Memory consistency model;store atomicity;multi-copy atomicity;load-to-store forwarding","Bridges;Microarchitecture;Memory management;Semantics;Optimized production technology;Software;Load modeling","","3","","41","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Boosting Store Buffer Efficiency with Store-Prefetch Bursts","J. M. Cebrian; S. Kaxiras; A. Ros","Computer Engineering Department, University of Murcia, Murcia, Spain; Department of Information Technology, Uppsala University, Uppsala, Sweden; Computer Engineering Department, University of Murcia, Murcia, Spain",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","568","580","Virtually all processors today employ a store buffer (SB) to hide store latency. However, when the store buffer is full, store latency is exposed to the processor causing pipeline stalls. The default strategies to mitigate these stalls are to issue prefetch for ownership requests when store instructions commit and to continuously increase the store buffer size. While these strategies considerably increase memory-level parallelism for stores, there are still applications that suffer deeply from stalls caused by the store buffer. Even worse, store-buffer induced stalls increase considerably when simultaneous multi-threading is enabled, as the store buffer is statically partitioned among the threads.In this paper, we propose a highly selective and very aggressive prefetching strategy to minimize store-buffer induced stalls. Our proposal, Store-Prefetch Burst (SPB), is based on the following insights: i) the majority of store-buffer induced stalls are caused by a few stores; ii) the access pattern of such stores are easily predictable; and iii) the latency of the stores is not commonly hidden by standard cache prefetchers, as hiding their latency would require tremendous prefetch aggressiveness. SPB accurately detects contiguous store-access patterns (requiring just 67 bits of storage) and prefetches the remaining memory blocks of the accessed page in a single burst request to the L1 controller. SPB matches the performance of a 1024-entry SB implementation on a 56-entry SB (i.e., Skylake architecture). For a 14-entry SB (e.g., running four logical cores), it achieves 95.0% of that ideal performance, on average, for SPEC CPU 2017. Additionally, a 20-entry store buffer that incorporates SPB achieves the average performance of a standard 56-entry store buffer.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00054","Agencia Estatal de Investigación; European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251940","","Microarchitecture;Prefetching;Buffer storage;Pipelines;Parallel processing;Proposals;Standards","","2","","33","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"D-SOAP: Dynamic Spatial Orientation Affinity Prediction for Caching in Multi-Orientation Memory Systems","M. J. Liao; J. Sampson","Department of Computer Science and Engineering, School of Electrical Engineering and Computer Science Pennsylvania State University, University Park, Pennsylvania, USA; Department of Computer Science and Engineering, School of Electrical Engineering and Computer Science Pennsylvania State University, University Park, Pennsylvania, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","581","595","Previous works have shown the possibility of constructing row-column and multi-stride memory systems that can exploit simultaneously dense access along multiple logical data orientations to offer more than 3x speedups on some workloads. However, existing multi-orientation memory (MOM) and MOM-caching approaches presume that the orientation preference of a memory request is statically determinable and rely on both ISA and compiler changes to express and extract these preferences for performance gains. Thus, current MOM-caching approaches cannot readily provide benefits in the presence of dynamism with respect to data layout, data-dependent code behavior, or access ordering. Accurate orientation prediction will allow MOMs to benefit a larger range of workloads.In this paper, we describe the sources of orientation preference dynamism and show that the sensitivity of orientation prediction to cache line utilization, as well as to access pattern, differentiates it from stride prediction. We introduce a hardware-managed utilization-focused orientation predictor, D-SOAP, and compare it with a set variants (D-SOAP-*) that make use of utilization, local stride analysis, and prefetcher feedback as sources of information, both in isolation and in combination, to predict orientation preference and evaluate the impact of each information source. We evaluate the D-SOAP mechanisms on workloads with both dynamic, data-value-dependent (DVD) and statically identifiable data-value-independent (DVI) orientation preferences. We demonstrate that the D-SOAP variants using utilization information 1) track the performance of the preferred orientation within 2%, on average, and 18%, at worst, across microbenchmarks sweeping data distributions for DVD patterns, avoiding the up to 267% slowdown seen with misaligned static preferences; 2) provide competitive (4% speedup, on average) performance, compared to prior static annotation approaches relying on a priori data profiling, in DVD scenarios; 3) closely track static annotations for DVI scenarios that lack any exploitable dynamism (1.8% speedup, on average).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251998","cache;symmetric memories;orientation prediction","Sensitivity;Microarchitecture;Annotations;DVD;Prefetching;Performance gain;Method of moments","","","","63","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism","Q. M. Nguyen; D. Sanchez",MIT CSAIL; MIT CSAIL,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","596","608","Applications with irregular memory accesses and control flow, such as graph algorithms and sparse linear algebra, use high-performance cores very poorly and suffer from dismal IPC. Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have abundant pipeline parallelism that can be used to boost utilization: these applications can be structured as a pipeline of stages decoupled by queues. Queues hide latency very effectively when they allow producer stages to run far ahead of consumers. Prior work has proposed decoupled architectures, such as DAE and streaming multicores, that implement queues in hardware to exploit pipeline parallelism. Unfortunately, prior decoupled architectures are ill-suited to irregular applications, as they lack the control mechanisms needed to achieve decoupling, and target decoupling across cores but suffer from poor utilization within each core due to load imbalance across stages.We present Pipette, a technique that enables cheap pipeline parallelism within each core. Pipette decouples threads within the core using architecturally visible queues. Pipette’s ISA features control mechanisms that allow effective decoupling under irregular control flow. By time-multiplexing stages on the same core, Pipette avoids load imbalance and achieves high core IPC. Pipette’s novel implementation uses the physical register file to implement queues at very low cost, putting otherwise-idle registers to use. Pipette also adds cheap hardware to accelerate common access patterns, enabling fine-grain composition of accelerated accesses and general-purpose computation. As a result, Pipette outperforms data-parallel implementations of several challenging irregular applications by gmean 1.9× (and up to 3.9×).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251856","","Multicore processing;Instruction sets;Pipelines;Computer architecture;Parallel processing;Registers;Acceleration","","22","","55","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"RnR: A Software-Assisted Record-and-Replay Hardware Prefetcher","C. Zhang; Y. Zeng; J. Shalf; X. Guo",Lehigh University; Lehigh University; Lawrence Berkeley National Lab; Lehigh University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","609","621","Applications with irregular memory access patterns do not benefit well from the memory hierarchy as applications that have good locality do. Relatively high miss ratio and long memory access latency can cause the processor to stall and degrade system performance. Prefetching can help to hide the miss penalty by predicting which memory addresses will be accessed in the near future and issuing memory requests ahead of the time. However, software prefetchers add instruction overhead, whereas hardware prefetchers cannot efficiently predict irregular memory access sequences with high accuracy. Fortunately, in many important irregular applications (e.g., iterative solvers, graph algorithms, and sparse matrix-vector multiplication), memory access sequences repeat over multiple iterations or program phases. When the patterns are long, a conventional spatial-temporal prefetcher can not achieve high prefetching accuracy, but these repeating patterns can be identified by programmers.In this work, we propose a software-assisted hardware prefetcher that focuses on repeating irregular memory access patterns for data structures that cannot benefit from conventional hardware prefetchers. The key idea is to provide a programming interface to record cache miss sequence on the first appearance of a memory access pattern and prefetch through replaying the pattern on the following repeats. The proposed Record-and-Replay (RnR) prefetcher provides a lightweight software interface so that the programmers can specify in the application code: 1) which data structures have irregular memory accesses, 2) when to start the recording, and 3) when to start the replay (prefetching). This work evaluated three irregular workloads with different inputs. For the evaluated workloads and inputs, the proposed RnR prefetcher can achieve on average 2.16× speedup for graph applications and 2.91× speedup for an iterative solver with a sparse matrix-vector multiplication kernel. By leveraging the knowledge from the programmers, the proposed RnR prefetcher can achieve over 95% prefetching accuracy and miss coverage.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00057","Lehigh University; Office of Science; U.S. Department of Energy; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251959","hardware prefetcher;irregular memory access;iterative algorithm","Prefetching;System performance;Memory management;Data structures;Hardware;Software;Sparse matrices","","9","","60","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning","S. -C. Kao; G. Jeong; T. Krishna","Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA; Computer Science Georgia Institute of Technology, Atlanta, GA; Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","622","636","DNN accelerators provide efficiency by leveraging reuse of activations/weights/outputs during the DNN computations to reduce data movement from DRAM to the chip. The reuse is captured by the accelerator's dataflow. While there has been significant prior work in exploring and comparing various dataflows, the strategy for assigning on-chip hardware resources (i.e., compute and memory) given a dataflow that can optimize for performance/energy while meeting platform constraints of area/power for DNN(s) of interest is still relatively unexplored. The design-space of choices for balancing compute and memory explodes combinatorially, as we show in this work (e.g., as large as O(1072) choices for running MobileNet-V2), making it infeasible to do manual-tuning via exhaustive searches. It is also difficult to come up with a specific heuristic given that different DNNs and layer types exhibit different amounts of reuse.In this paper, we propose an autonomous strategy called Con-fuciuX to find optimized HW resource assignments for a given model and dataflow style. ConfuciuX leverages a reinforcement learning method, REINFORCE, to guide the search process, leveraging a detailed HW performance cost model within the training loop to estimate rewards. We also augment the RL approach with a genetic algorithm for further fine-tuning. Con-fuciuX demonstrates the highest sample-efficiency for training compared to other techniques such as Bayesian optimization, genetic algorithm, simulated annealing, and other RL methods. It converges to the optimized hardware configuration 4.7 to 24 times faster than alternate techniques.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00058","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251929","DNN Accelerator;Machine Learning;Reinforcement Learning;Genetic Algorithm","Training;Reinforcement learning;Hardware;System-on-chip;Acceleration;Optimization;Genetic algorithms","","77","","98","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Gemini: Learning to Manage CPU Power for Latency-Critical Search Engines","L. Zhou; L. N. Bhuyan; K. K. Ramakrishnan","Computer Science and Engineering Department, University of California, Riverside, USA; Computer Science and Engineering Department, University of California, Riverside, USA; Computer Science and Engineering Department, University of California, Riverside, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","637","349","Saving energy for latency-critical applications like web search can be challenging because of their strict tail latency constraints. State-of-the-art power management frameworks use Dynamic Voltage and Frequency Scaling (DVFS) and Sleep states techniques to slow down the request processing and finish the search just-in-time. However, accurately predicting the compute demand of a request can be difficult. In this paper, we present Gemini, a novel power management framework for latency-critical search engines. Gemini has two unique features to capture the per query service time variation. First, at light loads without request queuing, a two-step DVFS is used to manage the CPU power. Our two-step DVFS selects the initial CPU frequency based on the query specific service time prediction and then judiciously boosts the initial frequency at the right time to catch-up to the deadline. The determination of boosting time further relies on estimating the error in the prediction of individual query's service time. At high loads, where there is request queuing, only the current request being executed and the critical request in the queue adopt a two-step DVFS. All the other requests in-between use the same frequency to reduce the frequency transition overhead. Second, we develop two separate neural network models, one for predicting the service time and the other for the error in the prediction. The combination of these two predictors significantly improves the power saving and tail latency results of our two-step DVFS. Gemini is implemented on the Solr search engine. Evaluations on three representative query traces show that Gemini saves 41% of the CPU power, and is better than other state-of-the-art techniques.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251983","power management;DVFS;search engine;neural network;service time prediction","Time-frequency analysis;Power system management;Query processing;Artificial neural networks;Search engines;Boosting;Web search","","14","","47","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"CuttleSys: Data-Driven Resource Management for Interactive Services on Reconfigurable Multicores","N. Kulkarni; G. Gonzalez-Pumariega; A. Khurana; C. A. Shoemaker; C. Delimitrou; D. H. Albonesi","Azure Hardware Architecture Microsoft, Redmond, WA, USA; Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; Industrial and Engineering Management, National University of Singapore, Singapore; Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","650","664","Multi-tenancy for latency-critical applications leads to resource interference and unpredictable performance. Core reconfiguration opens up more opportunities for application colocation, as it allows the hardware to adjust to the dynamic performance and power needs of a specific mix of co-scheduled services. However, reconfigurability also introduces challenges, as even for a small number of reconfigurable cores, exploring the design space becomes more time- and resource-demanding.We present CuttleSys, a runtime for reconfigurable multicores that leverages scalable and lightweight data mining to quickly identify suitable core and cache configurations for a set of co-scheduled applications. The runtime combines collaborative filtering to infer the behavior of each job on every core and cache configuration, with Dynamically Dimensioned Search to efficiently explore the configuration space. We evaluate CuttleSys on multicores with tens of reconfigurable cores and show up to 2.46× and 1.55× performance improvements compared to core-level gating and oracle-like asymmetric multicores respectively, under stringent power constraints.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251947","Heterogeneous architectures;datacenter;reconfigurable architectures;resource management","Runtime;Multicore processing;Quality of service;Reconstruction algorithms;Throughput;Space exploration;Resource management","","19","","104","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Jumanji: The Case for Dynamic NUCA in the Datacenter","B. C. Schwedock; N. Beckmann",Carnegie Mellon University; Carnegie Mellon University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","665","680","The datacenter introduces new challenges for computer systems around tail latency and security. This paper argues that dynamic NUCA techniques are a better solution to these challenges than prior cache designs. We show that dynamic NUCA designs can meet tail-latency deadlines with much less cache space than prior work, and that they also provide a natural defense against cache attacks. Unfortunately, prior dynamic NUCAs have missed these opportunities because they focus exclusively on reducing data movement.We present Jumanji, a dynamic NUCA technique designed for tail latency and security. We show that prior last-level cache designs are vulnerable to new attacks and offer imperfect performance isolation. Jumanji solves these problems while significantly improving performance of co-running batch applications. Moreover, Jumanji only requires lightweight hardware and a few simple changes to system software, similar to prior D-NUCAs. At 20 cores, Jumanji improves batch weighted speedup by 14% on average, vs. just 2% for a non-NUCA design with weaker security, and is within 2% of an idealized design.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251972","Multicore caching;non-uniform cache access (NUCA);tail latency;hardware security","Microarchitecture;Multicore processing;Heuristic algorithms;Software algorithms;Hardware;System software;Security","","8","","93","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks","S. Ghodrati; B. H. Ahn; J. Kyung Kim; S. Kinzer; B. R. Yatham; N. Alla; H. Sharma; M. Alian; E. Ebrahimi; N. S. Kim; C. Young; H. Esmaeilzadeh","Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Alternative Computing Technologies (ACT) Lab, University of California, San Diego; Bigstream Inc.; Kansas University; NVIDIA Research; University of Illinois Urbana-Champaign; Google Inc.; Alternative Computing Technologies (ACT) Lab, University of California, San Diego",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","681","697","Deep Neural Networks (DNNs) have reinvigorated real-world applications that rely on learning patterns of data and are permeating into different industries and markets. Cloud infrastructure and accelerators that offer INFerence-as-a-Service (INFaaS) have become the enabler of this rather quick and invasive shift in the industry. To that end, mostly accelerator-based INFaaS (Google's TPU [1], NVIDIA T4 [2], Microsoft Brainwave [3], etc.) has become the backbone of many real-life applications. However, as the demand for such services grows, merely scaling-out the number of accelerators is not economically cost-effective. Although multi-tenancy has propelled datacenter scalability, it has not been a primary factor in designing DNN accelerators due to the arms race for higher speed and efficiency. This paper sets out to explore this timely requirement of multi-tenancy through a new dimension: dynamic architecture fission. To that end, we define Planaria1 that can dynamically fission (break) into multiple smaller yet full-fledged DNN engines at runtime. This microarchitectural capability enables spatially co-locating multiple DNN inference services on the same hardware, offering simultaneous multi-tenant DNN acceleration. To realize this dynamic reconfigurability, we first devise breakable omni-directional systolic arrays for DNN acceleration that allows omni-directional flow of data. Second, it uses this capability and a unique organization of on-chip memory, interconnection, and compute resources to enable fission in systolic array based DNN accelerators. Architecture fission and its associated flexibility enables an extra degree of freedom for task scheduling, that even allows breaking the accelerator with regard to the server load, DNN topology, and task priority. As such, it can simultaneously co-locate DNNs to enhance utilization, throughput, QoS, and fairness. We compare the proposed design to PREMA [4], a recent effort that offers multi-tenancy by time-multiplexing the DNN accelerator across multiple tasks. We use the same frequency, the same amount of compute and memory resources for both accelerators. The results show significant benefits with (soft, medium, hard) QoS requirements, in throughput (7.4×, 7.2×, 12.2×), SLA satisfaction rate (45%, 15%, 16%), and fairness (2.1×, 2.3×, 1.9×).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00062","Google; Qualcomm; Microsoft; Xilinx; National Science Foundation; Air Force Office of Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251939","Accelerators;Deep Neural Networks;DNN;DNN Acceleration;Multi-Tenancy;Spatial DNN Task Co-Location;Multi-Tenant DNN Acceleration;Dynamic Architecture Fission;Omni-Directional Systolic Arrays","Industries;Microarchitecture;Computer architecture;Quality of service;Throughput;Acceleration;Task analysis","","66","","124","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"VR-DANN: Real-Time Video Recognition via Decoder-Assisted Neural Network Acceleration","Z. Song; F. Wu; X. Liu; J. Ke; N. Jing; X. Liang","Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","698","710","Nowadays, high-definition video object recognition (segmentation and detection) is not within the easy reach of a real-time task in a consumer SoC due to the limited on-chip computing power for neural network (NN) processing. Although many accelerators have been optimized heavily, they are still isolated from the intrinsic video compression expertise in a decoder. Given the fact that a great portion of frames can be dynamically reconstructed by a few key frames with high fidelity in a video, we envision that the recognition can also be reconstructed in a similar way so as to save a large amount of NN computing power. In this paper, we study the feasibility and efficiency of a novel decoder-assisted NN accelerator architecture for video recognition (VR-DANN) in a conventional SoC-styled design, which for the first time tightly couples the working principle of a video decoder with the NN accelerator to provide smooth high-definition video recognition experience. We leverage motion vectors, the simple tempo-spatial information already available in the decoding process to facilitate the recognition process, and propose a lightweight NN-based refinement scheme to suppress the non-pixel recognition noise. We also propose the corresponding microarchitecture design, which can be built upon any existing commercial IPs with minimal hardware overhead but significant speedup. Our experimental results show that the VR-DANN-parallel architecture achieves 2.9× performance improvement with less than 1% accuracy loss compared with the state-of-the-art ""FAVOS"" scheme widely used for video recognition. Compared with optical flow assisted ""DFF"" scheme, it can achieve 2.2× performance gain and 3% accuracy improvement. As to another ""Euphrates"" scheme, VR-DANN can achieve 40% performance gain and comparable accuracy.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00063","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251974","video decoder;motion vector;NN","Artificial neural networks;Streaming media;Video compression;Performance gain;Real-time systems;Decoding;Image reconstruction","","17","","48","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Procrustes: a Dataflow and Accelerator for Sparse Deep Neural Network Training","D. Yang; A. Ghasemazar; X. Ren; M. Golub; G. Lemieux; M. Lis",The University of British Columbia; The University of British Columbia; The University of British Columbia; Microsoft Corporation; The University of British Columbia; The University of British Columbia,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","711","724","The success of DNN pruning has led to the development of energy-efficient inference accelerators that support pruned models with sparse weight and activation tensors. Because the memory layouts and dataflows in these architectures are optimized for the access patterns during inference, however, they do not efficiently support the emerging sparse training techniques. In this paper, we demonstrate (a) that accelerating sparse training requires a co-design approach where algorithms are adapted to suit the constraints of hardware, and (b) that hardware for sparse DNN training must tackle constraints that do not arise in inference accelerators. As proof of concept, we adapt a sparse training algorithm to be amenable to hardware acceleration; we then develop dataflow, data layout, and load-balancing techniques to accelerate it. The resulting system is a sparse DNN training accelerator that produces pruned models with the same accuracy as dense models without first training, then pruning, and finally retraining, a dense model. Compared to training the equivalent unpruned models using a state-of-the-art DNN accelerator without sparse training support, Procrustes consumes up to 3.26× less energy and offers up to 4× speedup across a range of models, while pruning weights by an order of magnitude and maintaining unpruned accuracy.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00064","Air Force Research Laboratory; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251866","DNN acceleration;DNN training;deep neural networks;dataflow;load balancing","Training;Adaptation models;Layout;Data models;Inference algorithms;Acceleration;Load modeling","","50","","50","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Duplo: Lifting Redundant Memory Accesses of Deep Neural Networks for GPU Tensor Cores","H. Kim; S. Ahn; Y. Oh; B. Kim; W. W. Ro; W. J. Song","School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; EcoCloud, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Vaud, Switzerland; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","725","737","This paper introduces a GPU architecture named Duplo that minimizes redundant memory accesses of convolutions in deep neural networks (DNNs). Convolution is one of the fundamental operations used in various classes of DNNs, and it takes the majority of execution time. Various approaches have been proposed to accelerate convolutions via general matrix multiplication (GEMM), Winograd convolution, fast Fourier transform (FFT), etc. Recent introduction of tensor cores in NVIDIA GPUs particularly targets on accelerating neural network computations. A tensor core in a streaming multiprocessor (SM) is a specialized unit dedicated to handling matrix-multiply-and-accumulate (MMA) operations. The underlying operations of tensor cores represent GEMM calculations, and lowering a convolution can effectively exploit the tensor cores by transforming deeply nested convolution loops into matrix multiplication. However, lowering the convolution has a critical drawback since it requires a larger memory space (or workspace) to compute the matrix multiplication, where the expanded workspace inevitably creates multiple duplicates of the same data stored at different memory addresses. The proposed Duplo architecture tackles this challenge by leveraging compile-time information and microarchitectural supports to detect and eliminate redundant memory accesses that repeatedly load the duplicates of data in the workspace matrix. Duplo identifies data duplication based on memory addresses and convolution information generated by a compiler. It uses a load history buffer (LHB) to trace the recent load history of workspace data and their presence in register file. Every load instruction of workspace data refers to the LHB to find if potentially the same copies of data exist in the register file. If data duplicates are found, Duplo simply renames registers and makes them point to the ones containing the same values instead of issuing memory requests to load the same data. Our experiment results show that Duplo improves the performance of DNNs by 29.4% on average and saves 34.1% of energy using tensor cores.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00065","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251984","Deep Neural Network;GPU;Tensor Core","Tensors;Microarchitecture;Neural networks;Memory management;Graphics processing units;Registers;History","","21","","49","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"DUET: Boosting Deep Neural Network Efficiency on Dual-Module Architecture","L. Liu; Z. Qu; L. Deng; F. Tu; S. Li; X. Hu; Z. Gu; Y. Ding; Y. Xie",UC Santa Barbara; UC Santa Barbara; UC Santa Barbara; UC Santa Barbara; UC Santa Barbara; UC Santa Barbara; Alibaba DAMO Academy; UC Santa Barbara; UC Santa Barbara,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","738","750","Deep Neural Networks (DNNs) have been driving the mainstream of Machine Learning applications. However, deploying DNNs on modern hardware with stringent latency requirements and energy constraints is challenging because of the compute-intensive and memory-intensive execution patterns of various DNN models. We propose an algorithm-architecture co-design to boost DNN execution efficiency. Leveraging the noise resilience of nonlinear activation functions in DNNs, we propose dual-module processing that uses approximate modules learned from original DNN layers to compute insensitive activations. Therefore, we can save expensive computations and data accesses of unnecessary sensitive activations. We then design an Executor-Speculator dual-module architecture with support for balance execution and memory access reduction. With acceptable model inference quality degradation, our accelerator design can achieve 2.24x speedup and 1.97x energy efficiency improvement for compute-bound Convolutional Neural Networks (CNNs) and memory-bound Recurrent Neural Networks (RNNs).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251991","Neural networks;accelerator architecture","Recurrent neural networks;Computational modeling;Computer architecture;US Government;Approximation algorithms;Energy efficiency;Standards","","17","","55","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"TFE: Energy-efficient Transferred Filter-based Engine to Compress and Accelerate Convolutional Neural Networks","H. Mo; L. Liu; W. Hu; W. Zhu; Q. Li; A. Li; S. Yin; J. Chen; X. Jiang; S. Wei",Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Intel Corporation; Tsinghua University; Tsinghua University; Alibaba Group; Alibaba Group; Tsinghua University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","751","765","Although convolutional neural network (CNN) models have greatly enhanced the development of many fields, the untenable number of parameters and computations in these models yield significant performance and energy challenges in hardware implementations. Transferred filter-based methods, as very promising techniques that have not yet been explored in the architecture domain, can substantially compress CNN models. However, their straightforward hardware implementation inherently incurs massive redundant computations, causing significant energy and time consumption. In this work, a highly efficient transferred filter-based engine (TFE) is developed to alleviate this deficiency, with CNN models compressed and accelerated. First, the filters of CNN models are flexibly transferred according to specific tasks to reduce the model size. Then, two hardware-friendly mechanisms are proposed in the TFE to remove duplicate computations caused by transferred filters, which can further accelerate transferred CNN models. The first mechanism exploits the shared weights hidden in each row of transferred filters and reuses the corresponding same partial sums, reducing at least 25% of repetitive computations in each row. The second mechanism can intelligently schedule and access the memory system to reuse the repetitive partial sums among different rows of the transferred filters with at least 25% of computations eliminated. Furthermore, an efficient hardware architecture is proposed in the TFE to fully reap the benefits of the two proposed mechanisms such that different types of networks are flexibly supported. To achieve high energy efficiency, the sub-array-based filter mapping method (SAFM) is proposed, where the process element (PE) subarray is used as the elementary computational unit to support various filters. Therein, input data can be efficiently broadcast in each PE sub-array and the load can be stripped from each PE and intensively alleviated, which can dramatically reduce the area and power consumption. Excluding MobileNet-like networks that adopt depth-wise convolution, most mainstream networks can be compressed and accelerated by the proposed TFE. Two state-of-the-art transferred filter-based methods, i.e., doubly CNN and symmetry CNN are implemented by exploiting the TFE. Compared with Eyeriss, average speedup improvements of 2.93× and 3.17× are achieved in the convolutional layers of various modern CNNs. The overall energy efficiency can be improved by 12.66× and 13.31× on average. Compared with other state-of-the-art related works, the TFE can maximally achieve a parameter reduction of 4.0×, a speedup of 2.72× and an energy efficiency improvement of 10.74× on VGGNet.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00067","National Natural Science Foundation of China; Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251980","","Power demand;Computational modeling;Memory management;Filtering algorithms;Energy efficiency;Hardware;Acceleration","","6","","57","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product","N. Srivastava; H. Jin; J. Liu; D. Albonesi; Z. Zhang","School of ECE, Cornell University; School of ECE, Cornell University; School of ECE, Cornell University; School of ECE, Cornell University; School of ECE, Cornell University",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","766","780","Sparse-sparse matrix multiplication (SpGEMM) is a computation kernel widely used in numerous application domains such as data analytics, graph processing, and scientific computing. In this work we propose MatRaptor, a novel SpGEMM accelerator that is high performance and highly resource efficient. Unlike conventional methods using inner or outer product as the meta operation for matrix multiplication, our approach is based on row-wise product, which offers a better tradeoff in terms of data reuse and on-chip memory requirements, and achieves higher performance for large sparse matrices. We further propose a new hardware-friendly sparse storage format, which allows parallel compute engines to access the sparse data in a vectorized and streaming fashion, leading to high utilization of memory bandwidth. We prototype and simulate our accelerator architecture using gem5 on a diverse set of matrices. Our experiments show that MatRaptor achieves 129.2× speedup over single-threaded CPU, 8.8× speedup over GPU and 1.8× speedup over the state-of-the-art SpGEMM accelerator (OuterSPACE). MatRaptor also has 7.2× lower power consumption and 31.3× smaller area compared to OuterSPACE.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251978","sparse matrix multiplication;sparse formats;spatial hardware","Power demand;Prototypes;Graphics processing units;Bandwidth;Hardware;System-on-chip;Sparse matrices","","135","","65","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"TensorDash: Exploiting Sparsity to Accelerate Deep Neural Network Training","M. Mahmoud; I. Edo; A. H. Zadeh; O. Mohamed Awad; G. Pekhimenko; J. Albericio; A. Moshovos","University of Toronto; University of Toronto; University of Toronto; University of Toronto; Vector Institute, University of Toronto; Cerebras Systems; Vector Institute, University of Toronto",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","781","795","TensorDash is a hardware-based technique that enables data-parallel MAC units to take advantage of sparsity in their input operand streams. When used to compose a hardware accelerator for deep learning, TensorDash can speedup the training process while also increasing energy efficiency. TensorDash combines a low-cost sparse input operand interconnect with an area-efficient hardware scheduler. The scheduler can effectively extract sparsity in the activations, the weights, and the gradients. Over a wide set of state-of-the-art models covering various applications, TensorDash accelerates the training process by 1.95× while being 1.5× more energy efficient when incorporated on top of a Tensorcore-based accelerator at less than 5% area overhead. TensorDash is datatype agnostic and we demonstrate it with IEEE standard mixed-precision floating-point units and a popular optimized for machine learning floating-point format (BFloat16).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251995","","Training;Tensors;Neural networks;Hardware;Software;Energy efficiency;Acceleration","","56","","37","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"SAVE: Sparsity-Aware Vector Engine for Accelerating DNN Training and Inference on CPUs","Z. Gong; H. Ji; C. W. Fletcher; C. J. Hughes; S. Baghsorkhi; J. Torrellas","University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; Intel Labs; Intel Labs; University of Illinois, Urbana-Champaign",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","796","810","General Matrix Multiplication (GEMM) is the key operation in Deep Neural Networks (DNNs). While dense GEMM uses SIMD CPUs efficiently, sparse GEMM is much less efficient, especially at the modest levels of unstructured sparsity common in DNN inference/training. Thus, most DNNs use dense GEMM.In this paper, we propose SAVE, a novel vector engine for CPUs that efficiently skips ineffectual computation due to sparsity in dense DNN implementations. SAVE's hardware extensions to the vector pipeline are transparent to software. SAVE accelerates FP32 and mixed-precision kernels with unstructured sparsity from both weights and activations. Further, SAVE is not DNN-specific and can potentially speed-up any vector workload with sparsity. To evaluate SAVE, we use simulations of a 28-core machine and run VGG16, ResNet-50, and GNMT, with and without pruning. With realistic sparsity, SAVE accelerates inference by 1.37x-1.68x and end-to-end training by 1.28x-1.64x.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251932","deep neural network;CPU;training;inference;sparsity;vector processing unit","Training;Pipelines;Software;Acceleration;Sparse matrices;Engines;Optimization","","25","","69","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference","A. H. Zadeh; I. Edo; O. M. Awad; A. Moshovos",University of Toronto; University of Toronto; University of Toronto; University of Toronto,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","811","824","Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (i) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (ii) replaces most multiply-accumulations with additions, and (iii) reduces the off-chip traffic by amplifying on-chip memory capacity.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251854","","Quantization (signal);Tensors;Computational modeling;Memory management;Hardware;Energy efficiency;Task analysis","","87","","41","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"TrainBox: An Extreme-Scale Neural Network Training Server Architecture by Systematically Balancing Operations","P. Park; H. Jeong; J. Kim","Department of Electrical and Computer Engineering, Seoul National University; Department of Electrical and Computer Engineering, Seoul National University; Department of Electrical and Computer Engineering, Seoul National University",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","825","838","Neural network is a major driving force of another golden age of computing; the computer architects have proposed specialized accelerators (e.g., TPU), high-speed interconnects (e.g., NVLink), and algorithms (e.g., ring-based reduction) to efficiently support neural network applications. As a result, they achieve orders of magnitude higher efficiency for neural network computation and inter-accelerator communication over traditional computing platforms.In this paper, we identify that the emerging platforms have shifted the performance bottleneck of neural network from model computation and inter-accelerator communication to data preparation. Although overlapping data preparation and the others has hidden the preparation overhead, the higher input processing demands of emerging platforms start to reverse the situation; at scale, data preparation requires an infeasible amount of the host-side CPU, memory, and PCIe resources. Our detailed analysis reveals that this heavy resource consumption comes from data transformation for neural network specific formats, and buffering for communication among devices.Therefore, we propose a scalable neural network server architecture by balancing data preparation and the others. To achieve extreme scalability, our design relies on a scalable device array, rather than the limited host resources, with three key ideas. First, we offload CPU-intensive operations to the customized data preparation accelerators to scale the training performance regardless of the host-side CPU performance. Second, we apply direct inter-device communication to eliminate unnecessary data copies and reduce the pressure on the host memory. Lastly, we cluster underlying devices considering unique communication patterns of the neural network processing and interconnect characteristics to efficiently utilize aggregated interconnect band-width. Our evaluation shows that the proposed architecture achieves 44.4× higher training throughput on average over a naively extended server architecture with 256 neural network accelerators.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00072","Samsung; National Research Foundation of Korea; Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251955","Neural network;Training;Scalability;Data preparation;Server architecture","Training;Performance evaluation;Scalability;Neural networks;Computational efficiency;Servers;Synchronization","","8","","51","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Coordinated Priority-aware Charging of Distributed Batteries in Oversubscribed Data Centers","S. Malla; Q. Deng; Z. Ebrahimzadeh; J. Gasperetti; S. Jain; P. Kondety; T. Ortiz; D. Vieira","Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","839","851","Data centers employ batteries for uninterruptible operation during maintenance and power failures, for example, when switching to diesel generator power after a utility power failure. Depleted batteries start to recharge once the input power is back, creating a sudden power spike in the power hierarchy. If not properly controlled, a sustained power overload can potentially trip circuit breakers, leading to service outages. Power overloads due to battery recharging are even more likely in oversubscribed data centers where the power infrastructure is aggressively provisioned for high utilization. The problem caused by simultaneous recharging of batteries in a data center has not been extensively studied and no real-world solutions have been proposed in the literature.In this paper, we identify the problem due to battery recharging with case studies from Facebook's data centers. We describe the solutions we have developed to coordinate charging of batteries without exceeding the circuit breaker power limit. We explain in detail, the variable battery charging algorithm built into the distributed battery charger hardware deployed in Facebook data centers, and the system design considerations necessary on a large scale. The new variable charger is able to reduce battery recharge power by up to 80%. We further leverage individual battery charging control mechanism to coordinate the charging process such that we charge the batteries according to priorities of applications running on the servers supported by the batteries. We evaluate our coordinated priority-aware battery charging algorithm by building a prototype in a Facebook production data center as well as through simulation experiments using production power traces. Our results show that we are able to meet reliability service level agreements by using our battery recharging algorithm, while satisfying given power constraints.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252004","","Battery chargers;Data centers;Social networking (online);Circuit breakers;Production;Batteries;Servers","","6","","50","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"HyperPlane: A Scalable Low-Latency Notification Accelerator for Software Data Planes","A. Mirhosseini; H. Golestani; T. F. Wenisch",University of Michigan; University of Michigan; University of Michigan,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","852","867","I/O software stacks have evolved rapidly due to the growing speed of I/O devices-including network adapters, storage devices, and accelerators-and the emergence of microservice-based programming models. Datacenters rely on fast, efficient Software Data Planes (SDPs), which orchestrate data transfer between applications and I/O devices. Modern data planes are user-level software stacks, wherein cores spin-poll a large number of queues to avoid the attendant overheads of kernel-based I/O. Cores often poll empty queues before finding work in non-empty ones. Interrogating empty queues hurts peak throughput, tail latency, and energy efficiency as it often entails fruitless cache misses. In this work, we propose HyperPlane, an efficient accelerator for the notification mechanism of SDPs. The key features of HyperPlane are (1) avoiding iteration over empty I/O queues, unlike software-only designs, resulting in queue scalability, (2) halting execution when I/O queues are idle, leading to work proportionality and energy efficiency, and (3) efficiently sharing queues across cores to enjoy strong theoretical properties of scale-up queuing. HyperPlane is realized through a hardware subsystem associated with a familiar programming model. HyperPlane's microarchitecture consists of a monitoring set that watches for work arrival from I/O, and a ready set, which tracks ready queues and distributes work to cores based on various service policies and priority levels. We show that HyperPlane improves peak throughput by 4.1× and tail latency by 16.4× compared to a state-of-the-art SDP.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251997","software data plane;acceleration;spin-polling;notification","Microarchitecture;Scalability;Programming;Throughput;Software;Queueing analysis;Monitoring","","8","","101","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"ThymesisFlow: A Software-Defined, HW/SW co-Designed Interconnect Stack for Rack-Scale Memory Disaggregation","C. Pinto; D. Syrivelis; M. Gazzetti; P. Koutsovasilis; A. Reale; K. Katrinis; H. P. Hofstee","IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; IBM Systems, Austin, TX, United States",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","868","880","With cloud providers constantly seeking the best infrastructure trade-off between performance delivered to customers and overall energy/utilization efficiency of their data-centres, hardware disaggregation comes in as a new paradigm for dynamically adapting the data-centre infrastructure to the characteristics of the running workloads. Such an adaptation enables an unprecedented level of efficiency both from the standpoint of energy and the utilization of system resources. In this paper, we present - ThymesisFlow - the first, to our knowledge, full-stack prototype of the holy-grail of disaggregation of compute resources: pooling of remote system memory. Thymesis-Flow implements a HW/SW co-designed memory disaggregation interconnect on top of the POWER9 architecture, by directly interfacing the memory bus via the OpenCAPI port. We use ThymesisFlow to evaluate how disaggregated memory impacts a set of cloud workloads, and we show that for many of them the performance degradation is negligible. For those cases that are severely impacted, we offer insights on the underlying causes and viable cross-stack mitigation paths.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252003","Disaggregation;OpenCAPI;POWER9","Runtime;Microarchitecture;Switched mode power supplies;Prototypes;Hardware;Transceivers;Servers","","48","","70","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"A Benchmarking Framework for Interactive 3D Applications in the Cloud","T. Liu; S. He; S. Huang; D. Tsang; L. Tang; J. Mars; W. Wang","The University of Texas at San Antonio; The University of Texas at San Antonio; The University of Texas at San Antonio; The University of Texas at San Antonio; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; The University of Texas at San Antonio",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","881","894","With the growing popularity of cloud gaming and cloud virtual reality (VR), interactive 3D applications have become a major class of workloads for the cloud. However, despite their growing importance, there is limited public research on how to design cloud systems to efficiently support these applications due to the lack of an open and reliable research infrastructure, including benchmarks and performance analysis tools. The challenges of generating human-like inputs under various system/application nondeterminism and dissecting the performance of complex graphics systems make it very difficult to design such an infrastructure. In this paper, we present the design of a novel research infrastructure, Pictor, for cloud 3D applications and systems. Pictor employs AI to mimic human interactions with complex 3D applications. It can also track the processing of user inputs to provide in-depth performance measurements for the complex software and hardware stack used for cloud 3D-graphics rendering. With Pictor, we designed a benchmark suite with six interactive 3D applications. Performance analyses were conducted with these benchmarks, which show that cloud system designs, including both system software and hardware designs, are crucial to the performance of cloud 3D applications. The analyses also show that energy consumption can be reduced by at least 37% when two 3D applications share a could server. To demonstrate the effectiveness of Pictor, we also implemented two optimizations to address two performance bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering system. These two optimizations improved the frame rate by 57.7% on average.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251945","Cloud Computing;Cloud Gaming;Cloud Gaming Benchmarks;Cloud Gaming Performance Analysis;Cloud Graphics Systems","Measurement;Cloud computing;Three-dimensional displays;Benchmark testing;Rendering (computer graphics);Performance analysis;Optimization","","11","","86","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"A Locality-Aware Energy-Efficient Accelerator for Graph Mining Applications","P. Yao; L. Zheng; Z. Zeng; Y. Huang; C. Gui; X. Liao; H. Jin; J. Xue","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Engineering, UNSW Sydney, Australia",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","895","907","Graph mining is becoming increasingly important due to the ever-increasing demands on analyzing complex structures in graphs. Existing graph accelerators typically hold most of the randomly-accessed data in an on-chip memory to avoid off-chip communications. However, graph mining exhibits substantial random accesses from not only vertex dimension but also edge dimension (with the latter being excessively more complex than the former), leading to significant degradations in terms of both performance and energy efficiency.We observe that the most random memory requests arising in graph mining come from accessing a small fraction of valuable (vertex and edge) data when handling real-world graphs. To exploit this extension locality with maximum parallelism, we architect GRAMER, the first graph mining accelerator. GRAMER contains a specialized memory hierarchy, where the valuable data (precisely identified through a cost-efficient heuristic) is permanently resident in a high-priority memory while others are maintained in a cache-like memory under a lightweight replacement policy. The specific pipelined processing units are carefully designed to maximize computational parallelism. GRAMER is also equipped with a work-stealing mechanism to reduce load imbalance. We have implemented GRAMER on a Xilinx Alveo U250 accelerator card. Compared with two state-of-the-art CPU-based graph mining systems, Fractal and RStream, running on a 14-core Intel E5-2680 v4 processor, GRAMER achieves not only considerable speedups (1.11 × ~ 129.95 ) but also significant energy savings (5.79 × ~ 678.34×)","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00077","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251861","graph mining;extension locality;accelerator","Memory management;Parallel processing;Feature extraction;Energy efficiency;Fractals;System-on-chip;Data mining","","35","","51","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing","S. Rahman; N. Abu-Ghazaleh; R. Gupta","Computer Science and Engineering, University of California, Riverside, Riverside, USA; Computer Science and Engineering, University of California, Riverside, Riverside, USA; Computer Science and Engineering, University of California, Riverside, Riverside, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","908","921","Graph processing workloads are memory intensive with irregular access patterns and large memory footprint resulting in low data locality. Their popular software implementations typically employ either Push or Pull style propagation of changes through the graph over multiple iterations that follow the Bulk Synchronous Model. The performance of these algorithms on traditional computing systems is limited by random reads/writes of vertex values, synchronization overheads, and additional overheads for tracking active sets of vertices or edges across iterations. In this paper, we present GraphPulse, a hardware framework for asynchronous graph processing with event-driven scheduling that overcomes the performance limitations of software frameworks. Event-driven computation model enables a parallel dataflow-style execution where atomic updates and active sets tracking are inherent to the model; thus, scheduling complexity is reduced and scalability is enhanced. The dataflow nature of the architecture also reduces random reads of vertex values by carrying the values in the events themselves. We capitalize on the update properties commonly present in graph algorithms to coalesce in-flight events and substantially reduce the event storage requirement and the processing overheads incurred. GraphPulse event-model naturally supports asynchronous graph processing, enabling substantially faster convergence by exploiting available parallelism, reducing work, and eliminating synchronization at iteration boundaries. The framework provides easy to use programming interface for faster development of hardware graph accelerators. A single GraphPulse accelerator achieves up to 74x speedup (28x on average) over Ligra, a state of the art software framework, running on a 12 core CPU. It also achieves an average of 6.2x speedup over Graphicionado, a state of the art graph processing accelerator.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00078","University of California; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251946","Graph Processing;Hardware Accelerator;Event-driven Model;Domain-specific Architecture","Processor scheduling;Computational modeling;Software algorithms;Computer architecture;Software;Hardware;Synchronization","","48","","66","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing","T. Geng; A. Li; R. Shi; C. Wu; T. Wang; Y. Li; P. Haghi; A. Tumeo; S. Che; S. Reinhardt; M. C. Herbordt",Boston University; Pacific Northwest National Laboratory; The University of Hong Kong; Boston University; Boston University; Zhejiang University; Boston University; Pacific Northwest National Laboratory; Microsoft; Microsoft; Boston University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","922","936","Deep learning systems have been successfully applied to Euclidean data such as images, video, and audio. In many applications, however, information and their relationships are better expressed with graphs. Graph Convolutional Networks (GCNs) appear to be a promising approach to efficiently learn from graph data structures, having shown advantages in many critical applications. As with other deep learning modalities, hardware acceleration is critical. The challenge is that real-world graphs are often extremely large and unbalanced; this poses significant performance demands and design challenges. In this paper, we propose Autotuning-Workload-Balancing GCN (AWB-GCN) to accelerate GCN inference. To address the issue of workload imbalance in processing real-world graphs, three hardware-based autotuning techniques are proposed: dynamic distribution smoothing, remote switching, and row remapping. In particular, AWB-GCN continuously monitors the sparse graph pattern, dynamically adjusts the workload distribution among a large number of processing elements (up to 4K PEs), and, after converging, reuses the ideal configuration. Evaluation is performed using an Intel D5005 FPGA with five commonly-used datasets. Results show that 4K-PE AWB-GCN can significantly elevate PE utilization by 7.7× on average and demonstrate considerable performance speedups over CPUs (3255×), GPUs (80.3×), and a prior GCN accelerator (5.1×).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00079","Pacific Northwest National Laboratory; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252000","Graph Neural Network;Graph Convolutional Network;Sparse Matrix Multiplication;Computer Architecture;Machine Learning Accelerator;Dynamic Scheduling","Deep learning;Smoothing methods;Runtime;Switches;Acceleration;Sparse matrices;Field programmable gate arrays","","212","","67","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"SeedEx: A Genome Sequencing Accelerator for Optimal Alignments in Subminimal Space","D. Fujiki; S. Wu; N. Ozog; K. Goliya; D. Blaauw; S. Narayanasamy; R. Das",University of Michigan; University of Michigan; University of Michigan; University of Michigan; University of Michigan; University of Michigan; University of Michigan,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","937","950","Innovations in genome sequencing techniques are enabling remarkably fast and low cost production of raw genome data. As Moore's law tapers off, bottlenecks in genome sequencing are shifting to computational resources for mapping reads to reference DNA. This paper presents SeedEx, a read-alignment accelerator focused on the seed-extension step. SeedEx is based on the observation that only a small fraction of reads require large edit distance for alignment, hence an area efficient narrow-band seed-extension accelerator can suffice in practice. However, due to the highly error-sensitive nature of genomic workloads, guaranteeing optimality of alignment result is of cardinal importance. Towards this end, we propose a speculation-and-test based framework by using strict but powerful optimality checking mechanisms. We demonstrate SeedEx by an implementation on a cloud FPGA. SeedEx achieves 6.0× iso-area throughput speedup when compared to a banded Smith-Waterman baseline, and achieving 43.9 M seed extentions/s on AWS f1.2xlarge instance. Integration with BWA-MEM2 improves the execution time by 2.3×.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251992","genome sequencing;accelerator;FPGA","Sequential analysis;Technological innovation;Genomics;Throughput;Software;Bioinformatics;Field programmable gate arrays","","28","","50","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"GenASM: A High-Performance, Low-Power Approximate String Matching Acceleration Framework for Genome Sequence Analysis","D. S. Cali; G. S. Kalsi; Z. Bingöl; C. Firtina; L. Subramanian; J. S. Kim; R. Ausavarungnirun; M. Alser; J. Gomez-Luna; A. Boroumand; A. Norion; A. Scibisz; S. Subramoneyon; C. Alkan; S. Ghose; O. Mutlu","Carnegie Mellon University; Processor Architecture Research Lab, Intel Labs; Bilkent University; ETH Zürich; Facebook; ETH Zürich; King Mongkut’s University of Technology North Bangkok; ETH Zürich; ETH Zürich; Carnegie Mellon University; Processor Architecture Research Lab, Intel Labs; Carnegie Mellon University; Processor Architecture Research Lab, Intel Labs; Bilkent University; University of Illinois at Urbana–Champaign; ETH Zürich",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","951","966","Genome sequence analysis has enabled significant advancements in medical and scientific areas such as personalized medicine, outbreak tracing, and the understanding of evolution. To perform genome sequencing, devices extract small random fragments of an organism's DNA sequence (known as reads). The first step of genome sequence analysis is a computational process known as read mapping. In read mapping, each fragment is matched to its potential location in the reference genome with the goal of identifying the original location of each read in the genome. Unfortunately, rapid genome sequencing is currently bottlenecked by the computational power and memory bandwidth limitations of existing systems, as many of the steps in genome sequence analysis must process a large amount of data. A major contributor to this bottleneck is approximate string matching (ASM), which is used at multiple points during the mapping process. ASM enables read mapping to account for sequencing errors and genetic variations in the reads. We propose GenASM, the first ASM acceleration framework for genome sequence analysis. GenASM performs bitvectorbased ASM, which can efficiently accelerate multiple steps of genome sequence analysis. We modify the underlying ASM algorithm (Bitap) to significantly increase its parallelism and reduce its memory footprint. Using this modified algorithm, we design the first hardware accelerator for Bitap. Our hardware accelerator consists of specialized systolic-array-based compute units and on-chip SRAMs that are designed to match the rate of computation with memory capacity and bandwidth, resulting in an efficient design whose performance scales linearly as we increase the number of compute units working in parallel. We demonstrate that GenASM provides significant performance and power benefits for three different use cases in genome sequence analysis. First, GenASM accelerates read alignment for both long reads and short reads. For long reads, GenASM outperforms state-of-the-art software and hardware accelerators by 116× and 3.9×, respectively, while reducing power consumption by 37× and 2.7×. For short reads, GenASM outperforms state-of-the-art software and hardware accelerators by 111× and 1.9×. Second, GenASM accelerates pre-alignment filtering for short reads, with 3.7× the performance of a state-of-the-art pre-alignment filter, while reducing power consumption by 1.7× and significantly improving the filtering accuracy. Third, GenASM accelerates edit distance calculation, with 22-12501× and 9.3-400× speedups over the state-of-the-art software library and FPGA-based accelerator, respectively, while reducing power consumption by 548-582× and 67×. We conclude that GenASM is a flexible, high-performance, and low-power framework, and we briefly discuss four other use cases that can benefit from GenASM.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251930","","Sequential analysis;Sequences;Genomics;Hardware;Software;Acceleration;Bioinformatics","","67","","182","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Selective Replication in Memory-Side GPU Caches","X. Zhao; M. Jahre; L. Eeckhout","Academy of Military Science, China; Norwegian University of Science and Technology, Norway; Ghent University, Belgium",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","967","980","Data-intensive applications put immense strain on the memory systems of Graphics Processing Units (GPUs). To cater to this need, GPU memory systems distribute requests across independent units to provide high bandwidth by servicing requests (mostly) in parallel. We find that this strategy breaks down for shared data structures because the shared Last-Level Cache (LLC) organization used by contemporary GPUs stores shared data in a single LLC slice. Shared data requests are hence serialized - resulting in data-intensive applications not being provided with the bandwidth they require. A private LLC organization can provide high bandwidth, but it is often undesirable since it significantly reduces the effective LLC capacity. In this work, we propose the Selective Replication (SelRep) LLC which selectively replicates shared read-only data across LLC slices to improve bandwidth supply while ensuring that the LLC retains sufficient capacity to keep shared data cached. The compile-time component of SelRep LLC uses dataflow analysis to identify read-only shared data structures and uses a special-purpose load instruction for these accesses. The runtime component of SelRep LLC then monitors the caching behavior of these loads. Leveraging an analytical model, SelRep LLC chooses a replication degree that carefully balances the effective LLC bandwidth benefits of replication against its capacity cost. SelRep LLC consistently provides high performance to replication-sensitive applications across different data set sizes. More specifically, SelRep LLC improves performance by 19.7% and 11.1% on average (and up to 61.6% and 31.0%) compared to the shared LLC baseline and the state-of-the-art Adaptive LLC, respectively.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00082","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251999","GPGPU;Caches;Selective Replication","Runtime;Graphics processing units;Bandwidth;Organizations;Data structures;Monitoring;Strain","","6","","78","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Deterministic Atomic Buffering","Y. H. Chou; C. Ng; S. Cattell; J. Intan; M. D. Sinclair; J. Devietti; T. G. Rogers; T. M. Aamodt",University of British Columbia; University of British Columbia; University of British Columbia; University of Wisconsin; University of Wisconsin; University of Pennsylvania; Purdue University; University of British Columbia,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","981","995","Deterministic execution for GPUs is a desirable property as it helps with debuggability and reproducibility. It is also important for safety regulations, as safety critical workloads are starting to be deployed onto GPUs. Prior deterministic architectures, such as GPUDet, attempt to provide strong determinism for all types of workloads, incurring significant performance overheads due to the many restrictions that are required to satisfy determinism. We observe that a class of reduction workloads, such as graph applications and neural architecture search for machine learning, do not require such severe restrictions to preserve determinism. This motivates the design of our system, Deterministic Atomic Buffering (DAB), which provides deterministic execution with low area and performance overheads by focusing solely on ordering atomic instructions instead of all memory instructions. By scheduling atomic instructions deterministically with atomic buffering, the results of atomic operations are isolated initially and made visible in the future in a deterministic order. This allows the GPU to execute deterministically in parallel without having to serialize its threads for atomic operations as opposed to GPUDet. Our simulation results show that, for atomic-intensive applications, DAB performs 4× better than GPUDet and incurs only a 23% slowdown on average compared to a non-deterministic GPU architecture. We also characterize the bottlenecks and provide insights for future optimizations.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251962","GPU architecture;determinism;performance;parallel programming","Simulation;Semantics;Graphics processing units;Machine learning;Programming;Regulation;Safety","","6","","98","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"BOW: Breathing Operand Windows to Exploit Bypassing in GPUs","H. A. Esfeden; A. Abdolrashidi; S. Rahman; D. Wong; N. Abu-Ghazaleh","Department of Computer Science and Engineering, University of California Riverside; Department of Computer Science and Engineering, University of California Riverside; Department of Computer Science and Engineering, University of California Riverside; Department of Electrical and Computer Engineering, University of California Riverside; Department of Computer Science and Engineering, University of California Riverside",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","996","1008","The Register File (RF) is a critical structure in Graphics Processing Units (GPUs) responsible for a large portion of the area and power. To simplify the architecture of the RF, it is organized in a multi-bank configuration with a single port for each bank. Not surprisingly, the frequent accesses to the register file during kernel execution incur a sizeable overhead in GPU power consumption, and introduce delays as accesses are serialized when port conflicts occur. In this paper, we observe that there is a high degree of temporal locality in accesses to the registers: within short instruction windows, the same registers are often accessed repeatedly. We characterize the opportunities to reduce register accesses as a function of the size of the instruction window considered, and establish that there are many recurring reads and updates of the same register operands in most GPU computations. To exploit this opportunity, we propose Breathing Operand Windows (BOW), an enhanced GPU pipeline and operand collector organization that supports bypassing register file accesses and instead passes values directly between instructions within the same window. Our baseline design can only bypass register reads; we introduce an improved design capable of also bypassing unnecessary write operations to the RF. We introduce compiler optimizations to help guide the write-back destination of operands depending on whether they will be reused to further reduce the write traffic. To reduce the storage overhead, we analyze the occupancy of the bypass buffers and discover that we can significantly down size them without losing performance. BOW along with optimizations reduces dynamic energy consumption of the register file by 55% and increases the performance by 11%, with a modest overhead of 12KB increase in the size of the operand collectors (4% of the register file size).","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00084","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251966","operand bypassing;GPU;register file;microarchitecture;compiler","Radio frequency;Power demand;Pipelines;Graphics processing units;Organizations;Registers;Optimization","","14","","63","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"MDM: The GPU Memory Divergence Model","L. Wang; M. Jahre; A. Adileho; L. Eeckhout","Ghent University, Belgium; Norwegian University of Science and Technology; Huawei, Toga Networks; Ghent University, Belgium",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1009","1021","Analytical models enable architects to carry out early-stage design space exploration several orders of magnitude faster than cycle-accurate simulation by capturing first-order performance phenomena with a set of mathematical equations. However, this speed advantage is void if the conclusions obtained through the model are misleading due to model inaccuracies. Therefore, a practical analytical model needs to be sufficiently accurate to capture key performance trends across a broad range of applications and architectural configurations.In this work, we focus on analytically modeling the performance of emerging memory-divergent GPU-compute applications which are common in domains such as machine learning and data analytics. The poor spatial locality of these applications leads to frequent L1 cache blocking due to the application issuing significantly more concurrent cache misses than the cache can support, which cripples the GPU’s ability to use Thread-Level Parallelism (TLP) to hide memory latencies. We propose the GPU Memory Divergence Model (MDM) which faithfully captures the key performance characteristics of memory-divergent applications, including memory request batching and excessive NoC/DRAM queueing delays. We validate MDM against detailed simulation and real hardware, and report substantial improvements in (1) scope: the ability to model prevalent memory-divergent applications in addition to non-memory divergent applications; (2) practicality: 6.1× faster by computing model inputs using binary instrumentation as opposed to functional simulation; and (3) accuracy: 13.9% average prediction error versus 162% for the state-of-the-art GPUMech model.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00085","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251853","","Analytical models;Computational modeling;Graphics processing units;Predictive models;Hardware;Space exploration;Mathematical model","","11","","38","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Locality-Centric Data and Threadblock Management for Massive GPUs","M. Khairy; V. Nikiforov; D. Nellans; T. G. Rogers",Purdue University; Purdue University; NVIDIA; Purdue University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1022","1036","Recent work has shown that building GPUs with hundreds of SMs in a single monolithic chip will not be practical due to slowing growth in transistor density, low chip yields, and photoreticle limitations. To maintain performance scalability, proposals exist to aggregate discrete GPUs into a larger virtual GPU and decompose a single GPU into multiple-chip-modules with increased aggregate die area. These approaches introduce non-uniform memory access (NUMA) effects and lead to decreased performance and energy-efficiency if not managed appropriately. To overcome these effects, we propose a holistic Locality-Aware Data Management (LADM) system designed to operate on massive logical GPUs composed of multiple discrete devices, which are themselves composed of chiplets. LADM has three key components: a threadblock-centric index analysis, a runtime system that performs data placement and threadblock scheduling, and an adaptive cache insertion policy. The runtime combines information from the static analysis with topology information to proactively optimize data placement, threadblock scheduling, and remote data caching, minimizing off-chip traffic. Compared to state-of-the-art multi-GPU scheduling, LADM reduces inter-chip memory traffic by 4× and improves system performance by 1.8× on a future multi-GPU system.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251964","GPGPU;Multi-GPU;NUMA","Performance evaluation;Runtime;Instruction sets;System performance;Graphics processing units;Static analysis;Parallel processing","","23","","85","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Mesorasi: Architecture Support for Point Cloud Analytics via Delayed-Aggregation","Y. Feng; B. Tian; T. Xu; P. Whatmough; Y. Zhu",University of Rochester; University of Rochester; University of Rochester; Arm Research; University of Rochester,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1037","1050","Point cloud analytics is poised to become a key workload on battery-powered embedded and mobile platforms in a wide range of emerging application domains, such as autonomous driving, robotics, and augmented reality, where efficiency is paramount. This paper proposes Mesorasi, an algorithm-architecture co-designed system that simultaneously improves the performance and energy efficiency of point cloud analytics while retaining its accuracy.Our extensive characterizations of state-of-the-art point cloud algorithms show that, while structurally reminiscent of convolutional neural networks (CNNs), point cloud algorithms exhibit inherent compute and memory inefficiencies due to the unique characteristics of point cloud data. We propose delayed-aggregation, a new algorithmic primitive for building efficient point cloud algorithms. Delayed-aggregation hides the performance bottlenecks and reduces the compute and memory redundancies by exploiting the approximately distributive property of key operations in point cloud algorithms. Delayed-aggregation let point cloud algorithms achieve 1.6× speedup and 51.1% energy reduction on a mobile GPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the algorithmic benefits, we propose minor extensions to contemporary CNN accelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC) without modifying other SoC components. With additional hardware support, Mesorasi achieves up to 3.6× speedup.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251968","Point cloud;DNN;accelerator","Three-dimensional displays;Systematics;Transforms;Approximation algorithms;Robot sensing systems;Hardware;Sensors","","57","","67","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"FlexWatts: A Power- and Workload-Aware Hybrid Power Delivery Network for Energy-Efficient Microprocessors","J. Haj-Yahya; M. Alser; J. S. Kim; L. Orosa; E. Rotem; A. Mendelson; A. Chattopadhyay; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; Intel; Technion; Nanyang Technological University; ETH Zürich,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1051","1066","Modern client processors typically use one of three commonly-used power delivery network (PDN) architectures: 1) motherboard voltage regulators (MBVR), 2) integrated voltage regulators (IVR), and 3) low dropout voltage regulators (LDO). We observe that the energy-efficiency of each of these PDNs varies with the processor power (e.g, thermal design power (TDP) and dynamic power-state) and workload characteristics (e.g., work-load type and computational intensity). This leads to energy-inefficiency and performance loss, as modern client processors operate across a wide spectrum of power consumption and execute a wide variety of workloads. To address this inefficiency, we propose FlexWatts, a hybrid adaptive PDN for modern client processors whose goal is to provide high energy-efficiency across the processor's wide range of power consumption and workloads. FlexWatts provides high energy-efficiency by intelligently and dynamically allocating PDNs to processor domains depending on the processor's power consumption and workload. FlexWatts is based on three key ideas. First, FlexWatts combines IVRs and LDOs in a novel way to share multiple on-chip and off-chip resources and thus reduce cost, as well as board and die area overheads. This hybrid PDN is allocated for processor domains with a wide power consumption range (e.g., CPU cores and graphics engines) and it dynamically switches between two modes: IVR-Mode and LDO-Mode, depending on the power consumption. Second, for all other processor domains (that have a low and narrow power range, e.g., the IO domain), FlexWatts statically allocates off-chip VRs, which have high energy-efficiency for low and narrow power ranges. Third, FlexWatts introduces a novel prediction algorithm that automatically switches the hybrid PDN to the mode (IVR-Mode or LDO-Mode) that is the most beneficial based on processor power consumption and workload characteristics. To evaluate the tradeoffs of PDNs, we develop and open-source PDNspot, the first validated architectural PDN model that enables quantitative analysis of PDN metrics. Using PDNspot, we evaluate FlexWatts on a wide variety of SPEC CPU2006, graphics (3DMark06), and battery life (e.g., video playback) workloads against IVR, the state-of-the-art PDN in modern client processors. For a 4 W thermal design power (TDP) processor, FlexWatts improves the average performance of the SPEC CPU2006 and 3DMark06 workloads by 22% and 25%, respectively. For battery life workloads, FlexWatts reduces the average power consumption of video playback by 11% across all tested TDPs (4W-50W). FlexWatts has comparable cost and area overhead to IVR. We conclude that FlexWatts provides high energy-efficiency across a modern client processor's wide range of power consumption and wide variety of workloads, with minimal overhead.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251996","","Program processors;Power demand;Regulators;Energy efficiency;Hybrid power systems;Batteries;Voltage control","","5","","143","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Building the Computing System for Autonomous Micromobility Vehicles: Design Constraints and Architectural Optimizations","B. Yu; W. Hu; L. Xu; J. Tang; S. Liu; Y. Zhu",PerceptIn; PerceptIn; PerceptIn; South China University of Technology; PerceptIn; University of Rochester,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1067","1081","This paper presents the computing system design in our commercial autonomous vehicles, and provides a detailed performance, energy, and cost analyses. Drawing from our commercial deployment experience, this paper has two objectives. First, we highlight design constraints unique to autonomous vehicles that might change the way we approach existing architecture problems. Second, we identify new architecture and systems problems that are perhaps less studied before but are critical to autonomous vehicles.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251973","Autonomous vehicles;self-driving cars","Microarchitecture;Architecture;Computer architecture;Cost benefit analysis;Autonomous vehicles;System analysis and design;Optimization","","73","","94","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"AutoScale: Energy Efficiency Optimization for Stochastic Edge Inference Using Reinforcement Learning","Y. G. Kim; C. -J. Wu",Arizona State University; Arizona State University,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1082","1096","Deep learning inference is increasingly run at the edge. As the programming and system stack support becomes mature, it enables acceleration opportunities in a mobile system, where the system performance envelope is scaled up with a plethora of programmable co-processors. Thus, intelligent services designed for mobile users can choose between running inference on the CPU or any of the co-processors in the mobile system, and exploiting connected systems such as the cloud or a nearby, locally connected mobile system. By doing so, these services can scale out the performance and increase the energy efficiency of edge mobile systems. This gives rise to a new challenge—deciding when inference should run where. Such execution scaling decision becomes more complicated with the stochastic nature of mobile-cloud execution environment, where signal strength variation in the wireless networks and resource interference can affect real-time inference performance and system energy efficiency. To enable energy efficient deep learning inference at the edge, this paper proposes AutoScale, an adaptive and lightweight execution scaling engine built on the custom-designed reinforcement learning algorithm. It continuously learns and selects the most energy efficient inference execution target by considering characteristics of neural networks and available systems in the collaborative cloud-edge execution environment while adapting to stochastic runtime variance. Real system implementation and evaluation, considering realistic execution scenarios, demonstrate an average of 9.8x and 1.6x energy efficiency improvement over the baseline mobile CPU and cloud offloading, respectively, while meeting the real-time performance and accuracy requirements.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00090","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251950","","Deep learning;Cloud computing;Runtime;Reinforcement learning;Quality of service;Energy efficiency;Real-time systems","","53","","124","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"NCPU: An Embedded Neural CPU Architecture on Resource-Constrained Low Power Devices for Real-time End-to-End Performance","T. Jia; Y. Ju; R. Joseph; J. Gu","Northwestern Univeristy, Evanston, IL; Northwestern Univeristy, Evanston, IL; Northwestern Univeristy, Evanston, IL; Northwestern Univeristy, Evanston, IL",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1097","1109","Machine learning inference has become an essential task for embedded edge devices requiring the deployment of costly deep neural network accelerators onto extremely resource-constrained hardware. Although many optimization strategies have been proposed to improve the efficiency of standalone accelerators, the optimization for end-to-end performance of a computing device with heterogeneous cores is still challenging and often overlooked, especially for low power devices. In this paper, we propose a unified reconfigurable architecture, referred as Neural CPU (NCPU), for low-cost embedded systems. The proposed architecture is built on a binary neural network accelerator with the capability to emulate an in-order RISC-V CPU pipeline. The NCPU supports flexible programmability of RISC-V and maintains data locally to avoid costly core-to-core data transfer. A two-core NCPU SoC is designed and fabricated in a 65nm CMOS process. Compared with the conventional heterogeneous architecture, a single NCPU achieves 35% area reduction and 12% energy saving at 0.4V, which is suitable for low power and low-cost embedded edge devices. The NCPU design also features the capability of smooth switching between general-purpose CPU operation and a binary neural network inference to realize full utilization of the cores. The implemented two-core NCPU SoC achieves an end-to-end performance speed-up of 43% or an equivalent 74% energy saving based on use cases of real-time image classification and motion detection.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00091","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251958","Reconfigurable architecture;embedded systems;ultra-low power device;binary neural network;RISC-V;end-to-end performance;SoC silicon validation","Performance evaluation;Neural networks;Switches;CMOS process;Real-time systems;Task analysis;Optimization","","13","","60","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"CaSA: End-to-end Quantitative Security Analysis of Randomly Mapped Caches","T. Bourgeat; J. Drean; Y. Yang; L. Tsai; J. Emer; M. Yan",MIT CSAIL; MIT CSAIL; MIT/University of Chinese Academy of Sciences; MIT CSAIL; NVIDIA/MIT; MIT CSAIL,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1110","1123","It is well known that there are micro-architectural vulnerabilities that enable an attacker to use caches to exfiltrate secrets from a victim. These vulnerabilities exploit the fact that the attacker can detect cache lines that were accessed by the victim. Therefore, architects have looked at different forms of randomization to thwart the attacker's ability to communicate using the cache. The security analysis of those randomly mapped caches is based upon the increased difficulty for the attacker to determine the addresses that touch the same cache line that the victim has accessed. In this paper, we show that the analyses used to evaluate those schemes were incomplete in various ways. For example, they were incomplete because they only focused on one of the steps used in the exfiltration of secrets. Specifically, the step that the attacker uses to determine the set of addresses that can monitor the cache lines used by the transmitter address. Instead, we broaden the analysis of micro-architecture side channels by providing an overall view of the communication process. This allows us to identify the existence of other communication steps that can also affect the security of randomly mapped caches, but have been ignored by prior work. We design an analysis framework, CaSA, to comprehensively and quantitatively analyze the security of these randomly mapped caches. We comprehensively consider the end-to-end communication steps and study the statistical relationship between different steps. In addition, to perform quantitative analysis, we leverage the concepts from the field of telecommunications to formulate the security analysis into a statistical problem. We use CaSA to evaluate a wide range of attack strategies and cache configurations. Our result shows that the randomization mechanisms used in the state-of-the-art randomly mapped caches are insecure.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251961","Micro-architecture side channel;randomly mapped cache;security analysis","Microarchitecture;Transmitters;Statistical analysis;Telecommunications;Security;Monitoring","","24","","47","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"PerSpectron: Detecting Invariant Footprints of Microarchitectural Attacks with Perceptron","S. Mirbagher-Ajorpaz; G. Pokam; E. Mohammadian-Koruyeh; E. Garza; N. Abu-Ghazaleh; D. A. Jiménez","Computer Science and Engineering, Texas A&M University, College Station, USA; Intel Labs, Santa Clara, USA; Computer Science and Engineering, University of California Riverside, Riverside, USA; Computer Science and Engineering, Texas A&M University, College Station, USA; Computer Science and Engineering, Texas A&M University, College Station, USA; Computer Science and Engineering, Texas A&M University, College Station, USA",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1124","1137","Detecting microarchitectural attacks is critical given their proliferation in recent years. Many of these attacks exhibit intrinsic behaviors essential to the nature of their operation, such as creating contention or misspeculation. This study systematically investigates the microarchitectural footprints of hardware-based attacks and shows how they can be detected and classified using an efficient hardware predictor. We present a methodology to use correlated microarchitectural statistics to design a hardware-based neural predictor capable of detecting and classifying microarchitectural attacks before data is leaked. Once a potential attack is detected, it can be proactively mitigated by triggering appropriate countermeasures.Our hardware-based detector, PerSpectron, uses perceptron learning to identify and classify attacks. Perceptron-based prediction has been successfully used in branch prediction and other hardware-based applications. PerSpectron has minimal performance overhead. The statistics being monitored have similar overhead to already existing performance monitoring counters. Additionally, PerSpectron operates outside the processor's critical paths, offering security without added computation delay. Our system achieves a usable detection rate for detecting attacks such as SpectreV1, SpectreV2, SpectreRSB, Meltdown, breakingKSLR, Flush+Flush, Flush+Reload, Prime+Probe as well as cache-attack calibration programs. We also believe that the large number of diverse microarchitectural features offers both evasion resilience and interpretability-features not present in previous hardware security detectors. We detect these attacks early enough to avoid any data leakage, unlike previous work that triggers countermeasures only after data has been exposed.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00093","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251956","microarchitectural attack defenses;secure architectures;anomaly detection","Microarchitecture;Detectors;Feature extraction;Hardware;Security;Monitoring;Resilience","","17","","70","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Speculation Invariance (InvarSpec): Faster Safe Execution Through Program Analysis","Z. N. Zhao; H. Ji; M. Yan; J. Yu; C. W. Fletcher; A. Morrison; D. Marinov; J. Torrellas","University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; Massachusetts Institute of Technology; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; Tel Aviv University; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1138","1152","Many hardware-based defense schemes against speculative execution attacks use special mechanisms to protect instructions while speculative, and lift the mechanisms when the instructions turn non-speculative. In this paper, we observe that speculative instructions can sometimes become Speculation Invariant before turning non-speculative. Speculation invariance means that (i) whether the instruction will execute and (ii) the instruction's operands are not a function of speculative state. Hence, we propose to lift the protection mechanisms on these instructions early, when they become speculation invariant, and issue them without protection. As a result, we improve the performance of the defense schemes without changing their security properties. To exploit speculation invariance, we present the InvarSpec framework. InvarSpec includes a program analysis pass that identifies, for each relevant instruction i, the set of older instructions that are Safe for i-i.e., those that do not prevent i from becoming speculation invariant. At runtime, the InvarSpec micro-architecture loads this information and uses it to determine when speculative instructions can be issued without protection. InvarSpec is one of the first defense schemes for speculative execution that combines cooperative compiler and hardware mechanisms. Our evaluation shows that InvarSpec effectively reduces the execution overhead of hardware defense schemes. For example, on SPEC17, it reduces the average execution overhead of fence protections from 195.3% to 108.2%, of Delay-On-Miss from 39.5% to 24.4%, and of InvisiSpec from 15.4% to 10.9%.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251941","Speculative execution defense;Program analysis;Speculation","Runtime;Microarchitecture;Turning;Hardware;Security","","13","","53","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
"Hardware-based Always-On Heap Memory Safety","Y. Kim; J. Lee; H. Kim",Georgia Institute of Technology; Arm Research; Georgia Institute of Technology,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),"11 Nov 2020","2020","","","1153","1166","Memory safety violations, caused by illegal use of pointers in unsafe programming languages such as C and C++, have been a major threat to modern computer systems. However, implementing a low-overhead yet robust runtime memory safety solution is still challenging. Various hardware-based mechanisms have been proposed, but their significant hardware requirements have limited their feasibility, and their performance overhead is too high to be an always-on solution.In this paper, we propose AOS, a low-overhead always-on heap memory safety solution that implements a novel bounds-checking mechanism. We identify that the major challenges of existing bounds-checking approaches are 1) the extra instruction overhead for memory checking and metadata propagation and 2) the complex metadata addressing. To address these challenges, using Arm PA primitives, we leverage unused upper bits of a pointer to store a key and have it propagated along with the pointer address, eliminating propagation overhead. Then, we use the embedded key to index a hashed bounds table to achieve efficient metadata management. We also introduce a micro-architectural unit to remove the need for memory checking instructions. We show that AOS overcomes all the aforementioned challenges and demonstrate its feasibility as an efficient runtime memory safety solution. Our evaluation for SPEC 2006 workloads shows an 8.4% performance overhead on average.","","978-1-7281-7383-2","10.1109/MICRO50266.2020.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251969","Security;software and system safety;pointer authentication","Runtime;Memory management;Metadata;Picture archiving and communication systems;Safety;Security;Indexes","","23","","43","IEEE","11 Nov 2020","","","IEEE","IEEE Conferences"
