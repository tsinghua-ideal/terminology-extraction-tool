"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Amdahl's Law in the Datacenter Era: A Market for Fair Processor Allocation","S. M. Zahedi; Q. Llull; B. C. Lee","Duke University, Durham, NC, USA; VMware Inc., Palo Alto, CA, USA; Duke University, Durham, NC, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","1","14","We present a processor allocation framework that uses Amdahl's Law to model parallel performance and a market mechanism to allocate cores. First, we propose the Amdahl utility function and demonstrate its accuracy when modeling performance from processor core allocations. Second, we design a market based on Amdahl utility that optimizes users' bids for processors based on workload parallelizability. The framework uses entitlements to guarantee fairness yet outperforms existing proportional share algorithms.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326994","Proportional Sharing;Processor Allocation;Amdahl's Law;Karp Flatt Metric;Market Mechanisms","Servers;Resource management;Sparks;Mathematical model;Task analysis;Parallel processing;Computational modeling","","9","","47","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"iNPG: Accelerating Critical Section Access with In-network Packet Generation for NoC Based Many-Cores","Y. Yao; Z. Lu","KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","15","26","As recently studied, serialized competition overhead for entering critical section is more dominant than critical section execution itself in limiting performance of multi-threaded shared variable applications on NoC-based many-cores. We illustrate that the invalidation-acknowledgement delay for cache coherency between the home node storing the critical section lock and the cores running competing threads is the leading factor to high competition overhead in lock spinning, which is realized in various spin-lock primitives (such as the ticket lock, ABQL, MCS lock, etc.) and the spinning phase of queue spin-lock (QSL) in advanced operating systems. To reduce such high lock coherence overhead, we propose in-network packet generation (iNPG) to turn passive ""normal"" NoC routers which only transmit packets into active ""big"" ones that can generate packets. Instead of performing all coherence maintenance at the home node, big routers which are deployed nearer to competing threads can generate packets to perform early invalidation-acknowledgement for failing threads before their requests reach the home node, shortening the protocol round-trip delay and thus significantly reducing competition overhead in various locking primitives. We evaluate iNPG in Gem5 using PARSEC and SPEC OMP2012 programs with five different locking primitives. Compared to a state-of-the-art technique accelerating critical section access, experimental results show that iNPG can effectively reduce lock coherence overhead, expediting critical section access by 1.35x on average and 2.03x at maximum and consequently improving the program Region-of-Interest (ROI) runtime by 7.8% on average and 14.7% at maximum.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326995","In Network Packet Generation;Critical Section;Synchronisation Primitive;Cache Coherency;Network on Chip;CMP","Instruction sets;Spinning;Liquid crystal on silicon;Coherence;Acceleration;Routing protocols","","11","","41","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Enabling Efficient Network Service Function Chain Deployment on Heterogeneous Server Platform","Y. Hu; T. Li","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","27","39","Network Function Virtualization (NFV) aims to run software-implemented network functions on general hardware such as Commodity Off-the-Shelf (COTS) servers to trade the application-specific performance with generality and re-configurability. Nevertheless, with the wide adoption of general accelerators such as GPU, the researchers seek to boost the performance of software-based network functions while trying to maintain the reusability and programmability in the meantime. The Service Function Chain (SFC) is a key enabler of service flexibility of NFV. The network functions stitch into a chain to provide differentiated services to multi-tenants. However, our characterization results show that existing heterogeneous packet processing frameworks do not handle NFV SFC well since two new overheads, the aggregated processing overheads and co-existence interference overheads, are introduced by SFC.,,,, Motivated by our characterization, we propose NFCompass, a runtime framework that employs SFC re-organization technique and graph-partition based task scheduling technique to conquer the two challenges brought by SFC. By re-organizing the SFC components, the length and complexity of processing paths are reduced and the aggregated overheads are mitigated. By applying the graph-partition based task allocation, better load balance is achieved and the data transfer overheads are considerably reduced.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326996","NFV;GPU;Service Function Chain;Graph Partition","Graphics processing units;Interference;Network function virtualization;Kernel;Encryption;Noise measurement;Task analysis","","17","","58","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Reducing Data Transfer Energy by Exploiting Similarity within a Data Transaction","D. Lee; M. O'Connor; N. Chatterjee",NVIDIA; University of Texas at Austin; NVIDIA,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","40","51","Modern highly parallel GPU systems require high-bandwidth DRAM I/O interfaces that can consume a significant amount of energy. This energy increases in proportion to the number of 1 values in the data transactions due to the asymmetric energy consumption of Pseudo Open Drain (POD) I/O interface in contemporary Graphics DDR SDRAMs. In this work, we describe a technique to save energy by reducing the energy-expensive 1 values in the DRAM interface. We observe that multiple data elements within a single cache line/sector are often similar to one another. We exploit this characteristic to encode each transfer to the DRAM such that there is one reference copy of the data, with remaining similar data items being encoded predominantly as 0 values. Our proposed low energy data transfer mechanism, Base+XOR Transfer, encodes the data-similar portion by performing XOR operations between data elements within a single DRAM transaction. We address two challenges that influence the efficiency of our mechanism, i) the frequent appearance of zero data elements in transactions, and ii) the diversity in the underlying size of data types within a transaction. We describe two techniques, Zero Data Remapping and Universal Base+XOR Transfer, to efficiently address these issues. Our proposed encoding scheme requires no additional metadata or changes to existing DRAM devices. We evaluate our mechanism on a modern high performance GPU system with a variety of graphics and compute workloads. We show that our mechanism reduces energy-expensive 1 values by 35.3% with minimal overheads, and combining our mechanism with Dynamic Bus Inversion (DBI) reduces 1 values by 48.2% on average. These 1 value reductions lead to 5.8% and 7.1% DRAM energy savings, respectively.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326997","Memory;DRAM;GDDR;POD;I/O;Interface;Energy","Random access memory;Graphics processing units;Energy consumption;Wires;Data transfer;Bandwidth;Encoding","","22","1","41","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Making Memristive Neural Network Accelerators Reliable","B. Feinberg; S. Wang; E. Ipek","Dept. of Electr. & Comput. Eng., Univ. of Rochester, Rochester, NY, USA; Dept. of Comput. Sci., Univ. of Rochester, Rochester, NY, USA; Dept. of Electr. & Comput. Eng., Univ. of Rochester, Rochester, NY, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","52","65","Deep neural networks (DNNs) have attracted substantial interest in recent years due to their superior performance on many classification and regression tasks as compared to other supervised learning models. DNNs often require a large amount of data movement, resulting in performance and energy overheads. One promising way to address this problem is to design an accelerator based on in-situ analog computing that leverages the fundamental electrical properties of memristive circuits to perform matrix-vector multiplication. Recent work on analog neural network accelerators has shown great potential in improving both the system performance and the energy efficiency. However, detecting and correcting the errors that occur during in-memory analog computation remains largely unexplored. The same electrical properties that provide the performance and energy improvements make these systems especially susceptible to errors, which can severely hurt the accuracy of the neural network accelerators. This paper examines a new error correction scheme for analog neural network accelerators based on arithmetic codes. The proposed scheme encodes the data through multiplication by an integer, which preserves addition operations through the distributive property. Error detection and correction are performed through a modulus operation and a correction table lookup. This basic scheme is further improved by data-aware encoding to exploit the state dependence of the errors, and by knowledge of how critical each portion of the computation is to overall system accuracy. By leveraging the observation that a physical row that contains fewer 1s is less susceptible to an error, the proposed scheme increases the effective error correction capability with less than 4.5% area and less than 4.7% energy overheads. When applied to a memristive DNN accelerator performing inference on the MNIST and ILSVRC-2012 datasets, the proposed technique reduces the respective misclassification rates by 1.5x and 1.1x.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326998","Computer architecture;Accelerator architectures","Neural networks;Resistance;Arrays;Programming;Thermal noise;Computational modeling;Electric potential","","105","14","66","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Towards Efficient Microarchitectural Design for Accelerating Unsupervised GAN-Based Deep Learning","M. Song; J. Zhang; H. Chen; T. Li","Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","66","77","Recently, deep learning based approaches have emerged as indispensable tools to perform big data analytics. Normally, deep learning models are first trained with a supervised method and then deployed to execute various tasks. The supervised method involves extensive human efforts to collect and label the large-scale dataset, which becomes impractical in the big data era where raw data is largely un-labeled and uncategorized. Fortunately, the adversarial learning, represented by Generative Adversarial Network (GAN), enjoys a great success on the unsupervised learning. However, the distinct features of GAN, such as massive computing phases and non-traditional convolutions challenge the existing deep learning accelerator designs. In this work, we propose the first holistic solution for accelerating the unsupervised GAN-based Deep Learning. We overcome the above challenges with an algorithm and architecture co-design approach. First, we optimize the training procedure to reduce on-chip memory consumption. We then propose a novel time-multiplexed design to efficiently map the abundant computing phases to our microarchitecture. Moreover, we design high-efficiency dataflows to achieve high data reuse and skip the zero-operand multiplications in the non-traditional convolutions. Compared with traditional deep learning accelerators, our proposed design achieves the best performance (average 4.3X) with the same computing resource. Our design also has an average of 8.3X speedup over CPU and 6.2X energy-efficiency over NVIDIA GPU.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326999","GAN;Unsupervised Learning;Accelerator;Architecture","Gallium nitride;Training;Synchronization;Backpropagation;Optimization;Memory management","","45","","34","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks","M. Rhu; M. O'Connor; N. Chatterjee; J. Pool; Y. Kwon; S. W. Keckler","POSTECH, Pohang, South Korea; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; POSTECH, Pohang, South Korea; NVIDIA, Santa Clara, CA, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","78","91","Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform DNN computations. We introduce a high-performance virtualization strategy based on a ""compressing DMA engine"" (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6x (maximum 13.8x) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 53% (maximum 79%) when evaluated on an NVIDIA Titan Xp.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327000","GPU;Compression","Graphics processing units;Training;Feature extraction;Bandwidth;Neural networks;Resource management;Backpropagation","","116","","44","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"In-Situ AI: Towards Autonomous and Incremental Deep Learning for IoT Systems","M. Song; K. Zhong; J. Zhang; Y. Hu; D. Liu; W. Zhang; J. Wang; T. Li",University of Florida; Chongqing University; University of Florida; University of Texas at Dallas; Chongqing University; Capital Normal University; Capital Normal University; University of Florida,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","92","103","Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327001","IoT;Deep Learning;Architecture;Autonomous","Task analysis;Machine learning;Cloud computing;Data models;Training;Computer architecture","","86","3","35","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"KPart: A Hybrid Cache Partitioning-Sharing Technique for Commodity Multicores","N. El-Sayed; A. Mukkara; P. -A. Tsai; H. Kasture; X. Ma; D. Sanchez",Mit Computer Science and Artificial Intelligence Lab; Mit Computer Science and Artificial Intelligence Lab; Mit Computer Science and Artificial Intelligence Lab; Mit Computer Science and Artificial Intelligence Lab; Oracle Labs; Mit Computer Science and Artificial Intelligence Lab,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","104","117","Cache partitioning is now available in commercial hardware. In theory, software can leverage cache partitioning to use the last-level cache better and improve performance. In practice, however, current systems implement way-partitioning, which offers a limited number of partitions and often hurts performance. These limitations squander the performance potential of smart cache management. We present KPart, a hybrid cache partitioning-sharing technique that sidesteps the limitations of way-partitioning and unlocks significant performance on current systems. KPart first groups applications into clusters, then partitions the cache among these clusters. To build clusters, KPart relies on a novel technique to estimate the performance loss an application suffers when sharing a partition. KPart automatically chooses the number of clusters, balancing the isolation benefits of way-partitioning with its potential performance impact. KPart uses detailed profiling information to make these decisions. This information can be gathered either offline, or online at low overhead using a novel profiling mechanism. We evaluate KPart in a real system and in simulation. KPart improves throughput by 24% on average (up to 79%) on an Intel Broadwell-D system, whereas prior per-application partitioning policies improve throughput by just 1.7% on average and hurt 30% of workloads. Simulation results show that KPart achieves most of the performance of more advanced partitioning techniques that are not yet available in hardware.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327002","cache partitioning;multicore architecture;performance isolation;application clustering","Hardware;Throughput;Partitioning algorithms;Clustering algorithms;Monitoring;Software;Computer architecture","","84","1","67","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"SIPT: Speculatively Indexed, Physically Tagged Caches","T. Zheng; H. Zhu; M. Erez","Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","118","130","First-level (L1) data cache access latency is critical to performance because it services the vast majority of loads and stores. To keep L1 latency low while ensuring low-complexity and simple-to-verify operation, current processors most-typically utilize a virtually-indexed physically-tagged (VIPT) cache architecture. While VIPT caches decrease latency by proceeding with cache access and address translation concurrently, each cache way is constrained by the size of a virtual page. Thus, larger L1 caches are highly-associative, which degrades their access latency and energy. We propose speculatively-indexed physically-tagged (SIPT) caches to enable simultaneously larger, faster, and more efficient L1 caches. A SIPT cache speculates on the value of a few address bits beyond the page offset concurrently with address translation, maintaining the overall safe and reliable architecture of a VIPT cache while eliminating the VIPT design constraints. SIPT is a purely microarchitectural approach that can be used with any software and for all accesses. We evaluate SIPT with simulations of applications under standard Linux. SIPT improves performance by 8.1% on average and reduces total cache-hierarchy energy by 15.6%.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327003","Cache and memory architectures;Innovative hardware/software trade offs;Processor architecture","Program processors;Coherence;Computer architecture;Hardware;Indexing","","12","1","47","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Domino Temporal Data Prefetcher","M. Bakhshalipour; P. Lotfi-Kamran; H. Sarbazi-Azad","Institute for Research in Fundamental Sciences (IPM); School of Computer Science, Institute for Research in Fundamental Sciences (IPM); School of Computer Science, Institute for Research in Fundamental Sciences (IPM)",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","131","142","Big-data server applications frequently encounter data misses, and hence, lose significant performance potential. One way to reduce the number of data misses or their effect is data prefetching. As data accesses have high temporal correlations, temporal prefetching techniques are promising for them. While state-of-the-art temporal prefetching techniques are effective at reducing the number of data misses, we observe that there is a significant gap between what they offer and the opportunity. This work aims to improve the effectiveness of temporal prefetching techniques. We identify the lookup mechanism of existing temporal prefetchers responsible for the large gap between what they offer and the opportunity. Existing lookup mechanisms either not choose the right stream in the history, or unnecessarily delay the stream selection, and hence, miss the opportunity at the beginning of every stream. In this work, we introduce Domino prefetching to address the limitations of existing temporal prefetchers. Domino prefetcher is a temporal data prefetching technique that logically looks up the history with both one and two last miss addresses to find a match for prefetching. We propose a practical design for Domino prefetcher that employs an Enhanced Index Table that is indexed by just a single miss address. We show that Domino prefetcher captures more than 90% of the temporal opportunity. Through detailed evaluation targeting a quad-core processor and a set of server workloads, we show that Domino prefetcher improves system performance by 16% over the baseline with no data prefetcher and 6% over the state-of- the-art temporal data prefetcher.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327004","Temporal prefetcher;data miss;server workload;single address lookup;two address lookup","Prefetching;History;Servers;Correlation;Indexes;Media;Context","","72","","66","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"ProFess: A Probabilistic Hybrid Main Memory Management Framework for High Performance and Fairness","D. Knyaginin; V. Papaefstathiou; P. Stenstrom","Chalmers University of Technology, Gothenburg, Sweden; FORTH-ICS, Heraklion, Crete, Greece; Chalmers University of Technology, Gothenburg, Sweden",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","143","155","Non-Volatile Memory (NVM) technologies enable cost-effective hybrid main memories with two partitions: M1 (DRAM) and slower but larger M2 (NVM). This paper considers a flat migrating organization of hybrid memories. A challenging and open issue of managing such memories is to allocate M1 among co-running programs such that high fairness is achieved at the same time as high performance. This paper introduces ProFess: a Probabilistic hybrid main memory management Framework for high performance and fairness. It comprises: i) a Relative-Slowdown Monitor (RSM) that enables fair management by indicating which program suffers the most from competition for M1; and ii) a probabilistic Migration-Decision Mechanism (MDM) that unlocks high performance by realizing cost-benefit analysis that is individual for each pair of data blocks considered for migration. Within ProFess, RSM guides MDM towards high fairness. We show that for the multiprogrammed workloads evaluated, ProFess improves fairness by 15% (avg.; up to 29%), compared to the state-of-the-art, while outperforming it by 12% (avg.; up to 29%).","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327005","hybrid main memory;flat migrating organization;fairness;performance;hardware","Random access memory;Nonvolatile memory;Organizations;Memory management;Probabilistic logic;Three-dimensional displays;Topology","","6","","40","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"RCoal: Mitigating GPU Timing Attack via Subwarp-Based Randomized Coalescing Techniques","G. Kadam; D. Zhang; A. Jog","College of William and Mary, Williamsburg, VA; Penn State University, State College, PA; College of William and Mary, Williamsburg, VA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","156","167","Graphics processing units (GPUs) are becoming default accelerators in many domains such as high-performance computing (HPC), deep learning, and virtual/augmented reality. Recently, GPUs have also shown significant speedups for a variety of security-sensitive applications such as encryptions. These speedups have largely benefited from the high memory bandwidth and compute throughput of GPUs. One of the key features to optimize the memory bandwidth consumption in GPUs is intra-warp memory access coalescing, which merges memory requests originating from different threads of a single warp into as few cache lines as possible. However, this coalescing feature is also shown to make the GPUs prone to the correlation timing attacks as it exposes the relationship between the execution time and the number of coalesced accesses. Consequently, an attacker is able to correctly reveal an AES private key via repeatedly gathering encrypted data and execution time on a GPU. In this work, we propose a series of defense mechanisms to alleviate such timing attacks by carefully trading off performance for improved security. Specifically, we propose to randomize the coalescing logic such that the attacker finds it hard to guess the correct number of coalesced accesses generated. To this end, we propose to randomize: a) the granularity (called as subwarp) at which warp threads are grouped together for coalescing, and b) the threads selected by each subwarp for coalescing. Such randomization techniques result in three mechanisms: fixed-sized subwarp (FSS), random-sized subwarp (RSS), and random-threaded subwarp (RTS). We find that the combination of these security mechanisms offers 24- to 961-times improvement in the security against the correlation timing attacks with 5 to 28% performance degradation.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327006","GPUs;Hardware Security;Coalescing","Instruction sets;Graphics processing units;Timing;Cryptography;Correlation;Memory management","","37","","37","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Are Coherence Protocol States Vulnerable to Information Leakage?","F. Yao; M. Doroslovacki; G. Venkataramani","Dept. of Electr. & Comput. Eng., George Washington Univ., Washington, DC, USA; Dept. of Electr. & Comput. Eng., George Washington Univ., Washington, DC, USA; Dept. of Electr. & Comput. Eng., George Washington Univ., Washington, DC, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","168","179","Most commercial multi-core processors incorporate hardware coherence protocols to support efficient data transfers and updates between their constituent cores. While hardware coherence protocols provide immense benefits for application performance by removing the burden of software-based coherence, we note that understanding the security vulnerabilities posed by such oft-used, widely-adopted processor features is critical for secure processor designs in the future. In this paper, we demonstrate a new vulnerability exposed by cache coherence protocol states. We present novel insights into how adversaries could cleverly manipulate the coherence states on shared cache blocks, and construct covert timing channels to illegitimately communicate secrets to the spy. We demonstrate 6 different practical scenarios for covert timing channel construction. In contrast to prior works, we assume a broader adversary model where the trojan and spy can either exploit explicitly shared read-only physical pages (e.g., shared library code), or use memory deduplication feature to implicitly force create shared physical pages. We demonstrate how adversaries can manipulate combinations of coherence states and data placement in different caches to construct timing channels. We also explore how adversaries could exploit multiple caches and their associated coherence states to improve transmission bandwidth with symbols encoding multiple bits. Our experimental results on commercial systems show that the peak transmission bandwidths of these covert timing channels can vary between 700 to 1100 Kbits/sec. To the best of our knowledge, our study is the first to highlight the vulnerability of hardware cache coherence protocols to timing channels that can help computer architects to craft effective defenses against exploits on such critical processor features.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327007","coherence protocols;covert timing channels;information leakage;hardware security","Coherence;Trojan horses;Timing;Protocols;Hardware;Bandwidth;Security","","81","","41","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Record-Replay Architecture as a General Security Framework","Y. Shalabi; M. Yan; N. Honarmand; R. B. Lee; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; NA; Princeton University; University of Illinois at Urbana-Champaign,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","180","193","Hardware security features need to strike a careful balance between design intrusiveness and completeness of methods. In addition, they need to be flexible, as security threats continuously evolve. To help address these requirements, this paper proposes a novel framework where Record and Deterministic Replay (RnR) is used to complement hardware security features. We call the framework RnR-Safe. RnR-Safe reduces the cost of security hardware by allowing it to be less precise at detecting attacks, potentially reporting false positives. This is because it relies on on-the-fly replay that transparently verifies whether the alarm is a real attack or a false positive. RnR-Safe uses two replayers: an always-on, fast Checkpoint replayer that periodically creates checkpoints, and a detailed-analysis Alarm replayer that is triggered when there is a threat alarm. As an example application, we use RnR-Safe to thwart Return Oriented Programming (ROP) attacks, including on the Linux kernel. Our design augments the Return Address Stack (RAS) with relatively inexpensive hardware. We evaluate RnR-Safe using a variety of workloads on virtual machines running Linux. We find that RnR-Safe is very effective. Thanks to the judicious RAS hardware extensions and hypervisor changes, the checkpointing replayer has an execution speed comparable to the recorded execution. Also, the alarm replayer needs to handle very few false positives.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327008","Record and Deterministic Replay;Hardware Security;Return Oriented Programming","Hardware;Security;Kernel;Virtual machine monitors;Computer architecture;Linux;Monitoring","","10","","77","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices","J. S. Kim; M. Patel; H. Hassan; O. Mutlu",Carnegie Mellon University; ETH Zürich; ETH Zürich; Carnegie Mellon University,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","194","207","Physically Unclonable Functions (PUFs) are commonly used in cryptography to identify devices based on the uniqueness of their physical microstructures. DRAM-based PUFs have numerous advantages over PUF designs that exploit alternative substrates: DRAM is a major component of many modern systems, and a DRAM-based PUF can generate many unique identiers. However, none of the prior DRAM PUF proposals provide implementations suitable for runtime-accessible PUF evaluation on commodity DRAM devices. Prior DRAM PUFs exhibit unacceptably high latencies, especially at low temperatures (e.g., >125.8s on average for a 64KiB memory segment below 55C), and they cause high system interference by keeping part of DRAM unavailable during PUF evaluation. In this paper, we introduce the DRAM latency PUF, a new class of fast, reliable DRAM PUFs. The key idea is to reduce DRAM read access latency below the reliable datasheet specications using software-only system calls. Doing so results in error patterns that reect the compound eects of manufacturing variations in various DRAM structures (e.g., capacitors, wires, sense ampli- ers). Based on a rigorous experimental characterization of 223 modern LPDDR4 DRAM chips, we demonstrate that these error patterns 1) satisfy runtime-accessible PUF requirements, and 2) are quickly generated (i.e., at 88.2ms) irrespective of operating temperature using a real system with no additional hardware modications. We show that, for a constant DRAM capacity overhead of 64KiB, our implementation of the DRAM latency PUF enables an average (minimum, maximum) PUF evaluation time speedup of 152x (109x, 181x) at 70C and 1426x (868x, 1783x) at 55C when compared to a DRAM retention PUF and achieves greater speedups at even lower temperatures.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327009","","DRAM chips;Proposals;Runtime;Reliability;Servers;Authentication","","100","","145","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Accelerate GPU Concurrent Kernel Execution by Mitigating Memory Pipeline Stalls","H. Dai; Z. Lin; C. Li; C. Zhao; F. Wang; N. Zheng; H. Zhou","Departrnent of Electrical and Computer Engineering, North Carolina State University; Departrnent of Electrical and Computer Engineering, North Carolina State University; Departrnent of Electrical and Computer Engineering, North Carolina State University; School of Electrical and Information Engineering, Xi'an Jiaotong University; School of Electrical and Information Engineering, Xi'an Jiaotong University; School of Electrical and Information Engineering, Xi'an Jiaotong University; Departrnent of Electrical and Computer Engineering, North Carolina State University",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","208","220","Following the advances in technology scaling, graphics processing units (GPUs) incorporate an increasing amount of computing resources and it becomes difficult for a single GPU kernel to fully utilize the vast GPU resources. One solution to improve resource utilization is concurrent kernel execution (CKE). Early CKE mainly targets the leftover resources. However, it fails to optimize the resource utilization and does not provide fairness among concurrent kernels. Spatial multitasking assigns a subset of streaming multiprocessors (SMs) to each kernel. Although achieving better fairness, the resource underutilization within an SM is not addressed. Thus, intra-SM sharing has been proposed to issue thread blocks from different kernels to each SM. However, as shown in this study, the overall performance may be undermined in the intra-SM sharing schemes due to the severe interference among kernels. Specifically, as concurrent kernels share the memory subsystem, one kernel, even as computing-intensive, may starve from not being able to issue memory instructions in time. Besides, severe L1 D-cache thrashing and memory pipeline stalls caused by one kernel, especially a memory-intensive one, will impact other kernels, further hurting the overall performance. In this study, we investigate various approaches to overcome the aforementioned problems exposed in intra-SM sharing. We first highlight that cache partitioning techniques proposed for CPUs are not effective for GPUs. Then we propose two approaches to reduce memory pipeline stalls. The first is to balance memory accesses of concurrent kernels. The second is to limit the number of inflight memory instructions issued from individual kernels. Our evaluation shows that the proposed schemes significantly improve the weighted speedup of two state-of-the-art intra-SM sharing schemes, Warped-Slicer and SMK, by 24.6% and 27.2% on average, respectively, with lightweight hardware overhead.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327010","GPU;Resource Utilization;Concurrent Kernel Execution;Interference;Memory Pipeline Stall;Memory Subsystem","Kernel;Graphics processing units;Pipelines;Instruction sets;Resource management;Benchmark testing;Computer architecture","","32","","48","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"LATTE-CC: Latency Tolerance Aware Adaptive Cache Compression Management for Energy Efficient GPUs","A. Arunkumar; S. -Y. Lee; V. Soundararajan; C. -J. Wu","School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ; School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ; School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ; School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","221","234","General-purpose GPU applications are significantly constrained by the efficiency of the memory subsystem and the availability of data cache capacity on GPUs. Cache compression, while is able to expand the effective cache capacity and improve cache efficiency, comes with the cost of increased hit latency. This has constrained the application of cache compression to mostly lower level caches, leaving it unexplored for L1 caches and for GPUs. Directly applying state-of-the-art high performance cache compression schemes on GPUs results in a wide performance variation from -52% to 48%. To maximize the performance and energy benefits of cache compression for GPUs, we propose a new compression management scheme, called LATTE-CC. LATTE-CC is designed to exploit the dynamically-varying latency tolerance feature of GPUs. LATTE-CC compresses cache lines based on its prediction of the degree of latency tolerance of GPU streaming multiprocessors and by choosing between three distinct compression modes: no compression, low-latency, and high-capacity. LATTE-CC improves the performance of cache sensitive GPGPU applications by as much as 48.4% and by an average of 19.2%, outperforming the static application of compression algorithms. LATTE-CC also reduces GPU energy consumption by an average of 10%, which is twice as much as that of the state-of-the-art compression scheme.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327011","Graphics Processing Units;Cache Compression;Latency Tolerance","Computer architecture","","16","","43","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"High-Performance GPU Transactional Memory via Eager Conflict Detection","X. Ren; M. Lis",The University of British Columbia; The University of British Columbia,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","235","246","GPUs transactional memory (TM) proposals to date have relied on lazy, value-based conflict detection, assuming that GPUs can amortize the latency by executing other warps. In practice, however, concurrency must be throttled to a few warps per core to avoid high abort rates, and TM performance has remained far below that of fine-grained locks. We trace this to the latency cost of validating transactions: two round trips across the crossbar required for most commits and aborts. With limited concurrency, the warp scheduler cannot amortize this, and leaves the core idle most of the time. In this paper, we show that value-based validation does not scale to high thread counts, and eager conflict detection becomes more efficient as the number of threads grows. We leverage this insight to propose GETM, a GPU TM with eager conflict detection. GETM relies on a novel distributed logical clock scheme to implement eager conflict detection without the need for cache coherence or signature broadcasts. GETM is up to 2.1 times faster than the state-of-the art prior work WarpTM (gmean 1.2 times), with 3.6 times lower silicon area overheads and 2.2 times lower power overheads.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327012","GPU;transactional memory","Graphics processing units;Proposals;Concurrent computing;Clocks;Coherence;Hardware;Message systems","","3","","78","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Efficient and Fair Multi-programming in GPUs via Effective Bandwidth Management","H. Wang; F. Luo; M. Ibrahim; O. Kayiran; A. Jog",College of William and Mary; College of William and Mary; College of William and Mary; Advanced Micro Devices Inc.; College of William and Mary,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","247","258","Managing the thread-level parallelism (TLP) of GPGPU applications by limiting it to a certain degree is known to be effective in improving the overall performance. However, we find that such prior techniques can lead to sub-optimal system throughput and fairness when two or more applications are co-scheduled on the same GPU. It is because they attempt to maximize the performance of individual applications in isolation, ultimately allowing each application to take a disproportionate amount of shared resources. This leads to high contention in shared cache and memory. To address this problem, we propose new application-aware TLP management techniques for a multi-application execution environment such that all co-scheduled applications can make good and judicious use of all the shared resources. For measuring such use, we propose an application-level utility metric, called effective bandwidth, which accounts for two runtime metrics: attained DRAM bandwidth and cache miss rates. We find that maximizing the total effective bandwidth and doing so in a balanced fashion across all co-located applications can significantly improve the system throughput and fairness. Instead of exhaustively searching across all the different combinations of TLP configurations that achieve these goals, we find that a significant amount of overhead can be reduced by taking advantage of the trends, which we call patterns, in the way application's effective bandwidth changes with different TLP combinations. Our proposed pattern-based TLP management mechanisms improve the system throughput and fairness by 20% and 2x, respectively, over a baseline where each application executes with a TLP configuration that provides the best performance when it executes alone.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327013","Bandwidth Management;Fairness;GPUs","Bandwidth;Graphics processing units;Measurement;Throughput;Indexes;Random access memory;Interference","","35","","69","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"A Novel Register Renaming Technique for Out-of-Order Processors","H. Tabani; J. -M. Arnau; J. Tubella; A. Gonzalez","Department of Computer Architecture, Universitat Politécnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politécnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politécnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politécnica de Catalunya, Barcelona, Spain",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","259","270","Modern superscalar processors support a large number of in-flight instructions, which requires sizeable register files. Conventional register renaming techniques allocate a new storage location, i.e. physical register, for every instruction whose destination is a logical register in order to remove false dependences. Physical registers are released in a conservative manner when the same logical register is redefined. For this reason, many cycles may happen between the last read and the release of a physical register, leading to suboptimal utilization of the register file. We have observed that for more than 50% of the instructions in SPECfp and more than 30% of the instructions in SPECint that have a destination register, the produced value has only a single consumer. In this case, the RAW dependence guarantees that the producer-consumer instructions pair will be executed in program order and, hence, the same physical register can be used to store the value produced by both instructions. In this paper, we propose a renaming technique that exploits this property to reduce the pressure on the register file. Our technique leverages physical register sharing by introducing minor changes in the register map table and the issue queue. We also describe how our renaming scheme supports precise exceptions. We evaluated our renaming technique on top of a modern out-of-order processor. Our experimental results show that it provides 6% speedup on average for the SPEC2006 benchmarks. Alternatively, our renaming scheme achieves the same performance while reducing the number of physical registers by 10.5%.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327014","Register Renaming;Register File;Precise Exceptions","Registers;Benchmark testing;Out of order;Computer architecture;Hardware;Parallel processing","","12","","20","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Wait of a Decade: Did SPEC CPU 2017 Broaden the Performance Horizon?","R. Panda; S. Song; J. Dean; L. K. John",equivalent contribution to this work; equivalent contribution to this work; The University of Texas at Austin; The University of Texas at Austin,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","271","282","The recently released SPEC CPU2017 benchmark suite has already started receiving a lot of attention from both industry and academic communities. However, due to the significantly high size and complexity of the benchmarks, simulating all the CPU2017 benchmarks for design trade-off evaluation is likely to become extremely difficult. Simulating a randomly selected subset, or a random input set, may result in misleading conclusions. This paper analyzes the SPEC CPU2017 benchmarks using performance counter based experimentation from seven commercial systems, and uses statistical techniques such as principal component analysis and clustering to identify similarities among benchmarks. Such analysis can reveal benchmark redundancies and identify subsets for researchers who cannot use all benchmarks in pre-silicon design trade-off evaluations. Many of the SPEC CPU2006 benchmarks have been replaced with larger and complex workloads in the SPEC CPU2017 suite. However, compared to CPU2006, it is unknown whether SPEC CPU2017 benchmarks have different performance demands or whether they stress machines differently. Additionally, to evaluate the balance of CPU2017 benchmarks, we analyze the performance characteristics of CPU2017 workloads and compare them with emerging database, graph analytics and electronic design automation (EDA) workloads. This paper provides the first detailed analysis of SPEC CPU2017 benchmark suite for the architecture community.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327015","Performance Evaluation;SPEC CPU2017;Benchmark Redundancy Analysis","Benchmark testing;C++ languages;Measurement;Industries;Principal component analysis;Redundancy;Stress","","73","","42","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Architectural Support for Task Dependence Management with Flexible Software Scheduling","E. Castillo; L. Alvarez; M. Moreto; M. Casas; E. Vallejo; J. L. Bosque; R. Beivide; M. Valero",Universidad Politecnica de Catalunya; Universidad Politecnica de Catalunya; NA; Universidad Politecnica de Catalunya; Universidad de Cantabria; Universidad de Cantabria; Universidad de Cantabria; Universidad Politecnica de Catalunya,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","283","295","The growing complexity of multi-core architectures has motivated a wide range of software mechanisms to improve the orchestration of parallel executions. Task parallelism has become a very attractive approach thanks to its programmability, portability and potential for optimizations. However, with the expected increase in core counts, finer-grained tasking will be required to exploit the available parallelism, which will increase the overheads introduced by the runtime system. This work presents Task Dependence Manager (TDM), a hardware/software co-designed mechanism to mitigate runtime system overheads. TDM introduces a hardware unit, denoted Dependence Management Unit (DMU), and minimal ISA extensions that allow the runtime system to offload costly dependence tracking operations to the DMU and to still perform task scheduling in software. With lower hardware cost, TDM outperforms hardware-based solutions and enhances the flexibility, adaptability and composability of the system. Results show that TDM improves performance by 12.3% and reduces EDP by 20.4% on average with respect to a software runtime system. Compared to a runtime system fully implemented in hardware, TDM achieves an average speedup of 4.2% with 7.3x less area requirements and significant EDP reductions. In addition, five different software schedulers are evaluated with TDM, illustrating its flexibility and performance gains.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327016","fine grain;dataflow;task based programming models;task dynamic dependences","Task analysis;Runtime;Software;Time division multiplexing;Hardware;Programming;Benchmark testing","","13","","43","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"GDP: Using Dataflow Properties to Accurately Estimate Interference-Free Performance at Runtime","M. Jahre; L. Eeckhout","Department of Computer Science, Norwegian University of Science and Technology (NTNU); Department of Electronics and Information Systems, Ghent University",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","296","309","Multi-core memory systems commonly share resources between processors. Resource sharing improves utilization at the cost of increased inter-application interference which may lead to priority inversion, missed deadlines and unpredictable interactive performance. A key component to effectively manage multi-core resources is performance accounting which aims to accurately estimate interference-free application performance. Previously proposed accounting systems are either invasive or transparent. Invasive accounting systems can be accurate, but slow down latency-sensitive processes. Transparent accounting systems do not affect performance, but tend to provide less accurate performance estimates. We propose a novel class of performance accounting systems that achieve both performance-transparency and superior accuracy. We call the approach dataflow accounting, and the key idea is to track dynamic dataflow properties and use these to estimate interference-free performance. Our main contribution is Graph-based Dynamic Performance (GDP) accounting. GDP dynamically builds a dataflow graph of load requests and periods where the processor commits instructions. This graph concisely represents the relationship between memory loads and forward progress in program execution. More specifically, GDP estimates interference-free stall cycles by multiplying the critical path length of the dataflow graph with the estimated interference-free memory latency. GDP is very accurate with mean IPC estimation errors of 3.4% and 9.8% for our 4- and 8-core processors, respectively. When GDP is used in a cache partitioning policy, we observe average system throughput improvements of 11.9% and 20.8% compared to partitioning using the state-of-the-art Application Slowdown Model.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327017","Multi-core memory systems;Accounting","Economic indicators;Interference;Program processors;Load modeling;Resource management;Throughput;System performance","","14","","66","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Crash Consistency in Encrypted Non-volatile Main Memory Systems","S. Liu; A. Kolli; J. Ren; S. Khan",University of Virginia; Pennsylvania State University; Microsoft Research; University of Virginia,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","310","323","Non-Volatile Main Memory (NVMM) systems provide high performance by directly manipulating persistent data in-memory, but require crash consistency support to recover data in a consistent state in case of a power failure or system crash. In this work, we focus on the interplay between the crash consistency mechanisms and memory encryption. Memory encryption is necessary for these systems to protect data against the attackers with physical access to the persistent main memory. As decrypting data at every memory read access can significantly degrade the performance, prior works propose to use a memory encryption technique, counter-mode encryption, that reduces the decryption overhead by performing a memory read access in parallel with the decryption process using a counter associated with each cache line. Therefore, a pair of data and counter value is needed to correctly decrypt data after a system crash. We demonstrate that counter-mode encryption does not readily extend to crash consistent NVMM systems as the system will fail to recover data in a consistent state if the encrypted data and associated counter are not written back to memory atomically, a requirement we refer to as counter-atomicity. We show that näıvely enforcing counter-atomicity for all NVMM writes can serialize memory accesses and results in a significant performance degradation. In order to improve the performance, we make an observation that not all writes to NVMM need to be counter-atomic. The crash consistency mechanisms rely on versioning to keep one consistent copy of data intact while manipulating another version directly in-memory. As the recovery process only relies on the unmodified consistent version, it is not necessary to strictly enforce counter-atomicity for the writes that do not affect data recovery. Based on this insight, we propose selective counter-atomicity that allows reordering of writes to data and associated counters when the writes to persistent memory do not alter the recoverable consistent state. We propose efficient software and hardware support to enforce selective counter-atomicity. Our evaluation demonstrates that in a 1/2/4/8- core system, selective counter-atomicity improves performance by 6/11/22/40% compared to a system that enforces counter-atomicity for all NVMM writes. The performance of our selective counter-atomicity design comes within 5% of an ideal NVMM system that provides crash consistency of encrypted data at no cost.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327018","","Nonvolatile memory;Computer crashes;Encryption;Memory management;Hardware;Optimization","","64","","65","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Adaptive Memory Fusion: Towards Transparent, Agile Integration of Persistent Memory","D. Xue; C. Li; L. Huang; C. Wu; T. Li","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Intel Asia Pacific R&D Co. LTD",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","324","335","The great promise of in-memory computing inspires engineers to scale their main memory subsystems in a timely and efficient manner. Offering greatly expanded capacity at near-DRAM speed, today's new-generation persistent memory (PM) module is no doubt an ideal candidate for system upgrade. However, integrating DRAM-comparable PMs in current enterprise systems faces big barriers in terms of huge system modifications for software compatibility and complex runtime support. In addition, the very large PM capacity unavoidably results in massive metadata, which introduces significant performance and energy overhead. The inefficiency issue becomes even acute when the memory system reaches its capacity limit or the application requires large memory space allocation. In this paper we propose adaptive memory fusion (AMF), a novel PM integration scheme that jointly solves the above issues. Rather than struggle to adapt to the persistence property of PM through modifying the full software stack, we focus on exploiting the high capacity feature of emerging PM modules. AMF is designed to be totally transparent to user applications by carefully hiding PM devices and managing the available PM space in a DRAM-like way. To further improve the performance, we devise holistic optimization scheme that allows the system to efficiently utilize system resources. Specifically, AMF is able to adaptively release PM based on memory pressure status, smartly reclaim PM pages, and enable fast space expansion with direct PM pass-through. We implement AMF as a kernel subsystem in Linux. Compared to traditional approaches, AMF could decrease the page faults number of high-resident-set benchmarks by up to 67.8% with an average of 46.1%. Using realistic in-memory database, we show that AMF outperforms existing solutions by 57.7% on SQLite and 21.8% on Redis. Overall, AMF represents a more lightweight design approach and it would greatly encourage rapid and flexible adoption of PM in the near future.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327019","Persistent Memory;In Memory Computing;DRAM;Adaptive;Integration","Random access memory;Memory management;Kernel;Databases;Metadata","","7","","47","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Steal but No Force: Efficient Hardware Undo+Redo Logging for Persistent Memory Systems","M. A. Ogleari; E. L. Miller; J. Zhao","Univ. of California, Santa Cruz, Santa Cruz, CA, USA; Univ. of California, Santa Cruz, Santa Cruz, CA, USA; Univ. of California, Santa Cruz, Santa Cruz, CA, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","336","349","Persistent memory is a new tier of memory that functions as a hybrid of traditional storage systems and main memory. It combines the benefits of both: the data persistence of storage with the fast load/store interface of memory. Most previous persistent memory designs place careful control over the order of writes arriving at persistent memory. This can prevent caches and memory controllers from optimizing system performance through write coalescing and reordering. We identify that such write-order control can be relaxed by employing undo+redo logging for data in persistent memory systems. However, traditional software logging mechanisms are expensive to adopt in persistent memory due to performance and energy overheads. Previously proposed hardware logging schemes are inefficient and do not fully address the issues in software. To address these challenges, we propose a hardware undo+redo logging scheme which maintains data persistence by leveraging the write-back, write-allocate policies used in commodity caches. Furthermore, we develop a cache force-write-back mechanism in hardware to significantly reduce the performance and energy overheads from forcing data into persistent memory. Our evaluation across persistent memory microbenchmarks and real workloads demonstrates that our design significantly improves system throughput and reduces both dynamic energy and memory traffic. It also provides strong consistency guarantees compared to software approaches.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327020","NVM;persistent;memory;NVRAM;non volatile;nonvolatile;logging;hardware;architecture;microarchitecture;PRAM;efficient","Random access memory;Nonvolatile memory;Hardware;Program processors;Force;Control systems","","43","","54","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Enabling Fine-Grain Restricted Coset Coding Through Word-Level Compression for PCM","S. Seyedzadeh; A. Jones; R. Melhem","Computer Science Department, University of Pittsburgh; Electrical and Computer Engineering Department, University of Pittsburgh; Computer Science Department, University of Pittsburgh",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","350","361","Phase change memory (PCM) has recently emerged as a promising technology to meet the fast growing demand for large capacity memory in computer systems, replacing DRAM that is impeded by physical limitations. Multi-level cell (MLC) PCM offers high density with low per-byte fabrication cost. However, despite many advantages, such as scalability and low leakage, the energy for programming intermediate states is considerably larger than programming single-level cell PCM. In this paper, we study encoding techniques to reduce write energy for MLC PCM when the encoding granularity is lowered below the typical cache line size. We observe that encoding data blocks at small granularity to reduce write energy actually increases the write energy because of the auxiliary encoding bits. We mitigate this adverse effect by 1) designing suitable codeword mappings that use fewer auxiliary bits and 2) proposing a new Word-Level Compression (WLC) which compresses more than 91% of the memory lines and provides enough room to store the auxiliary data using a novel restricted coset encoding applied at small data block granularities. Experimental results show that the proposed encoding at 16-bit data granularity reduces the write energy by 39%, on average, versus the leading encoding approach for write energy reduction. Furthermore, it improves endurance by 20% and is more reliable than the leading approach. Hardware synthesis evaluation shows that the proposed encoding can be implemented on-chip with only a nominal area overhead.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327021","PCM;Word Level Compression;Coset Coding","Encoding;Phase change materials;Computer architecture;Microprocessors;Programming;Resistance;Energy states","","12","","37","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Perception-Oriented 3D Rendering Approximation for Modern Graphics Processors","C. Xie; X. Fu; S. Song","ECE Department, University of Houston; ECE Department, University of Houston; High Performance Computing Group, Pacific Northwest National Lab (PNNL)",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","362","374","Anisotropic filtering enabled by modern rasterization-based GPUs provides users with extremely authentic visualization experience, but significantly limits the performance and energy efficiency of 3D rendering process due to its large texture data requirement. To improve 3D rendering efficiency, we build a bridge between anisotropic filtering process and human visual system by analyzing users’ perception on image quality. We discover that anisotropic filtering does not impact user perceived image quality on every pixel. This motives us to approximate the anisotropic filtering process for non-perceivable pixels in order to improve the overall 3D rendering performance without damaging user experience. To achieve this goal, we propose a perceptionoriented runtime approximation model for 3D rendering by leveraging the inner-relationship between anisotropic and isotropic filtering. We also provide a low-cost texture unit design for enabling this approximation. Extensive evaluation on modern 3D games demonstrates that, under a conservative tuning point, our design achieves a significant average speedup of 17% for the overall 3D rendering along with 11% total GPU energy reduction, without visible image quality loss from users’ perception. It also reduces the texture filtering latency by an average of 29%. Additionally, it creates a unique perception-based tuning space for performance-quality tradeoffs on graphics processors.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327022","GPU;Approximate Computing;3D Rendering;User-Oriented Study","Three-dimensional displays;Rendering (computer graphics);Image quality;Image color analysis;Computer architecture;Runtime;Graphics processing units","","10","","49","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Warp Scheduling for Fine-Grained Synchronization","A. ElTantawy; T. M. Aamodt","The University of British Columbia Faculty of Medicine, Vancouver, BC, CA; University of British Columbia, Canada",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","375","388","Fine-grained synchronization is employed in many parallel algorithms and is often implemented using busy-wait synchronization (e.g., spin locks). However, busy-wait synchronization incurs significant overheads and existing CPU solutions do not readily translate to single-instruction, multiple-thread (SIMT) graphics processor unit (GPU) architectures. In this paper, we propose Back-Off Warp Spinning (BOWS), a hardware warp scheduling policy that extends existing warp scheduling policies to temporarily deprioritize warps executing busy wait code. In addition, we propose Dynamic Detection of Spinning (DDOS), a novel hardware mechanism for accurately and efficiently detecting busy-wait synchronization on GPUs. On a set of GPU kernels employing busy-wait synchronization, DDOS identifies all busy-wait loops incurring no false detections. BOWS improves performance by 1.5× and reduces energy consumption by 1.6× versus Criticality-Aware Warp Acceleration (CAWA) [14].,,,,","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327023","Warp Scheduling;Synchronization;SIMT Architectures;Busy Wait;GPGPUs","Synchronization;Delays;Graphics processing units;Spinning;Computer architecture;Hardware;Processor scheduling","","26","","37","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"WIR: Warp Instruction Reuse to Minimize Repeated Computations in GPUs","K. Kim; W. W. Ro","School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","389","402","Warp instructions with an identical arithmetic operation on same input values produce the identical computation results. This paper proposes warp instruction reuse to allow such repeated warp instructions to reuse previous computation results instead of actually executing the instructions. Bypassing register reading, functional unit, and register writing operations improves energy efficiency. This reuse technique is especially beneficial for GPUs since a GPU warp register is usually as wide as thousands of bits. In addition, we propose warp register reuse which allows identical warp register values to share a single physical register through register renaming. The register reuse technique enables to see if different logical warp registers have an identical value by only looking at their physical warp register IDs. Based on this observation, warp register reuse helps to perform all necessary operations for warp instruction reuse with register IDs, which is substantially more efficient than directly manipulating register values. Performance evaluation shows that 20.5% SM energy and 10.7% GPU energy can be saved by allowing 18.7% of warp instructions to reuse prior results.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327024","GPU;microarchitecture;instruction reuse;register renaming","Registers;Instruction sets;Graphics processing units;Pipelines;Microarchitecture;Benchmark testing;Resource management","","17","","66","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"G-TSC: Timestamp Based Coherence for GPUs","A. Tabbakh; X. Qian; M. Annavaram","Electrical Engineering Dept., University of Southern California, Los Angeles, USA; Electrical Engineering Dept., University of Southern California, Los Angeles, USA; Electrical Engineering Dept., University of Southern California, Los Angeles, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","403","415","Cache coherence has been studied extensively in the context of chip multiprocessors (CMP). It is well known that conventional directory-based and snooping coherence protocols generate considerable coherence traffic as the number of hardware thread contexts increase. Since GPUs support hundreds or even thousands of threads, conventional coherence mechanisms when applied to GPUs will exacerbate the the bandwidth constraints that GPUs already face. Recognizing this constraint, prior work has proposed time-based coherence protocols. The main idea is to assign a lease period to the accessed cache block, and after the lease expires the cache block is self-invalidated. However, time-based coherence protocols require global synchronized clocks. Furthermore, this approach may increase execution stalls since threads have to wait to access data with an unexpired lease. Tardis is timestamp-based coherence protocol that has been proposed recently to alleviate the need for global clocks in CPUs. This paper builds on this prior work and proposes G-TSC, a novel cache coherence protocol for GPUs that is based on timestamp ordering. G-TSC conducts its coherence transactions in logical time. This work demonstrates the challenges in adopting timestamp coherence for GPUs which support massive thread parallelism and have unique microarchitecture features. This work then presents a number of solutions that tackle GPU-centric challenges. Evaluation of G-TSC implemented in the GPGPU-Sim simulation framework shows that G-TSC outperforms time-based coherence by 38% with release consistency.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327025","GPU;cache coherence","Coherence;Protocols;Instruction sets;Graphics processing units;Clocks;Synchronization;Hardware","","9","","37","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"D-ORAM: Path-ORAM Delegation for Low Execution Interference on Cloud Servers with Untrusted Memory","R. Wang; Y. Zhang; J. Yang","Electr. & Comput. Eng. Dept., Univ. of Pittsburgh, Pittsburgh, PA, USA; Comput. Sci. Dept., Univ. of Pittsburgh, Pittsburgh, PA, USA; Electr. & Comput. Eng. Dept., Univ. of Pittsburgh, Pittsburgh, PA, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","416","427","Cloud computing has evolved into a promising computing paradigm. However, it remains a challenging task to protect application privacy and, in particular, the memory access patterns, on cloud servers. The Path ORAM protocol achieves high-level privacy protection but requires large memory bandwidth, which introduces severe execution interference. The recently proposed secure memory model greatly reduces the security enhancement overhead but demands the secure integration of cryptographic logic and memory devices, a memory architecture that is yet to prevail in mainstream cloud servers. In this paper, we propose D-ORAM, a novel Path ORAM scheme for achieving high-level privacy protection and low execution interference on cloud servers with untrusted memory. D-ORAM leverages the buffer-on-board (BOB) memory architecture to offload the Path ORAM primitives to a secure engine in the BOB unit, which greatly alleviates the contention for the off-chip memory bus between secure and non-secure applications. D-ORAM upgrades only one secure memory channel and employs Path ORAM tree split to extend the secure application flexibly across multiple channels, in particular, the non-secure channels. D-ORAM optimizes the link utilization to further improve the system performance. Our evaluation shows that D-ORAM effectively protects application privacy on mainstream computing servers with untrusted memory, with an improvement of NS-App performance by 22.5% on average over the Path ORAM baseline.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327026","ORAM;hardware security;server;memory system","Servers;Cryptography;Memory architecture;Interference;Memory management","","23","","48","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Secure DIMM: Moving ORAM Primitives Closer to Memory","A. Shafiee; R. Balasubramonian; M. Tiwari; F. Li","School of Computing, University of Utah, Salt Lake City, Utah, USA; School of Computing, University of Utah, Salt Lake City, Utah, USA; School of Computing, University of Utah, Salt Lake City, Utah, USA; Dept. of ECE, University of Texas Austin, Austin, Texas, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","428","440","As more critical applications move to the cloud, there is a pressing need to provide privacy guarantees for data and computation. While cloud infrastructures are vulnerable to a variety of attacks, in this work, we focus on an attack model where an untrusted cloud operator has physical access to the server and can monitor the signals emerging from the processor socket. Even if data packets are encrypted, the sequence of addresses touched by the program serves as an information side channel. To eliminate this side channel, Oblivious RAM constructs have been investigated for decades, but continue to pose large overheads. In this work, we make the case that ORAM overheads can be significantly reduced by moving some ORAM functionality into the memory system. We first design a secure DIMM (or SDIMM) that uses commodity low-cost memory and an ASIC as a secure buffer chip. We then design two new ORAM protocols that leverage SDIMMs to reduce bandwidth, latency, and energy per ORAM access. In both protocols, each SDIMM is responsible for part of the ORAM tree. Each SDIMM performs a number of ORAM operations that are not visible to the main memory channel. By having many SDIMMs in the system, we are able to achieve highly parallel ORAM operations. The main memory channel uses its bandwidth primarily to service blocks requested by the CPU, and to perform a small subset of the many shuffle operations required by conventional ORAM. The new protocols guarantee the same obliviousness properties as Path ORAM. On a set of memory-intensive workloads, our two new ORAM protocols – Independent ORAM and Split ORAM – are able to improve performance by 1.9x and energy by 2.55x, compared to Freecursive ORAM.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327027","Privacy;Oblivious RAM;Memory Systems","Bandwidth;Protocols;Servers;Memory management;Parallel processing;DRAM chips","","23","","37","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Comprehensive VM Protection Against Untrusted Hypervisor Through Retrofitted AMD Memory Encryption","Y. Wu; Y. Liu; R. Liu; H. Chen; B. Zang; H. Guan","Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University; Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University; Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University; Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University; Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University; Institute of Parallel and Distributed Systems-Shanghai, Jiao Tong University",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","441","453","The confidentiality of tenant's data is confronted with high risk when facing hardware attacks and privileged malicious software. Hardware-based memory encryption is one of the promising means to provide strong guarantees of data security. Recently AMD has proposed its new memory encryption hardware called SME and SEV, which can selectively encrypt memory regions in a fine-grained manner, e.g., by setting the C-bits in the page table entries. More importantly, SEV further supports encrypted virtual machines. This, intuitively, has provided a new opportunity to protect data confidentiality in guest VMs against an untrusted hypervisor in the cloud environment. In this paper, we first provide a security analysis on the (in)security of SEV and uncover a set of security issues of using SEV as a means to defend against an untrusted hypervisor. Based on the study, we then propose a software-based extension to the SEV feature, namely Fidelius, to address those issues while retaining performance efficiency. Fidelius separates the management of critical resources from service provisioning and revokes the permissions of accessing specific resources from the un-trusted hypervisor. By adopting a sibling-based protection mechanism with non-bypassable memory isolation, Fidelius embraces both security and efficiency, as it introduces no new layer of abstraction. Meanwhile, Fidelius reuses the SEV API to provide a full VM life-cycle protection, including two sets of para-virtualized I/O interfaces to encode the I/O data, which is not considered in the SEV hardware design. A detailed and quantitative security analysis shows its effectiveness in protecting tenant's data from a variety of attack surfaces, and the performance evaluation confirms the performance efficiency of Fidelius.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327028","VM protection;Untrusted hypervisor;Memory encryption","Virtual machine monitors;Encryption;Hardware;Virtualization;Memory management","","13","","43","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"SYNERGY: Rethinking Secure-Memory Design for Error-Correcting Memories","G. Saileshwar; P. J. Nair; P. Ramrakhyani; W. Elsasser; M. K. Qureshi",NA; IBM Research; ARM Research; ARM Research; Georgia Institute of Technology,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","454","465","Building trusted data-centers requires resilient memories which are protected from both adversarial attacks and errors. Unfortunately, the state-of-the-art memory security solutions incur considerable performance overheads due to accesses for security metadata like Message Authentication Codes (MACs). At the same time, commercial secure memory solutions tend to be designed oblivious to the presence of memory reliability mechanisms (such as ECC-DIMMs), that provide tolerance to memory errors. Fortunately, ECC-DIMMs possess an additional chip for providing error correction codes (ECC), that is accessed in parallel with data, which can be harnessed for security optimizations. If we can re-purpose the ECC-chip to store some metadata useful for security and reliability, it can prove beneficial to both. To this end, this paper proposes Synergy, a reliability-security co-design that improves performance of secure execution while providing strong reliability for systems with 9-chip ECC-DIMMs. Synergy uses the insight that MACs being capable of detecting data tampering are also useful for detecting memory errors. Therefore, MACs are best suited for being placed inside the ECC chip, to be accessed in parallel with each data access. By co-locating MAC and Data, Synergy is able to avoid a separate memory access for MAC and thereby reduce the overall memory traffic for secure memory systems. Furthermore, Synergy is able to tolerate 1 chip failure out of 9 chips by using a parity that is constructed over 9 chips (8 Data and 1 MAC), which is used for reconstructing the data of the failed chip. For memory intensive workloads, Synergy provides a speedup of 20% and reduces system Energy Delay Product by 31% compared to a secure memory baseline with ECC-DIMMs. At the same time, Synergy increases reliability by 185x compared to ECC-DIMMs that provide Single-Error Correction, Double-Error Detection (SECDED) capability. Synergy uses commercial ECC-DIMMs and does not incur any additional hardware overheads or reduction of security.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327029","Memory-Security;Reliability;ECC-DIMM","Encryption;Metadata;Memory management;Reliability engineering;Error correction codes","","63","1","59","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"A Case for Packageless Processors","S. Pal; D. Petrisko; A. A. Bajwa; P. Gupta; S. S. Iyer; R. Kumar","Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","466","479","Demand for increasing performance is far outpacing the capability of traditional methods for performance scaling. Disruptive solutions are needed to advance beyond incremental improvements. Traditionally, processors reside inside packages to enable PCB-based integration. We argue that packages reduce the potential memory bandwidth of a processor by at least one order of magnitude, allowable thermal design power (TDP) by up to 70%, and area efficiency by a factor of 5 to 18. Further, silicon chips have scaled well while packages have not. We propose packageless processors - processors where packages have been removed and dies directly mounted on a silicon board using a novel integration technology, Silicon Interconnection Fabric (Si-IF). We show that Si-IF-based packageless processors outperform their packaged counterparts by up to 58% (16% average), 136%(103% average), and 295% (80% average) due to increased memory bandwidth, increased allowable TDP, and reduced area respectively. We also extend the concept of packageless processing to the entire processor and memory system, where the area footprint reduction was up to 76%.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327030","Packageless Processors;Silicon Interconnect Fabric","Program processors;Silicon;Bandwidth;Thermal resistance;Pins;Heat sinks","","22","1","76","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Extending the Power-Efficiency and Performance of Photonic Interconnects for Heterogeneous Multicores with Machine Learning","S. Van Winkle; A. K. Kodi; R. Bunescu; A. Louri","School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; Department of Electrical and Computer Engineering, George Washington University, Washington DC",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","480","491","As communication energy exceeds computation energy in future technologies, traditional on-chip electrical interconnects face fundamental challenges in the many-core era. Photonic interconnects have been proposed as a disruptive technology solution due to superior performance per Watt, distance independent energy consumption and CMOS compatibility for on-chip interconnects. Static power due to the laser being always switched on, varying link utilization due to spatial and temporal traffic fluctuations and thermal sensitivity are some of the critical challenges facing photonics interconnects. In this paper, we propose photonic interconnects for heterogeneous multicores using a checkerboard pattern that clusters CPU-GPU cores together and implements bandwidth reconfiguration using local router information without global coordination. To reduce the static power, we also propose a dynamic laser scaling technique that predicts the power level for the next epoch using the buffer occupancy of previous epoch. To further improve power-performance trade-offs, we also propose a regression-based machine learning technique for scaling the power of the photonic link. Our simulation results demonstrate a 34% performance improvement over a baseline electrical CMESH while consuming 25% less energy per bit when dynamically reallocating bandwidth. When dynamically scaling laser power, our buffer-based reactive and ML-based proactive prediction techniques show 40 - 65% in power savings with 0 - 14% in throughput loss depending on the reservation window size.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327031","Network on Chips;Photonics;Power Scaling;Machine Learning","Photonics;Power lasers;Bandwidth;Multicore processing;System-on-chip;Graphics processing units","","16","","58","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Routerless Network-on-Chip","F. Alazemi; A. AziziMazreah; B. Bose; L. Chen","Oregon State University, USA; Oregon State University, USA; Oregon State University, USA; Oregon State University, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","492","503","Traditional bus-based interconnects are simple and easy to implement, but the scalability is greatly limited. While router-based networks-on-chip (NoCs) offer superior scalability, they also incur significant power and area overhead due to complex router structures. In this paper, we explore a new class of on-chip networks, referred to as Routerless NoCs, where routers are completely eliminated. We propose a novel design that utilizes on-chip wiring resources smartly to achieve comparable hop count and scalability as router-based NoCs. Several effective techniques are also proposed that significantly reduce the resource requirement to avoid new network abnormalities in routerless NoC designs. Evaluation results show that, compared with a conventional mesh, the proposed routerless NoC achieves 9.5X reduction in power, 7.2X reduction in area, 2.5X reduction in zero-load packet latency, and 1.7X increase in throughput. Compared with a state-of-the-art low-cost NoC design, the proposed approach achieves 7.7X reduction in power, 3.3X reduction in area, 1.3X reduction in zero-load packet latency, and 1.6X increase in throughput.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327032","Network on Chip;topology;CMP","Wiring;System-on-chip;Switches;Scalability;Throughput;Wires;Program processors","","40","","39","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"HeatWatch: Improving 3D NAND Flash Memory Device Reliability by Exploiting Self-Recovery and Temperature Awareness","Y. Luo; S. Ghose; Y. Cai; E. F. Haratsch; O. Mutlu",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Seagate Technology; Carnegie Mellon University,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","504","517","NAND flash memory density continues to scale to keep up with the increasing storage demands of data-intensive applications. Unfortunately, as a result of this scaling, the lifetime of NAND flash memory has been decreasing. Each cell in NAND flash memory can endure only a limited number of writes, due to the damage caused by each program and erase operation on the cell. This damage can be partially repaired on its own during the idle time between program or erase operations (known as the dwell time), via a phenomenon known as the self-recovery effect. Prior works study the self-recovery effect for planar (i.e., 2D) NAND flash memory, and propose to exploit it to improve flash lifetime, by applying high temperature to accelerate self-recovery. However, these findings may not be directly applicable to 3D NAND flash memory, due to significant changes in the design and manufacturing process that are required to enable practical 3D stacking for NAND flash memory. In this paper, we perform the first detailed experimental characterization of the effects of self-recovery and temperature on real, state-of-the-art 3D NAND flash memory devices. We show that these effects influence two major factors of NAND flash memory reliability: (1) retention loss speed (i.e., the speed at which a flash cell leaks charge), and (2) program variation (i.e., the difference in programming speed across flash cells). We find that self-recovery and temperature affect 3D NAND flash memory quite differently than they affect planar NAND flash memory, rendering prior models of self-recovery and temperature ineffective for 3D NAND flash memory. Using our characterization results, we develop a new model for 3D NAND flash memory reliability, which predicts how retention, wearout, self-recovery, and temperature affect raw bit error rates and cell threshold voltages. We show that our model is accurate, with an error of only 4.9%. Based on our experimental findings and our model, we propose HeatWatch, a new mechanism to improve 3D NAND flash memory reliability. The key idea of HeatWatch is to optimize the read reference voltage, i.e., the voltage applied to the cell during a read operation, by adapting it to the dwell time of the workload and the current operating temperature. HeatWatch (1) efficiently tracks flash memory temperature and dwell time online, (2) sends this information to our reliability model to predict the current voltages of flash cells, and (3) predicts the optimal read reference voltage based on the current cell voltages. Our detailed experimental evaluations show that HeatWatch improves flash lifetime by 3.85× over a baseline that uses a fixed read reference voltage, averaged across 28 real storage workload traces, and comes within 0.9% of the lifetime of an ideal read reference voltage selection mechanism.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327033","","Flash memories;Three-dimensional displays;Threshold voltage;Heating systems;Reliability;Transistors;Solid modeling","","84","","67","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"RC-NVM: Enabling Symmetric Row and Column Memory Accesses for In-memory Databases","P. Wang; S. Li; G. Sun; X. Wang; Y. Chen; H. Li; J. Cong; N. Xiao; T. Zhang","Center for Energy-efficient Computing and Applications, Peking University, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, China; Center for Energy-efficient Computing and Applications, Peking University, China; Center for Energy-efficient Computing and Applications, Peking University, China; Department of Electrical and Computer Engineering, Duke University, USA; Department of Electrical and Computer Engineering, Duke University, USA; University of California, Los Angeles, USA; State Key Laboratory of High Performance Computing, National University of Defense Technology, China; Pennsylvania State University, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","518","530","Ever increasing DRAM capacity has fostered the development of in-memory databases (IMDB). The massive performance improvements provided by IMDBs have enabled transactions and analytics on the same database. In other words, the integration of OLTP (on-line transactional processing) and OLAP (on-line analytical processing) systems is becoming a general trend. However, conventional DRAM-based main memory is optimized for row-oriented accesses generated by OLTP workloads in row-based databases. OLAP queries scanning on specified columns cause so-called strided accesses and result in poor memory performance. Since memory access latency dominates in IMDB processing time, it can degrade overall performance significantly. To overcome this problem, we propose a dual-addressable memory architecture based on non-volatile memory, called RC-NVM, to support both row-oriented and column-oriented accesses. We first present circuit-level analysis to prove that such a dual-addressable architecture is only practical with RC-NVM rather than DRAM technology. Then, we rethink the addressing schemes, data layouts, cache synonym, and coherence issues of RC-NVM in architectural level to make it applicable for IMDBs. Finally, we propose a group caching technique that combines the IMDB knowledge with the memory architecture to further optimize the system. Experimental results show that the memory access performance can be improved up to 14.5X with only 15% area overhead.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327034","Non volatitle Memory;In Memory Database;OLTP;OLAP","Random access memory;Databases;Layout;Nonvolatile memory;Memory architecture;Decoding","","17","","51","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"GraphR: Accelerating Graph Processing Using ReRAM","L. Song; Y. Zhuo; X. Qian; H. Li; Y. Chen",Duke University; University of southern California; University of southern California; Duke University; Duke University,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","531","543","Graph processing recently received intensive interests in light of a wide range of needs to understand relationships. It is well-known for the poor locality and high memory bandwidth requirement. In conventional architectures, they incur a significant amount of data movements and energy consumption which motivates several hardware graph processing accelerators. The current graph processing accelerators rely on memory access optimizations or placing computation logics close to memory. Distinct from all existing approaches, we leverage an emerging memory technology to accelerate graph processing with analog computation. This paper presents GRAPHR, the first ReRAM-based graph processing accelerator. GRAPHR follows the principle of near-data processing and explores the opportunity of performing massive parallel analog operations with low hardware and energy cost. The analog computation is suitable for graph processing because: 1) The algorithms are iterative and could inherently tolerate the imprecision; 2) Both probability calculation (e.g., PageRank and Collaborative Filtering) and typical graph algorithms involving integers (e.g., BFS/SSSP) are resilient to errors. The key insight of GRAPHR is that if a vertex program of a graph algorithm can be expressed in sparse matrix vector multiplication (SpMV), it can be efficiently performed by ReRAM crossbar. We show that this assumption is generally true for a large set of graph algorithms. GRAPHR is a novel accelerator architecture consisting of two components: memory ReRAM and graph engine (GE). The core graph computations are performed in sparse matrix format in GEs (ReRAM crossbars). The vector/matrix-based graph computation is not new, but ReRAM offers the unique opportunity to realize the massive parallelism with unprecedented energy efficiency and low hardware cost. With small subgraphs processed by GEs, the gain of performing parallel operations overshadows the wastes due to sparsity. The experiment results show that GRAPHR achieves a 16.01× (up to 132.67×) speedup and a 33.82× energy saving on geometric mean compared to a CPU baseline system. Compared to GPU, GRAPHR achieves 1.69× to 2.19× speedup and consumes 4.77× to 8.91× less energy. GRAPHR gains a speedup of 1.16× to 4.12×, and is 3.67× to 10.96× more energy efficiency compared to PIM-based architecture.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327035","Accelerator Architecture;Domain specific Architecture;Graph Processing;ReRAM;SpMV","Sparse matrices;Hardware;Memory management;Bandwidth;Random access memory;Parallel processing","","222","","70","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"GraphP: Reducing Communication for PIM-Based Graph Processing with Efficient Data Partition","M. Zhang; Y. Zhuo; C. Wang; M. Gao; Y. Wu; K. Chen; C. Kozyrakis; X. Qian","Sangfor Technologies Inc.; University of Southern California; University of Southern California; Stanford University; Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology (TNLIST), Beijing, China; Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology (TNLIST), Beijing, China; Stanford University; University of Southern California",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","544","557","Processing-In-Memory (PIM) is an effective technique that reduces data movements by integrating processing units within memory. The recent advance of “big data” and 3D stacking technology make PIM a practical and viable solution for the modern data processing workloads. It is exemplified by the recent research interests on PIM-based acceleration. Among them, TESSERACT is a PIM-enabled parallel graph processing architecture based on Micron’s Hybrid Memory Cube (HMC), one of the most prominent 3D-stacked memory technologies. It implements a Pregel-like vertex-centric programming model, so that users could develop programs in the familiar interface while taking advantage of PIM. Despite the orders of magnitude speedup compared to DRAM-based systems, TESSERACT generates excessive crosscube communications through SerDes links, whose bandwidth is much less than the aggregated local bandwidth of HMCs. Our investigation indicates that this is because of the restricted data organization required by the vertex programming model. In this paper, we argue that a PIM-based graph processing system should take data organization as a first-order design consideration. Following this principle, we propose GraphP, a novel HMC-based software/hardware co-designed graph processing system that drastically reduces communication and energy consumption compared to TESSERACT. GraphP features three key techniques. 1) “Source-cut” partitioning, which fundamentally changes the cross-cube communication from one remote put per cross-cube edge to one update per replica. 2) “Two-phase Vertex Program”, a programming model designed for the “source-cut” partitioning with two operations: GenUpdate and ApplyUpdate. 3) Hierarchical communication and overlapping, which further improves performance with unique opportunities offered by the proposed partitioning and programming model. We evaluate GraphP using a cycle accurate simulator with 5 real-world graphs and 4 algorithms. The results show that it provides on average 1.7 speedup and 89% energy saving compared to TESSERACT.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327036","Graph processing;Processing In Memory;Hybrid Memory Cube","Bandwidth;Programming;Organizations;Three-dimensional displays;Partitioning algorithms;Memory management","","158","","47","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"PM3: Power Modeling and Power Management for Processing-in-Memory","C. Zhang; T. Meng; G. Sun","Center for Energy-efficient Computing and Applications, Peking University, Beijing, China; Center for Energy-efficient Computing and Applications, Peking University, Beijing, China; Center for Energy-efficient Computing and Applications, Peking University, Beijing, China",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","558","570","Processing-in-Memory (PIM) has been proposed as a solution to accelerate data-intensive applications, such as real-time Big Data processing and neural networks. The acceleration of data processing using a PIM relies on its high internal memory bandwidth, which always comes with the cost of high power consumption. Consequently, it is important to have a comprehensive quantitative study of the power modeling and power management for such PIM architectures. In this work, we first model the relationship between the power consumption and the internal bandwidth of PIM. This model not only provides a guidance for PIM designs but also demonstrates the potential of power management via bandwidth throttling. Based on bandwidth throttling, we propose three techniques, Power-Aware Subtask Throttling (PAST), Processing Unit Boost (PUB), and Power Sprinting (PS), to improve the energy efficiency and performance. In order to demonstrate the universality of the proposed methods, we applied them to two kinds of popular PIM designs. Evaluations show that the performance of PIM can be further improved if the power consumption is carefully controlled. Targeting at the same performance, the peak power consumption of HMC-based PIM can be reduced from 20W to 15W. The proposed power management schemes improve the speedup of prior RRAM-based PIM from 69 × to 273 ×, after pushing the power usage from about 1W to 10W safely. The model also shows that emerging RRAM is more suitable for large processing-in-memory designs, due to its low power cost to store the data.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327037","Processing in Memory;Non volatile Memory;Power Management","Bandwidth;Power demand;Random access memory;Power system management;Memory management;Nonvolatile memory","","10","","78","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Don’t Correct the Tags in a Cache, Just Check Their Hamming Distance from the Lookup Tag","A. Gendler; A. Bramnik; A. Szapiro; Y. Sazeides","Intel Israel Design Center, Haifa, Israel; Intel Israel Design Center, Haifa, Israel; Intel Israel Design Center, Haifa, Israel; Univ. of Cyprus, Nicosia, Cyprus",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","571","582","This paper describes the design of an efficient technique for correcting errors in the tag array of set-associative caches. The main idea behind this scheme is that for a cache tag array protected with ECC code, the stored tags do not need to be corrected prior to the comparison against a lookup tag for cache hit/miss definition. This eliminates the need for costly hardware to correct the cache tags before checking for a hit or a miss. The paper reveals the various optimizations needed to translate this idea into a design that delivers a practical improvement in a product. An analysis of our design, as compared to state of the art methods, shows that it can provide the same correction and detection strength with less area, power and timing overheads and better performance. An Intel Core® microprocessor is implementing this technique in its second level and third level caches.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327038","Caches;Soft Error;Error Correction","Error correction codes;Arrays;Hamming distance;Microprocessors;Optimization;Organizations;Timing","","6","1","22","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Reliability-Aware Data Placement for Heterogeneous Memory Architecture","M. Gupta; V. Sridharan; D. Roberts; A. Prodromou; A. Venkat; D. Tullsen; R. Gupta","CSE Dept., University of California, San Diego; RAS Architecture; RAS Architecture; University of California San Diego, La Jolla, CA, US; CSE Dept., University of California, San Diego; CSE Dept., University of California, San Diego; CSE Dept., University of California, San Diego",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","583","595","System reliability is a first-class concern as technology continues to shrink, resulting in increased vulnerability to traditional sources of errors such as single event upsets. By tracking access counts and the Architectural Vulnerability Factor (AVF), application data can be partitioned into groups based on how frequently it is accessed (its ""hotness"") and its likelihood to cause program execution error (its ""risk""). This is particularly useful for memory systems which exhibit heterogeneity in their performance and reliability such as Heterogeneous Memory Architectures – with a typical configuration combining slow, highly reliable memory with faster, less reliable memory. This work demonstrates that current state of the art, performance-focused data placement techniques affect reliability adversely. It shows that page risk is not necessarily correlated with its hotness; this makes it possible to identify pages that are both hot and low risk, enabling page placement strategies that can find a good balance of performance and reliability. This work explores heuristics to identify and monitor both hotness and risk at run-time, and further proposes static, dynamic, and program annotation-based reliability-aware data placement techniques. This enables an architect to choose among available memories with diverse performance and reliability characteristics. The proposed heuristic-based reliability-aware data placement improves reliability by a factor of 1.6x compared to performance-focused static placement while limiting the performance degradation to 1%. A dynamic reliability-aware migration scheme, which does not require prior knowledge about the application, improves reliability by a factor of 1.5x on average while limiting the performance loss to 4.9%. Finally, program annotation-based data placement improves the reliability by 1.3x at a performance cost of 1.1%.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327039","Memory;Heterogenous Memory Architecture;Reliability","Reliability;Random access memory;Memory architecture;Bandwidth;Memory management;Transient analysis;Error correction codes","","29","","67","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"SmarCo: An Efficient Many-Core Processor for High-Throughput Applications in Datacenters","D. Fan; W. Li; X. Ye; D. Wang; H. Zhang; Z. Tang; N. Sun","School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, China; SKL Computer Architecture, CAS, Beijing, China; SKL Computer Architecture, CAS, Beijing, China; SmarCo Co. Ltd., Beijing, China; SmarCo Co. Ltd., Beijing, China; SKL Computer Architecture, CAS, Beijing, China; SKL Computer Architecture, CAS, Beijing, China",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","596","607","Fast-growing high-throughput applications, such as web services, are characterized by high-concurrency processing, hard real-time response, and high-bandwidth memory access. The newly-born applications bring severe challenges to processors in datacenters, both in concurrent processing performance and energy efficiency. To offer a satisfactory quality of services, it is of critical importance to meet these newly emerging demands of high-throughput applications in the future datacenters in a more efficient way. In this paper, we propose a novel architecture, called SmarCo, which allows high-throughput applications to be processed more efficiently in datacenters. Based on the dominant characteristics of high-throughput applications, we implement large-scale many-core architecture with in-pair threads to support high-concurrency processing; we also introduce a hierarchical ring topology and laxity-aware task scheduler to guarantee hard real-time response; furthermore, we propose high-throughput datapath to improve memory access efficiency. We verify the efficiency of SmarCo by using simulators, large-scale FPGA and prototype with TSMC 40-nm technology node. The experimental results show that, compared to Intel Xeon E7-8890V4, SmarCo achieves 10.11X performance improvement and 6.95X energy-efficiency improvement with higher throughput and a better guarantee of real-time response.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327040","high throughput applications;Many Core Processor;high concurrency processing;hard real time response;low memory access latency","Instruction sets;Task analysis;Indexes;Real-time systems;Memory management;Throughput","","13","","40","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level","A. Gutierrez; B. M. Beckmann; A. Dutu; J. Gross; M. LeBeane; J. Kalamatianos; O. Kayiran; M. Poremba; B. Potter; S. Puthoor; M. D. Sinclair; M. Wyse; J. Yin; X. Zhang; A. Jain; T. Rogers",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","608","619","Modern GPU frameworks use a two-phase compilation approach. Kernels written in a high-level language are initially compiled to an implementation agnostic intermediate language (IL), then finalized to the machine ISA only when the target GPU hardware is known. Most GPU microarchitecture simulators available to academics execute IL instructions because there is substantially less functional state associated with the instructions, and in some situations, the machine ISA's intellectual property may not be publicly disclosed. In this paper, we demonstrate the pitfalls of evaluating GPUs using this higher-level abstraction, and make the case that several important microarchitecture interactions are only visible when executing lower-level instructions. Our analysis shows that given identical application source code and GPU microarchitecture models, execution behavior will differ significantly depending on the instruction set abstraction. For example, our analysis shows the dynamic instruction count of the machine ISA is nearly 2× that of the IL on average, but contention for vector registers is reduced by 3× due to the optimized resource utilization. In addition, our analysis highlights the deficiencies of using IL to model instruction fetching, control divergence, and value similarity. Finally, we show that simulating IL instructions adds 33% error as compared to the machine ISA when comparing absolute runtimes to real hardware.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327041","ABI;GPU;Intermediate Language;Intermediate Representation;ISA;Simulation","Kernel;Graphics processing units;Registers;Hardware;Computer architecture;Microarchitecture;Runtime","","51","","39","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective","K. Hazelwood; S. Bird; D. Brooks; S. Chintala; U. Diril; D. Dzhulgakov; M. Fawzy; B. Jia; Y. Jia; A. Kalro; J. Law; K. Lee; J. Lu; P. Noordhuis; M. Smelyanskiy; L. Xiong; X. Wang",Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","620","629","Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327042","computer architecture;hardware software codesign;machine learning;facebook","Facebook;Training;Machine learning algorithms;Neural networks;Feeds;Support vector machines;Servers","","406","1","17","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Amdahl's Law in Big Data Analytics: Alive and Kicking in TPCx-BB (BigBench)","D. Richins; T. Ahmed; R. Clapp; V. Janapa Reddi",The University of Texas at Austin; Stanford University; Intel; Google,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","630","642","Big data, specifically data analytics, is responsible for driving many of consumers' most common online activities, including shopping, web searches, and interactions on social media. In this paper, we present the first (micro)architectural investigation of a new industry-standard, open source benchmark suite directed at big data analytics applications—TPCx-BB (BigBench). Where previous work has usually studied benchmarks which oversimplify big data analytics, our study of BigBench reveals that there is immense diversity among applications, owing to their varied data types, computational paradigms, and analyses. In our analysis, we also make an important discovery generally restricting processor performance in big data. Contrary to conventional wisdom that big data applications lend themselves naturally to parallelism, we discover that they lack sufficient thread-level parallelism (TLP) to fully utilize all cores. In other words, they are constrained by Amdahl's law. While TLP may be limited by various factors, ultimately we find that single-thread performance is as relevant in scale-out workloads as it is in more classical applications. To this end we present core packing: a software and hardware solution that could provide as much as 20% execution speedup for some big data analytics applications.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327043","big data analytics;Hadoop;BigBench;TPCx BB;Amdahl's law;core packing","Benchmark testing;Big Data;Industries;Data analysis;Throughput;Usability;Parallel processing","","10","","35","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Memory Hierarchy for Web Search","G. Ayers; J. H. Ahn; C. Kozyrakis; P. Ranganathan",NA; Seoul National University; Stanford University; Google,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","643","656","Online data-intensive services, such as search, serve billions of users, utilize millions of cores, and comprise a significant and growing portion of datacenter-scale workloads. However, the complexity of these workloads and their proprietary nature has precluded detailed architectural evaluations and optimizations of processor design trade-offs. We present the first detailed study of the memory hierarchy for the largest commercial search engine today. We use a combination of measurements from longitudinal studies across tens of thousands of deployed servers, systematic microarchitectural evaluation on individual platforms, validated trace-driven simulation, and performance modeling – all driven by production workloads servicing real-world user requests. Our data quantifies significant differences between production search and benchmarks commonly used in the architecture community. We identify the memory hierarchy as an important opportunity for performance optimization, and present new insights pertaining to how search stresses the cache hierarchy, both for instructions and data. We show that, contrary to conventional wisdom, there is significant reuse of data that is not captured by current cache hierarchies, and discuss why this precludes state-of-the-art tiled and scale-out architectures. Based on these insights, we rethink a new cache hierarchy optimized for search that trades off the inefficient use of L3 cache transistors for higher-performance cores, and adds a latency-optimized on-package eDRAM L4 cache. Compared to state-of-the-art processors, our proposed design performs 27% to 38% better.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327044","web search;warehouse scale computing;memory systems","Production;Servers;Indexes;Microarchitecture;Web search;Benchmark testing;Hardware","","49","1","60","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Characterizing Resource Sensitivity of Database Workloads","R. Sen; K. Ramachandra","Gray Systems Lab, Microsoft Corporation; Gray Systems Lab, Microsoft Corporation",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","657","669","The performance of real world database workloads is heavily influenced by the resources available to run the workload. Therefore, understanding the performance impact of changes in resource allocations on a workload is key to achieving predictable performance. In this work, we perform an in-depth study of the sensitivity of several database workloads, running on Microsoft SQL Server on Linux, to resources such as cores, caches, main memory, and non-volatile storage. We consider transactional, analytical, and hybrid workloads that model real-world systems, and use recommended configurations such as storage layouts and index organizations at different scale factors. Our study lays out the wide spectrum of resource sensitivities, and leads to several findings and insights that are highly valuable to computer architects, cloud DBaaS (Database-as-a-Service) providers, database researchers, and practitioners. For instance, our results indicate that throughput improves more with more cores than with more cache beyond a critical cache size; depending upon the compute vs. I/O activity of a workload, hyper-threading may be detrimental in some cases. We discuss our extensive experimental results and present insights based on a comprehensive analysis of query plans and various query execution statistics.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327045","SQL-Server;DSS;HTAP;OLTP;hyper-threading;CAT;LLC-MPKI;memory-capacity;bandwidth;parallelism;concurrency;query-plan;cloud;DBaaS;database-performance","Benchmark testing;Sensitivity;Resource management;Indexes;Servers;Bandwidth","","14","1","50","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"ERUCA: Efficient DRAM Resource Utilization and Resource Conflict Avoidance for Memory System Parallelism","S. Lym; H. Ha; Y. Kwon; C. -k. Chang; J. Kim; M. Erez",The University of Texas at Austin; Stanford University; The University of Texas at Austin; The University of Texas at Austin; Microsoft; The University of Texas at Austin,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","670","682","Memory system performance is measured by access latency and bandwidth, and DRAM access parallelism critically impacts for both. To improve DRAM parallelism, previous research focused on increasing the number of effective banks by sub-dividing one physical bank. We find that without avoiding conflicts on the shared resources among (sub)banks, the benefits are limited. We propose mechanisms for efficient DRAM resource utilization and resource-conflict avoidance (ERUCA). ERUCA reduces conflicts on shared (sub)bank resources utilizing row address locality between sub-banks and improving the DRAM chip-level data bus. Area overhead for ERUCA is kept near zero with a unique implementation that exploits under-utilized resources available in commercial DRAM chips. Overall ERUCA provides 15% speedup while incurring <0.3% DRAM die area overhead.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327046","Memory System;Memory System Performance;DRAM Access parallelism","Parallel processing;DRAM chips;Arrays;Decoding;Bandwidth;Servers","","7","","43","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"DUO: Exposing On-Chip Redundancy to Rank-Level ECC for High Reliability","S. -L. Gong; J. Kim; S. Lym; M. Sullivan; H. David; M. Erez",UT Austin; Microsoft; UT Austin; NVIDIA; Huawei; UT Austin,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","683","695","DRAM row and column sparing cannot efficiently tolerate the increasing inherent fault rate caused by continued process scaling. In-DRAM ECC (IECC), an appealing alternative to sparing, can resolve inherent faults without significant changes to DRAM, but it is inefficient for highly-reliable systems where rank-level ECC (RECC) is already used against operational faults. In addition, DRAM design in the near future (possibly as early as DDR5) may transfer data in longer bursts, which complicates high-reliability RECC due to fewer devices being used per rank and increased fault granularity. We propose dual use of on-chip redundancy (DUO), a mech- anism that bypasses the IECC module and transfers on-chip redundancy to be used directly for RECC. Due to its increased redundancy budget, DUO enables a strong and novel RECC for highly-reliable systems, called DUO SDDC. The long codewords of DUO SDDC provide fundamentally higher detection and correction capabilities, and several novel secondary-correction techniques integrate together to further expand its correction capability. According to our evaluation results, DUO shows performance degradation on par with or better than IECC (average 2–3%), while consuming less DRAM energy than IECC (average 4–14% overheads). DUO provides higher reliability than either IECC or the state-of-the-art ECC technique. We show the robust reliability of DUO SDDC by comparing it to other ECC schemes using two different inherent fault-error models.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327047","DRAM reliability;scaling errors;ECC","Random access memory;Redundancy;System-on-chip;Error correction codes;Decoding;Organizations","","29","","57","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Memory System Design for Ultra Low Power, Computationally Error Resilient Processor Microarchitectures","S. Srikanth; P. G. Rabbat; E. R. Hein; B. Deng; T. M. Conte; E. DeBenedictis; J. Cook; M. P. Frank",Georgia Institute of Technoloey; Intel Corporation; Georgia Institute of Technoloey; NA; Georgia Institute of Technoloey; Sandia National Laboratories; Sandia National Laboratories; Sandia National Laboratories,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","696","709","Dennard scaling ended a decade ago. Energy reduction by lowering supply voltage has been limited because of guard bands and a subthreshold slope of over 60mV/decade in MOSFETs. On the other hand, newly-proposed logic devices maintain a high on/off ratio for drain currents even at significantly lower operating voltages. However, such ultra low power technology would eventually suffer from intermittent errors in logic as a result of operating close to the thermal noise floor. Computational error correction mitigates this issue by efficiently correcting stochastic bit errors that may occur in computational logic operating at low signal energies, thereby allowing for energy reduction by lowering supply voltage to tens of millivolts. Cores based on a Redundant Residual Number System (RRNS), which represents a number using a tuple of smaller numbers, are a promising candidate for implementing energyefficient computational error correction. However, prior RRNS core microarchitectures abstract away the memory hierarchy and do not consider the power-performance impact of RNS-based memory addressing. When compared with a non-error-correcting core addressing memory in binary, naive RNS-based memory addressing schemes cause a slowdown of over 3x/2x for inorder/out-of-order cores respectively. In this paper, we analyze RNS-based memory access pattern behavior and provide solutions in the form of novel schemes and the resulting design space exploration, thereby, extending and enabling a tangible, ultra low power RRNS based architecture.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327048","Architectures for emerging technology and applications;Cache and memory architectures;Dependable/secure architectures;Impact of technology on architecture;Power efficient architectures and techniques","Error correction;Computer architecture;Microarchitecture;MOSFET;Rendering (computer graphics);Error correction codes;Electronic mail","","4","","118","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"NACHOS: Software-Driven Hardware-Assisted Memory Disambiguation for Accelerators","N. Vedula; A. Shriraman; S. Kumar; W. N. Sumner","School of Computing Sciences, Simon Fraser University, Burnaby, Canada; School of Computing Sciences, Simon Fraser University, Burnaby, Canada; School of Computing Sciences, Simon Fraser University, Burnaby, Canada; School of Computing Sciences, Simon Fraser University, Burnaby, Canada",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","710","723","Hardware accelerators have relied on the compiler to extract instruction parallelism but may waste significant energy in enforcing memory ordering and discovering memory parallelism. Accelerators tend to either serialize memory operations [43] or reuse power hungry load-store queues (LSQs) [8], [27]. Recent works [11], [15] use the compiler for scheduling but continue to rely on LSQs for memory disambiguation. NACHOS is a hardware assisted software-driven approach to memory disambiguation for accelerators. In NACHOS, the compiler classifies pairs of memory operations as NO alias (i.e., independent memory operations), MUST alias (i.e., ordering required), or MAY alias (i.e., compiler uncertain). We developed a compiler-only approach called NACHOS-SW that serializes memory operations both when the compiler is certain (MUST alias) and uncertain (MAY alias). Our study analyzes multiple stages of alias analysis on 135 acceleration regions extracted from SPEC2K, SPEC2k6, and PARSEC. NACHOS-SW is en- ergy efficient, but serialization limits performance; 18%-100% slowdown compared to an optimized LSQ. We then proposed NACHOS a low-overhead, scalable, hardware comparator assist that dynamically verifies MAY alias and executes independent memory operations in parallel. NACHOS is a pay-as-you-go approach where the compiler filters out memory operations to save dynamic energy, and the hardware dynamically checks to find MLP. NACHOS achieves performance comparable to an optimized LSQ; in fact, it improved performance in 6 benchmarks(6%-70%) by reducing load-to-use latency for cache hits. NACHOS imposes no energy overhead in 15 out of 27 benchmarks i.e., compiler accurately determines all memory dependencies; the average energy overhead is ≃6% of total (accelerator and L1 cache); in comparison, an optimized LSQ consumes 27% of total energy. NACHOS is released as free and open source software. Github: https://github.com/sfu-arch/ nachos.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327049","Hardware accelerators;Dataflow;Memory disambiguation;Load Store Queues;LSQ;Alias Analysis;Hardware Software co design","Hardware;Memory management;Acceleration;Out of order;Parallel processing;Compounds;Benchmark testing","","5","","46","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator","S. Pal; J. Beaumont; D. -H. Park; A. Amarnath; S. Feng; C. Chakrabarti; H. -S. Kim; D. Blaauw; T. Mudge; R. Dreslinski","University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; Arizona State University, Tempe, AZ; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","724","736","Sparse matrices are widely used in graph and data analytics, machine learning, engineering and scientific applications. This paper describes and analyzes OuterSPACE, an accelerator targeted at applications that involve large sparse matrices. OuterSPACE is a highly-scalable, energy-efficient, reconfigurable design, consisting of massively parallel Single Program, Multiple Data (SPMD)-style processing units, distributed memories, high-speed crossbars and High Bandwidth Memory (HBM). We identify redundant memory accesses to non-zeros as a key bottleneck in traditional sparse matrix-matrix multiplication algorithms. To ameliorate this, we implement an outer product based matrix multiplication technique that eliminates redundant accesses by decoupling multiplication from accumulation. We demonstrate that traditional architectures, due to limitations in their memory hierarchies and ability to harness parallelism in the algorithm, are unable to take advantage of this reduction without incurring significant overheads. OuterSPACE is designed to specifically overcome these challenges. We simulate the key components of our architecture using gem5 on a diverse set of matrices from the University of Florida's SuiteSparse collection and the Stanford Network Analysis Project and show a mean speedup of 7.9× over Intel Math Kernel Library on a Xeon CPU, 13.0× against cuSPARSE and 14.0× against CUSP when run on an NVIDIA K40 GPU, while achieving an average throughput of 2.9 GFLOPS within a 24 W power budget in an area of 87 mm2.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327050","Sparse matrix processing;Application specific hardware;Parallel computer architecture;Hardware software co design;Hardware accelerators","Sparse matrices;Computer architecture;Kernel;Matrix decomposition;Parallel processing;Graphics processing units;Libraries","","232","","60","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Searching for Potential gRNA Off-Target Sites for CRISPR/Cas9 Using Automata Processing Across Different Platforms","C. Bo; V. Dang; E. Sadredini; K. Skadron","Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA; Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA; Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA; Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","737","748","The CRISPR/Cas system is a bacteria immune system protecting cells from foreign genetic elements. One version that attracted special interest is CRISPR/Cas9, because it can be modified to edit genomes at targeted locations. However, the risk of binding and damaging off-target locations limits its power. Identifying all these potential off-target sites is thus important for users to effectively use the system to edit genomes. This process is computationally expensive, especially when one allows more differences in gRNA targeting sequences. In this paper, we propose using automata to search for off-target sites while allowing differences between the reference genome and gRNA targeting sequences. We evaluate the automata-based approach on four different platforms, including conventional architectures such as the CPU and the GPU, and spatial architectures such as the FPGA and Micron's Automata Processor. We compare the proposed approach with two off-target search tools (CasOFFinder (GPU) and CasOT (CPU)), and achieve over 83x speedups on the FPGA compared with CasOFFinder and over 600x speedups compared with CasOT. More customized hardware such as the AP can provide additional speedups (1.5x for the kernel execution) compared with the FPGA. We also evaluate the automata-based solution using single-thread HyperScan (a high-performance automata processing library) on the CPU. HyperScan outperforms CasOT by over 29.7x. The automata-based approach on iNFAnt2 (a DFA/NFA engine on the GPU) does not consistently work better than CasOFFinder, and only show a slightly better speedup compared with single-thread HyperScan on the CPU (4.4x for the best case). These results show that the automata-based approach provides significant algorithmic benefits, and that accelerators such as the FPGA and the AP can provide substantial additional speedups. However, iNFAnt2 does not confer a clear advantage because the proposed method does not map well to the GPU architecture. Furthermore, we propose several methods to further improve the performance on spatial architectures, and some potential architectural modifications for future automata processing hardware.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327051","CRISPR;gRNA off target;Automata processing;FPGA;GPU","Automata;Genomics;Computer architecture;Field programmable gate arrays;Graphics processing units;Hardware;Tools","","24","","35","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Characterizing and Mitigating Output Reporting Bottlenecks in Spatial Automata Processing Architectures","J. Wadden; K. Angstadt; K. Skadron","Department of Computer Science, University of Virginia, Charlottesville, VA; Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Department of Computer Science, University of Virginia, Charlottesville, VA",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","749","761","Automata processing has seen a resurgence in importance due to its usefulness for pattern matching and pattern mining of ""big data."" While large-scale automata processing is known to bottleneck von Neumann processors due to unpredictable memory accesses, spatial architectures excel at automata processing. Spatial architectures can implement automata graphs by wiring together automata states in reconfigurable arrays, allowing parallel automata state computation, and point-to-point state transitions on-chip. However, spatial automata processing architectures can suffer from output constraints (up to 255x in commercial systems!) due to the physical placement of states, output processing architecture design, I/O resources, and the massively parallel nature of the architecture. To understand this bottleneck, we conduct the first known characterization of output requirements of a realistic set of automata processing benchmarks. We find that most benchmarks report fairly frequently, but that few states report at any one time. This observation motivates new output compression schemes and reporting architectures. We evaluate the benefit of one purely software automata transformation and show that output reporting costs can be greatly reduced (improving performance by up to 40% without hardware modification. We then explore bottlenecks in the reporting architecture of a commercial spatial automata processor and propose a new architecture that improves performance by up to 5.1x.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327052","finite automata;regular expression;spatial;reconfigurable;non deterministic finite automata","Automata;Computer architecture;Benchmark testing;Hardware;Learning automata;Field programmable gate arrays;Standards","","16","","32","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Power and Energy Characterization of an Open Source 25-Core Manycore Processor","M. McKeown; A. Lavrov; M. Shahrad; P. J. Jackson; Y. Fu; J. Balkind; T. M. Nguyen; K. Lim; Y. Zhou; D. Wentzlaff",Princeton University; Princeton University; Princeton University; Princeton University; Princeton University; Princeton University; Princeton University; Princeton University; Baidu; Princeton University,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","762","775","The end of Dennard's scaling and the looming power wall have made power and energy primary design goals for modern processors. Further, new applications such as cloud computing and Internet of Things (IoT) continue to necessitate increased performance and energy efficiency. Manycore processors show potential in addressing some of these issues. However, there is little detailed power and energy data on manycore processors. In this work, we carefully study detailed power and energy characteristics of Piton, a 25-core modern open source academic processor, including voltage versus frequency scaling, energy per instruction (EPI), memory system energy, network-on-chip (NoC) energy, thermal characteristics, and application performance and power consumption. This is the first detailed power and energy characterization of an open source manycore design implemented in silicon. The open source nature of the processor provides increased value, enabling detailed characterization verified against simulation and the ability to correlate results with the design and register transfer level (RTL) model. Additionally, this enables other researchers to utilize this work to build new power models, devise new research directions, and perform accurate power and energy research using the open source processor. The characterization data reveals a number of interesting insights, including that operand values have a large impact on EPI, recomputing data can be more energy efficient than loading it from memory, on-chip data transmission (NoC) energy is low, and insights on energy efficient multithreaded core design. All data collected and the hardware infrastructure used is open source and available for download at http://www.openpiton.org.","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327053","processor;manycore;power;energy;thermal;characterization","Field programmable gate arrays;Power supplies;Bridge circuits;Coherence;Silicon;Protocols;Monitoring","","26","","71","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"A Spot Capacity Market to Increase Power Infrastructure Utilization in Multi-tenant Data Centers","M. Islam; X. Ren; S. Ren; A. Wierman",UC Riverside; Caltech; UC Riverside; Caltech,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","776","788","Despite the common practice of oversubscription, power capacity is largely under-utilized in data centers. A significant factor driving this under-utilization is fluctuation of the aggregate power demand, resulting in unused “spot (power) capacity”. In this paper, we tap into spot capacity for improving power infrastructure utilization in multi-tenant data centers, an important but under-explored type of data center where multiple tenants house their own physical servers. We propose a novel market, called SpotDC, to allocate spot capacity to tenants on demand. Specifically, SpotDC extracts tenants' racklevel spot capacity demand through an elastic demand function, based on which the operator sets the market price for spot capacity allocation. We evaluate SpotDC using both testbed experiments and simulations, demonstrating that SpotDC improves power infrastructure utilization and creates a “win-win” situation: the data center operator increases its profit (by nearly 10%), while tenants improve their performance (by 1.2-1.8x on average compared to the no spot capacity case, yet at a marginal cost).","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327054","Data center;market approach;power management;spot capacity","Data centers;Servers;Degradation;Power demand;Aggregates;Cooling;Uninterruptible power systems","","6","","45","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"GPGPU Power Modeling for Multi-domain Voltage-Frequency Scaling","J. Guerreiro; A. Ilic; N. Roma; P. Tomas","INESC-ID, Universidade de Lisboa, Portugal; INESC-ID, Universidade de Lisboa, Portugal; INESC-ID, Universidade de Lisboa, Portugal; INESC-ID, Universidade de Lisboa, Portugal",2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),"29 Mar 2018","2018","","","789","800","Dynamic Voltage and Frequency Scaling (DVFS) on Graphics Processing Units (GPUs) components is one of the most promising power management strategies, due to its potential for significant power and energy savings. However, there is still a lack of simple and reliable models for the estimation of the GPU power consumption under a set of different voltage and frequency levels. Accordingly, a novel GPU power estimation model with both core and memory frequency scaling is herein proposed. This model combines information from both the GPU architecture and the executing GPU application and also takes into account the non-linear changes in the GPU voltage when the core and memory frequencies are scaled. The model parameters are estimated using a collection of 83 microbenchmarks carefully crafted to stress the main GPU components. Based on the hardware performance events gathered during the execution of GPU applications on a single frequency configuration, the proposed model allows to predict the power consumption of the application over a wide range of frequency configurations, as well as to decompose the contribution of different parts of the GPU pipeline to the overall power consumption. Validated on 3 GPU devices from the most recent NVIDIA microarchitectures (Pascal, Maxwell and Kepler), by using a collection of 26 standard benchmarks, the proposed model is able to achieve accurate results (7%, 6% and 12% mean absolute error) for the target GPUs (Titan Xp, GTX Titan X and Tesla K40c).","2378-203X","978-1-5386-3659-6","10.1109/HPCA.2018.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327055","GPU Power Modeling;DVFS;GPU Utilization Metrics;Microbenchmarking;Power Decoupling","Graphics processing units;Power demand;Memory management;Predictive models;Frequency estimation;Frequency-domain analysis","","38","","40","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
