"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"SGCN: Exploiting Compressed-Sparse Features in Deep Graph Convolutional Network Accelerators","M. Yoo; J. Song; J. Lee; N. Kim; Y. Kim; J. Lee","Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University; Samsung Electronics; Department of Computer Science, Yonsei University; Department of Electrical and Computer Engineering, Seoul National University",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1","14","Graph convolutional networks (GCNs) are becoming increasingly popular as they overcome the limited applicability of prior neural networks. One recent trend in GCNs is the use of deep network architectures. As opposed to the traditional GCNs, which only span only around two to five layers deep, modern GCNs now incorporate tens to hundreds of layers with the help of residual connections. From such deep GCNs, we find an important characteristic that they exhibit very high intermediate feature sparsity. This reveals a new opportunity for accelerators to exploit in GCN executions that was previously not present.In this paper, we propose SGCN, a fast and energy-efficient GCN accelerator which fully exploits the sparse intermediate features of modern GCNs. SGCN suggests several techniques to achieve significantly higher performance and energy efficiency than the existing accelerators. First, SGCN employs a GCN-friendly feature compression format. We focus on reducing the off-chip memory traffic, which often is the bottleneck for GCN executions. Second, we propose microarchitectures for seamlessly handling the compressed feature format. Specifically, we modify the aggregation phase of GCN to process compressed features, and design a combination engine that can output compressed features at no extra memory traffic cost. Third, to better handle locality in the existence of the varying sparsity, SGCN employs sparsity-aware cooperation. Sparsity-aware cooperation creates a pattern that exhibits multiple reuse windows, such that the cache can capture diverse sizes of working sets and therefore adapt to the varying level of sparsity. Through a thorough evaluation, we show that SGCN achieves 1.66× speedup and 44.1% higher energy efficiency compared to the existing accelerators in geometric mean.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071102","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071102","Graph Convolutional Networks;Sparsity;Compressed Format;Accelerators","Degradation;Microarchitecture;Costs;Neural networks;Computer architecture;Network architecture;Market research","","16","","100","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"PhotoFourier: A Photonic Joint Transform Correlator-Based Neural Network Accelerator","S. Li; H. Yang; C. W. Wong; V. J. Sorger; P. Gupta","University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; The George Washington University; University of California, Los Angeles",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","15","28","The last few years have seen a lot of work to address the challenge of low-latency and high-throughput convolutional neural network inference. Integrated photonics has the potential to dramatically accelerate neural networks because of its low-latency nature. Combined with the concept of Joint Transform Correlator (JTC), the computationally expensive convolution functions can be computed instantaneously (time of flight of light) with almost no cost. This ‘free’ convolution computation provides the theoretical basis of the proposed PhotoFourier JTC-based CNN accelerator. PhotoFourier addresses a myriad of challenges posed by on-chip photonic computing in the Fourier domain including 1D lenses and high-cost optoelectronic conversions. The proposed PhotoFourier accelerator achieves more than 28× better energy-delay product compared to state-of-art photonic neural network accelerators.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070931","","Costs;Convolution;Neural networks;Transforms;Computer architecture;System-on-chip;Convolutional neural networks","","14","","75","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"INCA: Input-stationary Dataflow at Outside-the-box Thinking about Deep Learning Accelerators","B. Kim; S. Li; H. Li","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","29","41","This paper first presents an input-stationary (IS) implemented crossbar accelerator (INCA), supporting inference and training for deep neural networks (DNNs). Processing-in-memory (PIM) accelerators for DNNs have been actively researched, specifically, with resistive random-access memory (RRAM), due to RRAM’s computing and memorizing capabilities and device merits. To the best of our knowledge, all previous PIM accelerators have saved weights into RRAMs and inputs (activations) into conventional memories—it naturally forms weight-stationary (WS) dataflow. WS has generally been considered the most optimized choice for high parallelism and data reuse. How-ever, WS-based PIM accelerators show fundamental limitations: first, remaining high dependency on DRAM and buffers for fetching and saving inputs (activations); second, a remarkable number of extra RRAMs for transposed weights and additional computational intermediates in training; third, coarse-grained arrays demanding high-bit analog-to-digital converters (ADCs) and introducing poor utilization in depthwise and pointwise convolution; last, degraded accuracy due to its sensitivity to weights which are affected by RRAM’s nonideality. On the other hand, we observe that IS dataflow, where RRAMs retain inputs (activations), can effectively address the limitations of WS, because of low dependency by only loading weights, no need for extra RRAMs, feasibility of fine-grained accelerator design, and less impact of input (activation) variance on accuracy. But IS dataflow is hardly achievable by the existing crossbar structure because it is difficult to implement kernel sliding and preserve the high parallelism. To support kernel movement, we constitute a cell structure with two-transistor-one-RRAM (2T1R). Based on the 2T1R cell, we design a novel three-dimensional (3D) architecture for high parallelism in batch training. Our experiment results prove the potential of INCA. Compared to the WS accelerator, INCA achieves up to 20.6× and 260× energy efficiency improvement in inference and training, respectively; 4.8× (inference) and 18.6× (training) speedup as well. While accuracy in WS drops to 15% in our high-noise simulation, INCA presents an even more robust result as 86% accuracy.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070992","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070992","","Training;Deep learning;Three-dimensional displays;Sensitivity;Convolution;Microprocessors;Computer architecture","","16","","72","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks","R. Hwang; M. Kang; J. Lee; D. Kam; Y. Lee; M. Rhu","School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; Department of Electrical Engineering, POSTECH; Department of Electrical Engineering, POSTECH; School of Electrical Engineering, KAIST",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","42","55","Graph convolutional neural networks (GCNs) have emerged as a key technology in various application domains where the input data is relational. A unique property of GCNs is that its two primary execution stages, aggregation and combination, exhibit drastically different dataflows. Consequently, prior GCN accelerators tackle this research space by casting the aggregation and combination stages as a series of sparse-dense matrix multiplication. However, prior work frequently suffers from inefficient data movements, leaving significant performance left on the table. We present GROW, a GCN accelerator based on Gustavson’s algorithm to architect a row-wise product based sparse-dense GEMM accelerator. GROW co-designs the software/ hardware that strikes a balance in locality and parallelism for GCNs, reducing the average memory traffic by 2×, and achieving an average 2.8× and 2.3× improvement in performance and energy-efficiency, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070983","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070983","","Casting;Software algorithms;Computer architecture;Parallel processing;Hardware;Energy efficiency;Software","","32","","53","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Logical/Physical Topology-Aware Collective Communication in Deep Learning Training","S. Cho; H. Son; J. Kim","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","56","68","Training is an important aspect of deep learning to enable network models to be deployed. To scale training, multiple GPUs are commonly used with data parallelism to exploit the additional GPU compute and memory capacity. However, one challenge in scalability is the collective communication between GPUs. In this work, we propose to accelerate the AllReduce collective. AllReduce communication is often based on a logical topology (e.g., ring or tree algorithms) that is mapped to a physical topology or the physical connectivity between the nodes. In this work, we propose a logical/physical topology-aware collective communication that we refer to as C-Cube architecture – Chaining Collective Communication with Computation. C-Cube exploits the opportunity to overlap or chain different phases of collective communication as well as forward computation in a tree algorithm AllReduce. We exploit the communication pattern in a logical tree topology to overlap the different phases of communication. Since ordering is maintained in the tree collective algorithm, we propose gradient queuing to enable chaining of communication with forward computation to accelerate overall performance while having no impact on training accuracy. We also exploit the physical topology characteristics to further improve the performance, including proposing detour connections for collective communication while leveraging the additional connectivity to enable a double-tree C-Cube implementation. We implement a C-Cube proof-of-concept on a real system (8-GPU NVIDIA DGX-1) and show C-Cube results in performance improvement in communication performance compared to non-overlapped tree algorithms as well as overall performance.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071117","","Training;Deep learning;System performance;Scalability;Computational modeling;Graphics processing units;Computer architecture;Parallel processing;Throughput;Topology","","6","","59","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Sibia: Signed Bit-slice Architecture for Dense DNN Acceleration with Slice-level Sparsity Exploitation","D. Im; G. Park; Z. Li; J. Ryu; H. -J. Yoo","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST)",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","69","80","Deep neural networks (DNNs) have achieved high performance in many AI fields such as 1-D language, 2-D image, and 3-D point cloud processing applications. Since recent DNN tasks require dense matrix operations with various bit-precision and non-ReLU activation functions, mobile neural processing units (NPUs) suffer from the acceleration of diverse DNN tasks within their limited hardware resources and power budget. Although bit-slice architectures benefit from slice-level computation and slice-level sparsity exploitation, the conventional bit-slice representation is inefficient in bit-slice architectures resulting in poor dense DNN execution. This paper proposes the efficient signed bit-slice architecture, Sibia, with the signed bit-slice representation (SBR) for efficient dense DNN acceleration. The SBR adds a sign bit to each bit-slice and changes signed 11112 bit-slice to 00002 by borrowing a value of 1 from its lower order of the bit-slice. This scheme generates large numbers of zero bit-slices in dense DNNs even not relying on accuracy-sensitive pruning methods or retraining processes. Moreover, the SBR balances positive and negative values of 2’s complement data, allowing accurate bit-slice-based output speculation that pre-computes high orders of bit-slices. Sibia integrates the signed multiplier-and-accumulate (MAC) units for efficient signed bit-slice computations, and the flexible zero skipping processing element (PE) supports the zero input bit-slice skipping and output skipping for high throughput and energy-efficiency. Additionally, the dynamic sparsity monitoring unit monitors sparsity ratio between input and weight data and determines the more sparse one for zero bit-slice skipping. The heterogeneous network-on-chip (NoC) benefits from data reusability during bit-slice computation, reducing transmission bandwidth. Finally, Sibia outperforms the previous bit-slice architecture, Bit-fusion, over 3.65× higher area-efficiency, 3.88× higher energy-efficiency, and 5.35× higher throughput.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071031","Hardware accelerator;deep neural network;binary representation;bit-slice;sparsity;output speculation;non-ReLU","Point cloud compression;Neural networks;Computer architecture;Bandwidth;Network-on-chip;Throughput;Hardware","","16","","43","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"AstriFlash A Flash-Based System for Online Services","S. Gupta; Y. Oh; L. Yan; M. Sutherland; A. Bhattacharjee; B. Falsafi; P. Hsu","EcoCloud, EPFL; Korea University; EcoCloud, EPFL; EcoCloud, EPFL; Yale University; EcoCloud, EPFL; Peter Hsu & Associates",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","81","93","Modern datacenters host datasets in DRAM to offer large-scale online services with tight tail-latency requirements. Unfortunately, as DRAM is expensive and increasingly difficult to scale, datacenter operators are forced to consider denser storage technologies. While modern flash-based storage exhibits μs-scale access latency, which is well within the tail-latency constraints of many online services, traditional demand paging abstraction used to manage memory and storage incurs high overheads and prohibits flash usage in online services. We introduce AstriFlash, a hardware-software co-design that tightly integrates flash and DRAM with ns-scale overheads. Our evaluation of server workloads with cycle-accurate full-system simulation shows that AstriFlash achieves 95% of a DRAM-only system’s throughput while maintaining the required 99th-percentile tail latency and reducing the memory cost by 20x.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070955","Qualcomm; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070955","","Costs;Memory management;Random access memory;Tail;Switches;Throughput;Servers","","3","","81","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Thoth: Bridging the Gap Between Persistently Secure Memories and Memory Interfaces of Emerging NVMs","X. Han; J. Tuck; A. Awad","North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","94","107","Emerging non-volatile memories (NVMs) are expected to be part of future computing systems, including cloud systems and edge devices. In addition to the high density (and hence large capacities) NVMs can provide, they feature ultra-low idle power which makes them very promising for edge computing and data centers. Additionally, NVMs’ ability to retain data upon system crash (e.g., power outage or software bug) makes them a great candidate for high-availability and persistent applications. However, NVMs’ data retention capability brings in security challenges and further complicates today’s secure memory implementations; to ensure correct and secure system recovery, the data and security metadata must be persisted atomically (i.e., up-to-date in memory upon a crash).Despite the many efforts for rethinking secure memory implementations to enable crash-consistency, we observe that the state-of-the-art solutions are based on a major assumption that may not be suitable for future memory interfaces. Specifically, the majority of today’s solutions assume that either the encryption counter and/or message-authentication code (MAC) can be co-located with data by directly or indirectly leveraging the otherwise Error-Correcting Codes (ECC) bits. However, we observe that emerging interfaces and standards delegate the ECC calculation and management to happen inside the memory module, which makes it possible to remove extra bits for ECC in memory interfaces. Thus, all today’s solutions may need to separately persist the encrypted data, its MAC, and its encryption counter upon each memory write. To mitigate this issue, we propose a novel solution, Thoth, which leverages a novel off-chip persistent partial updates combine buffer that can ensure crash consistency at the cost of a fraction of the write amplification by the state-of-the-art solutions when adapted to future interfaces. Based on our evaluation, Thoth improves the performance by an average of 1.22x (up to 1.44x) while reducing write traffic by an average of 32% (up to 40%) compared to the baseline Anubis when adapted to future interfaces.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070991","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070991","Persistent Memory;Security Metadata;Secure NVM","Nonvolatile memory;Memory modules;Metadata;System recovery;Elliptic curve cryptography;Software;Error correction codes","","2","","51","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Multi-Granularity Shadow Paging with NVM Write Optimization for Crash-Consistent Memory-Mapped I/O","H. Du; Q. Li; R. Pan; T. -W. Kuo; C. J. Xue","Department of Computer Science, City University of Hong Kong; Department of Computer Science and Technology, Xiamen University; Department of Computer Science, City University of Hong Kong; Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science, City University of Hong Kong",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","108","121","The complex software stack has become the performance bottleneck of the system with high-speed Non-Volatile Memory (NVM). Memory-mapped I/O (MMIO) could avoid the long-stack overhead by bypassing the kernel, but the performance is limited by existing crash-resilient mechanisms. We propose a Multi-Granularity Shadow Paging (MGSP) strategy, which smartly utilizes the redo and undo logs as shadow logs to provide a light-weight crash-resilient mechanism for MMIO. In addition, a multi-granularity strategy is designed to provide high-performance updating and locking for reducing runtime overhead, where strong consistency is preserved with a lockfree metadata log. Experimental results show that the proposed MGSP achieves 1.1 ~ 4.21× performance improvement with write and 2.56 ~ 3.76× improvement with multi-threads write compared with the underlying file system. For SQLite, MGSP can improve the database performance by 29.4% for Mobibench and 36.5% for TPCC, on average.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071009","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071009","","Runtime;Nonvolatile memory;File systems;Databases;Scalability;Full stack;Metadata","","2","","46","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"MGC: Multiple-Gray-Code for 3D NAND Flash based High-Density SSDs","Y. Lv; L. Shi; Q. Li; C. Gao; Y. Song; L. Luo; Y. Zhang","School of Computer Science and Technology, East China Normal University, Shanghai, China; Ministry of Education, Software/Hardware Co-Design Engineering Research Center; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; Computer Science Department, University of Pittsburgh, Pittsburgh, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","122","136","QLC (4-bit-per-cell) and more-bit-per-cell 3D NAND flash memories are increasingly adopted in large storage systems. While achieving significant cost reduction, these memories face degraded performance and reliability issues. The industry has adopted two-step programming (TSP), rather than one-step programming, to perform fine-granularity program control and choose gray-code encoding, as well as LDPC (Low-Density Parity-Check Code) for error correction. Different flash manufacturers often integrate different gray-codes in their products, which exhibit different performance and reliability characteristics. Unfortunately, a fixed gray-code encoding design lacks the ability to meet the dynamic read and program performance requirements at both application and device levels.In this paper, we propose MGC, a multiple-gray-code encoding strategy, that adaptively chooses the best gray-code to meet the optimization goals at runtime. In particular, MGC first extracts the performance and reliability requirements based on application-level access patterns and detects the reliability degree of SSD. It then determines the appropriate gray-code to encode the data, either from host/user application or due to garbage collection, before writing the pages to the flash memory. MGC is integrated in FTL (flash translation layer) and enhances the flash controller to enable runtime gray-code arbitration. We evaluate the proposed MGC scheme. The results show that MGC achieves better performance and lifetime guarantee compared with state-of-the-arts and introduces little overhead.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070946","East China Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070946","Storage System;3D NAND Flash;Gray-Code;Reliability;Performance","Performance evaluation;Three-dimensional displays;Runtime;Programming;Writing;Encoding;Parity check codes","","17","","71","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking","Y. Li; M. Gao",Tsinghua University; Tsinghua University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","137","151","Hybrid memory systems are able to achieve both high performance and large capacity when combining fast commodity DDR memories with larger but slower non-volatile memories in a heterogeneous way. However, it is critical to best utilize the limited fast memory capacity and slow memory bandwidth in such systems to gain the maximum efficiency. In this paper, we propose a novel hybrid memory design, Baryon, that leverages both memory compression and data sub-blocking techniques to improve the utilization of fast memory capacity and slow memory bandwidth, with only moderate metadata overheads and management complexity. Baryon reserves a small fast memory area to efficiently manage and stabilize the irregular and frequently varying data layouts resulted from compression and sub-blocking, and selectively commits only stable blocks to the rest fast memory space. It also adopts a novel dual-format metadata scheme to support flexible address remapping under such complex data layouts with low storage cost. Baryon is completely transparent to software, and works with both cache and flat schemes of hybrid memories. Our evaluation shows Baryon achieves up to 1.68× and on average 1.27× performance improvements over state-of-the-art designs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071115","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071115","Hybrid memory;compression;sub-blocking","Costs;Nonvolatile memory;Memory management;Layout;Systems architecture;Data compression;Bandwidth","","7","","78","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Root Crash Consistency of SGX-style Integrity Trees in Secure Non-Volatile Memory Systems","J. Huang; Y. Hua","Wuhan National Laboratory for Optoelectronics School of Computer, Huazhong University of Science and Technology; Wuhan National Laboratory for Optoelectronics School of Computer, Huazhong University of Science and Technology",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","152","164","Data integrity is important for non-volatile memory (NVM) systems that maintain data even without power. The data integrity in NVM may be compromised by integrity attacks, which can be defended against by integrity verification via integrity trees. After NVM system failures and reboots, the integrity tree root is responsible for providing a trusted execution environment. However, updating the root incurs latency to propagate the modifications from leaf nodes to the root. If system crashes occur during the propagation process, the root is inconsistent with the updated leaf nodes, resulting in misreporting the attacks after the system reboots. In this paper, we propose a ShortCut UpdatE scheme, called SCUE, which is an efficient and low-latency scheme to instantaneously update the root on the SGX-style integrity tree (SIT) by judiciously overlooking the updates upon most intermediate tree nodes. The idea behind SCUE explores and exploits the observation that consistent leaf nodes and root are enough to ensure data integrity after system failures and reboots. Moreover, SIT is difficult to be reconstructed from the leaf nodes since updating one tree node needs its parent node as input. Root in SIT thus cannot verify the data after the system crashes and reboots even though the root is correctly updated. To provide the ability of verification via root in SIT, we use a counter-summing approach to efficiently reconstructing the SIT from leaf nodes. Extensive evaluation results show that compared with the state-of-the-art integrity tree update schemes, our SCUE delivers high performance while ensuring data integrity.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071003","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071003","","Nonvolatile memory;Data integrity;Computer architecture;System recovery;Computer crashes;System-on-chip;Low latency communication","","4","","55","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ACIC: Admission-Controlled Instruction Cache","Y. Wang; C. -H. Chang; A. Sivasubramaniam; N. Soundararajan","The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; Intel Labs, India",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","165","178","The front end bottleneck in datacenter workloads has come under increased scrutiny, with the growing code footprint, involvement of numerous libraries and OS services, and the unpredictability in the instruction stream. Our examination of these workloads points to burstiness in accesses to instruction blocks, which has also been observed in data accesses [61]. Such burstiness is largely due to spatial and short-duration temporal localities, that LRU fails to recognize and optimize for, when a single cache caters to both forms of locality. Instead, we incorporate a small i-Filter as in previous works [29], [49] to separate spatial from temporal accesses. However, a simple separation does not suffice, and we additionally need to predict whether the block will continue to have temporal locality, after the burst of spatial locality. This combination of i-Filter and temporal locality predictor constitutes our Admission-Controlled Instruction Cache (ACIC). ACIC outperforms a number of state-of-the-art pollution reduction techniques (replacement algorithms, bypassing mechanisms, victim caches), providing 1.0223 speedup on the average over a baseline LRU based conventional i-cache (bridging over half of the gap between LRU and OPT) across several datacenter workloads.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071033","","Pollution;Codes;Optimized production technology;Computer architecture;Prediction algorithms;Libraries","","1","","93","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Compression-Aware and Performance-Efficient Insertion Policies for Long-Lasting Hybrid LLCs","C. Escuin; A. A. Khan; P. Ibáñez; T. Monreal; J. Castrillon; V. Viñals","Dept. Informática e Ingeniería de Sistemas - I3A, Universidad de Zaragoza, Zaragoza, Spain; Chair for Compiler Construction, TU Dresden, Dresden, Germany; Dept. Informática e Ingeniería de Sistemas - I3A, Universidad de Zaragoza, Zaragoza, Spain; Universitat Politècnica de Catalunya • BarcelonaTech, Barcelona, Spain; Chair for Compiler Construction, TU Dresden, Dresden, Germany; Dept. Informática e Ingeniería de Sistemas - I3A, Universidad de Zaragoza, Zaragoza, Spain",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","179","192","Emerging non-volatile memory (NVM) technologies can potentially replace large SRAM memories such as the last-level cache (LLC). However, despite recent advances, NVMs suffer from higher write latency and limited write endurance. Recently, NVM-SRAM hybrid LLCs are proposed to combine the best of both worlds. Several policies have been proposed to improve the performance and lifetime of hybrid LLCs by intelligently steering the incoming LLC blocks into either the SRAM or NVM part, regarding the cache behavior of the LLC blocks and the SRAM/NVM device properties. However, these policies neither consider compressing the contents of the cache block nor using partially worn-out NVM cache blocks.This paper proposes new insertion policies for byte-level fault-tolerant hybrid LLCs that collaboratively optimize for lifetime and performance. Specifically, we leverage data compression to utilize partially defective NVM cache entries, thereby improving the LLC hit rate. The key to our approach is to guide the insertion policy by both the reuse properties of the block and the size resulting from its compression. A block is inserted in NVM only if it is a read-reuse block or its compressed size is lower than a threshold. It will be inserted in SRAM if the block is a write-reuse or its compressed size is greater than the threshold. We use set-dueling to tune the compression threshold at runtime. This compression threshold provides a knob to control the NVM write rate and, together with a rule-based mechanism, allows balancing performance and lifetime.Overall, our evaluation shows that, with affordable hardware overheads, the proposed schemes can nearly reach the performance of an SRAM cache with the same associativity while improving lifetime by 17× compared to a hybrid NVM-unaware LLC. Our proposed scheme outperforms the state-of-the-art insertion policies by 9% while achieving a comparative lifetime. The rule-based mechanism shows that by compromising, for instance, 1.1% and 1.9% performance, the NVM lifetime can be further increased by 28% and 44%, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070968","","Performance evaluation;Fault tolerance;Runtime;Nonvolatile memory;Fault tolerant systems;Random access memory;Hardware","","1","","49","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"NOMAD: Enabling Non-blocking OS-managed DRAM Cache via Tag-Data Decoupling","Y. Kim; H. Kim; W. J. Song","School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","193","205","This paper introduces a DRAM cache architecture that provides near-ideal access time and non-blocking miss handling. Previous DRAM cache (DC) designs are classified into two categories, HW-based and OS-managed schemes. Hardware-based designs implement non-blocking caches that can handle multiple DC misses using MSHRs, but they have drawbacks in metadata management since storing tags in on-package DRAM significantly increases the effective cycle time of DC accesses. In contrast, OS-managed schemes utilize PTEs for storing tags and caching them in TLBs, which can achieve ideal DC access time. However, they implement blocking caches that stall application threads on misses until cache fills are completed. To overcome the limitations of both HW-based and OS-managed schemes, this paper introduces a DRAM cache architecture named Non-blocking OS-managed DRAM cache (NOMAD). Unlike conventional caches that guarantee the presence of data on tag hits, NOMAD decouples tag and data management to enable non-blocking miss handling in an OS-managed DRAM cache. The front-end OS routines of NOMAD manage DC tags using PTEs and TLBs, and its back-end hardware handles data management in the DRAM cache. On a DC miss, the OS updates a tag, offloads a cache-fill command to the back-end, and immediately resumes an application thread without waiting for the cache fill to complete. Instead, the back-end hardware handles the cache fill without blocking the application thread. By decoupling tag and data management in NOMAD, a tag hit does not necessarily guarantee the presence of data in the DRAM cache. The back-end traces which DC lines are still in transfers and checks if the demanded part of a cache line has been transferred yet for every DC access. Notably, this back-end procedure does not require an OS intervention, thereby implementing a non-blocking DRAM cache. Experiment results show that NOMAD reduces application stall cycles by 76.1% and improves IPC by 16.7% over a state-of-the-art OS-managed scheme.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071016","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071016","","Random access memory;Computer architecture;Metadata;Hardware;Complexity theory","","7","","50","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Safety Hints for HTM Capacity Abort Mitigation","A. Jain; D. K. Kadiyala; A. Daglis","School of Computer Science, Georgia Institute of Technology, Atlanta, Georgia, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, Georgia, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","206","219","Hardware Transactional Memory (HTM) is a high-performance instantiation of the powerful programming abstraction of transactional memory, which simplifies the daunting— yet critically important—task of parallel programming. While many HTM implementations with variable complexity exist in the literature, commercially available HTMs impose rigid restrictions to transaction and system behavior, limiting their practical use. A key constraint is the limited size of supported transactions, implicitly capped by hardware buffering capacity. We identify the opportunity to expand the effective capacity of these limited hardware structures by being more selective in memory accesses that need to be tracked. We leverage compiler and virtual memory support to identify safe memory accesses, which can never cause a transaction abort, subsequently passed as safety hints to the underlying HTM. With minor extensions over a conventional HTM implementation, HinTM uses these hints to selectively allocate transactional state tracking resources to unsafe accesses only, thus expanding the HTM’s effective capacity, and conversely reducing capacity aborts. We demonstrate that HinTM effectively augments the performance of a range of baseline HTM configurations. When coupled with a POWER8 HTM implementation, HinTM eliminates 64% of transactional capacity aborts, achieving 1.4× average speedup, and up to 8.7×.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071113","","Couplings;Limiting;Parallel programming;Memory management;Performance gain;Hardware;Safety","","","","74","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"iCache: An Importance-Sampling-Informed Cache for Accelerating I/O-Bound DNN Model Training","W. Chen; S. He; Y. Xu; X. Zhang; S. Yang; S. Hu; X. -H. Sun; G. Chen",Zhejiang University; Zhejiang University; Zhejiang University; Washington State University Vancouver; Zhejiang University; Zhejiang University; Illinois Institute of Technology; Zhejiang University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","220","232","Fetching a large amount of DNN training data from storage systems incurs long I/O latency and fetch stalls of GPUs. Importance sampling in DNN training can reduce the amount of data computing on GPUs while maintaining a similar model accuracy. However, existing DNN training frameworks do not have a cache layer that reduces the number of data fetches and manages cached items according to sample importance, resulting in unnecessary data fetches, poor cache hit ratios, and random I/Os when importance sampling is used.In this paper, we design a new importance-sampling-informed cache, namely, iCache, to accelerate I/O bound DNN training jobs. iCache only fetches parts of samples instead of all samples in the dataset. The cache is partitioned into two regions: H-cache and L-cache, which store samples of high importance and low importance respectively. Rather than using recency or frequency, we manage data items in H-cache according to their corresponding sample importance. When there is a cache miss in L-cache, we use sample substitutability and dynamic packaging to improve the cache hit ratio and reduce the number of random I/Os. When multiple concurrent jobs access the same datasets in H-cache, we design a model to assign the relative importance values to cached samples to avoid cache thrashing, which may happen when there is no coordination among the concurrent training jobs. Our experimental results show that iCache has a negligible impact on training accuracy and speeds up the DNN training time by up to 2.0× compared to the state-of-the-art caching systems.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070964","Research and Development; National Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070964","","Training;Deep learning;Monte Carlo methods;Computational modeling;Training data;Computer architecture;Packaging","","11","","54","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Are Randomized Caches Truly Random? Formal Analysis of Randomized-Partitioned Caches","A. Chakraborty; S. Bhattacharya; S. Saha; D. Mukhopadhyay","Indian Institute of Technology, Kharagpur; Imec, Belgium; Nanyang Technological University, Singapore; Indian Institute of Technology, Kharagpur",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","233","246","Cache based side-channel attacks exploit the fact that an adversary can setup the shared cache memory (the last level cache in modern systems) into a known state and detect any microarchitectural state changes made by the victim on the cache. Different mitigation techniques have been proposed in the literature that aims to mitigate these attacks by randomizing the address to cache location mappings. The security guarantees in these schemes are based on the degree of difficulty for an attacker to reliably determine the cache lines accessed by the victim within practical time settings. However, prior attacks have shown that newer and more improved algorithms can be envisaged that discover conflicting sets in the secured randomized caches. In this work, we first categorize different types of cache designs into four broad classes based on the extent of non-determinism and randomness of allocating an address in those caches. We then develop a mathematical framework to formally analyse the security implications of the randomized and partitioned cache designs in terms of collision probability, self-collision probability and size of the eviction set required to perform a successful eviction-based attack. We further empirically demonstrate set associative eviction on recently proposed randomization schemes called Mirage and Scattercache. Next, we propose two algorithms to generate efficient eviction set on these schemes and analytically evaluate the efficacy of our algorithms against the one proposed in the literature. Finally, we argue that mere randomization using a cryptographic primitive as used in popular schemes like Scattercache, CEASER-S, Mirage etc. does not provide the required randomness. Although the randomized-partitioned caches provide some resilience against eviction-set generation techniques, they are still vulnerable to eviction-based attacks.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071041","","Microarchitecture;Cache memory;Side-channel attacks;Computer architecture;Signal generators;Partitioning algorithms;Reliability","","2","","34","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"HIRAC: A Hierarchical Accelerator with Sorting-based Packing for SpGEMMs in DNN Applications","H. Shabani; A. Singh; B. Youhana; X. Guo","Lehigh University, USA; Lehigh University, USA; Lehigh University, USA; Lehigh University, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","247","258","The state-of-the-art deep neural network (DNN) models use pruning to avoid over-fitting and reduce the number of parameters. In order to improve storage and computational efficiency, only nonzero elements are stored, and their locations are encoded into a sparse format. Sparse General Matrix Multiplication (SpGEMM) is the kernel computation of DNN-based applications. One challenge of computing SpGEMM is to avoid multiplying zero elements while keeping hardware utilization high in hardware accelerators that consist of processing element (PE) arrays. Prior work tackling this challenge typically requires complex interconnection networks, which adds high area and energy costs.This work proposes a HW/SW co-design architecture to compute SpGEMM efficiently without requiring complex interconnection networks. A novel fast packing algorithm, SorPack, is proposed to convert a sparse matrix into a dense matrix that increases PE utilization. The key idea is to sort columns and rows inside each submatrix based on the number of nonzero elements. The goal is to keep the partial sums that need to be added together close to each other, hence can be added locally and avoid the use of complex interconnection networks. In addition, a new tile-based hierarchical architecture, HIRAC, is proposed to provide a scalable system that maximizes the parallelism of the PEs. The HIRAC architecture consists of a novel PE array design and interconnection network tailored for DNN applications. The SorPack algorithm complements the HIRAC to further improve hardware utilization and overall system performance. Based on the evaluation results, HIRAC achieves an average of 3.2× speedup on a single layer of DNN as compared to the state-of-the-art sparse DNN accelerator SIGMA. In addition, HIRAC has a 9.5% area reduction and a 32% power reduction as compared to SIGMA. An end-to-end evaluation on a DNN model shows an 8.2× runtime reduction over the TPU.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070977","","Runtime;Multiprocessor interconnection;System performance;Neural networks;Parallel processing;Sparse matrices;Matrix converters","","7","","31","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs","G. Jeong; S. Damani; A. R. Bambhaniya; E. Qin; C. J. Hughes; S. Subramoney; H. Kim; T. Krishna",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Intel Labs; Intel Labs; Georgia Institute of Technology; Georgia Institute of Technology,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","259","272","Deep Learning (DL) acceleration support in CPUs has recently gained a lot of traction, with several companies (Arm, Intel, IBM) announcing products with specialized matrix engines accessible via GEMM instructions. CPUs are pervasive and need to handle diverse requirements across DL workloads running in edge/HPC/cloud platforms. Therefore, as DL workloads embrace sparsity to reduce the computations and memory size of models, it is also imperative for CPUs to add support for sparsity to avoid under-utilization of the dense matrix engine and inefficient usage of the caches and registers. This work presents VEGETA, a set of ISA and microarchitecture extensions over dense matrix engines to support flexible structured sparsity for CPUs, enabling programmable support for diverse DL models with varying degrees of sparsity. Compared to the state-of-the-art (SOTA) dense matrix engine in CPUs, a VEGETA engine provides 1.09×, 2.20×, 3.74×, and 3.28× speed-ups when running 4:4 (dense), 2:4, 1:4, and unstructured (95%) sparse DNN layers.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071058","","Deep learning;Microarchitecture;Computational modeling;Computer architecture;Companies;Registers;Sparse matrices","","20","","56","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design","H. You; Z. Sun; H. Shi; Z. Yu; Y. Zhao; Y. Zhang; C. Li; B. Li; Y. Lin","Georgia Institute of Technology, Atlanta, GA; Rice University, Houston, TX; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Rice University, Houston, TX; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Oracle Health and AI, Redwood, CA; Georgia Institute of Technology, Atlanta, GA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","273","286","Vision Transformers (ViTs) have achieved state-of-the-art performance on various vision tasks. However, ViTs’ self-attention module is still arguably a major bottleneck, limiting their achievable hardware efficiency and more extensive applications to resource constrained platforms. Meanwhile, existing accelerators dedicated to NLP Transformers are not optimal for ViTs. This is because there is a large difference between ViTs and Transformers for natural language processing (NLP) tasks: ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns, without severely hurting the model accuracy (e.g., <=1.5% under 90% pruning ratio); while NLP Transformers need to handle input sequences of varying numbers of tokens and rely on on-the-fly predictions of dynamic sparse attention patterns for each input to achieve a decent sparsity (e.g., >=50%). To this end, we propose a dedicated algorithm and accelerator co-design framework dubbed ViTCoD for accelerating ViTs. Specifically, on the algorithm level, ViTCoD prunes and polarizes the attention maps to have either denser or sparser fixed patterns for regularizing two levels of workloads without hurting the accuracy, largely reducing the attention computations while leaving room for alleviating the remaining dominant data movements; on top of that, we further integrate a lightweight and learnable auto-encoder module to enable trading the dominant high-cost data movements for lower-cost computations. On the hardware level, we develop a dedicated accelerator to simultaneously coordinate the aforementioned enforced denser and sparser workloads for boosted hardware utilization, while integrating on-chip encoder and decoder engines to leverage ViTCoD’s algorithm pipeline for much reduced data movements. Extensive experiments and ablation studies validate that ViTCoD largely reduces the dominant data movement costs, achieving speedups of up to 235.3×, 142.9×, 86.0×, 10.1×, and 6.8× over general computing platforms CPUs, EdgeGPUs, GPUs, and prior-art Transformer accelerators SpAtten and Sanger under an attention sparsity of 90%, respectively. Our code implementation is available at https://github.com/GATECH-EIC/ViTCoD.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071027","","Heuristic algorithms;Predictive models;Transformers;Prediction algorithms;Natural language processing;Hardware;Decoding","","73","","53","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Leveraging Domain Information for the Efficient Automated Design of Deep Learning Accelerators","C. Sakhuja; Z. Shi; C. Lin",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","287","301","Deep learning accelerators are important tools for feeding the growing demand for deep learning applications. The automated design of such accelerators—which is important for reducing development costs—can be viewed as a search over a vast and complex design space that consists of all possible accelerators and all the possible software that could run on them. Unfortunately, this search is complicated by the existence of many ordinal and categorical values, which are critical to explore for the ultimate design but are not handled well by existing search techniques.This paper presents a technique for efficiently searching this space by injecting domain information—in this case information about hardware/software (HW/SW) co-design—into the automated search process. Specifically, this paper introduces a novel Bayesian optimization framework called daBO (domain-aware BO) that accepts domain information as input, including those describing ordinal and categorical values.This paper also introduces Spotlight, a design tool based on daBO, and this paper empirically shows that Spotlight produces accelerator designs and software schedules that are orders of magnitude better than those created by the state-of-the-art. For example, for the ResNet-50 deep learning model, Spotlight produces a HW/SW configuration that reduces delay by 135× over the configuration produced by ConfuciuX, a state-of-the-art HW/SW co-design tool, and Spotlight reduces energy-delay product (EDP) by 44× over an Eyeriss-like accelerator, which is an edge-scale hand-designed accelerator. In the realm of cloud-scale accelerators, Spotlight reduces the EDP of a scaled-up Eyeriss-like accelerator by 23×. Our evaluation shows that Spotlight benefits from the efficiency of daBO, which allows Spotlight to identify accelerator designs and software schedules that prior work cannot identify.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071095","","Deep learning;Schedules;Emulation;Computer architecture;Software;Space exploration;Bayes methods","","8","","75","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing","Z. Zhou; C. Li; F. Yang; G. Sun","School of Integrated Circuits; School of Integrated Circuits; School of Computer Science, Nankai University; School of Integrated Circuits",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","302","316","DIMM-based near-memory processing architectures (DIMM-NMP) have received growing interest from both academia and industry. They have the advantages of large memory capacity, low manufacturing cost, high flexibility, compatible form factor, etc. However, inter-DIMM communication (IDC) has become a critical obstacle for generic DIMM-NMP architectures because it involves costly forwarding transactions through the host CPU. Recent research has demonstrated that, for many applications, the overhead induced by IDC may even offset the performance and energy benefits of near-memory processing.To tackle this problem, we propose DIMM-Link, which enables high-performance IDC in DIMM-NMP architectures and supports seamless integration with existing host memory systems. It adopts bidirectional external data links to connect DIMMs, via which point-to-point communication and inter-DIMM broadcast are efficiently supported in a packet-routing way. We present the full-stack design of DIMM-Link, including the hardware architecture, interconnect protocol, system organization, routing mechanisms, optimization strategies, etc. Comprehensive experiments on typical data-intensive tasks demonstrate that the DIMM-Link-equipped NMP system can achieve a 5.93× average speedup over the 16-core CPU baseline. Compared to other IDC methods, DIMM-Link outperforms MCN, AIM, and ABC-DIMM by 2.42×, 1.87×, and 1.77×, respectively. More importantly, DIMM-Link fully considers the implementation feasibility and system integration constraints, which are critical for designing NMP architectures based on modern DDR4/DDR5 DIMMs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071005","","Industries;Protocols;Computer architecture;Organizations;System integration;Routing;Hardware","","22","","91","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"AutoCAT: Reinforcement Learning for Automated Exploration of Cache-Timing Attacks","M. Luo; W. Xiong; G. Lee; Y. Li; X. Yang; A. Zhang; Y. Tian; H. -H. S. Lee; G. E. Suh",Cornell University; Virginia Tech; Virginia Tech; Cornell University; Meta AI; Meta AI; Meta AI; Intel Corporation; Cornell University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","317","332","The aggressive performance optimizations in modern microprocessors can result in security vulnerabilities. For example, timing-based attacks in processor caches can steal secret keys or break randomization. So far, finding cache-timing vulnerabilities is mostly performed by human experts, which is inefficient and laborious. There is a need for automatic tools that can explore vulnerabilities given that unreported vulnerabilities leave the systems at risk.In this paper, we propose AutoCAT, an automated exploration framework that finds cache timing-channel attack sequences using reinforcement learning (RL). Specifically, AutoCAT formulates the cache timing-channel attack as a guessing game between an attack program and a victim program holding a secret. This guessing game can thus be solved via modern deep RL techniques. AutoCAT can explore attacks in various cache configurations without knowing design details and under different attack and victim program configurations. AutoCAT can also find attacks to bypass certain detection and defense mechanisms. In particular, AutoCAT discovered StealthyStreamline, a new attack that is able to bypass performance counter-based detection and has up to a 71% higher information leakage rate than the state-of-the-art LRU-based attacks on real processors. AutoCAT is the first of its kind in using RL for crafting microarchitectural timing-channel attack sequences and can accelerate cache timing-channel exploration for secure microprocessor designs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070947","","Microarchitecture;Microprocessors;Bit rate;Closed box;Reinforcement learning;Games;Computer architecture","","2","","86","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"SHADOW: Preventing Row Hammer in DRAM with Intra-Subarray Row Shuffling","M. Wi; J. Park; S. Ko; M. J. Kim; N. Sung Kim; E. Lee; J. H. Ahn",Seoul National University; Seoul National University; Seoul National University; Seoul National University; University of Illinois at Urbana-Champaign; Inha University; Seoul National University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","333","346","As Row Hammer (RH) attacks have been a critical threat to computer systems, numerous hardware-based (HWbased) RH mitigation strategies have been proposed. However, the advent of non-adjacent RH attacks and lower RH thresholds significantly increase the area and performance overhead of these prior solutions due to their conservative design characteristics.We propose a new in-DRAM RH protection solution named Shuffling Aggressor DRAM Rows (SHADOW). SHADOW dynamically randomizes DRAM row mapping information, preventing an attacker from targeting a specific victim row that may hold critical data. SHADOW is robust against non-adjacent RH attacks because it utilizes the in-DRAM row-shuffle technique. To realize the in-DRAM row-shuffle operation with low performance and energy overhead, we use a novel DRAM microarchitecture optimization technique. We also utilize the recently introduced JEDEC RFM interface to enable in-DRAM RH mitigation without any DRAM interface changes. By exploiting an additional DRAM row per subarray, SHADOW does not require costly SRAM- or CAM-based tracking structures other than intrinsic counters for the RFM interface. We demonstrate the strong probabilistic protection of SHADOW against RH attacks through adversarial pattern analysis and highlight the compelling performance, area, and energy overheads compared to those of state-of-the-art HW-based RH prevention solutions.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070966","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070966","","Microarchitecture;Random access memory;Computer architecture;Probabilistic logic;Pattern analysis;Computer security;Optimization","","20","","99","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Efficient Distributed Secure Memory with Migratable Merkle Tree","E. Feng; D. Du; Y. Xia; H. Chen","Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","347","360","Hardware-assisted enclaves with memory encryption have been widely adopted in the prevailing architectures, e.g., Intel SGX/TDX, AMD SEV, ARM CCA, etc. However, existing enclave designs fall short in supporting efficient cooperation among cross-node enclaves (i.e., multi-machines) because the range of hardware memory protection is within a single node. A naive approach is to leverage cryptography at the application level and transfer data between nodes through secure channels (e.g., SSL). However, it incurs orders of magnitude costs due to expensive encryption/decryption, especially for distributed applications with large data transfer, e.g., MapReduce and graph computing. A secure and efficient mechanism of distributed secure memory is necessary but still missing.This paper proposes Migratable Merkle Tree (MMT), a design enabling efficient distributed secure memory to support distributed confidential computing. MMT sets up an integrity forest for distributed memory on multiple nodes. It allows an enclave to securely delegate an MMT closure, which contains both data and metadata of a subtree, to a remote enclave. By reusing the memory encryption mechanisms of existing enclaves, our design achieves secure data transfer without software re-encryption. We have implemented a prototype of MMT and a trusted firmware for management, and further applied MMT to real-world distributed applications. The evaluation results show that compared with existing systems using the AES-NI instruction, MMT can achieve up to 13x speedup on data transferring, and gain 12%~58% improvement on the end-to-end performance of MapReduce and PageRank.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071130","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071130","","Protocols;Program processors;Memory management;Prototypes;Metadata;Data transfer;Software","","4","","70","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"AB-ORAM: Constructing Adjustable Buckets for Space Reduction in Ring ORAM","M. Raoufi; J. Yang; X. Tang; Y. Zhang","Computer Science Department, University of Pittsburgh, Pittsburgh, PA, USA; Electrical and Computer Engineering Department, University of Pittsburgh, Pittsburgh, PA, USA; Computer Science Department, University of Pittsburgh, Pittsburgh, PA, USA; Computer Science Department, University of Pittsburgh, Pittsburgh, PA, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","361","373","Ring ORAM (Oblivious RAM) is a secure primitive that mitigates the large performance degradation of ORAM through reduced online memory bandwidth demand, i.e., the number of memory accesses at servicing a real memory request. Ring ORAM requires 4× or more of the protected data space to enable the optimization and thus presents high capacity pressure on modern memory systems. While recent studies strive to reduce its space consumption through bucket compaction, the large space consumption remains a major design challenge for Ring ORAM.In this paper, we propose AB-ORAM to reduce the space capacity demand in Ring ORAM. AB-ORAM identifies two inefficient use of memory space in Ring ORAM: (i) accessed blocks hold useless data until the next reshuffle operation; and (ii) large buckets provide a diminishing performance benefit for tree levels close to the leaves. AB-ORAM then proposes two schemes to exploit the optimization opportunities, respectively. Specifically, it reclaims accessed blocks early by allocating them to buckets that need a reshuffle; and shrinks the bucket size for tree level close to the leaves for a better space/performance trade-off. We evaluate the proposed AB-ORAM design and compare it to the state-of-the-art. Our results show that AB-ORAM achieves an average of 36% space reduction over the state-of-the-art while introducing very low performance overhead.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071064","Oblivious RAM;ORAM;security;memory access pattern;space efficiency","Degradation;Memory management;Random access memory;Bandwidth;Compaction;Resource management;Optimization","","2","","40","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Scalable and Secure Row-Swap: Efficient and Safe Row Hammer Mitigation in Memory Systems","J. Woo; G. Saileshwar; P. J. Nair",University of British Columbia; University of British Columbia; University of British Columbia,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","374","389","As Dynamic Random Access Memories (DRAM) scale, they are becoming increasingly susceptible to Row Hammer. By rapidly activating rows of DRAM cells (aggressor rows), attackers can exploit inter-cell interference through Row Hammer to flip bits in neighboring rows (victim rows). A recent work, called Randomized Row-Swap (RRS), proposed proactively swapping aggressor rows with randomly selected rows before an aggressor row can cause Row Hammer.Our paper observes that RRS is neither secure nor scalable. We first propose the ‘Juggernaut attack pattern’ that breaks RRS in under 1 day. Juggernaut exploits the fact that the mitigative action of RRS, a swap operation, can itself induce additional target row activations, defeating such a defense. Second, this paper proposes a new defense Secure Row-Swap mechanism that avoids the additional activations from swap (and unswap) operations and protects against Juggernaut. Furthermore, this paper extends Secure Row-Swap with attack detection to defend against even future attacks. While this provides better security, it also allows for securely reducing the frequency of swaps, thereby enabling Scalable and Secure Row-Swap. The Scalable and Secure Row-Swap mechanism provides years of Row Hammer protection with 3.3× lower storage overheads as compared to the RRS design. It incurs only a 0.7% slowdown as compared to a not-secure baseline for a Row Hammer threshold of 1200.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070999","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070999","","Correlation;Memory management;Random access memory;System-on-chip;Security;Intercell interference","","21","","60","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Post0-VR: Enabling Universal Realistic Rendering for Modern VR via Exploiting Architectural Similarity and Data Sharing","Y. Wen; C. Xie; S. L. Song; X. Fu","ECE Department, ECMOS Lab, University of Houston; School of Computer Science and Engineering, Beihang University; FSA Lab, School of Computer Science, University of Sydney; ECE Department, ECMOS Lab, University of Houston",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","390","402","To provide users with a fully immersive environment, VR post-processing, which adds numerous realistic effects on the frame after rendering, plays a key role in modern VR systems. Current post-processing is processed separately from normal rendering by the graphics processing unit (GPU). As a result, the GPU needs to first render a high-resolution frame and then add the post-processing effects within a very short time frame. Our in-depth experimental results on commercial VR products demonstrate that the post-processing in VR applications extends the VR frame time by approximately 2X on average. Furthermore, the ever-increasing resolution requirements of modern VR significantly increase the workloads for post-processing in the execution pipeline. This long delay causes VR real-time execution to frequently miss the critical frame-time deadline, thus hurting users’ quality of experience.Based on the analysis of VR post-processing workflow and its common realistic effects, we observe that post-processing shares the same hardware pipeline with normal rendering, and even reuses the intermediate data produced by normal rendering. To fully utilize this hardware-level similarity and capture the data locality, we propose a novel universal realistic rendering architecture for VR, named Post0-VR, which eliminates post-processing by directly merging the common realistic effects into the normal rendering process. Based on our newly proposed VR architecture design, we further propose a dynamic accuracy adjustment method to simplify the normal rendering without hurting users’ perception. The evaluation results on real-world applications demonstrate that Post0-VR can support different types of realistic effects while significantly improving the overall VR rendering performance.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071097","","Pipelines;Merging;Graphics processing units;Computer architecture;Virtual reality;Quality of service;Rendering (computer graphics)","","2","","94","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ParallelNN: A Parallel Octree-based Nearest Neighbor Search Accelerator for 3D Point Clouds","F. Chen; R. Ying; J. Xue; F. Wen; P. Liu","Brain-Inspired Application Technology Center (BATC), Shanghai Jiao Tong University, Shanghai, China; Brain-Inspired Application Technology Center (BATC), Shanghai Jiao Tong University, Shanghai, China; Brain-Inspired Application Technology Center (BATC), Shanghai Jiao Tong University, Shanghai, China; Brain-Inspired Application Technology Center (BATC), Shanghai Jiao Tong University, Shanghai, China; Brain-Inspired Application Technology Center (BATC), Shanghai Jiao Tong University, Shanghai, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","403","414","As Light Detection And Ranging (LiDAR) increasingly becomes an essential component in robotic navigation and autonomous driving, the processing of high throughput 3D point clouds in real time is widely required. This work considers the point cloud k-Nearest Neighbor (kNN) search, which is an important 3D processing kernel. Although applying fine-grained parallelism optimization on internal processing, e.g., using multiple workers, has demonstrated high efficiency, previous accelerators with DDR external memory are fundamentally limited by the external bandwidth bottleneck. To break this bottleneck, this work proposes a highly parallel architecture, namely ParallelNN, for highly efficient kNN search processing of high throughput point clouds. First, we optimize the multichannel cache based on High Bandwidth Memory (HBM) and on-chip memory to provide large external bandwidth. Then, a novel parallel depth-first octree construction algorithm is proposed and mapped onto multiple construction branches with trace-coded construction queues, which can regularize random accesses and perform multi-branch octree construction efficiently. Furthermore, in the search stage, we present algorithm-architecture co-optimization, including parallel keyframe-based scheduling and multi-branch flexible search engines, to provide conflict-free access and maximum reuse opportunities for reference points, which achieves more than 27.0× speedup compared with baseline architectures. We prototype ParallelNN on Virtex HBM FPGA and perform extensive benchmarking on the KITTI dataset. The results demonstrate that ParallelNN achieves up to 107.7× and 12.1× speedup over CPU and GPU implementations, while being more energy efficient, e.g., outperforming CPU and GPU implementations by 73.6× and 31.1×, respectively. Besides, with the proposed algorithm-architecture co-optimization, ParallelNN achieves 11.4× speedup over state-of-the-art architecture. Moreover, ParallelNN is configurable and can be easily generalized to similar octree-based applications.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070940","","Point cloud compression;Three-dimensional displays;Octrees;Graphics processing units;Bandwidth;Parallel processing;Search problems","","14","","48","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention","J. Dass; S. Wu; H. Shi; C. Li; Z. Ye; Z. Wang; Y. Lin","Rice University, Houston, TX; Rice University, Houston, TX; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Rice University, Houston, TX; Nanjing University, Nanjing; Georgia Institute of Technology, Atlanta, GA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","415","428","Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViTs’ multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from VITALITY’s linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that VITALITY offers boosted end-to-end efficiency (e.g., 3× faster and 3× energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution. We make the codes available on https://github.com/GATECH-EIC/ViTaLiTy","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071081","","Training;Costs;Systematics;Approximation algorithms;Transformers;Boosting;Sparse representation","","39","","51","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"CTA: Hardware-Software Co-design for Compressed Token Attention Mechanism","H. Wang; H. Xu; Y. Wang; Y. Han","CICS, Institute of Computing Technology, Chinese Academy of Sciences; CICS, Institute of Computing Technology, Chinese Academy of Sciences; CICS, Institute of Computing Technology, Chinese Academy of Sciences; CICS, Institute of Computing Technology, Chinese Academy of Sciences",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","429","441","The attention mechanism is becoming an integral part of modern neural networks, bringing breakthroughs to Natural Language Processing (NLP) applications and even Computer Vision (CV) applications. Unfortunately, the superiority of attention mechanism comes from its ability to model relations between any two positions in long sequence, which incurs high inference overhead. For state-of-the-art AI workloads such as Bert or GPT-2, attention mechanism is reported to account up to 50% of the inference overhead. Previous works seek to alleviate this performance bottleneck by removing useless relations for each position and accelerate position-specific operations. However their attempts require selecting from a sequence of relations once for each position, which is essentially frequent on-the-fly pruning and breaks the inherent parallelism in attention mechanism. In this paper, we propose CTA, an algorithm-architecture co-designed solution that can substantially reduce theoretic complexity of attention mechanism, enabling significant speedup and energy saving. Inspired by the fact that the feature sequence encoded by attention mechanism contain a large number of semantic feature repetition, we propose a novel approximation scheme that can efficiently remove that repetition, only calculating attention among necessary features thus reducing computation complexity quadratically. To utilize this algorithmic bonus and empower high performance attention mechanism inference, we devise specialized architecture to efficiently support the proposed approximation scheme. Extensive experiments show that, on average, CTA achieves 27.7× speedup, 634.0× energy savings with no accuracy loss, and 44.2× speedup, 950.0× energy savings with around 1% accuracy loss over Nvidia V100-SXM2 GPU. Also, CTA achieves 22.8× speedup, 479.6× energy savings over ELSA accelerator+GPU system.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070997","National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070997","","Computational modeling;Semantics;Neural networks;Graphics processing units;Computer architecture;Parallel processing;Approximation algorithms","","12","","65","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers","P. Dong; M. Sun; A. Lu; Y. Xie; K. Liu; Z. Kong; X. Meng; Z. Li; X. Lin; Z. Fang; Y. Wang",Northeastern University; Northeastern University; Simon Fraser University; Northeastern University; Simon Fraser University; Northeastern University; Northeastern University; Northeastern University; Northeastern University; Simon Fraser University; Northeastern University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","442","455","While vision transformers (ViTs) have continuously achieved new milestones in the field of computer vision, their sophisticated network architectures with high computation and memory costs have impeded their deployment on resource-limited edge devices. In this paper, we propose a hardware-efficient image-adaptive token pruning framework called HeatViT for efficient yet accurate ViT acceleration on embedded FPGAs. Based on the inherent computational patterns in ViTs, we first adopt an effective, hardware-efficient, and learnable head-evaluation token selector, which can be progressively inserted before transformer blocks to dynamically identify and consolidate the non-informative tokens from input images. Moreover, we implement the token selector on hardware by adding miniature control logic to heavily reuse existing hardware components built for the backbone ViT. To improve the hardware efficiency, we further employ 8-bit fixed-point quantization and propose polynomial approximations with regularization effect on quantization error for the frequently used nonlinear functions in ViTs. Compared to existing ViT pruning studies, under the similar computation cost, HeatViT can achieve 0.7% ~ 8.9% higher accuracy; while under the similar model accuracy, HeatViT can achieve more than 28.4% ~ 65.3% computation reduction, for various widely used ViTs, including DeiT-T, DeiT-S, DeiT-B, LV-ViT-S, and LV-ViT-M, on the ImageNet dataset. Compared to the baseline hardware accelerator, our implementations of HeatViT on the Xilinx ZCU102 FPGA achieve 3.46×~4.89× speedup with a trivial resource utilization overhead of 8%~11% more DSPs and 5%~8% more LUTs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071047","Vision Transformer;FPGA Accelerator;Hardware and Software Co-design;Data-level Sparsity","Heating systems;Training;Quantization (signal);Costs;Image edge detection;Transformers;Software","","67","","66","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding","B. Li; J. Yin; A. Holey; Y. Zhang; J. Yang; X. Tang","University of Pittsburgh, Pittsburgh, Pennsylvania, USA; Lehigh University, Bethlehem, Pennsylvania, USA; NVIDIA, Santa Clara, California, USA; University of Pittsburgh, Pittsburgh, Pennsylvania, USA; University of Pittsburgh, Pittsburgh, Pennsylvania, USA; University of Pittsburgh, Pittsburgh, Pennsylvania, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","456","470","Multi-GPU systems have become a popular platform to meet the ever-growing application demands. However, employing multiple GPUs does not guarantee proportional performance improvements. While prior works have extensively studied the optimizations to mitigate the non-uniform memory accesses (NUMA) overheads, the address translation process also plays an important role in shaping the overall execution performance. In this paper, we investigate the address translation process in multi-GPU systems under unified virtual memory (UVM). We specifically focus on the efficiency of page table walk and identify three major latency penalties: i) queuing for available page table walk threads, ii) memory accesses for page walk cache misses, and iii) handling page faults. Based on our observations, we propose Trans-FW, which short circuits the page table walk by leveraging substantial translation sharing and eager remote translation forwarding. Experimental results on 10 representative multi-GPU applications show that our proposed approach improves the overall performance by 53.8% on average.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071054","University of Pittsburgh; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071054","multi-GPU;page fault;page table walk","Fault diagnosis;Scalability;Prefetching;Memory management;Circuit faults;Optimization","","11","","80","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Ah-Q: Quantifying and Handling the Interference within a Datacenter from a System Perspective","Y. Liu; X. Deng; J. Zhou; M. Chen; Y. Bao","State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","471","484","Interference among applications frequently occurs in a datacenter and significantly influences the cost-efficiency and the user experience. However, it is challenging for us to quantify the exact intensity of the interference that occurred in the overall system of a datacenter, because there are many concurrent applications in a datacenter, and their type can be either latency-critical (LC) and best-effort (BE). To address this issue, we present the Ah-Q which includes a theory and a strategy.First, we propose the ""system entropy"" (ES) theory to holistically and analytically quantify the interference in a datacenter to address this vital issue. The interference is caused by the scarcity of resources or/and the irrationality of scheduling. As more appropriate scheduling can compensate for resource scarcity, we derive the concept of ""resource equivalence"" to quantify the effectiveness of a resource scheduling strategy. We evaluate different resource scheduling strategies to validate the correctness and effectiveness of the proposed theory.Moreover, using the theory to eliminate interference, we propose a new resource scheduling strategy; i.e., ARQ, which dynamically allocates the isolated resources and the shared resources to simultaneously harvest the benefits of isolation and sharing. Our results show that compared to the state-of-the-art strategies (PARTIES and CLITE), ARQ is more effective to reduce the tail latency of the LC applications and to increase the IPC of the BE applications. Compared with PARTIES and CLITE, ARQ increases the yield (the ratio of satisfactory LC applications) by 25% and 20%, respectively; when the load is low, ARQ increases IPC of BE applications by 63.8% and 37.1%, respectively; ARQ reduces ES by 36.4% and 33.3%, respectively. The effectiveness of ARQ has saved resources significantly to achieve the same satisfactory overall user experience.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071128","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071128","","Concurrent computing;Interference;Computer architecture;Tail;Quality of service;Dynamic scheduling;User experience","","10","","57","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Market Mechanism-Based User-in-the-Loop Scalable Power Oversubscription for HPC Systems","M. R. Hossen; K. Ahmed; M. A. Islam",The University of Texas at Arlington; The University of Toledo; The University of Texas at Arlington,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","485","498","Significant power consumption is one of the major challenges for current and future high-performance computing (HPC) systems. All the while, HPC systems generally remain power underutilized, making them a great candidate for applying power oversubscription to reclaim unused capacity. However, an oversubscribed HPC system may occasionally get overloaded. In this paper, we propose MPR (Market-based Power Reduction), a scalable market-based approach where users actively participate in reducing the HPC system’s power consumption to mitigate overloads. In MPR, HPC users bid to supply, in exchange for incentives, the resource reduction required for handling the overloads. Using several real-world trace-based simulations, we extensively evaluate MPR and show that, by participating in MPR, users always receive more rewards than the cost of performance loss. At the same time, the HPC manager enjoys orders of magnitude more resource gain than her incentive payoff to the users. We also demonstrate the real-world effectiveness of MPR on a prototype system.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071006","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071006","","Power demand;Costs;High performance computing;Prototypes;Computer architecture","","3","","66","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Rambda: RDMA-driven Acceleration Framework for Memory-intensive µs-scale Datacenter Applications","Y. Yuan; J. Huang; Y. Sun; T. Wang; J. Nelson; D. R. K. Ports; Y. Wang; R. Wang; C. Tai; N. S. Kim",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Microsoft Research; Microsoft Research; Intel Labs; Intel Labs; Intel Labs; University of Illinois at Urbana-Champaign,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","499","515","Responding to the ""datacenter tax"" and ""killer microseconds"" problems for memory-intensive datacenter applications, diverse solutions including Smart NIC-based ones have been proposed. Nonetheless, they often suffer from high overhead of communications over network and/or PCIe links. To tackle the limitations of the current solutions, this paper proposes RAMBDA, a holistic network and architecture co-design solution that leverages current RDMA and emerging cache-coherent off-chip interconnect technologies. Specifically, RAMBDA consists of four hardware and software components: (1) unified abstraction of inter- and intra-machine communications synergistically managed by one-sided RDMA write and cache-coherent memory write; (2) efficient notification of requests to accelerators assisted by cache coherence; (3) cache-coherent accelerator architecture directly interacting with NIC; and (4) adaptive device-to-host data transfer for modern server memory systems comprising both DRAM and NVM exploiting state-of-the-art features in CPUs and PCIe. We prototype RAMBDA with a commercial system and evaluate three popular datacenter applications: (1) in-memory key-value store, (2) chain replication-based distributed transaction system, and (3) deep learning recommendation model inference. The evaluation shows that RAMBDA provides 30.1~69.1% lower latency, 0.2~2.5× throughput, and ~ 3× higher energy efficiency than the current state-of-the-art solutions, including Smart NIC. For those cases where Rambda performs poorly, we also envision future architecture to improve it.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071127","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071127","cache-coherent interconnects and accelerators;RDMA;heterogeneous and disaggregated memory;datacenters","Nonvolatile memory;Memory management;Random access memory;Prototypes;Throughput;Software;Hardware","","17","","180","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"FinePack: Transparently Improving the Efficiency of Fine-Grained Transfers in Multi-GPU Systems","H. Muthukrishnan; D. Lustig; O. Villa; T. Wenisch; D. Nellans",NVIDIA; NVIDIA; NVIDIA; University of Michigan; NVIDIA,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","516","529","Recent studies have shown that using fine-grained peer-to-peer (P2P) stores to communicate among devices in multi-GPU systems is a promising path to achieve strong performance scaling. In many irregular applications, such as graph algorithms and sparse linear algebra, small sub-cache line (4-32B) stores arise naturally when using the P2P paradigm. This is particularly problematic in multi-GPU systems because inter-GPU interconnects are optimized for bulk transfers rather than small operations. As a consequence, application developers either resort to complex programming techniques to work around this small transfer inefficiency or fall back to bulk inter-GPU DMA transfers that have limited performance scalability. We propose FinePack, a set of limited I/O interconnect and GPU hardware enhancements that enable small peer-to-peer stores to achieve interconnect efficiency that rivals bulk transfers while maintaining the simplicity of a peer-to-peer memory access programming model. Exploiting the GPU’s weak memory model, FinePack dynamically coalesces and compresses small writes into a larger I/O message that reduces link-level protocol overhead. FinePack is fully transparent to software and requires no changes to the GPU’s virtual memory system. We evaluate FinePack on a system comprising 4 Volta GPUs on a PCIe 4.0 interconnect to show FinePack improves interconnect efficiency for small peer-to-peer stores by 3×. This results in 4-GPU strong scaling performance 1.4× better than traditional DMA based multi-GPU programming and comes within 71% of the maximum achievable strong scaling performance.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070949","","Performance evaluation;Technological innovation;Protocols;Scalability;Memory management;Graphics processing units;Programming","","7","","56","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Mitigating GPU Core Partitioning Performance Effects","A. Barnes; F. Shen; T. G. Rogers","School of ECE, Purdue University, West Lafayette, IN; School of ECE, Purdue University, West Lafayette, IN; School of ECE, Purdue University, West Lafayette, IN",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","530","542","Modern GPU Streaming Multiprocessors (SMs) have several warp schedulers, execution units, and register file banks. To reduce area and energy-consumption, recent generations divide SMs into sub-cores. Each sub-core contains a distinct warp scheduler, register file, and execution units, sharing L1 memory and scratchpad resources with sub-cores in the same SM. Although partitioning the SM into sub-cores decreases the area and energy demands of larger SMs, it comes at a performance cost. Warps assigned to the SM have access to a fraction of the SM’s resources, resulting in contention and imbalance issues. In this paper, we examine the effect SM sub-division has on performance and propose novel mechanisms to mitigate the negative impacts. We identify four orthogonal effects caused by sub-dividing SMs and demonstrate that two of these effects have a significant impact on performance in practice. Based on these findings, we propose register-bank-aware warp scheduling to avoid bank conflicts that arise when instruction operands are placed in the limited number of register file banks available to each sub-core, and randomly hashed sub-core assignment to mitigate imbalance issues. Our intelligent scheduling mechanisms result in an average 11.2% speedup across a diverse set of applications capturing 81% of the performance lost to SM sub-division.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070957","GPU;Scheduling;Register File;Bank Conflict","Pathology;Scheduling algorithms;Databases;Instruction sets;Query processing;Graphics processing units;Computer architecture","","14","","60","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Plutus: Bandwidth-Efficient Memory Security for GPUs","R. Abdullah; H. Zhou; A. Awad","North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","543","555","Graphic-Processing Units (GPUs) are increasingly used in systems where security is a critical design requirement. Such systems include cloud computing, safety-critical systems, and edge devices, where sensitive data is processed or/and generated. Thus, the ability to reduce the attack surface while achieving high performance is of utmost importance. However, adding security features to GPUs comes at the expense of high-performance overheads due to the extra memory bandwidth required to handle security metadata. In particular, memory authentication metadata (e.g., authentication tags) along with encryption counters can lead to significant performance overheads due to the memory bandwidth used to fetch the metadata. Such metadata can lead to more than 200% extra bandwidth usage for irregular access patterns.In this work, we propose a novel design, Plutus, which enables low-overhead secure GPU memory. Plutus has three key ideas. The first is to leverage value locality to reduce authentication metadata. Our observation is that a large percentage of memory accesses could be verified without the need to bring the authentication tags. Specifically, through comparing decrypted blocks against known/verified values, we can with high confidence guarantee that no tampering occurred. Our analysis shows that the probability of the decryption of a tampered (and/or replayed) block leading to a known value is extremely low, in fact, lower than the collision probability in the most secure hash functions. Second, based on the observation that many GPU workloads have limited numbers of dirty block evictions, Plutus proposes a second layer of compact counters to reduce the memory traffic due to both the encryption counters and integrity tree. Third, by exploring the interesting tradeoff between the integrity tree organization vs. metadata fetch granularity, Plutus uses smaller block sizes for security metadata caches to optimize the number of security metadata memory requests. Based on our evaluation, Plutus can improve the GPU throughput by 16.86% (up to 58.38%) and reduce the memory bandwidth usage of secure memory by 48.14% (up to 80.30%).","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071100","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071100","","Trusted computing;Authentication;Graphics processing units;Bandwidth;Organizations;Metadata;Throughput","","7","","37","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism","Q. Zhou; H. Wang; X. Yu; C. Li; Y. Bai; F. Yan; Y. Xu",University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Houston; University of Science and Technology of China,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","556","569","It remains challenging to train billion-scale DNN models on a single modern multi-GPU server due to the GPU memory wall. Unfortunately, existing memory-saving techniques such as GPU-CPU swap, recomputation, and ZeRO-Series come at the price of extra computation, communication overhead, or limited memory reduction.We present MPress, a new single-server multi-GPU system that breaks the GPU memory wall of billion-scale model training while minimizing extra cost. MPress first discusses the trade-offs of various memory-saving techniques and offers a holistic solution, which alternatively chooses the inter-operator parallelism with low cross-GPU communication traffics, and combines with recomputation and swap, to balance training performance and sustained model sizes. Additionally, MPress employs a novel, fast D2D swap technique, which simultaneously utilizes multiple high-bandwidth NVLink to swap tensors to light-load GPUs, based on a key observation that inter-operator parallel training may result in imbalanced GPU memory utilization and spare memory space from least used devices plus the high-end interconnects among them have the opportunity to support low-overhead swapping. Finally, we integrate MPress with PipeDream and DAPPLE, two representative inter-operator parallel training systems. Experimental results with two popular DNN models, Bert, and GPT, on two modern GPU servers from the DGX-1 and DGX-2 generation, equipped with 8 V100 or A100 cards, respectively, demonstrate that MPress significantly improves the training throughput over ZeRO-Series with the identical memory reduction, while being able to train larger models than the recomputation baseline.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071077","National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071077","Inter-Operator Parallelism;DNN Training;Swap;Recomputation","Training;Performance evaluation;Tensors;Costs;Computational modeling;Graphics processing units;Parallel processing","","6","","64","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"DeFiNES: Enabling Fast Exploration of the Depth-first Scheduling Space for DNN Accelerators through Analytical Modeling","L. Mei; K. Goetschalckx; A. Symons; M. Verhelst","Department of Electrical Engineering, MICAS, KU Leuven, Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Leuven, Belgium",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","570","583","DNN workloads can be scheduled onto DNN accelerators in many different ways: from layer-by-layer scheduling to cross-layer depth-first scheduling (a.k.a. layer fusion, or cascaded execution). This results in a very broad scheduling space, with each schedule leading to varying hardware (HW) costs in terms of energy and latency. To rapidly explore this vast space for a wide variety of hardware architectures, analytical cost models are crucial to estimate scheduling effects on the HW level. However, state-of-the-art cost models are lacking support for exploring the complete depth-first scheduling space, for instance focusing only on activations while ignoring weights, or modeling only DRAM accesses while overlooking on-chip data movements. These limitations prevent researchers from systematically and accurately understanding the depth-first scheduling space.After formalizing this design space, this work proposes a unified modeling framework, DeFiNES, for layer-by-layer and depth-first scheduling to fill in the gaps. DeFiNES enables analytically estimating the hardware cost for possible schedules in terms of both energy and latency, while considering data access at every memory level. This is done for each schedule and HW architecture under study by optimally choosing the active part of the memory hierarchy per unique combination of operand, layer, and feature map tile. The hardware costs are estimated, taking into account both data computation and data copy phases. The analytical cost model is validated against measured data from a taped-out depth-first DNN accelerator, DepFiN, showing good modeling accuracy at the end-to-end neural network level. A comparison with generalized state-of-the-art demonstrates up to 10× better solutions found with DeFiNES.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071098","","Analytical models;Schedules;Costs;Computational modeling;Random access memory;Computer architecture;Hardware","","29","","39","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"CEGMA: Coordinated Elastic Graph Matching Acceleration for Graph Matching Networks","Y. Dai; Y. Zhang; X. Tang","Computer Science Department, University of Pittsburgh, Pittsburgh, PA; Computer Science Department, University of Pittsburgh, Pittsburgh, PA; Computer Science Department, University of Pittsburgh, Pittsburgh, PA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","584","597","The recently proposed Graph Matching Network models (GMNs) effectively improve the inference accuracy of graph similarity analysis tasks. GMNs often take graph pairs as input, embed nodes features, and match nodes between graphs for similarity analysis. While GMNs deliver high inference accuracy, the all-to-all node matching stage in GMNs introduces quadratic computing complexity with excessive memory accesses, resulting in significant computing and memory overhead that cannot be handled by existing approaches. In this paper, we propose the Coordinated Elastic Graph Matching Accelerator (CEGMA), a software and hardware co-design accelerator to address the challenges of GMNs. Specifically, by exploiting duplicate subgraphs in the input graphs, we develop an elastic matching filter to significantly reduce the quadratic computing overhead. By exploring the substantial data reuses oriented from accessing node features, we propose a cross-graph coordinator that fuses cross-graph similarity computing with intra-graph computing to enhance data locality. Experimental results show that, on average, CEGMA achieves 353× and 6.5× speedups in GMN computing compared to state-of-the-art GPU implementation and GNN accelerators, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070956","Graph Matching Networks;Graph Neural Networks;Accelerator","Matched filters;Fuses;Computational modeling;Neural networks;Memory management;Graphics processing units;Software","","5","","43","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ISOSceles: Accelerating Sparse CNNs through Inter-Layer Pipelining","Y. Yang; J. S. Emer; D. Sanchez",MIT CSAIL; MIT CSAIL / NVIDIA; MIT CSAIL,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","598","610","Sparse CNNs dramatically reduce computation and storage costs over dense ones. But sparsity also makes CNNs more data-intensive, as each value is reused fewer times. Thus, current sparse CNN accelerators, which process one layer at a time, are bottlenecked by memory traffic.We present ISOSceles, a new sparse CNN accelerator that dramatically reduces data movement through inter-layer pipelining: overlapping the execution of consecutive layers so that a layer’s output activations are quickly consumed by the next layer without spilling them off-chip. Pipelining greatly increases reuse, but it is challenging to implement with existing approaches, which are limited to dense CNNs. ISOSceles relies on a novel input-stationary output-stationary (IS-OS) dataflow that consumes inputs and produces outputs in the same order, greatly reducing intermediate sizes over existing dataflows. ISOSceles implements IS-OS efficiently and leverages time-multiplexing and dynamic scheduling to pipeline multiple layers despite the large variations in work that sparsity induces.On a wide range of sparse CNNs, ISOSceles outperforms a state-of-the-art accelerator by gmean 4.3× (up to 6.7×), and reduces traffic by 4.7× (up to 8.5×) while using less area.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071080","Semiconductor Research Corporation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071080","","Costs;Computer architecture;Dynamic scheduling;Pipeline processing","","18","","45","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"OptimStore: In-Storage Optimization of Large Scale DNNs with On-Die Processing","J. Kim; M. Kang; Y. Han; Y. -G. Kim; L. -S. Kim","SAIT, Samsung Electronics; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","611","623","Training deep neural network (DNN) models is a resource-intensive, iterative process. For this reason, nowadays, complex optimizers like Adam are widely adopted as it increases the speed and efficiency of training. These optimizers, however, employ additional variables and raise the memory demand 2× to 3× of model parameters, worsening the memory capacity bottleneck. Moreover, as the size of DNN models is projected to grow even further, it is not practical to assume that the future models will fit in accelerator memory. This has triggered various efforts to offload models to flash-based storage. However, when the model, especially the optimizer, is offloaded to flash, the limited I/O bandwidth severely slows down the overall training process. To this end, we present OptimStore, a solid-state drive (SSD) system with on-die processing (ODP) architectures for gradient descent-based machine learning models. OptimStore accelerates the training process of such large-scale models by processing model optimization in the storage device, specifically inside the flash dies. ODP capability of OptimStore eliminates the heavy data movement over external interconnect and internal flash channels. Overall, OptimStore achieves, on average, a 2.8× speedup and a 3.6× improved energy efficiency in the weight update stage over baseline SSD offloading.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071024","K2; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071024","","Training;Solid modeling;Protocols;Solid state drives;Scalability;Computer architecture;Parallel processing","","9","","65","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"KRISP: Enabling Kernel-wise RIght-sizing for Spatial Partitioned GPU Inference Servers","M. Chow; A. Jahanshahi; D. Wong","Department of Computer Science & Engineering, University of California, Riverside; Department of Computer Science & Engineering, University of California, Riverside; Department of Computer Science & Engineering, University of California, Riverside",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","624","637","Machine learning (ML) inference workloads present significantly different challenges than ML training workloads. Typically, inference workloads are shorter running and under-utilize GPU resources. To overcome this, co-locating multiple instances of a model has been proposed to improve the utilization of GPUs. Co-located models share the GPU through GPU spatial partitioning facilities, such as Nvidia’s MPS, MIG, or AMD’s CU Masking API. Existing spatially partitioned inference servers create model-wise partitions by ""right-sizing"" based on a model’s latency tolerance to restricting resources. We show that model-wise right-sizing is under-utilized due to varying resource restriction tolerance of individual kernels within an inference pass.We propose Kernel-wise Right-sizing for Spatial Partitioned GPU Inference Servers (KRISP) to enable kernel-wise right-sizing of spatial partitions at the granularity of individual kernels. We demonstrate that KRISP can support a greater level of concurrently running inference models compared to existing spatially partitioned inference servers. KRISP improves overall throughput by 2x when compared with an isolated inference (1.22x vs prior works) and reduce energy per inference by 33%.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071121","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071121","GPU Inference Server;Compute Unit Masking;GPU Spatial Partitioning","Training;Graphics processing units;Machine learning;Computer architecture;Throughput;Servers;Kernel","","9","","69","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"MERCURY: Accelerating DNN Training By Exploiting Input Similarity","V. Janfaza; K. Weston; M. Razavi; S. Mandal; F. Mahmud; A. Hilty; A. Muzahid","Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computer Science and Engineering, Texas A&M University, College Station, TX, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","638","650","Deep Neural Networks (DNN) are computationally intensive to train. It consists of a large number of multidimensional dot products between many weights and input vectors. However, there can be significant similarities among input vectors. If one input vector is similar to another, its computations with the weights are similar to those of the other and, therefore, can be skipped by reusing the already-computed results. We propose a novel scheme, called MERCURY, to exploit input similarity during DNN training in a hardware accelerator. MERCURY uses Random Projection with Quantization (RPQ) to convert an input vector to a bit sequence, called Signature. A cache (MCACHE) stores signatures of recent input vectors along with the computed results. If the Signature of a new input vector matches that of an already existing vector in the MCACHE, the two vectors are found to have similarities. Therefore, the already-computed result is reused for the new vector. To the best of our knowledge, MERCURY is the first work that exploits input similarity using RPQ for accelerating DNN training in hardware. The paper presents a detailed design, workflow, and implementation of the MERCURY. Our experimental evaluation with twelve different deep learning models shows that MERCURY saves a significant number of computations and speeds up the model training by an average of 1.97× with an accuracy similar to the baseline system.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071051","","Training;Deep learning;Quantization (signal);Computational modeling;Impedance matching;Neural networks;Computer architecture","","3","","66","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Silo: Speculative Hardware Logging for Atomic Durability in Persistent Memory","M. Zhang; Y. Hua","Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology; Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","651","663","Persistent memory (PM) provides data persistency. Due to this property, guaranteeing atomic durability becomes important for applications running on PM in order to ensure the crash consistency for a group of updates. To this end, hardware logging has received many attentions by overlapping the log operations and transaction execution. Unfortunately, existing approaches regard logs as backups, which inevitably increases the log writes to PM, thus exacerbating the limited endurance of PM and imposing constraints on the write ordering.This paper proposes Silo, a speculative hardware logging design to ensure atomic durability with ultra-low overheads. Unlike existing studies, Silo exploits a speculative methodology and regards logs as data to make the common case fast. In practice, system crashes or power failures rarely occur for a machine. Hence, we do not need to write logs to back up data in most of the running time. Based on this observation, Silo temporarily maintains the undo+redo logs on chip during transaction execution. After the transaction commits, Silo leverages the new data in these on-chip logs to in-place update the PM data region, instead of conservatively writing logs as useless backups in common cases where no crash occurs. In this way, Silo significantly reduces the write overheads. If a crash occurs, Silo still efficiently flushes these on-chip logs to PM for recovery without any loss of correctness. Experimental results demonstrate that Silo significantly improves the transaction throughput by 4.3×, and reduces the memory writes by 76.5% compared with state-of-the-art designs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071034","National Natural Science Foundation of China; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071034","","Computer architecture;Writing;Throughput;Computer crashes;Hardware;System-on-chip","","3","","64","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Reconciling Selective Logging and Hardware Persistent Memory Transaction","C. Ye; Y. Xu; X. Shen; Y. Sha; X. Liao; H. Jin; Y. Solihin","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; North Carolina State University, Raleigh, North Carolina, USA; North Carolina State University, Raleigh, North Carolina, USA; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; University of Central Florida, Florida, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","664","676","Log creation, maintenance, and its persist ordering are known to be performance bottlenecks for durable transactions on persistent memory. Existing hardware persistent memory transactions overlook an important opportunity for improving performance: some persistent data is algorithmically redundant such that it can be recovered from other data, removing the need for logging such data. The paper presents an ISA extension that enables selective logging for hardware persistent memory transactions for the first time. The ISA extension features two novel components: fine-grain logging and lazy persistency. Fine-grain logging allows hardware to log updates on data in the granularity of words without lengthening the critical path of data accesses. Lazy persistency allows updated data to remain in the cache after the transaction commits. Together, the new hardware persistent memory transaction outperforms the state-of-the-art hardware counterpart by 1.8× on average.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071088","National Key Research and Development Program of China; National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071088","","Computer architecture;Maintenance engineering;Hardware;Optimization;IEEE transactions","","3","","58","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"SecPB: Architectures for Secure Non-Volatile Memory with Battery-Backed Persist Buffers","A. Freij; H. Zhou; Y. Solihin","North Carolina State University, North Carolina, USA; North Carolina State University, North Carolina, USA; University of Central Florida, Florida, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","677","690","The durability of data stored in persistent memory (PM) exposes data to potentially data leakage attacks. Recent research has identified the requirements for crash recoverable secure PM, but do not consider recent trends of the persistency domain extending on-chip to include cache hierarchies. In this paper, we explore this design space and identify performance and energy optimization opportunities.We propose secure persistent buffers (SecPB), a battery-backed persistent structure that moves the point of secure data persistency from the memory controller closer to the core. We revisit the fundamentals of how data in PM is secured and show how various subsets of security metadata can be generated lazily while still guaranteeing crash recoverability and integrity verification. We analyze the metadata dependency chain required in securing PM and expose optimization opportunities that allow for SecPB to reduce performance overheads by up to 32.8×, with average performance overheads as low as 1.3% observed for reasonable battery capacities.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071082","","Nonvolatile memory;Computer architecture;Metadata;Market research;Computer crashes;System-on-chip;Space exploration","","4","","76","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"EVE: Ephemeral Vector Engines","K. Al-Hawaj; T. Ta; N. Cebry; S. Agwa; O. Afuye; E. Hall; C. Golden; A. B. Apsel; C. Batten","School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","691","704","There has been a resurgence of interest in vector architectures evident by recent adoption of vector extensions in mainstream instruction set architectures. Traditionally, vector engines leverage this abstraction by exploiting its inherent regularity to increase performance and efficiency. Recent work on SRAM-based compute-in-memory has shown promise in reducing the area overhead of these engines. In this work, we propose ephemeral vector engines (EVE) where we leverage SRAM-based compute-in-memory techniquesas well as bit-peripheral computations to facilitate efficient vector execution. EVE uses a novel approach of bit-hybrid execution, striking a balance between throughput and latency. Evaluated on the Rodinia and RiVEC benchmark suites, EVE achieves almost 8× speed-up compared to an out-of-order processor and 4.59× compared to an integrated vector unit. EVE achieves speed-ups comparable to an aggressive decoupled vector unit and increases the area-normalized performance by over 2 ×. By repurposing SRAM arrays in the L2 cache to create ephemeral vector execution units, EVE is able to efficiently achieve high performance while incurring as little as 11.7% area overhead.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071074","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071074","","Out of order;Support vector machines;Processor scheduling;Memory management;Random access memory;Programming;Throughput","","5","","64","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"On Consistency for Bulk-Bitwise Processing-in-Memory","B. Perach; R. Ronen; S. Kvatinsky","The Andrew and Erna Viterbi Faculty of Electrical & Computer Engineering, Technion – Israel Institute of Technology, Haifa, Israel; The Andrew and Erna Viterbi Faculty of Electrical & Computer Engineering, Technion – Israel Institute of Technology, Haifa, Israel; The Andrew and Erna Viterbi Faculty of Electrical & Computer Engineering, Technion – Israel Institute of Technology, Haifa, Israel",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","705","717","Processing-in-memory (PIM) architectures allow software to explicitly initiate computation in the memory. This effectively makes PIM operations a new class of memory operations, alongside standard memory operations (e.g., load, store). For software correctness, it is crucial to have ordering rules for a PIM operation with other PIM operations and other memory operations, i.e., a consistency model that takes into account PIM operations is vital. To the best of our knowledge, little attention to PIM operation consistency has been given in existing works. In this paper, we focus on a specific PIM approach, named bulk-bitwise PIM. In bulk-bitwise PIM, large bitwise operations are performed directly and stored in the memory array. We show that previous solutions for the related topic of maintaining coherency of bulk-bitwise PIM have broken the host native consistency model and prevent any guaranteed correctness. As a solution, we propose and evaluate four consistency models for bulk-bitwise PIM, from strict to relaxed. Our designs also preserve coherency between PIM and the host processor. Evaluating the proposed designs’ performance with a gem5 simulation, using the YCSB short-range scan benchmark and TPC-H queries, shows that the run time overhead of guaranteeing correctness is at most 6%, and in many cases the run time is even improved. The hardware overhead of our design is less than 0.22%.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071007","European Research Council; Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071007","","Databases;Computational modeling;Computer architecture;Benchmark testing;Software;Hardware;Standards","","2","","33","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications","M. Orenes-Vera; E. Tureci; D. Wentzlaff; M. Martonosi","Princeton University Princeton, New Jersey, USA; Princeton University Princeton, New Jersey, USA; Princeton University Princeton, New Jersey, USA; Princeton University Princeton, New Jersey, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","718","730","Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data-placement strategy that improves work balance.This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071089","Air Force Research Laboratory; Advanced Research Projects Agency; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071089","Distributed;scalable;parallel;data-local;near-memory;architecture;sparse;graph;NoC;network;bandwidth","Technological innovation;Software algorithms;Distributed databases;Linear algebra;Bandwidth;Energy efficiency;Software","","14","","76","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"HyQSAT: A Hybrid Approach for 3-SAT Problems by Integrating Quantum Annealer with CDCL","S. Tan; M. Yu; A. Python; Y. Shang; T. Li; L. Lu; J. Yin","Zhejiang University, P.R. China; Zhejiang University, P.R. China; Zhejiang University, P.R. China; Zhejiang University, P.R. China; Zhejiang University, P.R. China; Zhejiang University, P.R. China; Zhejiang University, P.R. China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","731","744","Propositional satisfiability problem (SAT) is represented in a conjunctive normal form with multiple clauses, which is an important non-deterministic polynomial-time (NP) complete problem that plays a major role in various applications including artificial intelligence, graph colouring, and circuit analysis. Quantum annealing (QA) is a promising methodology for solving complex SAT problems by exploiting the parallelism of quantum entanglement, where the SAT variables are embedded to the qubits. However, the long embedding time fundamentally limits existing QA-based methods, leading to inefficient hardware implementation and poor scalability.In this paper, we propose HyQSAT, a hybrid approach that integrates QA with the classical Conflict-Driven Clause Learning (CDCL) algorithm to enable end-to-end acceleration for solving SAT problems. Instead of embedding all clauses to QA hardware, we quantitatively estimate the conflict frequency of clauses and apply breadth-first traversal to choose their embedding order. We also consider the hardware topology to maximize the utilization of physical qubits in embedding to QA hardware. Besides, we adjust the embedding coefficients to improve the computation accuracy under qubit noise. Finally, we present how to interpret the satisfaction probability based on QA energy distribution and use this information to guide the CDCL search. Our experiments demonstrate that HyQSAT can effectively support larger-scale SAT problems that are beyond the capability of existing QA approaches, achieve up to 12.62X end-to-end speedup using D-Wave 2000Q compared to the classic CDCL algorithm on Intel E5 CPU, and considerably reduce the QA embedding time from 17.2s to 15.7µs compared to the D-Wave Minorminer algorithm [11].","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071022","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071022","","Annealing;Quantum entanglement;Qubit;Computer architecture;Parallel processing;Quantum annealing;Hardware","","8","","81","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Duet: Creating Harmony between Processors and Embedded FPGAs","A. Li; A. Ning; D. Wentzlaff","Princeton University, Princeton, NJ, USA; Princeton University, Princeton, NJ, USA; Princeton University, Princeton, NJ, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","745","758","The demise of Moore’s Law has led to the rise of hardware acceleration. However, the focus on accelerating stable algorithms in their entirety neglects the abundant fine-grained acceleration opportunities available in broader domains and squanders host processors’ compute power.This paper presents Duet, a scalable, manycore-FPGA architecture that promotes embedded FPGAs (eFPGA) to be equal peers with processors through non-intrusive, bi-directionally cache-coherent integration. In contrast to existing CPU-FPGA hybrid systems in which the processors play a supportive role, Duet unleashes the full potential of both the processors and the eFPGAs with two classes of post-fabrication enhancements: fine-grained acceleration, which partitions an application into small tasks and offloads the frequently-invoked, compute-intensive ones onto various small accelerators, leveraging the processors to handle dynamic control flow and less accelerable tasks; hardware augmentation, which employs eFPGA-emulated hardware widgets to improve processor efficiency or mitigate software overheads in certain execution models.An RTL-level implementation of Duet is developed to evaluate the architecture with high fidelity. Experiments using synthetic benchmarks show that Duet can reduce the processor-accelerator communication latency by up to 82% and increase the bandwidth by up to 9.5x. The RTL implementation is further evaluated with seven application benchmarks, achieving 1.5-24.9x speedup.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070989","Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070989","","Program processors;Computer architecture;Bandwidth;Benchmark testing;Software;Registers;Task analysis","","5","","60","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Co-Designed Architectures for Modular Superconducting Quantum Computers","E. McKinney; M. Xia; C. Zhou; P. Lu; M. Hatridge; A. K. Jones","Department of Electrical and Computer Engineering, University of Pittsburgh; Department of Physics and Astronomy, University of Pittsburgh; Department of Physics and Astronomy, University of Pittsburgh; Department of Physics and Astronomy, University of Pittsburgh; Department of Physics and Astronomy, University of Pittsburgh; Department of Electrical and Computer Engineering, University of Pittsburgh",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","759","772","Noisy, Intermediate Scale Quantum (NISQ) computers have reached the point where they can show the potential for quantum advantage over classical computing. Unfortunately, NISQ machines introduce sufficient noise that even for moderate size quantum circuits the results can be unreliable. We propose a collaboratively designed superconducting quantum computer using a Superconducting Nonlinear Asymmetric Inductive eLement (SNAIL) modulator. The SNAIL modulator is designed by considering both the ideal fundamental qubit gate operation while maximizing the qubit coupling capabilities. First, the SNAIL natively implements $\sqrt[n]{{{\text{iSWAP}}}}$ gates realized through proportionally scaled pulse lengths. This naturally includes $\sqrt {{\text{iSWAP}}} $, which provides an advantage over CNOT as a basis gate. Second, the SNAIL enables high-degree couplings that allow rich and highly parallel qubit connection topologies without suffering from frequency crowding. Building on our previously demonstrated SNAIL-based quantum state router we propose a quantum 4-ary tree and a hypercube inspired corral built from interconnected quantum modules. We compare their advantage in data movement based on necessary SWAP gates to the traditional lattice and heavy-hex lattice used in latest commercial quantum computers. We demonstrate the co-design advantage of our SNAIL-based machine with $\sqrt {{\text{iSWAP}}} $ basis gates and rich topologies against CNOT/heavy-hex and FSIM/lattice for 16-20 qubit and extrapolated designs circa 80 qubit architectures. We compare total circuit time and total gate count to understand fidelity for systems dominated by decoherence and control imperfections, respectively. Finally, we provide a gate duration sensitivity study on further decreasing the SNAIL pulse length to realize $\sqrt[n]{{{\text{iSWAP}}}}$ qubit systems to reduce decoherence times.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071036","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071036","","Computers;Couplings;Qubit;Modulation;Lattices;Computer architecture;Logic gates","","5","","50","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"A Pulse Generation Framework with Augmented Program-aware Basis Gates and Criticality Analysis","Y. Chen; Y. Jin; F. Hua; A. Hayes; A. Li; Y. Shi; E. Z. Zhang",Rutgers University; Rutgers University; Rutgers University; Rutgers University; Pacific Northwest National Laboratory; Amazon Braket; Rutgers University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","773","786","Near-term intermediate-scale quantum (NISQ) devices are subject to considerable noise and short coherence time. Consequently, it is critical to minimize circuit execution latency and improve fidelity. Traditionally, each basis gate of a transpiled circuit is decoded into a fixed episode of the device control pulses. Recent studies investigate the merged pulse generation method for customized gates through quantum optimal control (QOC). In this work, we propose PAQOC, a novel QOC framework that can (i) exploit an augmented program-aware (APA) basis gate set for the tradeoff between compilation time and circuit performance, (ii) prune the search space based on a criticality-centric analytical model and experiment observations we learned from 150 benchmarks. Evaluations using seventeen applications show that PAQOC can achieve an average 54% reduction of the circuit latency, on average 43% reduction in compilation overhead, and a 1.27× improvement in fidelity. PAQOC is available on GitHub1.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070990","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070990","","Circuit optimization;Program processors;Databases;Optimal control;Transforms;Computer architecture;Logic gates","","7","","55","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"The Imitation Game: Leveraging CopyCats for Robust Native Gate Selection in NISQ Programs","P. Das; E. Kessler; Y. Shi","Georgia Institute of Technology, Atlanta, USA; Amazon Braket (AWS), New York, USA; Amazon Braket (AWS), New York, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","787","801","Quantum programs are written in high-level languages, whereas quantum hardware can only execute low-level native gates. To run programs on quantum systems, each high- level instruction must be decomposed into native gates. This process is called gate nativization and is performed by the compiler. Recent quantum computers support a richer native gate set to reduce crosstalk by tackling frequency crowding and enable compilers to generate quantum executables with fewer native gates. On these systems, any two-qubit CNOT instruction can be decomposed using more than a single two-qubit native gate. For example, a CNOT can be decomposed using either XY, CPHASE, or CZ native gates on Rigetti machines. Unfortunately, two-qubit native gates have high-error rates and exhibit temporal and spatial variations, which limits the success-rate of quantum programs. Therefore, identifying the native gate that maximizes the success-rate of each CNOT operation in a program is crucial.Our experiments on Rigetti machines show that noise-adaptive gate nativization to select the native gate with the highest fidelity for each CNOT operation is often sub-optimal at the application level. This is because the performance of such nativization heavily depends on the correctness of the device calibration data which only provides the average gate fidelities and may not accurately capture the error trends specific to the qubit state space of a program. Moreover, the calibration data may go stale due to device drifts going undetected. To overcome these limitations, we propose Application-specific Native Gate Selection (ANGEL). ANGEL designs a CopyCat that imitates a given program but has a known solution. Then, ANGEL employs the CopyCat to test different combinations of native gates and learn the optimal combination, which is then used to nativize the given program. To avoid an exponential search, ANGEL uses a divide-and-conquer- based localized search, the complexity of which scales linear with the number of device links used by the program. Our evaluations on Rigetti Aspen-11 show that ANGEL improves the success-rate of programs by 1.40x on average and by up-to 2x.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071025","Quantum;Gate nativization;NISQ Compilation","Computers;Performance evaluation;Quantum system;Qubit;Logic gates;Market research;Hardware","","9","","132","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"eNODE: Energy-Efficient and Low-Latency Edge Inference and Training of Neural ODEs","J. Zhu; Y. Tao; Z. Zhang","University of Michigan, Ann Arbor, Ann Arbor, Michigan, USA; University of Michigan, Ann Arbor, Ann Arbor, Michigan, USA; University of Michigan, Ann Arbor, Ann Arbor, Michigan, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","802","813","Neural ordinary differential equations (NODEs) provide better modeling performance with smaller amount of model parameters in many tasks by embedding neural networks (NNs) in ordinary differential equations (ODEs). They have been shown to outperform in representing continuous-time data and learning dynamic systems, and are promising for on-device inference and training. However, an edge device is limited by area and energy budget, and real-time operations have a tight latency requirement. State-of-the-art NN accelerators are not optimized for the area- and power-hungry memory storage and access for NODE inference and training, and lack the flexibility to incorporate dynamic latency reduction techniques. We present eNODE by architecture-algorithm co-design to achieve efficient and fast inference and training of NODEs. eNODE adopts compact-size depth-first integration and depth-first training for higher energy efficiency. Through function reuse, packetized processing and a unified NN core design, the efficiency of eNODE’s depth-first processing is further enhanced. We propose algorithm innovations, including slope-adaptive stepsize search and priority processing with early stop, to substantially shorten the latency. A hardware prototype is synthesized in a 28 nm CMOS technology for evaluation and benchmarking. eNODE demonstrates up to 6.59× better energy efficiency, 2.38× higher speed, and better area scalability over a SIMD ASIC baseline.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070935","","Training;Memory management;Prototypes;Artificial neural networks;Ordinary differential equations;Energy efficiency;Real-time systems","","2","","34","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"SpecFaaS: Accelerating Serverless Applications with Speculative Function Execution","J. Stojkovic; T. Xu; H. Franke; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","814","827","Serverless computing has emerged as a popular cloud computing paradigm. Serverless environments are convenient to users and efficient for cloud providers. However, they can induce substantial application execution overheads, especially in applications with many functions.In this paper, we propose to accelerate serverless applications with a novel approach based on software-supported speculative execution of functions. Our proposal is termed Speculative Function-as-a-Service (SpecFaaS). It is inspired by out-of-order execution in modern processors, and is grounded in a characterization analysis of FaaS applications. In SpecFaaS, functions in an application are executed early, speculatively, before their control and data dependences are resolved. Control dependences are predicted like in pipeline branch prediction, and data dependences are speculatively satisfied with memoization. With this support, the execution of downstream functions is overlapped with that of upstream functions, substantially reducing the end-to-end execution time of applications. We prototype SpecFaaS on Apache OpenWhisk, an open-source serverless computing platform. For a set of applications in a warmed-up environment, SpecFaaS attains an average speedup of 4.6×. Further, on average, the application throughput increases by 3.9× and the tail latency decreases by 58.7%.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071120","Cloud computing;Serverless computing;Function-as-a-Service","Out of order;Program processors;Pipelines;Serverless computing;Prototypes;Tail;Computer architecture","","10","","87","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"MoCA: Memory-Centric, Adaptive Execution for Multi-Tenant Deep Neural Networks","S. Kim; H. Genc; V. V. Nikiforov; K. Asanović; B. Nikolić; Y. S. Shao","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","828","841","Driven by the wide adoption of deep neural networks (DNNs) across different application domains, multi-tenancy execution, where multiple DNNs are deployed simultaneously on the same hardware, has been proposed to satisfy the latency requirements of different applications while improving the overall system utilization. However, multi-tenancy execution could lead to undesired system-level resource contention, causing quality-of-service (QoS) degradation for latency-critical applications.To address this challenge, we propose MoCA1, an adaptive multi-tenancy system for DNN accelerators. Unlike existing solutions that focus on compute resource partition, MoCA dynamically manages shared memory resources of co-located applications to meet their QoS targets. Specifically, MoCA leverages the regularities in both DNN operators and accelerators to dynamically modulate memory access rates based on their latency targets and user-defined priorities so that co-located applications get the resources they demand without significantly starving their co-runners. We demonstrate that MoCA improves the satisfaction rate of the service level agreement (SLA) up to 3.9× (1.8× average), system throughput by 2.3× (1.7× average), and fairness by 1.3× (1.2× average), compared to prior work.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071035","","Deep learning;Degradation;Adaptive systems;Neural networks;Accelerator architectures;Quality of service;Throughput","","20","","53","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Know Your Enemy To Save Cloud Energy: Energy-Performance Characterization of Machine Learning Serving","J. Yu; J. Kim; E. Seo","Dept. of Computer Science and Engineering, Sungkyunkwan University; Dept. of Computer Science and Engineering, Sungkyunkwan University; Dept. of Computer Science and Engineering, Sungkyunkwan University",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","842","854","The proportion of machine learning (ML) inference in modern cloud workloads is rapidly increasing, and graphic processing units (GPUs) are the most preferred computational accelerators for it. The massively parallel computing capability of GPUs is well-suited to the inference workloads but consumes more power than conventional CPUs. Therefore, GPU servers contribute significantly to the total power consumption of a data center. However, despite their heavy power consumption, GPU power management in cloud-scale has not yet been actively researched. In this paper, we reveal three findings about energy efficiency of ML inference clusters in the cloud. ❶ GPUs of different architectures have comparative advantages in energy efficiency to each other for a set of ML models. ❷ The energy efficiency of a GPU set may significantly vary depending on the number of active GPUs and their clock frequencies even when producing the same level of throughput. ❸ The service level objective(SLO)-blind dynamic voltage and frequency scaling (DVFS) driver of commercial GPUs maintain an immoderately high clock frequency. Based on these implications, we propose a hierarchical GPU resource management approach for cloud-scale inference services. The proposed approach consists of energy-aware cluster allocation, intra-cluster node scaling, intra-node GPU scaling and GPU clock scaling schemes considering the inference service architecture hierarchy. We evaluated our approach with its prototype implementation and cloud-scale simulation. The evaluation with real-world traces showed that the proposed schemes can save up to 28.3% of the cloud-scale energy consumption when serving five ML models with 105 servers having three different kinds of GPUs.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070943","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070943","","Energy consumption;Power demand;Graphics processing units;Prototypes;Computer architecture;Machine learning;Energy efficiency","","5","","58","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Adrias: Interference-Aware Memory Orchestration for Disaggregated Cloud Infrastructures","D. Masouros; C. Pinto; M. Gazzetti; S. Xydis; D. Soudris","Microprocessors and Digital Systems Laboratory, School of Electrical and Computer Engineering National Technical University of Athens, Greece; IBM Research Europe, Dublin, Ireland; IBM Research Europe, Dublin, Ireland; Microprocessors and Digital Systems Laboratory, School of Electrical and Computer Engineering National Technical University of Athens, Greece; Microprocessors and Digital Systems Laboratory, School of Electrical and Computer Engineering National Technical University of Athens, Greece",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","855","869","Workload co-location has become the de-facto approach for hosting applications in Cloud environments, leading, however, to interference and fragmentation in shared resources of the system. To this end, hardware disaggregation is introduced as a novel paradigm, that allows fine-grained tailoring of cloud resources to the characteristics of the deployed applications. Towards the realization of hardware disaggregated clouds, novel orchestration frameworks must provide additional knobs to manage the increased scheduling complexity.We present Adrias, a memory orchestration framework for disaggregated cloud systems. Adrias exploits information from low-level performance events and applies deep learning techniques to effectively predict the system state and performance of arriving workloads on memory disaggregated systems, thus, driving cognitive scheduling between local and remote memory allocation modes. We evaluate Adrias on a state-of-art disaggregated testbed and show that it achieves 0.99 and 0.942 R2 score for system state and application’s performance prediction on average respectively. Moreover, Adrias manages to effectively utilize disaggregated memory, by offloading almost 1/3 of deployed applications with less than 15% performance overhead compared to a conventional local memory scheduling, while clearly outperforms naive scheduling approaches (random and round-robin), by providing up to ×2 better performance.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070939","Memory disaggregation;Data placement;Deep Learning;Cloud Infrastructures;Orchestration;Interference","Deep learning;Memory management;Interference;Hardware;Resource management","","3","","97","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Poseidon: Practical Homomorphic Encryption Accelerator","Y. Yang; H. Zhang; S. Fan; H. Lu; M. Zhang; X. Li","State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","870","881","With the development of the important solution for privacy computing, the explosion of data size and computing intensity in Fully Homomorphic Encryption (FHE) has brought enormous challenges to the hardware design. In this paper, we propose a practical FHE accelerator - ""Poseidon"", which focuses on improving the hardware resource and bandwidth consumption. Poseidon supports complex FHE operations like Bootstrapping, Keyswitch, Rotation and so on, under limited FPGA resources. It refines these operations by abstracting five key operators: Modular Addition (MA), Modular Multiplication (MM), Number Theoretic Transformation (NTT), Automorphsim and Shared Barret Reduction (SBT). These operators are combined and reused to implement higher-level FHE operations. To utilize the FPGA resources more efficiently and improve the parallelism, we adopt the radix-based NTT algorithm and propose HFAuto, an optimized automorphism implementation suitable for FPGA. Then, we design the hardware accelerator based on the optimized key operators and HBM to maximize computational efficiency. We evaluate Poseidon with four domain-specific FHE benchmarks on Xilinx Alveo U280 FPGA. Empirical results show that the efficient reuse of the operator cores and on-chip storage enables superior performance compared with the state-of-the-art GPU, FPGA and accelerator ASICs. We highlight the following results: (1) up to 370× speedup over CPU for the basic operations of FHE; (2) up to 1300×/52× speedup over CPU and the FPGA solution for the key operators; (3) up to 10.6×/8.7× speedup over GPU and the ASIC solution for the FHE benchmark.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070984","National Natural Science Foundation of China; Chinese Academy of Sciences; Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070984","","Data privacy;Graphics processing units;Bandwidth;Benchmark testing;Parallel processing;Explosions;System-on-chip","","52","","42","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"FAB: An FPGA-based Accelerator for Bootstrappable Fully Homomorphic Encryption","R. Agrawal; L. de Castro; G. Yang; C. Juvekar; R. Yazicigil; A. Chandrakasan; V. Vaikuntanathan; A. Joshi","Boston University, Boston, MA; MIT, Cambridge, MA, USA; Boston University, Boston, MA; Analog Devices, Boston, MA, USA; Boston University, Boston, MA; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; Boston University, Boston, MA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","882","895","Fully Homomorphic Encryption (FHE) offers protection to private data on third-party cloud servers by allowing computations on the data in encrypted form. To support general-purpose encrypted computations, all existing FHE schemes require an expensive operation known as ""bootstrapping"". Unfortunately, the computation cost and the memory bandwidth required for bootstrapping add significant overhead to FHE-based computations, limiting the practical use of FHE.In this work, we propose FAB, an FPGA-based accelerator for bootstrappable FHE. Prior FPGA-based FHE accelerators have proposed hardware acceleration of basic FHE primitives for impractical parameter sets without support for bootstrapping. FAB, for the first time ever, accelerates bootstrapping (along with basic FHE primitives) on an FPGA for a secure and practical parameter set. The key contribution of this work is the architecture of a balanced FAB design, which is not memory bound. In our design, we leverage recent algorithms for bootstrapping while being cognizant of the compute and memory constraints of our FPGA. In addition, we use a minimal number of functional units for computing, operate at a low frequency, leverage high data rates to and from main memory, utilize the limited on-chip memory effectively, and perform careful operation scheduling.We evaluate FAB using a single Xilinx Alveo U280 FPGA and by scaling it to a multi-FPGA system consisting of eight such FPGAs. For bootstrapping a fully-packed ciphertext, while operating at 300MHz, FAB outperforms existing state-of-the-art CPU and GPU implementations by 213× and 1.5× respectively. Our target FHE application is training a logistic regression model over encrypted data. For logistic regression model training scaled to 8 FPGAs on the cloud, FAB outperforms a CPU and GPU by 456× and 9.5× respectively, providing practical performance at a fraction of the ASIC design cost.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070953","","Training;Costs;Memory management;Graphics processing units;Bandwidth;System-on-chip;Cryptography","","83","","56","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"FxHENN: FPGA-based acceleration framework for homomorphic encrypted CNN inference","Y. Zhu; X. Wang; L. Ju; S. Guo","Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University; Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University; School of Cyber Science and Technology, Shandong University, Qingdao, China; Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","896","907","Fully homomorphic encryption (FHE) is a promising data privacy solution for machine learning, which allows the inference to be performed with encrypted data. However, it typically leads to 5-6 orders of magnitude higher computation and storage overhead. This paper proposes the first full-fledged FPGA acceleration framework for FHE-based convolution neural network (HE-CNN) inference. We then design parameterized HE operation modules with intra- and inter- HE-CNN layer resource management based on FPGA high-level synthesis (HLS) design flow. With sophisticated resource and performance modeling of the HE operation modules, the proposed FxHENN framework automatically performs design space exploration to determine the optimized resource provisioning and generates the accelerator circuit for a given HE-CNN model on a target FPGA device. Compared with the state-of-the-art CPU-based HE-CNN inference solution, FxHENN achieves up to 13.49X speedup of inference latency, and 1187.12X energy efficiency. Meanwhile, given this is the first attempt in the literature on FPGA acceleration of fullfledged non-interactive HE-CNN inference, our results obtained on low-power FPGA devices demonstrate HE-CNN inference for edge and embedded computing is practical.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071133","fully homomorphic encryption;convolution neural network;FPGA acceleration;high-level synthesis;design space exploration","Performance evaluation;Data privacy;Computational modeling;Memory management;Space exploration;System-on-chip;Resource management","","22","","28","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"D-Shield: Enabling Processor-side Encryption and Integrity Verification for Secure NVMe Drives","M. H. I. Chowdhuryy; M. Jung; F. Yao; A. Awad",University of Central Florida; KAIST; University of Central Florida; North Carolina State University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","908","921","Ensuring the confidentiality and integrity of data stored in storage disks is essential to protect users’ sensitive and private data. Recent developments of hardware-based attacks have motivated the need to secure storage data not only at rest but also in transit. Unfortunately, existing techniques such as software-based disk encryption and hardware-based self-encrypting disks fail to offer such comprehensive protection in today’s adversarial settings. With the advances of NVMe SSDs promising ultralow I/O latencies and high parallelism, architecting a storage subsystem that ensures the security of data storage in fast disks without adversely sacrificing their performance is critical.In this paper, we present D-Shield, a processor-side secure framework to holistically protect NVMe storage data confidentiality and integrity with low overheads. D-Shield integrates a novel DMA Interception Engine that allows the processor to perform security metadata maintenance and data protection without any modification to the NVMe protocol and NVMe disks. We further propose optimized D-Shield schemes that minimize decryption/re-encryption overheads for data transfer crossing security domains and utilize efficient in-memory caching of storage metadata to further boost system performance. We implement D-Shield prototypes and evaluate their efficacy using a set of synthetic and real-world benchmarks. Our results show that D-Shield can introduce up to 17× speedup for I/O intensive workloads compared to software-based protection schemes. For server-class database and graph applications, D-Shield achieves up to 96% higher throughput over software-based encryption and integrity checking mechanisms, while providing strong security guarantee against off-chip storage attacks. Meanwhile, D-Shield shows only 6% overhead on effective performance on real-world workloads and has modest in-storage metadata overhead and on-chip hardware cost.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070924","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070924","","Protocols;System performance;Data protection;Metadata;Throughput;Hardware;Encryption","","4","","72","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"TensorFHE: Achieving Practical Computation on Encrypted Data Using GPGPU","S. Fan; Z. Wang; W. Xu; R. Hou; D. Meng; M. Zhang","State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","922","934","In the cloud computing era, privacy protection is becoming pervasive in a broad range of applications (e.g., machine learning, data mining, etc). Fully Homomorphic Encryption (FHE) is considered the perfect solution as it enables privacy-preserved computation on untrusted servers. Unfortunately, the prohibitive performance overhead blocks the wide adoption of FHE (about 10, 000× slower than the normal computation). As heterogeneous architectures have gained remarkable success in several fields, achieving high performance for FHE with specifically designed accelerators seems to be a natural choice. Until now, most FHE accelerators have focused on efficiently implementing one FHE operation at a time based on ASIC and with significantly higher performance than GPU and FPGA. However, recent state-of-the-art FHE accelerators rely on an expensive and large on-chip storage and a high-end manufacturing process (i.e., 7nm), which increase the cost of FHE adoption.In this paper, we propose TensorFHE, an FHE acceleration solution based on GPGPU for real applications on encrypted data. TensorFHE utilizes Tensor Core Units (TCUs) to boost the computation of Number Theoretic Transform (NTT), which is the part of FHE with highest time-cost. Moreover, TensorFHE focuses on performing as many FHE operations as possible in a certain time period rather than reducing the latency of one operation. Based on such an idea, TensorFHE introduces operation-level batching to fully utilize the data parallelism in GPGPU. We experimentally prove that it is possible to achieve comparable performance with GPGPU as with state-of-the-art ASIC accelerators. TensorFHE performs 913 KOPS and 88 KOPS for NTT and HMULT (key FHE kernels) within NVIDIA A100 GPGPU, which is 2.61× faster than state-of-the-art FHE implementation on GPGPU; Moreover, TensorFHE provides comparable performance to the ASIC FHE accelerators, which makes it even 2.9× faster than the F1+ with a specific workload. Such a pure software acceleration based on commercial hardware with high performance can open up usage of state-of-the-art FHE algorithms for a broad set of applications in real systems.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071017","National Natural Science Foundation of China; Chinese Academy of Sciences; Natural Science Foundation of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071017","","Cloud computing;Software algorithms;Graphics processing units;Computer architecture;Parallel processing;Software;Servers","","28","","63","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"AVGI: Microarchitecture-Driven, Fast and Accurate Vulnerability Assessment","G. Papadimitriou; D. Gizopoulos","Department of Informatics and Telecommunications, National and Kapodistrian University of Athens; Department of Informatics and Telecommunications, National and Kapodistrian University of Athens",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","935","948","We propose AVGI, a new Statistical Fault Injection (SFI)-based methodology, which delivers orders of magnitude faster assessment of the Architectural Vulnerability Factor (AVF) of a microprocessor chip, while retaining the high accuracy of SFI. The proposed methodology is based on three key insights about the way that faults traverse complex out-of-order microarchitectures: (1) the distribution of the different ways that hardware faults manifest at the software (i.e., the first effects of faults to the software layer) is relatively uniform across workloads, (2) the final effects of faults in a specific hardware structure (i.e., their effect on the program execution) is relatively uniform for different workloads and depends on the distribution of the above fault manifestations, and (3) the majority of first manifestations occur in certain timeframe from the fault occurrence, which is significantly shorter than the complete execution of the workload, and depends on the type of hardware structure. Based on these insights, the proposed AVGI methodology accurately estimates the complete cross-layer vulnerability (i.e., AVF) for every hardware structure in fine granularity (SDCs and Crashes). Our experimental analysis shows that preserving high levels of accuracy, the proposed AVF assessment methodology is up to 337x and 440x faster than an accelerated exhaustive SFI, for two different microarchitectures of 64-bit Armv8 and 32-bit Armv7 CPU models, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071105","microarchitecture;reliability;hardware/software interface;microprocessors;microarchitecture-level fault injection","Out of order;Cross layer design;Microarchitecture;Microprocessor chips;Computer architecture;Hardware;Software","","20","","50","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Realizing Extreme Endurance Through Fault-aware Wear Leveling and Improved Tolerance","J. Zhang; C. Wang; Z. Zhu; D. Kline; A. K. Jones; H. Yang; Y. Wang","Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Intel Corporation; Department of Electrical and Computer Engineering, University of Pittsburgh; Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","964","976","Phase-change memory (PCM) and resistive memory (RRAM) are promising alternatives to traditional memory technologies. However, both PCM and RRAM suffer from limited write endurance. Wear-leveling (WL) techniques are essential to extend the lifetime of these memories before experiencing endurance faults. Beyond the additional usage afforded by WL, row-sparing and focused error correction can extend the lifetime further after wear faults appear. Unfortunately, the need for extended WL techniques continues to become more pressing as scaling exacerbates process variation. Similarly, scaling causes challenges such as more severe noise and crosstalk to traditional DRAM.In this paper, we propose novel fault-aware WL schemes to allocate write frequencies according to the strength of the rows and handle the imbalance of writes in columns. We use runtime detection schemes to identify weak rows and protect them prior to wear out. In particular, row-level WL, aka RETROFIT, leverages the spare rows provided for redundancy to be used strategically to guard against early cell wear out. RETROFIT is compatible with error correction schemes that guarantee to mitigate hard faults and error-correcting codes (ECC). Rather than discard retired rows, when any spare row completely replaces a retired row, we retarget the retired row to assist with column sparing. It becomes a group of Page Protecting Pointers (PPPs), which utilizes otherwise discarded error correction potential to further enhance the leveling ability of RETROFIT. To relieve column-level imbalance, we apply idle error correction bits before they are used to reduce average bit flips. The evaluation demonstrates that RETROFIT and enhanced RETROFIT with the PPPs improve lifetime by as much as 0.64× and 5.4× in the average case, respectively, over state-of-the-art row-level method while also reducing area overhead. In the worst-case scenario, these improvements further increase to 2.6× and 16.0×. Combined with the proposed column-level WL, enhanced RETROFIT realizes an overall 1.5× memory lifetime improvement over the perfectly uniform wear-leveling with equal storage overhead.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071093","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071093","","Phase change materials;Runtime;Microprocessors;Redundancy;Crosstalk;Random access memory;Computer architecture","","3","","65","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"ESD: An ECC-assisted and Selective Deduplication for Encrypted Non-Volatile Main Memory","C. Du; S. Wu; J. Wu; B. Mao; S. Wang","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","977","990","Reducing write data to encrypted Non-Volatile Main Memory (NVMM) can directly improve NVMM’s endurance, performance, and energy efficiency. However, existing works that straightforwardly apply inline deduplication on encrypted NVMM can significantly lead to system performance degradation due to high computing, memory footprint, and index-lookup overhead to generate, store, and query the cryptographic hash (fingerprint). This paper proposes ESD, an ECC-assisted and Selective Deduplication for encrypted NVMM by exploiting both the device characteristics (ECC mechanism) and the workload characteristics (content locality). First, ESD utilizes the ECC information associated with each cache line evicted from the Last-Level Cache (LLC) as the fingerprint to identify data similarity and avoids the costly hash calculating overhead on the non-duplicate cache lines. Second, ESD leverages selective deduplication to exploit the content locality within cache lines by only storing the fingerprints with high reference counts in the memory cache to reduce the memory space overhead and avoid fingerprints NVMM_lookup operations. The experimental results show that ESD can significantly speed up the writes by up to 3.4x, 4.3x, and 2.6x, speed up the reads by up to 5.3x, 5.0x, and 2.0x, and reduce the energy consumption by up to 96.3%, 96.2%, and 56.6% than Baseline, Dedup SHA1, and DeWrite, respectively. Meanwhile, ESD also can significantly outperform other schemes in tail latency.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071011","National Natural Science Foundation of China; Wuhan National Laboratory for Optoelectronics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071011","","Nonvolatile memory;System performance;Memory management;Tail;Fingerprint recognition;Media;Electrostatic discharges","","6","","73","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"A Systematic Study of DDR4 DRAM Faults in the Field","M. V. Beigi; Y. Cao; S. Gurumurthi; C. Recchia; A. Walton; V. Sridharan","RAS Architecture, Advanced Micro Devices, Inc, Boxborough, MA; Google, Sunnyvale, CA; RAS Architecture, Advanced Micro Devices, Inc, Austin, TX; Advanced Micro Devices, Inc, Austin, TX; Google, Sunnyvale, CA; RAS Architecture, Advanced Micro Devices, Inc, Boxborough, MA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","991","1002","This paper presents a study of DDR4 DRAM faults in a large fleet of commodity servers, covering several billion memory device-hours of data. The goal of this study is to understand faults in DDR4 DRAM devices to measure the efficacy of existing hardware resilience techniques and aid in designing more resilient systems for future large-scale systems.The study has several key findings about the fault characteristics of DDR4 DRAMs and adds several novel insights about system reliability to the existing literature. Specifically, the data show sixteen unique fault modes in the DDR4 DRAM under study, including several that have not been previously reported. Over 45% of the faults that occurred affected multiple DRAM bits. The time-to-failure characteristics of faults internal to the DRAM die differ from those external to the DRAM die. We also examine faults from multiple DRAM vendors, finding that fault rates vary by more than 1.34x among vendors.Finally, we use the data to compare chipkill ECC and an ECC that covers a DDR5 ""bounded fault."" Given the fault rates in this data, a bounded fault ECC increases the rate of faults that cause uncorrectable errors by up to 5.71 FIT per DRAM device compared to chipkill ECC.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071066","","Industries;Systematics;Random access memory;Computer architecture;Reliability engineering;Error correction codes;Reliability","","31","","38","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"High Performance and Power Efficient Accelerator for Cloud Inference","J. Yao; H. Zhou; Y. Zhang; Y. Li; C. Feng; S. Chen; J. Chen; Y. Wang; Q. Hu","SJTU and Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China; Enflame-Tech Inc., Shanghai, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1003","1016","Facing the growing complexity of Deep Neural Networks (DNNs), high-performance and power-efficient AI accelerators are desired to provide effective and affordable cloud inference services. We introduce our flagship product, i.e., the Cloudblazer i20 accelerator, which integrates the innovated Deep Thinking Unit (DTU 2.0). The design is driven by requests drawn from various AI inference applications and insights learned from our previous products. With careful tradeoffs in hardware-software co-design, Cloudblazer i20 delivers impressive performance and energy efficiency while maintaining acceptable hardware costs and software complexity/flexibility. To tackle computation- and data-intensive workloads, DTU 2.0 integrates powerful vector/matrix engines and a large-capacity multi-level memory hierarchy with high bandwidth. It supports comprehensive data flow and synchronization patterns to fully exploit parallelism in computation/memory access within or among concurrent tasks. Moreover, it enables sparse data compression/decompression, data broadcasting, repeated data transfer, and kernel code prefetching to optimize bandwidth utilization and reduce data access overheads. To utilize the underlying hardware and simplify the development of customized DNNs/operators, the software stack enables automatic optimizations (such as operator fusion and data flow tuning) and provides diverse programming interfaces for developers. Lastly, the energy consumption is optimized through dynamic power integrity and efficiency management, eliminating integrity risks and energy wastes. Based on the performance requirement, developers also can assign their workloads with the entire or partial hardware resources accordingly. Evaluated with 10 representative DNN models widely adopted in various domains, Cloudblazer i20 outperforms Nvidia T4 and A10 GPUs with a geometric mean of 2.22x and 1.16x in performance and 1.04x and 1.17x in energy efficiency, respectively. The improvements demonstrate the effectiveness of Cloudblazer i20’s design that emphasizes performance, efficiency, and flexibility.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070941","","Full stack;Bandwidth;Programming;Hardware;Energy efficiency;Synchronization;Task analysis","","2","","98","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"LightTrader: A Standalone High-Frequency Trading System with Deep Learning Inference Accelerators and Proactive Scheduler","S. Yoo; H. Kim; J. Kim; S. Park; J. -Y. Kim; J. Oh",KAIST; Rebellions Inc; Rebellions Inc; Rebellions Inc; KAIST; Rebellions Inc,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1017","1030","Recent research shows that artificial intelligence (AI) algorithms can dramatically improve the profitability of high-frequency trading (HFT) with accurate market prediction, overcoming the limitation of conventional latency-oriented approaches. However, it is challenging to integrate the computationally intensive AI algorithm into the existing trading pipeline due to its excessively long latency and insufficient throughput, necessitating a breakthrough in hardware. Furthermore, harsh HFT environments such as bursty data traffic and stringent power constraint make it even more difficult to achieve system-level performance without missing crucial market signals.In this paper, we present LightTrader, the world’s first AI-enabled HFT system that incorporates an FPGA and custom AI accelerators for short-latency-high-throughput trading systems. Leveraging the computing power of brand-new AI accelerators fabricated in TSMC’s 7nm FinFET technology, LightTrader optimizes the tick-to-trade latency and response rate for stock market data. The AI accelerators, adopting Coarse-Grained Reconfigurable Array (CGRA) architecture, which maximizes the hardware utilization from the flexible dataflow architecture, achieve a throughput of 16 TFLOPS and 64 TOPS. In addition, we propose both workload scheduling and dynamic voltage and frequency scaling (DVFS) scheduling algorithms to find an optimal offloading strategy under bursty market data traffic and limited power condition. Finally, we build a reliable and rerunnable simulation framework that can back-test the historical market data, such as Chicago Mercantile Exchange (CME), to evaluate the LightTrader system. We thoroughly explore the performance of LightTrader when the number of AI accelerators, power conditions, and complexity of deep neural network models change. As a result, LightTrader achieves 13.92× and 7.28× speed-up of AI algorithm processing compared to existing GPU-based, FPGA-based systems, respectively. LightTrader with multiple AI accelerators achieves up to 99.5% response rates, while LightTrader with the proposed workload scheduling and DVFS scheduling algorithm relieves the miss rate from 17.1% to 23.1%.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070930","National IT Industry Promotion Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070930","","Scheduling algorithms;Pipelines;AI accelerators;Computer architecture;Throughput;Prediction algorithms;Dynamic scheduling","","5","","77","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"BM-Store: A Transparent and High-performance Local Storage Architecture for Bare-metal Clouds Enabling Large-scale Deployment","Y. Chen; J. Xu; C. Wei; Y. Wang; X. Yuan; Y. Zhang; X. Yu; Y. Chen; Z. Wang; S. He; W. Chen",Zhejiang University; Zhejiang University; Zhejiang University; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Zhejiang University; Alibaba Group; Zhejiang University; Zhejiang University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1031","1044","Bare-metal instances are crucial for high-value, mission-critical applications on the cloud. Tenants exclusively use these dedicated hardware resources. Local virtualized disks are essential for bare-metal instances to provide flexible and high-performance storage resources. Traditionally tenants can choose polling-based software virtualization techniques, but they consume too many valuable host CPU cores and suffer from performance degradation. Cloud vendors are hard to deploy existing hardware-assisted local storage solutions in bare-metal instances due to no access to the host OS to install customized drivers. Moreover, cloud vendors have difficulties managing and maintaining the local storage devices in bare-metal instances because hardware resources and host operating systems are completely utilized by tenants, then it will impact the availability of storage devices.This paper presents our design and experience with BM-Store, a novel high-performance hardware-assisted virtual local storage architecture for bare-metal clouds. BM-Store is transparent to the host that tenants are unaware of the underlying hardware architecture. Therefore, it can be deployed on a large scale in cloud vendors. BM-Store consists of two components: an FPGA-based BMS-Engine and an ARM-based BMS-Controller. The BMS-Engine accelerates the I/O path to enable high-performance virtual storage independent of disk devices without consuming any CPU resource on the host. The BMS-Controller is responsible for resource management and maintenance to achieve flexible and high available local storage. The results of the extensive experiments show that BM-Store can achieve near-native performance, which only introduces about 3 µs extra latency and average 4.0% throughput overhead to native disks. Compared to SPDK vhost, BM-Store achieves an average bandwidth improvement of 15.7% in microbenchmark and a maximum throughput enhancement of 13.4% in real-world applications.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071029","","Performance evaluation;Cloud computing;Computer architecture;Production;Maintenance engineering;Throughput;Hardware","","7","","43","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Turbo: SmartNIC-enabled Dynamic Load Balancing of µs-scale RPCs","H. Seyedroudbari; S. Vanavasam; A. Daglis","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA, Georgia; School of Computer Science, Georgia Institute of Technology, Atlanta, USA, Georgia; School of Computer Science, Georgia Institute of Technology, Atlanta, USA, Georgia",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1045","1058","Online services are decomposed into fine-grained software components that communicate over the network using fine-grained Remote Procedure Calls (RPCs). Inter-server communication often exhibits patterns of wide RPC fan-outs between software tiers, raising the well-known tail at scale effect and necessitating mechanisms that curb long response tail latencies. When handling µs-scale RPCs, request distribution across the cores of multicore servers is a major determinant of the resulting tail latency. Software approaches for inter-core RPC balancing introduce considerable overheads, throttling a server’s peak throughput. On the other hand, existing NIC-based hardware mechanisms ameliorate software and inter-core synchronization overheads, but result in inter-core load imbalance that leaves significant performance improvement headroom.We introduce Turbo, a hardware on-NIC load-balancing mechanism that achieves near-optimal inter-core load distribution for the most fine-grained, light-tailed RPCs with service times of only a couple of µs. We implement Turbo on a programmable NIC and evaluate it on a range of different service time distributions and with a high-performance Key-Value store. Compared to hardware NIC-based mechanisms that statically spread load across cores, Turbo boosts throughput under a 99% latency Service Level Objective (SLO) of 30× the service time by up to 5×, and by up to 95× for a more aggressive 10× SLO target.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071135","","Multicore processing;Prototypes;Tail;Throughput;Load management;Software;Hardware","","7","","48","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"A Scalable Methodology for Designing Efficient Interconnection Network of Chiplets","Y. Feng; D. Xiang; K. Ma","Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1059","1071","The Chiplet methodology can accelerate VLSI system development and provide better flexibility. However, it is not easy to build interconnection networks across multiple chiplets and maintain high-performance deadlock-free routing in systems of various hierarchical topologies. In particular, most on-chiplet networks are based on flat topologies such as 2D-mesh, which are inflexible and insufficient for large-scale multi-chiplet systems.To take full advantage of the multi-chiplet architecture and advanced packaging, we propose an interconnection method that can flexibly establish high-radix interconnection networks from typical 2D-mesh-NoC-based chiplets. A minus-first-based deadlock-free adaptive routing algorithm and a safe/unsafe flow control policy are introduced for these multi-chiplet interconnection networks. Additionally, a general approach network interleaving is used to balance the communication bandwidth within and between chiplets.We evaluate different architectures and traffic patterns on a cycle-accurate C++ simulator. Compared with traditional adaptive routing in 2D-mesh, our methodology can significantly improve network performance in various cases. The more chiplets there are, the more effective the method is. For 64 4×4-2D-mesh-based chiplets, The maximum injection rate increase is up to 2×, and the average latency reduction is up to 45%.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070981","Chiplet;network-on-chip;interconnection network;adaptive routing;deadlock-free","Adaptive systems;Multiprocessor interconnection;Network topology;C++ languages;System recovery;Very large scale integration;Traffic control","","17","","55","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"VVQ: Virtualizing Virtual Channel for Cost-Efficient Protocol Deadlock Avoidance","H. Kasan; J. Kim","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1072","1084","Deadlock freedom is a critical component of interconnection networks in large-scale systems. In particular, protocol or high-level deadlock can occur from dependency based on network endpoints. Virtual channels (VCs) are commonly used to avoid such protocol deadlocks in large-scale systems; however, the cost of VCs is high in large-scale networks because of the deep input buffers and VCs need to be replicated. In this work, we propose to virtualize virtual channels to create Virtualizing Virtual Queues (VVQ) buffer architecture. VVQ is based on the observation that a FIFO buffer organization is sufficient when protocol deadlocks do not occur. However, when potential protocol deadlock can occur through blocking, VVQ takes a proactive approach to allow packets from different traffic classes to ""jump"" the queue and ensure protocol deadlock does not occur. Our proposal shows how a ghost pointer can be leveraged to enable such buffer organization without introducing complex buffer management such as dynamic buffer organizations. Our evaluations show VVQ can match the performance of the baseline VC buffer organization with only half of the total buffer storage.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071059","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071059","","Out of order;Protocols;Costs;Organizations;Tail;System recovery;Throughput","","2","","39","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Mix-GEMM: An efficient HW-SW Architecture for Mixed-Precision Quantized Deep Neural Networks Inference on Edge Devices","E. Reggiani; A. Pappalardo; M. Doblas; M. Moreto; M. Olivieri; O. S. Unsal; A. Cristal","Polytechnic University of Catalonia; AMD AECG Research Labs, Dublin, Ireland; Polytechnic University of Catalonia; Polytechnic University of Catalonia; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Polytechnic University of Catalonia",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1085","1098","Deep Neural Network (DNN) inference based on quantized narrow-precision integer data represents a promising research direction toward efficient deep learning computations on edge and mobile devices. On one side, recent progress of Quantization-Aware Training (QAT) frameworks aimed at improving the accuracy of extremely quantized DNNs allows achieving results close to Floating-Point 32 (FP32), and provides high flexibility concerning the data sizes selection. Unfortunately, current Central Processing Unit (CPU) architectures and Instruction Set Architectures (ISAs) targeting resource-constrained devices present limitations on the range of data sizes supported to compute DNN kernels.This paper presents Mix-GEMM, a hardware-software co-designed architecture capable of efficiently computing quantized DNN convolutional kernels based on byte and sub-byte data sizes. Mix-GEMM accelerates General Matrix Multiplication (GEMM), representing the core kernel of DNNs, supporting all data size combinations from 8- to 2-bit, including mixed-precision computations, and featuring performance that scale with the decreasing of the computational data sizes. Our experimental evaluation, performed on representative quantized Convolutional Neural Networks (CNNs), shows that a RISC-V based edge System-on-Chip (SoC) integrating Mix-GEMM achieves up to 1.3 TOPS/W in energy efficiency, and up to 13.6 GOPS in throughput, gaining from 5.3× to 15.1× in performance over the OpenBLAS GEMM frameworks running on a commercial RISC-V based edge processor. By performing synthesis and Place and Route (PnR) of the enhanced SoC in Global Foundries 22nm FDX technology, we show that Mix-GEMM only accounts for 1% of the overall area consumption.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071076","Generalitat de Catalunya; Ministry of Economy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071076","","Performance evaluation;Deep learning;Training;Neural networks;Computer architecture;Energy efficiency;Computational efficiency","","17","","80","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference","R. Sarkar; S. Abi-Karam; Y. He; L. Sathidevi; C. Hao","School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1099","1112","Graph neural networks (GNNs) have recently exploded in popularity thanks to their broad applicability to graph-related problems such as quantum chemistry, drug discovery, and high energy physics. However, meeting demand for novel GNN models and fast inference simultaneously is challenging due to the gap between developing efficient accelerators and the rapid creation of new GNN models. Prior art focuses on accelerating specific classes of GNNs, such as Graph Convolutional Networks (GCN), but lacks generality to support a wide range of existing or new GNN models. Furthermore, most works rely on graph pre-processing to exploit data locality, making them unsuitable for real-time applications. To address these limitations, in this work, we propose a generic dataflow architecture for GNN acceleration, named FlowGNN, which is generalizable to the majority of message-passing GNNs. The contributions are three-fold. First, we propose a novel and scalable dataflow architecture, which generally supports a wide range of GNN models with message-passing mechanism. The architecture features a configurable dataflow optimized for simultaneous computation of node embedding, edge embedding, and message passing, which is generally applicable to all models. We also propose a rich library of model-specific components. Second, we deliver ultra-fast real-time GNN inference without any graph pre-processing, making it agnostic to dynamically changing graph structures. Third, we verify our architecture on the Xilinx Alveo U50 FPGA board and measure the on-board end-to-end performance. We achieve a speed-up of up to 24–254× against CPU (6226R) and 1.3–477× against GPU (A6000) (with batch sizes 1 through 1024); we also outperform the SOTA GNN accelerator I-GCN by 1.26× speedup and 1.55× energy efficiency over four datasets. Our implementation code and on-board measurement are publicly available on GitHub.1","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071015","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071015","","Computational modeling;Quantum chemistry;Message passing;Graphics processing units;Computer architecture;Parallel processing;Multicast communication","","54","","64","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion","S. Zheng; S. Chen; P. Song; R. Chen; X. Li; S. Yan; D. Lin; J. Leng; Y. Liang",Peking University; Peking University; Peking University; Peking University; Sensetime Research; Sensetime Research; The Chinese University of Hong Kong; Shanghai Jiao Tong University; Peking University,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1113","1126","Machine learning models with various tensor operators are becoming ubiquitous in recent years. There are two types of operators in machine learning: compute-intensive operators (e.g., GEMM and convolution) and memory-intensive operators (e.g., ReLU and softmax). In emerging machine learning models, compute-intensive operators are usually organized in a chain structure. With the continual specialization of hardware, the gap between computing performance and memory bandwidth has become more prominent. Consequently, the implementations of many compute-intensive operator chains are bounded by memory bandwidth, and generating fused kernels to improve locality for these compute-intensive operators becomes necessary. But in existing machine learning compilers, there lack both precise analysis and efficient optimization for compute-intensive operator chains on different accelerators. As a result, they usually produce sub-optimal performance for these operator chains.In this paper, we propose Chimera, an optimizing framework that can efficiently improve the locality of compute-intensive operator chains on different hardware accelerators. In Chimera, each compute-intensive operator is composed of a series of computation blocks. To generate efficient fused kernels for the operator chains, optimizations for both inter-block and intra-block are required. For inter-block optimization, Chimera decides the optimized block execution order by minimizing the data movement volume among blocks using an analytical model. For intra-block optimization, Chimera uses unified replaceable micro kernels to apply hardware-specific optimizations for different accelerators. Finally, Chimera generates fused kernels for compute-intensive operator chains. Evaluation of batch GEMM chains and convolution chains on CPU, GPU, and NPU shows that Chimera achieves up to 2.87×, 2.29×, and 2.39× speedups to hand-tuned libraries. Compared to state-of-the-art compilers, the speedups are up to 2.29×, 1.64×, and 1.14× for CPU, GPU, and NPU.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071018","National Natural Science Foundation of China; Baidu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071018","","Analytical models;Tensors;Convolution;Computational modeling;Graphics processing units;Machine learning;Bandwidth","","24","","65","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Securator: A Fast and Secure Neural Processing Unit","N. Shrivastava; S. R. Sarangi","Electrical Engineering Department, Indian Institute of Technology, Delhi, India; Electrical Engineering Department, Indian Institute of Technology, Delhi, India",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1127","1139","Securing deep neural networks (DNNs) is a problem of significant interest since an ML model incorporates high-quality intellectual property, features of data sets painstakingly collated by mechanical turks, and novel methods of training on large cluster computers. Sadly, attacks to extract model parameters are on the rise, and thus designers are being forced to create architectures for securing such models. State-of-the-art proposals in this field take the deterministic memory access patterns of such networks into cognizance (albeit partially), group a set of memory blocks into a tile, and maintain state at the level of tiles (to reduce storage space). For providing integrity guarantees (tamper avoidance), they don’t propose any significant optimizations, and still maintain block-level state.We observe that it is possible to exploit the deterministic memory access patterns of DNNs even further, and maintain state information for only the current tile and current layer, which may comprise a large number of tiles. This reduces the storage space, reduces the number of memory accesses, increases performance, and simplifies the design without sacrificing any security guarantees. The key techniques in our proposed accelerator architecture, Securator, are to encode memory access patterns to create a small HW-based tile version number generator for a given layer, and to store layer-level MACs. We completely eliminate the need for having a MAC cache and a tile version number store (as used in related work). We show that using intelligently-designed mathematical operations, these structures are not required. By reducing such overheads, we show a speedup of 20.56% over the closest competing work.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071091","","Training;Computational modeling;Neural networks;Memory management;Random access memory;Throughput;Generators","","4","","49","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Tensor Movement Orchestration in Multi-GPU Training Systems","S. -F. Lin; Y. -J. Chen; H. -Y. Cheng; C. -L. Yang","Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Chi Nan University, Nantou, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1140","1152","As deep neural network (DNN) models grow deeper and wider, one of the main challenges for training large-scale neural networks is overcoming limited GPU memory capacity. One common solution is to utilize the host memory as the external memory for swapping tensors in and out of GPU memory. However, the effectiveness of such tensor swapping can be impaired in data-parallel training systems due to contention on the shared PCIe channel to the host. In this paper, we propose the first large-model support framework that coordinates tensor movements among GPUs to alleviate PCIe channel contention. We design two types of coordination mechanisms. In the first mechanism, PCIe channel accesses from different GPUs are interleaved by selecting disjoint swapped-out tensors for each GPU. In the second method, swap commands are orchestrated to avoid contention. The effectiveness of these two methods depends on the model size and how often the GPUs synchronize on gradients. Experimental results show that compared to large-model support that is oblivious to channel contention, the proposed solution achieves average speedups of 38.3% to 31.8% when the memory footprint size is 1.33 to 2 times the GPU memory size.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071043","Facebook; Ministry of Science and Technology; Delta; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071043","","Training;Deep learning;Tensors;Computational modeling;Neural networks;Graphics processing units;Full-duplex system","","3","","47","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"A Storage-Effective BTB Organization for Servers","T. Asheim; B. Grot; R. Kumar","NTNU, Norway; University of Edinburgh, UK; NTNU, Norway",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1153","1167","Many contemporary applications feature multi-megabyte instruction footprints that overwhelm the capacity of branch target buffers (BTB) and instruction caches (L1-I), causing frequent front-end stalls that inevitably hurt performance. BTB capacity is crucial for performance as a sufficiently large BTB enables the front-end to accurately resolve the upcoming execution path and steer instruction fetch appropriately. Moreover, it also enables highly effective fetch-directed instruction prefetching that can eliminate a large portion L1-I misses. For these reasons, commercial processors allocate vast amounts of storage capacity to BTBs.This work aims to reduce BTB storage requirements by optimizing the organization of BTB entries. Our key insight is that storing branch target offsets, instead of full or compressed targets, can drastically reduce BTB storage cost as the vast majority of dynamic branches have short offsets requiring just a handful of bits to encode. Based on this insight, we size the ways of a set associative BTB to hold different number of target offset bits such that each way stores offsets within a particular range. Doing so enables a dramatic reduction in storage for target addresses. Our final design, called BTB-X, uses an 8-way set associative BTB with differently sized ways that enables it to track about 2.24x more branches than a conventional BTB and 1.3x more branches than a storage-optimized state-of-the-art BTB organization, called PDede, with the same storage budget.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070938","","Costs;Target tracking;Program processors;Prefetching;Pipelines;Organizations;Computer architecture","","8","","54","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"<monospace>HoPP</monospace>: Hardware-Software Co-Designed Page Prefetching for Disaggregated Memory","H. Li; K. Liu; T. Liang; Z. Li; T. Lu; H. Yuan; Y. Xia; Y. Bao; M. Chen; Y. Shan","Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Huawei Technologies; Huawei Technologies; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Chinese Academy of Sciences, State Key Lab of Processors, Institute of Computing Technology; Huawei Cloud",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1168","1181","Memory disaggregation is a promising direction to mitigate memory contention in datacenters. To make memory disaggregation practical, prior efforts expose remote memory to applications transparently via virtual memory subsystem’s swapping interface. However, due to the semantic gap between OS and applications – OS cannot know the memory accessing sequences of an application but via page faults. This approach has two limitations. First, it learns little from page faults’ access history, which leads to sub-optimal prefetching predictions. Second, a page fault can still occur even if there is a prefetch-hit which leads to a large kernel overhead.To address such limitations, our key insight is to decouple the address capturing from page faults by collecting full memory access traces in the memory controller. Using this idea, we build <monospace>HoPP</monospace> – a hardware-software co-designed prefetching framework. <monospace>HoPP</monospace> adds hardware modules to the memory controller to feed sufficient hot pages to OS in real-time, which has three benefits in <monospace>HoPP</monospace>’s software design: 1) it improves existing prefetching algorithms with simple revamps, also offers more insights to build better policies; 2) the prefetch algorithm can run as a separate data path alongside the normal remote data path via page faults, potentially hiding the swap latency from applications, and enabling fine-grained control over prefetching behaviors; 3) the prefetch-hit overhead can be eliminated by early page table entry (PTE) injection, i.e., inject PTE for the prefetched page as soon as it returns. We implemented a proof-of-concept prototype using commodity servers along with a hardware-based memory tracking tool called <monospace>HMTT</monospace> to emulate a modified memory controller. Results show that compared to Fastswap and Leap, <monospace>HoPP</monospace>-optimized prefetching algorithm achieves over 90% accuracy and coverage, which leads to up to 59% completion time improvement for various datacenter applications.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070986","Research and Development; Beijing Municipal Natural Science Foundation; National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070986","","Software design;Prefetching;Software algorithms;Semantics;Prototypes;Real-time systems;Hardware","","9","","69","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Speculative Register Reclamation","S. Mehta","Hardware Software Co-Design HPE, Bloomington, MN, USA",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1182","1194","Large number of in-flight instructions were envisioned two decades ago. They are finally happening now. While more in-flight instructions enable higher ILP and therefore better single-thread performance, it comes at a price. The price is larger structures within the core such as the physical register file. In this work, we propose to reduce register file size while maintaining (or even increasing) number of in-flight instructions. We leverage the insight that within loops, where most time is spent in general, most logical registers are redefined in the same or immediate next iteration. The physical registers allocated to most of these logical registers can thus be aggressively and speculatively released at redefinition instead of being released when the redefining instruction finally commits as in current designs. For correct mis-speculation recovery, only registers that are actually used (i.e used without prior redefinition) across iterations need to remain allocated beyond redefinition, leading to much reduced register file pressure. We show that using our design, register file sizes can be reduced by 50% while still achieving a 1.05x performance improvement over existing designs on a variety of applications even when other core resources are kept the same. The power consumption among various core structures in reduced by 26% on average. In addition, the performance improvement jumps to 1.14x when this reduction in register file size is complemented with an increase of other structures within the core.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071122","","Power demand;Microarchitecture;Source coding;Random access memory;Computer architecture;Registers;Proposals","","1","","44","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs","J. Lee; J. M. Lee; Y. Oh; W. J. Song; W. W. Ro","School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical Engineering, Korea University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1195","1207","This paper presents an address translation scheme in GPUs named SnakeByte that can dynamically manage variable-sized pages and maximize TLB reach by recursively merging contiguous pages. Memory virtualization has become an integral part of GPUs to enhance programmability and memory management efficiency. However, conventional memory virtualization methods using multi-level page tables and caching them in TLBs are insufficient to provide GPUs with enough address translation coverage for the massive volume of data. SnakeByte implements a hardware-based address translation mechanism that recursively merges contiguous pages into larger page groups and effectively extends TLB coverage. SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE). It records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages. If all pages covered by the PTE are allocated with contiguity, the PTE is promoted to be further coalesced into a larger page group. The recursive coalescence of contiguous pages enables SnakeByte to handle variable-sized page groups with the exponentially increasing TLB reach. Associated with a contiguity-aware memory allocator, SnakeByte can consolidate vastly contiguous address spaces into a few TLB entries. Consequently, it significantly reduces TLB misses for large working sets in GPUs and achieves substantial performance improvements. Experiment results show that SnakeByte decreases the number of page table walks by 6.5x and enhances the GPU performance by 2.0x on average over the conventional paging scheme.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071063","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071063","GPU;virtual memory;address translation;TLB","Merging;Memory management;Graphics processing units;Virtualization","","13","","55","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"CARE: A Concurrency-Aware Enhanced Lightweight Cache Management Framework","X. Lu; R. Wang; X. -H. Sun","Department of Compute Science, Illinois Institute of Technology, Chicago, Illinois; Department of Compute Science, Illinois Institute of Technology, Chicago, Illinois; Department of Compute Science, Illinois Institute of Technology, Chicago, Illinois",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1208","1220","Improving cache performance is a lasting research topic. While utilizing data locality to enhance cache performance becomes more and more difficult, data access concurrency provides a new opportunity for cache performance optimization. In this work, we propose a novel concurrency-aware cache management framework that outperforms state-of-the-art locality-only cache management schemes. First, we investigate the merit of data access concurrency and pinpoint that reducing the miss rate may not necessarily lead to better overall performance. Next, we introduce the pure miss contribution (PMC) metric, a lightweight and versatile concurrency-aware indicator, to accurately measure the cost of each outstanding miss access by considering data concurrency. Then, we present CARE, a dynamic adjustable, concurrency-aware, low-overhead cache management framework with the help of the PMC metric. We evaluate CARE with extensive experiments across different application domains and show significant performance gains with the consideration of data concurrency. In a 4-core system, CARE improves IPC by 10.3% over LRU replacement. In 8 and 16-core systems where more concurrent data accesses exist, CARE outperforms LRU by 13.0% and 17.1%, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071125","","Concurrent computing;Measurement;Costs;Memory management;Performance gain;Currencies;Optimization","","6","","53","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Memory-Efficient Hashed Page Tables","J. Stojkovic; N. Mantri; D. Skarlatos; T. Xu; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Carnegie Mellon University; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1221","1235","Conventional radix-tree page tables have scalability challenges, as address translation following a TLB miss potentially requires multiple memory accesses in sequence. An alternative is hashed page tables (HPTs) where, conceptually, address translation needs only one memory access. Traditionally, HPTs have been shunned due to high costs of handling conflicts and other limitations. However, recent advances have made HPTs compelling. Still, a major issue in HPT designs is their requirement for substantial contiguous physical memory.This paper addresses this problem. To minimize HPTs’ contiguous memory needs, it introduces the Logical to Physical (L2P) Table and the use of Dynamically-Changing Chunk Sizes. These techniques break down the HPT into discontiguous physical-memory chunks. In addition, the paper also introduces two techniques that minimize HPTs’ total memory needs and, indirectly, reduce the memory contiguity requirements. These techniques are In-place Page Table Resizing and Per-way Resizing. We call our complete design Memory-Efficient HPTs (ME-HPTs). Compared to state-of-the-art HPTs, ME-HPTs: (i) reduce the contiguous memory allocation needs by 92% on average, and (ii) improve the performance by 8.9% on average. For the two most demanding workloads, the contiguous memory requirements decrease from 64MB to 1MB. In addition, compared to state-of-the-art radix-tree page tables, ME-HPTs achieve an average speedup of 1.23× (without huge pages) and 1.28× (with huge pages).","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071061","Virtual memory;Page tables;Hashed page tables","Costs;Scalability;Memory management;Resource management","","8","","91","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"NvWa: Enhancing Sequence Alignment Accelerator Throughput via Hardware Scheduling","Y. Li; X. Li; R. Gao; W. Liu; G. Tan","State Key Lab of Processors, ICT, CAS; State Key Lab of Processors, ICT, CAS; State Key Lab of Processors, ICT, CAS; State Key Lab of Processors, ICT, CAS; State Key Lab of Processors, ICT, CAS",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1236","1248","Sequence alignment is the most time-consuming step in the genome analysis pipeline. Since sequence alignment generally follows the seed-and-extension paradigm, prior proposed hardware accelerators either opt to accelerate the seeding phase or the seed-extension phase. However, the diversity of each sequence in the alignment workflow leads to the pipeline stall or bubbles, which finally results in a decreased throughput for the end-to-end sequence alignment.In this paper, we propose NvWa, which is a hardware scheduling accelerator for sequence alignment. To solve the diversity problem, we propose three novel scheduling mechanisms and corresponding architecture, which target the seeding phase, the seed-extension phase, and the interaction between the two phases, respectively. For the seeding phase, we propose a Seeding Scheduler to schedule all idle seeding units in only one cycle. For the seed-extension phase, the Extension Scheduler can achieve both lower latency and higher parallelism when facing seed-extension tasks with different scales. Between the two phases, an efficient Coordinator caches and dispatches seeding hits to optimal and sub-optimal seed-extension units. Furthermore, to avoid algorithmic obsolescence for the new sequence technologies, we propose a loosely coupled design, which decouples the data path and the control scheduling path. Experimental results show that NvWa can achieve 493×, 200×, 12.11×, 2.30× speedup and 14.21×, 5.60×, 4.34×, 5.85× energy reduction when compared with a 16-thread CPU baseline, an NVIDIA A100 GPU, and two state-of-the-art accelerators, respectively.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070978","National Postdoctoral Program for Innovative Talents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070978","","Schedules;Pipelines;Graphics processing units;Genomics;Computer architecture;Parallel processing;Throughput","","4","","69","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Efficient Supernet Training Using Path Parallelism","Y. Xu; L. Cheng; X. Cai; X. Ma; W. Chen; L. Zhang; Y. Wang","Chinese Academy of Sciences, Institute of Computing Technology; School of Control and Computer Engineering, North China Electric Power University; Chinese Academy of Sciences, Institute of Computing Technology; Chinese Academy of Sciences, Institute of Computing Technology; Chinese Academy of Sciences, Institute of Computing Technology; Chinese Academy of Sciences, Institute of Computing Technology; Chinese Academy of Sciences, Institute of Computing Technology",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1249","1261","Compared to conventional neural networks, training a supernet for Neural Architecture Search (NAS) is very time consuming. Although current works have demonstrated that parallel computing can significantly speed up the training process, almost all of their parallelism still follow the conventional data- and model-based paradigms, which actually face performance issues in both computation and inter-node communication of the supernet training. To further improve the performance of current methods, we discover the unique path-parallelism that exists in supernet training, and proposed a novel training approach designed specifically for supernet. In detail, we focus on analyzing path correlations between subnets in a supernet and exploiting effective path-merging methods to reduce redundant computations and communications raised by concurrent subnets. Moreover, we also try to combine the proposed path parallelism with traditional intra-subnet parallelism to perform multi-level parallelization to further optimize the parallel performance. We present the detailed design and implementation of our method, and our experimental results show that our proposed approach can achieve up to 3.2x end-to-end speedup over conventional parallel training solutions, and 1.46x–5.78x speedup compared to the state-of-art supernet training frameworks.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071099","","Training;Correlation;Computational modeling;Neural networks;Merging;Graphics processing units;Computer architecture","","1","","40","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"Phloem: Automatic Acceleration of Irregular Applications with Fine-Grain Pipeline Parallelism","Q. M. Nguyen; D. Sanchez",MIT CSAIL; MIT CSAIL,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1262","1274","Irregular applications are increasingly common in diverse domains, like graph analytics and sparse linear algebra. Accelerating these applications is challenging because of their unpredictable data reuse and control flow. Recent work has proposed hardware support for fine-grain pipeline parallelism, hiding long latencies by decoupling irregular applications into pipeline stages. However, this prior work requires programmers to manually decouple applications. This tedious and error-prone process limits the usefulness of such architectural support.We address this problem with Phloem, a compiler that automatically discovers and exploits pipeline parallelism in irregular applications. Prior compilers for pipeline parallelism target regular applications, which contain simple pipeline stages with known latencies and fixed buffering needs. Designing Phloem to target irregular applications, where these properties do not hold, requires treating their unique challenges as first-class considerations throughout its design. Phloem breaks down this complex transformation into a series of simple passes that together encode the insights that have been previously applied by hand, producing code that targets architectures with support for queue-based communication.We evaluate Phloem by generating efficient pipelines on a variety of irregular applications. Phloem’s contributions improve performance by 1.7× on average, approaching (and sometimes exceeding) the performance of manually optimized pipeline-parallel code. These results show that, for the first time, automatic parallelization for irregular applications is not only feasible, but also profitable.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071026","","Codes;Pipelines;Computer architecture;Transforms;Linear algebra;Parallel processing;Programming","","3","","62","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"CHOPPER: A Compiler Infrastructure for Programmable Bit-serial SIMD Processing Using Memory in DRAM","X. Peng; Y. Wang; M. -C. Yang",The Chinese University of Hong Kong; National University of Defense Technology; The Chinese University of Hong Kong,2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1275","1288","Increasing interests in Bit-serial SIMD Processing-Using-DRAM (PUD) architectures amplify the needs for a compiler to automate code generation, credited to their ultra-wide SIMD width and reduction of data movements. The state-of-the-art Bit-serial SIMD PUD architectures (1) only provide assembly SIMD programming interfaces, which heavily saddles with programmers to exploit the ultra-wide SIMD width on these architectures; and (2) encapsulate 1-bit operations into multi-bit abstractions, which incurs a granularity mismatch and restricts the optimization space to minimize data movements.We present CHOPPER, a new compiler infrastructure to make Bit-serial SIMD PUD more programmable and efficient. For the better programmability, the design of CHOPPER (1) exploits bit-slicing compilers to enable automatic memory allocation and code generation, from naturally-expressive codes (i.e. similar to Parallel Haskell) into the ""SIMD-Within-A-Register""-style codes; and (2) introduces a new abstraction called ""Virtual Code Emitter"", to make Bit-serial SIMD PUD architecture exploit Memory-Level Parallelism (i.e. Bank or Subarray) more effectively. For the better efficiency, we propose three novel optimizations for CHOPPER to better exploit the potentials of Bit-serial SIMD PUD architectures, which (1) minimize the amount of intra-subarray data movements; and (2) mitigate the overheads of spilling data outside Bit-serial SIMD PUD architectures. These optimizations can greatly improve the overall efficiency of Bit-serial SIMD PUD architectures. We also discuss (1) the limitations of the current CHOPPER; and (2) the potentials of CHOPPER for other types of Processing-In-Memory architectures.We evaluate CHOPPER by hosting it on three state-of-the-art Bit-serial SIMD PUD architectures. We compare CHOPPER-generated codes against the state-of-the-art hands-tuned codes for Bit-serial SIMD PUD architectures. We highlight that, averaged across 16 real-world workloads from 4 PUD-friendly application domains, CHOPPER achieves (A) 1.20X, 1.29X and 1.26X speedup when data can fit within DRAM subarrays; and (B) 12.61X, 9.05X and 9.81X speedup when data need to spill to the secondary storage, on Ambit [50], ELP2IM [56] and SIMDRAM [22], compared with hands-tuned codes using the state-of-the-art methodology [22] for Bit-serial SIMD PUD architectures. These performance benefits also accompany with a great reduction of Lines-of-Codes (LoC) in CHOPPER (i.e. by 4.3X less LoCs for hands-tuning a single subarray, and >103X less for hands-tuning all subarrays in a rank). We also perform breakdown and sensitivity studies of CHOPPER, to better understand its source benefits and examine its robustness under various architectural features.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10071070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071070","","Codes;Program processors;Sensitivity;Random access memory;Computer architecture;Choppers (circuits);Programming","","8","","56","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
"VAQUERO: A Scratchpad-based Vector Accelerator for Query Processing","J. Pavón; I. V. Valdivieso; J. Marimon; R. Figueras; F. Moll; O. Unsal; M. Valero; A. Cristal","Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain",2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"24 Mar 2023","2023","","","1289","1302","Database Management Systems (DBMS) have be-come an essential tool for industry and research and are often a significant component of data centers. There have been many efforts to accelerate DBMS application performance. One of the most explored techniques is the use of vector processing. Unfortunately, conventional vector architectures have not been able to exploit the full potential of DBMS acceleration.In this paper, we present VAQUERO, our Scratchpad-based Vector Accelerator for QUEry pROcessing. VAQUERO improves the efficiency of vector architectures for DBMS operations such as data aggregation and hash joins featuring lookup tables. Lookup tables are significant contributors to the performance bottlenecks in DBMS processing suffering from insufficient ISA support in the form of scatter-gather instructions. VAQUERO introduces a novel Advanced Scratchpad Memory specifically designed with two mapping modes — direct- and associative-mode. These map-ping modes enable VAQUERO to accelerate real-world databases with workload sizes that significantly exceed the scratchpad memory capacity. Additionally, the associative-mode allows to use VAQUERO with DBMS operators that use hashed keys, e.g. hash-join and hash-aggregate. VAQUERO has been designed considering general DBMS algorithm requirements instead of being based on a particular database organization. For this reason, VAQUERO is capable to accelerate DBMS operators for both row- and column-oriented databases.In this paper, we evaluate the efficiency of VAQUERO using two highly optimized popular open-source DBMS, namely the row-based PostgreSQL and column-based MonetDB. We imple-mented VAQUERO at the RTL level and prototype it, by performing Place&Route, at the 7nm technology node. VAQUERO incurs a modest 0.15% area overhead compared with an Intel Ice Lake processor. Our evaluation shows that VAQUERO significantly outperforms PostgreSQL and MonetDB by 2.09× and 3.32× respectively, when processing operators and queries from the TPC-H benchmark.","2378-203X","978-1-6654-7652-2","10.1109/HPCA56546.2023.10070958","Barcelona Supercomputing Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070958","Vector Computing;Scratchpad Memory;Database Accelerator;PostgreSQL;MonetDB","Industries;Data centers;Query processing;Prototypes;Organizations;Computer architecture;Data aggregation","","2","","54","IEEE","24 Mar 2023","","","IEEE","IEEE Conferences"
