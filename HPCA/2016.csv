"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Memristive Boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning","M. N. Bojnordi; E. Ipek","University of Rochester, Rochester, NY, USA; University of Rochester, Rochester, NY, US",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","1","13","The Boltzmann machine is a massively parallel computational model capable of solving a broad class of combinatorial optimization problems. In recent years, it has been successfully applied to training deep machine learning models on massive datasets. High performance implementations of the Boltzmann machine using GPUs, MPI-based HPC clusters, and FPGAs have been proposed in the literature. Regrettably, the required all-to-all communication among the processing units limits the performance of these efforts. This paper examines a new class of hardware accelerators for large-scale combinatorial optimization and deep learning based on memristive Boltzmann machines. A massively parallel, memory-centric hardware accelerator is proposed based on recently developed resistive RAM (RRAM) technology. The proposed accelerator exploits the electrical properties of RRAm to realize in situ, fine-grained parallel computation within memory arrays, thereby eliminating the need for exchanging data between the memory cells and the computational units. Two classical optimization problems, graph partitioning and boolean satisfiability, and a deep belief network application are mapped onto the proposed hardware. As compared to a multicore system, the proposed accelerator achieves 57x higher performance and 25x lower energy with virtually no loss in the quality of the solution to the optimization problems. The memristive accelerator is also compared against an RRAM based processing-in-memory (PIM) system, with respective performance and energy improvements of 6.89x and 5.2x.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446049","","Hardware;Machine learning;Computational modeling;Arrays;Simulated annealing;Training","","154","5","28","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"TABLA: A unified template-based framework for accelerating statistical machine learning","D. Mahajan; J. Park; E. Amaro; H. Sharma; A. Yazdanbakhsh; J. K. Kim; H. Esmaeilzadeh","Georgia Institute of Technology, Atlanta, GA, US; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology; Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","14","26","A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650 Ti, and Tesla K40) using real hardware measurements. TABLA-generated accelerators provide 19.4x and 2.9x average speedup over the ARM and Xeon processors, respectively. These accelerators provide 17.57x, 20.2x, and 33.4x higher Performance-per-Watt in comparison to Tegra, GTX 650 Ti and Tesla, respectively. These benefits are achieved while the programmers write less than 50 lines of code.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446050","","Machine learning algorithms;Field programmable gate arrays;Linear programming;Stochastic processes;Algorithm design and analysis;Hardware;Data models","","125","","58","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Pushing the limits of accelerator efficiency while retaining programmability","T. Nowatzki; V. Gangadhan; K. Sankaralingam; G. Wright",University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; Qualcomm,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","27","39","The waning benefits of device scaling have caused a push towards domain specific accelerators (DSAs), which sacrifice programmability for efficiency. While providing huge benefits, DSAs are prone to obsoletion due to domain volatility, have recurring design and verification costs, and have large area footprints when multiple DSAs are required in a single device. Because of the benefits of generality, this work explores how far a programmable architecture can be pushed, and whether it can come close to the performance, energy, and area efficiency of a DSA-based approach. Our insight is that DSAs employ common specialization principles for concurrency, computation, communication, data-reuse and coordination, and that these same principles can be exploited in a programmable architecture using a composition of known microarchitectural mechanisms. Specifically, we propose and study an architecture called LSSD, which is composed of many low-power and tiny cores, each having a configurable spatial architecture, scratchpads, and DMA. Our results show that a programmable, specialized architecture can indeed be competitive with a domain-specific approach. Compared to four prominent and diverse DSAs, LSSD can match the DSAs' 10× to 150× speedup over an OOO core, with only up to 4x more area and power than a single DSA, while retaining programmability.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446051","","Hardware;Concurrent computing;Engines;Algorithm design and analysis;Multicore processing;Out of order","","40","1","47","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A low power software-defined-radio baseband processor for the Internet of Things","Y. Chen; S. Lu; H. -S. Kim; D. Blaauw; R. G. Dreslinski; T. Mudge","University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","40","51","In this paper, we define a configurable Software Defined Radio (SDR) baseband processor design for the Internet of Things (IoT). We analyzed the fundamental algorithms in communications systems on IoT devices to enable a microarchitecture design that supports many IoT standards and custom nonstandard communications. Based on this analysis, we propose a custom SIMD execution model coupled with a scalar unit. We introduce several architectural optimizations to this design: streaming registers, variable bit width datapath, dedicated ALUs for critical kernels, and an optimized flexible reduction network. We employ voltage scaling and clock gating to further reduce the power, while more than a 100% time margin has been reserved for reliable operation in the near-threshold region. Together our architectural enhancements lead to a 71× power reduction compared to a classic general purpose SDR SIMD architecture. Our IoT SDR datapath has sub-mW power consumption based on SPICE simulation, and is placed and routed to fit within an area of 0.074mm2 in a 28nm process. We implemented several essential elementary signal processing kernels and combined them to demonstrate two end-to-end upper bound systems, 802.15.4-OQPSK and Bluetooth Low Energy. Our full SDR baseband system consists of a configurable SIMD with a control plane MCU and memory. For comparison, the best commercial wireless transceiver consumes 23.8mW for the entire wireless system (digital/RF/ analog). We show that our digital system power is below 2mW, in other words only 8% of the total system power. The wireless system is dominated by RF/analog power comsumption, thus the price of flexibility that SDR affords is small. We believe this work is unique in demonstrating the value of baseband SDR in the low power IoT domain.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446052","","Standards;Kernel;Finite impulse response filters;Baseband;Wireless communication;Wireless sensor networks;Synchronization","","30","","38","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Improving smartphone user experience by balancing performance and energy with probabilistic QoS guarantee","B. Gaudette; C. -J. Wu; S. Vrudhula","School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ; School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ; School of Computing Informatics, & Decision Systems Engineering Arizona State University, Tempe, AZ",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","52","63","User satisfaction is pivotal to the success of a mobile application. A recent study has shown that 49% of users would abandon a web-based application if it failed to load within 10 seconds. At the same time, it is imperative to maximize energy efficiency to ensure maximum usage of the limited energy source available to smartphones while maintaining the necessary levels of user satisfaction. An important factor to consider, that has been previously neglected, is variability of execution times of an application, requiring them to be modeled as stochastic quantities. This changes the nature of the objective function and the constraints of the underlying optimization problem. In this paper, we present a new approach to optimal energy control of mobile applications running on modern smartphone devices, focusing on the need to ensure a specified level of user satisfaction. The proposed statistical models address both single and multi-stage applications and are used in the formulation of an optimization problem, the solution to which is a static, lightweight controller that optimizes energy efficiency of mobile applications, subject to constraints on the likelihood that the application execution time meets a given deadline. We demonstrate the proposed models and the corresponding optimization method on three common mobile applications running on a real Qualcomm Snapdragon 8074 mobile chipset. The results show that the proposed statistical estimates of application execution times are within 99.34% of the measured values. Additionally, on the actual Qualcomm Snapdragon 8074 mobile chipset, the proposed control scheme achieves a 29% power savings over commonly-used Linux governors while maintaining an average web page load time of 2 seconds with a likelihood of 90%.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446053","","Quality of service;Frequency control;Time-frequency analysis;Mobile communication;Optimization;Complexity theory;Mobile applications","","34","","53","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Mobile CPU's rise to power: Quantifying the impact of generational mobile CPU design trends on performance, energy, and user satisfaction","M. Halpern; Y. Zhu; V. J. Reddi","University of Texas at Austin, Austin, TX, US; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","64","76","In this paper, we assess the past, present, and future of mobile CPU design. We study how mobile CPU designs trends have impacted the end-user, hardware design, and the holistic mobile device. We analyze the evolution often cutting-edge mobile CPU designs released over the past seven years. Specifically, we report measured performance, power, energy and user satisfaction trends across mobile CPU generations. A key contribution of our work is that we contextualize the mobile CPU's evolution in terms of user satisfaction, which has largely been absent from prior mobile hardware studies. To bridge the gap between mobile CPU design and user satisfaction, we construct and conduct a novel crowdsourcing study that spans over 25,000 survey participants using the Amazon Mechanical Turk service. Our methodology allows us to identify what mobile CPU design techniques provide the most benefit to the end-user's quality of user experience. Our results quantitatively demonstrate that CPUs play a crucial role in modern mobile system-on-chips (SoCs). Over the last seven years, both single-and multicore performance improvements have contributed to end-user satisfaction by reducing user-critical application response latencies. Mobile CPUs aggressively adopted many power-hungry desktop-oriented design techniques to reach these performance levels. Unlike other smartphone components (e.g. display and radio) whose peak power consumption has decreased over time, the mobile CPU's peak power consumption has steadily increased. As the limits of technology scaling restrict the ability of desktop-like scaling to continue for mobile CPUs, specialized accelerators appear to be a promising alternative that can help sustain the power, performance, and energy improvements that mobile computing necessitates. Such a paradigm shift will redefine the role of the CPU within future SoCs, which merit several design considerations based on our findings.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446054","","Mobile communication;Market research;Multicore processing;Hardware;Benchmark testing;Power demand;Technological innovation","","108","","60","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Atomic persistence for SCM with a non-intrusive backend controller","K. Doshi; E. Giles; P. Varman","Intel Corp., Portland, Oregon; Rice University, Houston, Texas; Rice University, Houston, Texas",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","77","89","Non-volatile byte-addressable memory has the potential to revolutionize system architecture by providing instruction-grained direct access to vast amounts of persistent data. We describe a non-intrusive memory controller that uses backend operations for achieving lightweight failure atomicity. By moving synchronous persistent memory operations to the background, the performance overheads are minimized. Our solution avoids costly software intervention by decoupling isolation and concurrency-driven atomicity from failure atomicity and durability, and does not require changes to the front-end cache hierarchy. Two implementation alternatives - one using a hardware structure, and the other extending the memory controller with a firmware managed volatile space - are described. Our results show the performance is significantly better than traditional approaches.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446055","","Random access memory;Nonvolatile memory;Software;Hardware;Computer crashes;Retirement;Libraries","","53","","28","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"CompEx: Compression-expansion coding for energy, latency, and lifetime improvements in MLC/TLC NVM","P. M. Palangappa; K. Mohanram","Department of Electrical and Computer Engineering, University of Pittsburgh, PA; Department of Electrical and Computer Engineering, University of Pittsburgh, PA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","90","101","Multi-level/triple-level cell non-volatile memories (MLC/TLC NVMs) such as PCM and RRAM are the subject of active research and development as replacement candidates for DRAM, which is limited by its high refresh power and poor scaling potential. Besides the benefits of non-volatility (low refresh power) and improved scalability, MLC/TLC NVMs offer high data density and memory capacity over DRAM. However, the viability of MLC/TLC NVMs is limited primarily due to the high programming energy and latency as well as the low endurance of NVM cells; these are primarily attributable to the iterative program-and-verify procedure necessary for programming the NVM cells. In this paper, we propose compression-expansion (CompEx) coding, a low overhead scheme that synergistically integrates statistical compression with expansion coding to realize simultaneous energy, latency, and lifetime improvements in MLC/TLC NVMs. CompEx coding is agnostic to the choice of compression technique; in this paper, we evaluate CompEx coding using both frequent pattern compression (FPC) and base-delta-immediate (BΔI) compression. CompEx coding integrates FPC/BΔI with (k, m)q `expansion' coding; expansion codes are a class of q-ary linear block codes that encode data using only the low energy states of a q-ary NVM cell. CompEx coding simultaneously reduces energy and latency and improves lifetime for no memory overhead and negligible logic overhead (≈ 10k gates, which is <; 0.1% per NVM module). Our full-system simulations of a system that integrates TLC RRAM show that CompEx coding reduces total memory energy by 53% and write latency by 24%; these improvements translate to a 5.7% improvement in IPC, a 11.8% improvement in main memory bandwidth, and 1.8× improvement in lifetime over classical binary coding using data-comparison write. CompEx coding thus addresses the programming energy/latency and lifetime challenges of MLC/-TLC NVMs that pose a serious technological roadblock to their adoption in high performance computing systems.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446056","","Nonvolatile memory;Encoding;Resistance;Programming;Codecs;Random access memory;Benchmark testing","","38","","68","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A low-power hybrid reconfigurable architecture for resistive random-access memories","M. A. Lastras-Montaño; A. Ghofrani; K. -T. Cheng","Electrical and Computer Engineering Department, University of California, Santa Barbara; Electrical and Computer Engineering Department, University of California, Santa Barbara; Electrical and Computer Engineering Department, University of California, Santa Barbara",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","102","113","Access-transistor-free memristive crossbars have shown to be excellent candidates for next generation non-volatile memories. While the elimination of the transistor per memory element enables higher memory densities, it also introduces parasitic currents during the normal operation of the memory that increases both the overall power consumption of the crossbar, and the current requirements of the line drivers. In this work we present a hybrid reconfigurable memory architecture that takes advantage of the fact that a complementary resistive switch (CRS) can behave both as a memristor and as a CRS. By dynamically keeping frequently accessed regions of the memory in the memristive mode and others in the CRS mode, our hybrid memory offer all the benefits that a memristor and a CRS offer individually, without any of their drawbacks. We validate our architecture using the SPEC CPU2006 benchmark and found that our hybrid memory offers average energy savings of 3.6x with respect to a memristive-only memory. In addition, we can offer a memory lifetime that is, on average, 6.4x longer than that of a CRS-only memory.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446057","","Memristors;Resistance;Memory management;Transistors;Microprocessors;Energy consumption","","12","","34","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A performance analysis framework for optimizing OpenCL applications on FPGAs","Z. Wang; B. He; W. Zhang; S. Jiang","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; HKUST; Cornell University, Nanyang Technological University, Singapore",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","114","125","Recently, FPGA vendors such as Altera and Xilinx have released OpenCL SDK for programming FPGAs. However, the architecture of FPGA is significantly different from that of CPU/GPU, for which OpenCL is originally designed. Tuning the OpenCL code for good performance on FPGAs is still an open problem, since the existing OpenCL tools and models designed for CPUs/GPUs are not directly applicable to FPGAs. In the paper, we present an FPGA-based performance analysis framework that can shed light on the performance bottleneck and thus guide the code tuning for OpenCL applications on FPGAs. Particularly, we leverage static and dynamic analysis to develop an analytical performance model, which has captured the key architectural features of FPGA abstractions under OpenCL. Then, we provide four programmer-interpretable metrics to quantify the performance potentials of the OpenCL program with input optimization combination for the next optimization step. We evaluate our framework with a number of user cases, and demonstrate that 1) our analytical performance model can accurately predict the performance of OpenCL programs with different optimization combinations on FPGAs, and 2) our tool can be used to effectively guide the code tuning on alleviating the performance bottleneck.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446058","","Field programmable gate arrays;Optimization;Kernel;Pipelines;Analytical models;Performance analysis;Measurement","","66","","33","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"HRL: Efficient and flexible reconfigurable logic for near-data processing","M. Gao; C. Kozyrakis",Stanford University; Stanford University,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","126","137","The energy constraints due to the end of Dennard scaling, the popularity of in-memory analytics, and the advances in 3D integration technology have led to renewed interest in near-data processing (NDP) architectures that move processing closer to main memory. Due to the limited power and area budgets of the logic layer, the NDP compute units should be area and energy efficient while providing sufficient compute capability to match the high bandwidth of vertical memory channels. They should also be flexible to accommodate a wide range of applications. Towards this goal, NDP units based on fine-grained (FPGA) and coarse-grained (CGRA) reconfigurable logic have been proposed as a compromise between the efficiency of custom engines and the flexibility of programmable cores. Unfortunately, FPGAs incur significant area overheads for bit-level reconfiguration, while CGRAs consume significant power in the interconnect and are inefficient for irregular data layouts and control flows. This paper presents Heterogeneous Reconfigurable Logic (HRL), a reconfigurable array for NDP systems that improves on both FPGA and CGRA arrays. HRL combines both coarse-grained and fine-grained logic blocks, separates routing networks for data and control signals, and uses specialized units to effectively support branch operations and irregular data layouts in analytics workloads. HRL has the power efficiency of FPGA and the area efficiency of CGRA. It improves performance per Watt by 2.2x over FPGA and 1.7x over CGRA. For NDP systems running MapReduce, graph processing, and deep neural networks, HRL achieves 92% of the peak performance of an NDP system based on custom accelerators for each application.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446059","","Field programmable gate arrays;Arrays;Three-dimensional displays;Bandwidth;Random access memory;Layout","","153","1","60","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Software transparent dynamic binary translation for coarse-grain reconfigurable architectures","M. A. Watkins; T. Nowatzki; A. Carno","Lafayette College, Easton, PA; Univ. of Wisconsin-Madison, Madison, WI; Virginia Tech, Blacksburg, VA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","138","150","The end of Dennard Scaling has forced architects to focus on designing for execution efficiency. Course-grained reconfigurable architectures (CGRAs) are a class of architectures that provide a configurable grouping of functional units that aim to bridge the gap between the power and performance of custom hardware and the flexibility of software. Despite their potential benefit, CGRAs face a major adoption challenge as they do not execute a standard instruction stream. Dynamic translation for CGRAs has the potential to solve this problem, but faces non-trivial challenges. Existing attempts either do not achieve the full power and performance potential CGRAs offer or suffer from excessive translation time. In this work we propose DORA, a Dynamic Optimizer for Reconfigurable Architectures, which achieves substantial (2X) power and performance improvements while having low hardware and insertion overhead and benefiting the current execution. In addition to traditional optimizations, DORA leverages dynamic register information to perform optimizations not available to compilers and achieves performance similar to or better than CGRA-targeted compiled code.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446060","","Optimization;Computer architecture;Hardware;Software;Coprocessors;Radiation detectors;Monitoring","","23","","50","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Core tunneling: Variation-aware voltage noise mitigation in GPUs","R. Thomas; K. Barber; N. Sedaghati; L. Zhou; R. Teodorescu","Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","151","162","Voltage noise and manufacturing process variation represent significant reliability challenges for modern microprocessors. Voltage noise is caused by rapid changes in processor activity that can lead to timing violations and errors. Process variation is caused by manufacturing challenges in low-nanometer technologies and can lead to significant heterogeneity in performance and reliability across the chip. To ensure correct execution under worst-case conditions, chip designers generally add operating margins that are often unnecessarily conservative for most use cases, which results in wasted energy. This paper investigates the combined effects of process variation and voltage noise on modern GPU architectures. A distributed power delivery and process variation model at functional unit granularity was developed and used to simulate supply voltage behavior in a multicore GPU system. We observed that, just like in CPUs, large changes in power demand can lead to significant voltage droops. We also note that process variation makes some cores much more vulnerable to noise than others in the same GPU. Therefore, protecting the chip against large voltage droops by using fixed and uniform voltage guardbands is costly and inefficient. This paper presents core tunneling, a variation-aware solution for dynamically reducing voltage margins. The system relies on hardware critical path monitors to detect voltage noise conditions and quickly reacts by clock-gating vulnerable cores to prevent timing violations. This allows a substantial reduction in voltage margins. Since clock gating is enabled infrequently and only on the most vulnerable cores, the performance impact of core tunneling is very low. On average, core tunneling reduces energy consumption by 15%.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446061","","Graphics processing units;Tunneling;Histograms;Power demand;Monitoring;Delays;Kernel","","26","2","39","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Warped-preexecution: A GPU pre-execution approach for improving latency hiding","K. Kim; S. Lee; M. K. Yoon; G. Koo; W. W. Ro; M. Annavaram","School of Electrical and Electronic Engineering, Yonsei University; School of Electrical and Electronic Engineering, Yonsei University; School of Electrical and Electronic Engineering, Yonsei University; University of Southern California, Los Angeles, CA, US; School of Electrical and Electronic Engineering, Yonsei University; Ming Hsieh Department of Electrical Engineering, University of Southern California",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","163","175","This paper presents a pre-execution approach for improving GPU performance, called P-mode (pre-execution mode). GPUs utilize a number of concurrent threads for hiding processing delay of operations. However, certain long-latency operations such as off-chip memory accesses often take hundreds of cycles and hence leads to stalls even in the presence of thread concurrency and fast thread switching capability. It is unclear if adding more threads can improve latency tolerance due to increased memory contention. Further, adding more threads increases on-chip storage demands. Instead we propose that when a warp is stalled on a long-latency operation it enters P-mode. In P-mode, a warp continues to fetch and decode successive instructions to identify any independent instruction that is not on the long latency dependence chain. These independent instructions are then pre-executed. To tackle write-after-write and write-after-read hazards, during P-mode output values are written to renamed physical registers. We exploit the register file underutilization to re-purpose a few unused registers to store the P-mode results. When a warp is switched from P-mode to normal execution mode it reuses pre-executed results by reading the renamed registers. Any global load operation in P-mode is transformed into a pre-load which fetches data into the L1 cache to reduce future memory access penalties. Our evaluation results show 23% performance improvement for memory intensive applications, without negatively impacting other application categories.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446062","","Registers;Graphics processing units;Instruction sets;Hazards;Pipelines;Memory management;Switches","","38","","46","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Approximating warps with intra-warp operand value similarity","D. Wong; N. S. Kim; M. Annavaram","University of California, Riverside; University of Illinois, Urbana-Champaign; University of Southern California",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","176","187","Value locality, the recurrence of a previously-seen value, has been the enabler of myriad optimization techniques in traditional processors. Value similarity relaxes the constraint of value locality by allowing values to differ in the lowest significant bits where values are micro-architecturally near. With the end of Dennard Scaling and the turn towards massively parallel accelerators, we revisit value similarity in the context of GPUs. We identify a form of value similarity called intra-warp operand value similarity, which is abundant in GPUs. We present Warp Approximation, which leverages intra-warp operand value similarity to trade off accuracy for energy. Warp Approximation dynamically identifies intra-warp operand value similarity in hardware, and executes a single representative thread on behalf of all the active threads in a warp, thereby producing a representative value with approximate value locality. This representative value can then be stored compactly in the register file as a value similar scalar, reducing the read and write energy when dealing with approximate data. With Warp Approximation, we can reduce execution unit energy by 37%, register file energy by 28%, and improve overall GPGPU energy efficiency by 26% with minimal quality degradation.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446063","","Registers;Graphics processing units;Hardware;Writing;Computers;Microarchitecture","","35","","41","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A case for toggle-aware compression for GPU systems","G. Pekhimenko; E. Bolotin; N. Vijaykumar; O. Mutlu; T. C. Mowry; S. W. Keckler","Carnegie Mellon University; NVIDIA; Carnegie Mellon University, Pittsburgh, PA, US; Carnegie Mellon University, Pittsburgh, PA, US; Carnegie Mellon University; Carnegie Mellon University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","188","200","Data compression can be an effective method to achieve higher system performance and energy efficiency in modern data-intensive applications by exploiting redundancy and data similarity. Prior works have studied a variety of data compression techniques to improve both capacity (e.g., of caches and main memory) and bandwidth utilization (e.g., of the on-chip and off-chip interconnects). In this paper, we make a new observation about the energy-efficiency of communication when compression is applied. While compression reduces the amount of transferred data, it leads to a substantial increase in the number of bit toggles (i.e., communication channel switchings from 0 to 1 or from 1 to 0). The increased toggle count increases the dynamic energy consumed by on-chip and off-chip buses due to more frequent charging and discharging of the wires. Our results show that the total bit toggle count can increase from 20% to 2.2x when compression is applied for some compression algorithms, averaged across different application suites. We characterize and demonstrate this new problem across 242 GPU applications and six different compression algorithms. To mitigate the problem, we propose two new toggle-aware compression techniques: Energy Control and Metadata Consolidation. These techniques greatly reduce the bit toggle count impact of the data compression algorithms we examine, while keeping most of their bandwidth reduction benefits.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446064","","Bandwidth;Data compression;System-on-chip;Graphics processing units;Compression algorithms;Data transfer;Random access memory","","57","2","76","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Minimal disturbance placement and promotion","E. Teran; Y. Tian; Z. Wang; D. A. Jiménez","Texas A&M University; Advanced Micro Devices, Inc.; Intel Labs; Texas A&M University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","201","211","Cache replacement policies often order blocks into distinct positions. A block is placed into a set in some initial position. A re-referenced block is promoted into a higher position while other blocks may move into lower positions. A block in the lowest position is a candidate for replacement. Tree-based PseudoLRU is a well-known space-efficient replacement policy based on representing block positions as distinct paths in a binary tree. We find that a placement or promotion for one block often needlessly disturbs the non-promoted blocks. Guided by the principle of minimal disturbance, i.e. that a policy should seek to disturb the order of non-promoted blocks to the smallest extent possible, we develop a simple modification to PseudoLRU resulting in a policy that improves performance over previous techniques while retaining the low cost of PseudoLRU. The result is a minimal disturbance placement and promotion (MDPP) policy. We first give a static formulation of MDPP and show that it provides superior performance to LRU, PseudoLRU and matches performance for SRRIP for both single-threaded and multi-core workloads. We then give a dynamic formulation that uses dead block prediction for placement and bypass and show that it meets or exceeds state-of-the-art performance with lower overhead. For single-threaded workloads, dynamic MDPP matches the 5.9% speedup over LRU of the state-of-the-art policy SHiP. For multi-core workloads, dynamic MDPP gives a normalized weighted speedup of 14.3% over LRU, compared with SHiP that yields a speedup of 12.3% over LRU and requires double the storage overhead per set. We show that minimal disturbance policies can reduce the frequency of a costly read-modify-write cycle for replacement state, making them potentially suitable for future work in DRAM caches.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446065","","Radiation detectors;Binary trees;Marine vehicles;Electronics packaging;Neural networks;Genetic algorithms;Random access memory","","14","","27","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Revisiting virtual L1 caches: A practical design using dynamic synonym remapping","H. Yoon; G. S. Sohi","Computer Sciences Department, University of Wisconsin-Madison; Computer Sciences Department, University of Wisconsin-Madison",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","212","224","Virtual caches have potentially lower access latency and energy consumption than physical caches because they do not consult the TLB prior to cache access. However, they have not been popular in commercial designs. The crux of the problem is the possibility of synonyms. This paper makes several empirical observations about the temporal characteristics of synonyms, especially in caches of sizes that are typical of L1 caches. By leveraging these observations, the paper proposes a practical design of an L1 virtual cache that (1) dynamically decides a unique virtual page number for all the synonymous virtual pages that map to the same physical page and (2) uses this unique page number to place and look up data in the virtual caches. Accesses to this unique page number proceed without any intervention. Accesses to other synonymous pages are dynamically detected, and remapped to the corresponding unique virtual page number to correctly access data in the cache. Such remapping operations are rare, due to the temporal properties of synonyms, allowing a Virtual Cache with Dynamic Synonym Remapping (VC-DSR) to achieve most of the benefits of virtual caches but without software involvement. Experimental results based on real world applications show that VC-DSR can achieve about 92% of the dynamic energy savings for TLB lookups, and 99.4% of the latency benefits of ideal (but impractical) virtual caches for the configurations considered.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446066","","Computers;Software;Organizations;Energy consumption;Microarchitecture;Hardware;Coherence","","15","24","55","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Modeling cache performance beyond LRU","N. Beckmann; D. Sanchez",Massachusetts Institute of Technology; Massachusetts Institute of Technology,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","225","236","Modern processors use high-performance cache replacement policies that outperform traditional alternatives like least-recently used (LRU). Unfortunately, current cache models do not capture these high-performance policies as most use stack distances, which are inherently tied to LRU or its variants. Accurate predictions of cache performance enable many optimizations in multicore systems. For example, cache partitioning uses these predictions to divide capacity among applications in order to maximize performance, guarantee quality of service, or achieve other system objectives. Without an accurate model for high-performance replacement policies, these optimizations are unavailable to modern processors. We present a new probabilistic cache model designed for high-performance replacement policies. It uses absolute reuse distances instead of stack distances, and models replacement policies as abstract ranking functions. These innovations let us model arbitrary age-based replacement policies. Our model achieves median error of less than 1% across several high-performance policies on both synthetic and SPEC CPU2006 benchmarks. Finally, we present a case study showing how to use the model to improve shared cache performance.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446067","","Analytical models;Program processors;Optimization;Electronics packaging;Organizations;Computational modeling","","32","2","57","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Efficient footprint caching for Tagless DRAM Caches","H. Jang; Y. Lee; J. Kim; Y. Kim; J. Kim; J. Jeong; J. W. Lee","Sungkyunkwan University, Suwon, Korea; Sungkyunkwan University, Suwon, Korea; Sungkyunkwan University, Suwon, Korea; POSTECH, Pohang, Korea; POSTECH, Pohang, Korea; Sungkyunkwan University, Suwon, Korea; Sungkyunkwan University, Suwon, Korea",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","237","248","Efficient cache tag management is a primary design objective for large, in-package DRAM caches. Recently, Tagless DRAM Caches (TDCs) have been proposed to completely eliminate tagging structures from both on-die SRAM and in-package DRAM, which are a major scalability bottleneck for future multi-gigabyte DRAM caches. However, TDC imposes a constraint on DRAM cache block size to be the same as OS page size (e.g., 4KB) as it takes a unified approach to address translation and cache tag management. Caching at a page granularity, or page-based caching, incurs significant off-package DRAM bandwidth waste by over-fetching blocks within a page that are not actually used. Footprint caching is an effective solution to this problem, which fetches only those blocks that will likely be touched during the page's lifetime in the DRAM cache, referred to as the page's footprint. In this paper we demonstrate TDC opens up unique opportunities to realize efficient footprint caching with higher prediction accuracy and a lower hardware cost than the original footprint caching scheme. Since there are no cache tags in TDC, the footprints of cached pages are tracked at TLB, instead of cache tag array, to incur much lower on-die storage overhead than the original design. Besides, when a cached page is evicted, its footprint will be stored in the corresponding page table entry, instead of an auxiliary on-die structure (i.e., Footprint History Table), to prevent footprint thrashing among different pages, thus yielding higher accuracy in footprint prediction. The resulting design, called Footprint-augmented Tagless DRAM Cache (F-TDC), significantly improves the bandwidth efficiency of TDC, and hence its performance and energy efficiency. Our evaluation with 3D Through-Silicon-Via-based in-package DRAM demonstrates an average reduction of off-package bandwidth by 32.0%, which, in turn, improves IPC and EDP by 17.7% and 25.4%, respectively, over the state-of-the-art TDC with no footprint caching.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446068","","Random access memory;Bandwidth;Resource management;Tagging;History;Scalability;Three-dimensional displays","","40","","35","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"SCsafe: Logging sequential consistency violations continuously and precisely","Y. Duan; D. Koufaty; J. Torrellas",University of Illinois at Urbana-Champaign; Intel labs; University of Illinois at Urbana-Champaign,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","249","260","Sequential Consistency Violations (SCV) in relaxed consistency machines cause programs to malfunction and are hard to debug. While there are proposals for detecting and recording SCVs, they are limited in that they end program execution after detecting the first SCV because the program is now non-SC. Therefore, they cannot be used in production runs. In addition, such proposals rely on complicated hardware. To address these problems, this paper proposes the first architecture that detects and logs SCVs in a continuous manner, while retaining SC. In addition, the scheme is precise and uses substantially simpler hardware. The scheme, called SCsafe, operates continously because, after SCV detection and logging, it recovers and resumes execution while retaining SC. As a result, it can be used in production runs. In addition, SCsafe is precise in that it identifies only true SCVs - rather than dependence cycles due to false sharing. Finally, SCsafe's hardware is mostly local to each processor, and uses known recovery techniques. We evaluate SCsafe using simulations of 16-processor multicores with Total Store Order or Release Consistency. In codes with SCVs, SCsafe detects and reports SCVs while enforcing SC during the execution. In codes with few SCVs, it adds a negligible performance overhead. Finally, SCsafe is scalable with the processor count.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446069","","Hardware;Production;Load modeling;Coherence;Proposals;Debugging","","10","","34","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"LASER: Light, Accurate Sharing dEtection and Repair","L. Luo; A. Sriraman; B. Fugate; S. Hu; G. Pokam; C. J. Newburn; J. Devietti",University of Washington; University of Michigan; University of Pennsylvania; Intel Corporation; Intel Corporation; Intel Corporation; University of Pennsylvania,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","261","273","Contention for shared memory, in the forms of true sharing and false sharing, is a challenging performance bug to discover and to repair. Understanding cache contention requires global knowledge of the program's actual sharing behavior, and can even arise invisibly in the program due to the opaque decisions of the memory allocator. Previous schemes have focused only on false sharing, and impose significant performance penalties or require non-trivial alterations to the operating system or runtime system environment. This paper presents the Light, Accurate Sharing dEtection and Repair (LASER) system, which leverages new performance counter capabilities available on Intel's Haswell architecture that identify the source of expensive cache coherence events. Using records of these events generated by the hardware, we build a system for online contention detection and repair that operates with low performance overhead and does not require any invasive program, compiler or operating system changes. Our experiments show that LASER imposes just 2% average runtime overhead on the Phoenix, Parsec and Splash2x benchmarks. LASER can automatically improve the performance of programs by up to 19% on commodity hardware.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446070","","Maintenance engineering;Coherence;Hardware;Benchmark testing;Radiation detectors;Instruction sets","","20","","35","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Efficient GPU hardware transactional memory through early conflict resolution","S. Chen; L. Peng","Division of Electrical & Computer Engineering, Louisiana State University; Louisiana State University Paul M Hebert Law Center, Baton Rouge, LA, US",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","274","284","It has been proposed that Transactional Memory be added to Graphics Processing Units (GPUs) in recent years. One proposed hardware design, Warp TM, can scale to 1000s of concurrent transactions. As a programming method that can atomicize an arbitrary number of memory access locations and greatly reduce the efforts to program parallel applications, transactional memory handles the complexity of inter-thread synchronization. However, when thousands of transactions run concurrently on a GPU, conflicts and resource contentions arise, causing performance loss. In this paper, we identify and analyze the cause of conflicts and contentions and propose two enhancements that try to resolve conflicts early: (1) Early-Abort global conflict resolution that allows conflicts to be detected before they reach the Commit Units so that contention in the Commit Units is reduced and (2) Pause-and-Go execution scheme that reduces the chance of conflict and the performance penalty of re-executing long transactions. These two enhancements are enabled by a single hardware modification. Our evaluation shows the combination of the two enhancements greatly improves overall execution speed while reducing energy consumption.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446071","","Graphics processing units;Hardware;Spatial resolution;Instruction sets;Programming;Hazards;Partitioning algorithms","","15","","24","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"PleaseTM: Enabling transaction conflict management in requester-wins hardware transactional memory","S. Park; M. Prvulovic; C. J. Hughes",Georgia Institute of Technology; Georgia Institute of Technology; Intel,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","285","296","With recent commercial offerings, hardware transactional memory (HTM) has finally become an important tool in writing multithreaded applications. However, current offerings are commonly implemented in a way that keep the coherence protocol unmodified. Data conflicts are recognized by coherence messages sent by the requester to sharers of the cache block (e.g., a write to a speculatively read line), who are then aborted. This tends to abort transactions that have done more work, leading to suboptimal performance. Even worse, this can lead to live-lock situations where transactions repeatedly abort each other. In this paper, we present PleaseTM, a mechanism that allows more freedom in deciding which transaction to abort, while leaving the coherence protocol design unchanged. In PleaseTM, transactions insert plea bits into their responses to coherence requests as a simple payload, and use these bits to inform conflict management decisions. Coherence permission changes are then achieved with normal coherence requests. Our experiments show that this additional freedom can provide on average 43% speedup, with a maximum of 7-fold speedup, on STAMP benchmarks running at 32 threads compared to requester-wins HTM.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446072","","Coherence;Protocols;Message systems;Hardware;Proposals;Memory management;Benchmark testing","","11","","33","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Efficient synthetic traffic models for large, complex SoCs","J. Yin; O. Kayiran; M. Poremba; N. E. Jerger; G. H. Loh","Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; University of Toronto; Advanced Micro Devices, Inc.",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","297","308","The interconnect or network on chip (NoC) is an increasingly important component in processors. As systems scale up in size and functionality, the ability to efficiently model larger and more complex NoCs becomes increasingly important to the design and evaluation of such systems. Recent work proposed the ""SynFull"" methodology that performs statistical analysis of a workload's NoC traffic to create compact traffic generators based on Markov models. While the models generate synthetic traffic, the traffic is statistically similar to the original trace and can be used for fast NoC simulation. However, the original SynFull work only evaluated multi-core CPU scenarios with a very simple cache coherence protocol (MESI). We find the original SynFull methodology to be insufficient when modeling the NoC of a more complex system on a chip (SoC). We identify and analyze the shortcomings of SynFull in the context of a SoC consisting of a heterogeneous architecture (CPU and GPU), a more complex cache hierarchy including support for full coherence between CPU, GPU, and shared caches, and heterogeneous workloads. We introduce new techniques to address these shortcomings. Furthermore, the original SynFull methodology can only model a NoC with N nodes when the original application analysis is performed on an identically-sized N-node system, but one typically wants to model larger future systems. Therefore, we introduce new techniques to enable SynFull-like analysis to be extrapolated to model such larger systems. Finally, we present a novel synthetic memory reference model to replace SynFull's fixed latency model; this allows more realistic evaluation of the memory subsystem and its interaction with the NoC. The result is a robust NoC simulation methodology that works for large, heterogeneous SoC architectures.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446073","","Graphics processing units;Protocols;Solid modeling;Central Processing Unit;Coherence;Analytical models;Computational modeling","","12","","43","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"DVFS for NoCs in CMPs: A thread voting approach","Y. Yao; Z. Lu","KTH Royal Institute of Technoloqy, Stockholm, Sweden; KTH Royal Institute of Technoloqy, Stockholm, Sweden",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","309","320","As the core count grows rapidly, dynamic voltage/frequency scaling (DVFS) in networks-on-chip (NoCs) becomes critical in optimizing energy efficacy in chip multiprocessors (CMPs). Previously proposed techniques often exploit inherent network-level metrics to do so. However, such network metrics may contradictorily reflect application's performance need, leading to power over/under provisioning. We propose a novel on-chip DVFS technique for NoCs that is able to adjust per-region V/F level according to voted V/F levels of communicating threads. Each region is composed of a few adjacent routers sharing the same V/F level. With a voting-based approach, threads seek to influence the DVFS decisions independently by voting for a preferred V/F level that best suits their own performance interest according to their runtime profiled message generation rate and data sharing characteristics. The vote expressed in a few bits is then carried in the packet header and spread to the routers on the packet route. The final DVFS decision is made democratically by a region DVFS controller based on the majority election result of collected votes from all active threads. To achieve scalable V/F adjustment, each region works independently, and the voting-based V/F tuning forms a distributed decision making process. We evaluate our technique with detailed simulations of a 64-core CMP running a variety of multi-threaded PARSEC benchmarks. Compared with a network without DVFS and a network metric (router buffer occupancy) based approach, experimental results show that our voting based DVFS mechanism improves the network energy efficacy measured in MPPJ (million packets per joule) by about 17.9% and 9.7% on average, respectively, and the system energy efficacy measured in MIPJ (million instructions per joule) by about 26.3% and 17.1% on average, respectively.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446074","","Message systems;Nickel;System-on-chip;Tuning;Delays;Benchmark testing","","17","","23","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"SLaC: Stage laser control for a flattened butterfly network","Y. Demir; N. Hardavellas","Intel Corporation, Portland, OR, USA; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","321","332","Photonic interconnects have emerged as a promising candidate technology for high-performance energy-efficient on-chip, on-board, and datacenter-scale interconnects. However, the high optical loss of many nanophotonic components coupled with the low efficiency of current laser sources result in exceedingly high total power requirements for the laser. As optical interconnects stay on even during periods of system inactivity, most of this power is wasted, which has prompted research on laser gating. Unfortunately, prior work on laser gating has only focused on low-scalability on-chip photonic interconnects (photonic crossbars), and disrupts the connectivity of the network which renders a high-performance implementation challenging. In this paper we propose SLaC, a laser gating technique that turns on and off redundant paths in a photonic flattened-butterfly network to save laser energy while maintaining high performance and full connectivity. Maintaining full connectivity removes the laser turn-on latency from the critical path and results in minimal performance degradation. SLaC is equally applicable to on-chip, on-board, and datacenter level interconnects. For on-chip and multi-chip applications, SLaC saves up to 67% of the laser energy (43-57% on average) when running real-world workloads. On a datacenter network, SLaC saves 79% of the laser energy on average when running traffic traces collected from university datacenter servers.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446075","","Photonics;Optical waveguides;System-on-chip;Layout;Waveguide lasers;Energy efficiency;Delays","","16","","46","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"The runahead network-on-chip","Z. Li; J. S. Miguel; N. E. Jerger",University of Toronto; University of Toronto; University of Toronto,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","333","344","With increasing core counts and higher memory demands from applications, it is imperative that networks-on-chip (NoCs) provide low-latency, power-efficient communication. Conventional NoCs tend to be over-provisioned for worst-case bandwidth demands leading to ineffective use of network resources and significant power inefficiency; average channel utilization is typically less than 5% in real-world applications. In terms of performance, low-latency techniques often introduce power and area overheads and incur significant complexity in the router microarchitecture. We find that both low latency and power efficiency are possible by relaxing the constraint of lossless communication. This is inspired from internetworking where best effort delivery is commonplace. We propose the Runahead NoC, a lightweight, lossy network that provides single-cycle hops. Allowing for lossy delivery enables an extremely simple bufferless router microarchitecture that performs routing and arbitration within the same cycle as link traversal. The Runahead NoC operates either as a power-saver that is integrated into an existing conventional NoC to improve power efficiency, or as an accelerator that is added on top to provide ultra-low latency communication for select packets. On a range of PAR-SEC and SPLASH-2 workloads, we find that the Runahead NoC reduces power consumption by 1.81 as a power-saver and improves runtime and packet latency by 1.08× and 1.66× as an accelerator.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446076","","Ports (Computers);Routing;Complexity theory;Power demand;Turning;System-on-chip;Bandwidth","","25","","46","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Towards high performance paged memory for GPUs","T. Zheng; D. Nellans; A. Zulfiqar; M. Stephenson; S. W. Keckler",NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","345","357","Despite industrial investment in both on-die GPUs and next generation interconnects, the highest performing parallel accelerators shipping today continue to be discrete GPUs. Connected via PCIe, these GPUs utilize their own privately managed physical memory that is optimized for high bandwidth. These separate memories force GPU programmers to manage the movement of data between the CPU and GPU, in addition to the on-chip GPU memory hierarchy. To simplify this process, GPU vendors are developing software runtimes that automatically page memory in and out of the GPU on-demand, reducing programmer effort and enabling computation across datasets that exceed the GPU memory capacity. Because this memory migration occurs over a high latency and low bandwidth link (compared to GPU memory), these software runtimes may result in significant performance penalties. In this work, we explore the features needed in GPU hardware and software to close the performance gap of GPU paged memory versus legacy programmer directed memory management. Without modifying the GPU execution pipeline, we show it is possible to largely hide the performance overheads of GPU paged memory, converting an average 2× slowdown into a 12% speedup when compared to programmer directed transfers. Additionally, we examine the performance impact that GPU memory oversubscription has on application run times, enabling application designers to make informed decisions on how to shard their datasets across hosts and GPU instances.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446077","","Graphics processing units;Kernel;Bandwidth;Runtime;Memory management;Programming","","95","1","50","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Simultaneous Multikernel GPU: Multi-tasking throughput processors via fine-grained sharing","Z. Wang; J. Yang; R. Melhem; B. Childers; Y. Zhang; M. Guo","Department of Coumuter Science, Shanghai Jiao Tong University, P.R. China; Electrical and Computer Engineering Department, Department of Coumuter Science; Department of Coumuter Science, Shanghai Jiao Tong University, P.R. China; Department of Coumuter Science, Shanghai Jiao Tong University, P.R. China; Department of Coumuter Science, University of Pittsburgh, U.S.A.; Electrical and Computer Engineering Department, Department of Coumuter Science",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","358","369","Studies show that non-graphics programs can be less optimized for the GPU hardware, leading to significant resource under-utilization. Sharing the GPU among multiple programs can effectively improve utilization, which is particularly attractive to systems where many applications require access to the GPU (e.g., cloud computing). However, current GPUs lack proper architecture features to support sharing. Initial attempts are preliminary: They either provide only static sharing, which requires recompilation or code transformation, or they do not effectively improve GPU resource utilization. We propose Simultaneous Multikernel (SMK), a fine-grain dynamic sharing mechanism, that fully utilizes resources within a streaming multiprocessor by exploiting heterogeneity of different kernels. We propose several resource allocation strategies to improve system throughput while maintaining fairness. Our evaluation shows that for shared workloads with complementary resource occupancy, SMK improves GPU throughput by 52% over non-shared execution and 17% over a state-of-the-art design.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446078","","Kernel;Graphics processing units;Context;Instruction sets;Switches;Throughput;Hardware","","132","","27","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"iPAWS: Instruction-issue pattern-based adaptive warp scheduling for GPGPUs","M. Lee; G. Kim; J. Kim; W. Seo; Y. Cho; S. Ryu","KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; Samsung Electronics, Giheung, Korea; Samsung Electronics, Giheung, Korea; Samsung Electronics, Giheung, Korea",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","370","381","Thread or warp scheduling in GPGPUs has been shown to have a significant impact on overall performance. Recently proposed warp schedulers have been based on a greedy warp scheduler where some warps are prioritized over other warps. However, a single warp scheduling policy does not necessarily provide good performance across all types of workloads; in particular, we show that greedy warp schedulers are not necessarily optimal for workloads with inter-warp locality while a simple round-robin warp scheduler provides better performance. Thus, we argue that instead of single, static warp scheduling, an adaptive warp scheduler that dynamically changes the warp scheduler based on the workload characteristics should be leveraged. In this work, we propose an instruction-issue pattern-based adaptive warp scheduler (iPAWS) that dynamically adapts between a greedy warp scheduler and a fair, round-robin scheduler. We exploit the observation that workloads that favor a greedy warp scheduler will have an instruction-issue pattern that is biased towards some warps while workloads that favor a fair, round-robin warp scheduler will tend to issue instructions across all of the warps. Our evaluations show that iPAWS is able to adapt to the more optimal warp scheduler dynamically and achieve performance that is within a few percent of the statically determined, more optimal warp scheduler. We also show that iPAWS can be extended to other warp schedulers, including the cache-conscious wavefront scheduling (CCWS) and Memory Aware Scheduling and Cache Access Re-execution (MASCAR) to exploit the benefits of other warp schedulers while still providing adaptivity in warp scheduling.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446079","","Radio frequency;Computational fluid dynamics;Adaptive scheduling;Instruction sets;Computer architecture;Random access memory;Processor scheduling","","22","","26","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Lattice priority scheduling: Low-overhead timing-channel protection for a shared memory controller","A. Ferraiuolo; Y. Wang; D. Zhang; A. C. Myers; G. E. Suh","Cornell University, Ithaca, NY, USA; Cornell University, Ithaca, NY, USA; Penn State University, University Park, PA; Cornell University, Ithaca, NY, USA; Cornell University, Ithaca, NY, USA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","382","393","Computer hardware is increasingly shared by distrusting parties in platforms such as commercial clouds and web servers. Though hardware sharing is critical for performance and efficiency, this sharing creates timing-channel vulnerabilities in hardware components such as memory controllers and shared memory. Past work on timing-channel protection for memory controllers assumes all parties are mutually distrusting and require timing-channel protection. This assumption limits the capability of the memory controller to allocate resources effectively, and causes severe performance penalties. Further, the assumption that all entities are mutually distrusting is often a poor fit for the security needs of real systems. Often, some entities do not require timing-channel protection or trust others with information. We propose lattice priority scheduling (LPS), a secure memory scheduling algorithm that improves performance by more precisely meeting the target system's security requirements, expressed as a lattice policy. We evaluate LPS in a simulated 8-core microprocessor. Compared to prior solutions [34], lattice priority scheduling improves system throughput by over 30% on average and by up to 84% for some workloads.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446080","","Timing;Security;Lattices;Throughput;Processor scheduling;Hardware;Schedules","","14","1","44","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A complete key recovery timing attack on a GPU","Z. H. Jiang; Y. Fei; D. Kaeli","Electrical & Computer Engineering Department, Northeastern University Boston, MA, USA; Electrical & Computer Engineering Department, Northeastern University Boston, MA, USA; Electrical & Computer Engineering Department, Northeastern University Boston, MA, USA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","394","405","Graphics Processing Units (GPUs) have become mainstream parallel computing devices. They are deployed on diverse platforms, and an increasing number of applications have been moved to GPUs to exploit their massive parallel computational resources. GPUs are starting to be used for security services, where high-volume data is encrypted to ensure integrity and confidentiality. However, the security of GPUs has only begun to receive attention. Issues such as side-channel vulnerability have not been addressed. The goal of this paper is to evaluate the side-channel security of GPUs and demonstrate a complete AES (Advanced Encryption Standard) key recovery using known ciphertext through a timing channel. To the best of our knowledge, this is the first work that clearly demonstrates the vulnerability of a commercial GPU architecture to side-channel timing attacks. Specifically, for AES-128, we have been able to recover all key bytes utilizing a timing side channel in under 30 minutes.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446081","","Graphics processing units;Timing;Encryption;Kernel;Message systems","","84","","31","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"CATalyst: Defeating last-level cache side channel attacks in cloud computing","F. Liu; Q. Ge; Y. Yarom; F. Mckeen; C. Rozas; G. Heiser; R. B. Lee","Department of Electrical Engineering, Princeton University; NICTA; School of Computer Science, The University of Adelaide; Intel Labs; Intel Labs; NICTA; Department of Electrical Engineering, Princeton University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","406","418","Cache side channel attacks are serious threats to multi-tenant public cloud platforms. Past work showed how secret information in one virtual machine (VM) can be extracted by another co-resident VM using such attacks. Recent research demonstrated the feasibility of high-bandwidth, low-noise side channel attacks on the last-level cache (LLC), which is shared by all the cores in the processor package, enabling attacks even when VMs are scheduled on different cores. This paper shows how such LLC side channel attacks can be defeated using a performance optimization feature recently introduced in commodity processors. Since most cloud servers use Intel processors, we show how the Intel Cache Allocation Technology (CAT) can be used to provide a system-level protection mechanism to defend from side channel attacks on the shared LLC. CAT is a way-based hardware cache-partitioning mechanism for enforcing quality-of-service with respect to LLC occupancy. However, it cannot be directly used to defeat cache side channel attacks due to the very limited number of partitions it provides. We present CATalyst, a pseudo-locking mechanism which uses CAT to partition the LLC into a hybrid hardware-software managed cache. We implement a proof-of-concept system using Xen and Linux running on a server with Intel processors, and show that LLC side channel attacks can be defeated. Furthermore, CATalyst only causes very small performance overhead when used for security, and has negligible impact on legacy applications.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446082","","Cloud computing;Hardware;Servers;Resource management;Electronic mail;Security","","231","3","46","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Predicting the memory bandwidth and optimal core allocations for multi-threaded applications on large-scale NUMA machines","W. Wang; J. W. Davidson; M. L. Soffa","Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","419","431","Modern NUMA platforms offer large numbers of cores to boost performance through parallelism and multi-threading. However, because performance scalability is limited by available memory bandwidth, the strategy of allocating all cores can result in degraded performance. Consequently, accurately predicting optimal (best performing) core allocations, and executing applications with these allocations are crucial for achieving the best performance. Previous research focused on the prediction of optimal numbers of cores. However, in this paper, we show that, because of the asymmetric NUMA memory configuration and the asymmetric application memory behavior, optimal core allocations are not merely optimal numbers of cores. Additionally, previous studies do not adequately consider NUMA memory resources, which further limits their ability to accurately predict optimal core allocations. In this paper, we present a model, NuCore, which predicts both memory bandwidth usage and optimal core allocations. NuCore considers various memory resources and NUMA asymmetry, and employs Integer Programming to achieve high accuracy and low overhead. Experimental results from real NUMA machines show that the core allocations predicted by NuCore provide 1.27x average speedup over using all cores with only 75.6% cores allocated. NuCore also provides 1.18x and 1.21x average speedups over two state-of-the-art techniques. Our results also show that NuCore faithfully models NUMA memory systems and predicts memory bandwidth usages with only 10% average error.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446083","","Resource management;Bandwidth;Instruction sets;Benchmark testing;Scalability;Random access memory;Linear programming","","35","","60","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A market approach for handling power emergencies in multi-tenant data center","M. A. Islam; X. Ren; S. Ren; A. Wierman; X. Wang","University of California Riverside; California Institute of Technology, Pasadena, CA, US; University of California Riverside; California Institute of Technology; The Ohio State University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","432","443","Power oversubscription in data centers may occasionally trigger an emergency when the aggregate power demand exceeds the capacity. Handling such an emergency requires a graceful power capping solution that minimizes the performance loss. In this paper, we study power capping in a multi-tenant data center where the operator supplies power to multiple tenants that manage their own servers. Unlike owner-operated data centers, the operator lacks control over tenants' servers. To address this challenge, we propose a novel market mechanism based on supply function bidding, called COOP, to financially incentivize and coordinate tenants' power reduction for minimizing total performance loss (quantified in performance cost) while satisfying multiple power capping constraints. We build a prototype to show that COOP is efficient in terms of minimizing the total performance cost, even compared to the ideal but infeasible case that assumes the operator has full control over tenants' servers. We also demonstrate that COOP is ""win-win"", increasing the operator's profit (through oversubscription) and reducing tenants' cost (through financial compensation for their power reduction during emergencies).","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446084","","Servers;Power demand;Uninterruptible power systems;Aggregates;Cooling;Partial discharges;Power measurement","","35","1","50","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"SizeCap: Efficiently handling power surges in fuel cell powered data centers","Y. Li; D. Wang; S. Ghose; J. Liu; S. Govindan; S. James; E. Peterson; J. Siegler; R. Ausavarungnirun; O. Mutlu",Carnegie Mellon University; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Carnegie Mellon University; Carnegie Mellon University,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","444","456","Fuel cells are a promising power source for future data centers, offering high energy efficiency, low greenhouse gas emissions, and high reliability. However, due to mechanical limitations related to fuel delivery, fuel cells are slow to adjust to sudden increases in data center power demands, which can result in temporary power shortfalls. To mitigate the impact of power shortfalls, prior work has proposed to either perform power capping by throttling the servers, or to leverage energy storage devices (ESDs) that can temporarily provide enough power to make up for the shortfall while the fuel cells ramp up power generation. Both approaches have disadvantages: power capping conservatively limits server performance and can lead to service level agreement (SLA) violations, while ESD-only solutions must significantly overprovision the energy storage device capacity to tolerate the shortfalls caused by the worst-case (i.e., largest) power surges, which greatly increases the total cost of ownership (TCO). We propose SizeCap, the first ESD sizing framework for fuel cell powered data centers, which coordinates ESD sizing with power capping to enable a cost-effective solution to power shortfalls in data centers. SizeCap sizes the ESD just large enough to cover the majority of power surges, but not the worst-case surges that occur infrequently, to greatly reduce TCO. It then uses the smaller capacity ESD in conjunction with power capping to cover the power shortfalls caused by the worst-case power surges. As part of our new flexible framework, we propose multiple power capping policies with different degrees of awareness of fuel cell and workload behavior, and evaluate their impact on workload performance and ESD size. Using traces from Microsoft's production data center systems, we demonstrate that SizeCap significantly reduces the ESD size (by 85%ofor a workload with infrequent yet large power surges, and by 50% for a workload with frequent power surges) without violating any SLAs.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446085","","Fuel cells;Electrostatic discharges;Surges;Servers;Fuels;Power generation;Power demand","","12","","45","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"MaPU: A novel mathematical computing architecture","D. Wang; X. Du; L. Yin; C. Lin; H. Ma; W. Ren; H. Wang; X. Wang; S. Xie; L. Wang; Z. Liu; T. Wang; Z. Pu; G. Ding; M. Zhu; L. Yang; R. Guo; Z. Zhang; X. Lin; J. Hao; Y. Yang; W. Sun; F. Zhou; N. Xiao; Q. Cui; X. Wang","CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; Spreadtrum Comm, Inc.; Huawei Tech Co, Ltd.; Huawei Tech Co, Ltd.; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China; CASIA, Beijing, China",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","457","468","As the feature size of the semiconductor process is scaling down to 10nm and below, it is possible to assemble systems with high performance processors that can theoretically provide computational power of up to tens of PLOPS. However, the power consumption of these systems is also rocketing up to tens of millions watts, and the actual performance is only around 60% of the theoretical performance. Today, power efficiency and sustained performance have become the main foci of processor designers. Traditional computing architecture such as superscalar and GPGPU are proven to be power inefficient, and there is a big gap between the actual and peak performance. In this paper, we present the MaPU architecture, a novel architecture which is suitable for data-intensive computing with great power efficiency and sustained computation throughput. To achieve this goal, MaPU attempts to optimize the application from a system perspective, including the hardware, algorithm and corresponding program model. It uses an innovative multi-granularity parallel memory system with intrinsic shuffle ability, cascading pipelines with wide SIMD data paths and a state-machine-based program model. When executing typical signal processing algorithms, a single MaPU core implemented with a 40nm process exhibits a sustained performance of 134 GLOPS while consuming only 2.8 W in power, which increases the actual power efficiency by an order of magnitude comparable with the traditional CPU and GPGPU.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446086","","Computer architecture;Pipelines;Program processors;Kernel;Routing;Data models;Supercomputers","","17","","19","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Best-offset hardware prefetching","P. Michaud","Inria Campus de Beaulieu, Rennes, France",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","469","480","Hardware prefetching is an important feature of modern high-performance processors. When the application working set is too large to fit in on-chip caches, disabling hardware pre-fetchers may result in severe performance reduction. A new prefetcher was recently introduced, the Sandbox prefetcher, that tries to find dynamically the best prefetch offset using the sandbox method. The Sandbox prefetcher uses simple hardware and was shown to be quite effective. However, the sandbox method does not take into account prefetch timeliness. We propose an offset prefetcher with a new method for selecting the prefetch offset that takes into account prefetch timeliness. We show that our Best-Offset prefetcher outperforms the Sandbox prefetcher on the SPEC CPU2006 benchmarks, with equally simple hardware.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446087","","Prefetching;Hardware;System-on-chip;Pollution;History;Benchmark testing","","149","2","39","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"DUANG: Fast and lightweight page migration in asymmetric memory systems","H. Wang; J. Zhang; S. Shridhar; G. Park; M. Jung; N. S. Kim","University of Wisconsin, Madison; Yonsei University School of Integrated Technology Yonsei University Convergence Technology; University of Wisconsin, Madison; University of Texas, Dallas; Yonsei University School of Integrated Technology Yonsei University Convergence Technology; University of Illinois, Urbana-Champaign",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","481","493","Main memory systems have gone through dramatic increases in bandwidth and capacity. At the same time, their random access latency has remained relatively constant. For given memory technology, optimizing the latency typically requires sacrificing the density (i.e., cost per bit), which is one of the most critical concerns for memory industry. Recent studies have proposed memory architectures comprised of asymmetric (fast/low-density and slow/high-density) regions to optimize between overall latency and negative impact on density. Such memory architectures attempt to cost-effectively offer both high capacity and high performance. Yet they present a unique challenge, requiring direct placements of hot memory pages1 in the fast region and/or expensive runtime page migrations. In this paper, we propose a novel resistive memory architecture sharing a set of row buffers between a pair of neighboring banks. It enables two attractive techniques: (1) migrating memory pages between slow and fast banks with little performance overhead and (2) adaptively allocating more row buffers to busier banks based on memory access patterns. For an asymmetric memory architecture with both slow/high-density and fast/low-density banks, our shared row-buffer architecture can capture 87-93% of the performance of a memory architecture with only fast banks.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446088","","Phase change materials;Memory architecture;Random access memory;Memory management;Runtime;Resistance","","13","","40","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Selective GPU caches to eliminate CPU-GPU HW cache coherence","N. Agarwal; D. Nellans; E. Ebrahimi; T. F. Wenisch; J. Danskin; S. W. Keckler",NVIDIA; NVIDIA; NVIDIA; NVIDIA; University of Michigan; University of Michigan,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","494","506","Cache coherence is ubiquitous in shared memory multiprocessors because it provides a simple, high performance memory abstraction to programmers. Recent work suggests extending hardware cache coherence between CPUs and GPUs to help support programming models with tightly coordinated sharing between CPU and GPU threads. However, implementing hardware cache coherence is particularly challenging in systems with discrete CPUs and GPUs that may not be produced by a single vendor. Instead, we propose, selective caching, wherein we disallow GPU caching of any memory that would require coherence updates to propagate between the CPU and GPU, thereby decoupling the GPU from vendor-specific CPU coherence protocols. We propose several architectural improvements to offset the performance penalty of selective caching: aggressive request coalescing, CPU-side coherent caching for GPU-uncacheable requests, and a CPU-GPU interconnect optimization to support variable-size transfers. Moreover, current GPU workloads access many read-only memory pages; we exploit this property to allow promiscuous GPU caching of these pages, relying on page-level protection, rather than hardware cache coherence, to ensure correctness. These optimizations bring a selective caching GPU implementation to within 93% of a hardware cache-coherent implementation without the need to integrate CPUs and GPUs under a single hardware coherence protocol.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446089","","Graphics processing units;Coherence;Hardware;Memory management;Bandwidth;Protocols;System-on-chip","","42","8","70","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Venice: Exploring server architectures for effective resource sharing","J. Dong; R. Hou; M. Huang; T. Jiang; B. Zhao; S. A. McKee; H. Wang; X. Cui; L. Zhang","SKL Computer Architecture; SKL Computer Architecture; University of Rochester; SKL Computer Architecture; SKL Computer Architecture; Chalmers University of Technology; Huawei Technologies Co., Ltd; Huawei Technologies Co., Ltd; SKL Computer Architecture",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","507","518","Consolidated server racks are quickly becoming the backbone of IT infrastructure for science, engineering, and business, alike. These servers are still largely built and organized as when they were distributed, individual entities. Given that many fields increasingly rely on analytics of huge datasets, it makes sense to support flexible resource utilization across servers to improve cost-effectiveness and performance. We introduce Venice, a family of data-center server architectures that builds a strong communication substrate as a first-class resource for server chips. Venice provides a diverse set of resource-joining mechanisms that enables user programs to efficiently leverage non-local resources. To better understand the implications of design decisions about system support for resource sharing we have constructed a hardware prototype that allows us to more accurately measure end-to-end performance of at-scale applications and to explore tradeoffs among performance, power, and resource-sharing transparency. We present results from our initial studies analyzing these tradeoffs when sharing memory, accelerators, or NICs. We find that it is particularly important to reduce or hide latency, that data-sharing access patterns should match the features of the communication channels employed, and that inter-channel collaboration can be exploited for better performance.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446090","","Servers;Resource management;Fabrics;Memory management;Software;Prototypes","","11","","44","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"A large-scale study of soft-errors on GPUs in the field","B. Nie; D. Tiwari; S. Gupta; E. Smirni; J. H. Rogers",College of William and Mary; Oak Ridge National Laboratory; Oak Ridge National Laboratory; College of William and Mary; Oak Ridge National Laboratory,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","519","530","Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446091","","Graphics processing units;Supercomputers;Reliability;Computer architecture;Correlation;Error correction codes;Resource management","","71","1","37","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Design and implementation of a mobile storage leveraging the DRAM interface","S. Seo; Y. Cho; Y. Yoo; O. Bae; J. Park; H. Nam; S. Lee; Y. Lee; S. Chae; M. Kwon; J. -H. Choi; S. Cho; J. Jeong; D. Chang","Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd; Memory Business, Samsung Electronics Co., Ltd",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","531","542","Storage I/O performance remains a key factor that determines the overall user experience of a computer system. This is especially true for mobile systems as users commonly browse and navigate through many high-quality pictures and video clips stored in their device. The appetite for more appealing user interface has continuously pushed the mobile storage interface speed up; emerging UFS 2.0 standard provisions a maximum bandwidth of as high as 1,200 MB/s. In this work, we propose, design, and implement a mobile storage architecture that leverages the high-speed DRAM interface for communication, thus substantially expanding the storage performance headroom. In order to effectively turn the existing DRAM interface into a storage interface, we design a new storage protocol that runs on top of the DRAM interface. Our protocol builds on a small host interface buffer structure mapped to the system's memory space. Based on this protocol, we develop and fabricate a storage controller chip that natively supports the LPDDR3 interface. We also develop a host software stack (Linux device driver and boot loader) and a host platform board. Finally we show the feasibility of our proposal by constructing a full Android system running on the developed storage device and platform. Our detailed evaluation shows that the proposed storage architecture has very low protocol handling overheads and compares favorably to a UFS 2.0 device. The proposed architecture obviates the need for implementing a separate host-side storage controller on a mobile CPU chip.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446092","","Random access memory;Mobile communication;Protocols;Performance evaluation;Bandwidth;Computer architecture","","1","","30","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Restore truncation for performance improvement in future DRAM systems","X. Zhang; Y. Zhang; B. R. Childers; J. Yang","University of Pittsburgh, Pittsburgh, PA, US; Computer Science Department, University of Pittsburgh, PA, USA; Computer Science Department, University of Pittsburgh, PA, USA; Electrical and Computer Engineering Department, University of Pittsburgh, PA, USA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","543","554","Scaling DRAM below 20nm has become a major challenge due to intrinsic limitations in the structure of a bit cell. Future DRAM chips are likely to suffer from significant variations and degraded timings, such as taking much more time to restore cell data after read and write access. In this paper, we propose restore truncation (RT), a low-cost restore strategy to improve performance of DRAM modules that adopt relaxed restore timing. After an access, RT restores a bit cell's voltage only to the level required to persist data to the next scheduled refresh rather than to the default full voltage. Because restore time is shortened, the performance of the cell is improved under process variations. We devise two schemes to balance performance, energy consumption, and hardware overhead. We simulate our proposed RT schemes and compare them with the state of the art. Experimental results show that, on average, RT improves performance by 19.5% and reduces energy consumption by 17%.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446093","","Timing;Transistors;Capacitors;Sensors;DRAM chips;Hardware","","43","1","56","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Parity Helix: Efficient protection for single-dimensional faults in multi-dimensional memory systems","X. Jian; V. Sridharan; R. Kumar","University of Illinois at Urbana-Champaign; RAS Architecture Advanced Micro Devices, Inc.; University of Illinois at Urbana-Champaign",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","555","567","Emerging die-stacked DRAMs provide several factors higher bandwidth and energy efficiency than 2D DRAMs, making them excellent candidates for future memory systems. To be deployed in server and high-performance computing systems, however, die-stacked DRAMs need to provide equivalent or better reliability than existing 2D DRAMs. This includes protecting against channel and die faults, which have been observed in existing 2D DRAM production systems. In this paper, we observe that memory subsystems can be viewed as a multi-dimensional collection of memory banks in which faults generally affect memory banks that lie along a single dimension. For instance, in die-stacked DRAMs, a die consists of a group of DRAM banks that lie in a horizontal plane while a channel consists of a vertical group of banks spanning across multiple dies. We exploit this fault behavior to propose Parity Helix to efficiently protect against single-dimensional faults in multi-dimensional memory systems. Parity Helix shares the same error correction resources across all dimensions to minimize error correction overheads. For die-stacked DRAMs, our evaluation shows that compared to a straightforward extension of previous schemes, Parity Helix increases memory capacity by 16.7%, reduces memory energy per program access by 21%, on average, and by up to 45%.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446094","","Resilience;DRAM chips;Reliability;Error correction;Through-silicon vias;Supercomputers","","15","","41","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Low-Cost Inter-Linked Subarrays (LISA): Enabling fast inter-subarray data movement in DRAM","K. K. Chang; P. J. Nair; D. Lee; S. Ghose; M. K. Qureshi; O. Mutlu",Carnegie Mellon University; Georgia Institute of Technology; Carnegie Mellon University; Carnegie Mellon University; Georgia Institute of Technology; Carnegie Mellon University,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","568","580","This paper introduces a new DRAM design that enables fast and energy-efficient bulk data movement across subarrays in a DRAM chip. While bulk data movement is a key operation in many applications and operating systems, contemporary systems perform this movement inefficiently, by transferring data from DRAM to the processor, and then back to DRAM, across a narrow off-chip channel. The use of this narrow channel for bulk data movement results in high latency and energy consumption. Prior work proposed to avoid these high costs by exploiting the existing wide internal DRAM bandwidth for bulk data movement, but the limited connectivity of wires within DRAM allows fast data movement within only a single DRAM subarray. Each subarray is only a few megabytes in size, greatly restricting the range over which fast bulk data movement can happen within DRAM. We propose a new DRAM substrate, Low-Cost Inter-Linked Subarrays (LISA), whose goal is to enable fast and efficient data movement across a large range of memory at low cost. LISA adds low-cost connections between adjacent subarrays. By using these connections to interconnect the existing internal wires (bitlines) of adjacent subarrays, LISA enables wide-bandwidth data transfer across multiple subarrays with little (only 0.8%) DRAM area overhead. As a DRAM substrate, LISA is versatile, enabling an array of new applications. We describe and evaluate three such applications in detail: (1) fast inter-subarray bulk data copy, (2) in-DRAM caching using a DRAM architecture whose rows have heterogeneous access latencies, and (3) accelerated bitline precharging by linking multiple precharge units together. Our extensive evaluations show that each of LISA's three applications significantly improves performance and memory energy efficiency, and their combined benefit is higher than the benefit of each alone, on a variety of workloads and system configurations.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446095","","DRAM chips;Substrates;Energy efficiency;System performance;Arrays;Buffer storage","","171","4","84","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"ChargeCache: Reducing DRAM latency by exploiting row access locality","H. Hassan; G. Pekhimenko; N. Vijaykumar; V. Seshadri; D. Lee; O. Ergin; O. Mutlu","Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University, Pittsburgh, PA, US; Carnegie Mellon University; Carnegie Mellon University; TOBB University of Economics & Technology; Carnegie Mellon University",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","581","593","DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called Charge Cache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446096","","DRAM chips;Capacitors;Timing;Hardware;Parallel processing;Computer architecture","","109","","101","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Amdahl's law for lifetime reliability scaling in heterogeneous multicore processors","W. J. Song; S. Mukhopadhyay; S. Yalamanchili","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","594","605","Heterogeneous multicore processors have been suggested as alternative microarchitectural designs to enhance performance and energy efficiency. Using Amdahl's Law, heterogeneous models were primarily analyzed in performance and energy efficiency aspects to demonstrate its advantage over conventional homogeneous systems. In this paper, we further extend the study to understand the lifetime reliability consequences of heterogeneous multicore processors, as reliability becomes an increasingly important constraint. We present the lifetime reliability models of multicore processors based on Amdahl's Law, including compact thermal estimation that has strong correlation with device aging. Lifetime reliability is analyzed by varying i) core utilization (Amdahl's scaling factor), ii) processor composition (number of big and small cores), and iii) thread scheduling method. The study shows that the heterogeneous processor may have a serious reliability challenge. If the processor is comprised of only one big core and many small cores, stresses can be biased to the big core especially when workloads spend more time on sequential operations. Our study reveals that incorporating multiple big cores can mitigate reliability bottleneck in big cores and enhance processor lifetime, but adding too many big cores will have an adverse impact on lifetime reliability as well as performance.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446097","","Program processors;Multicore processing;Reliability;Processor scheduling;Computational modeling;Stress;Microarchitecture","","5","","37","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"LiveSim: Going live with microarchitecture simulation","S. Hassani; G. Southern; J. Renau","Dept. of Computer Engineering, University of California, Santa Cruz; Dept. of Computer Engineering, University of California, Santa Cruz; Dept. of Computer Engineering, University of California, Santa Cruz",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","606","617","Computer architects rely heavily on software-based microarchitecture simulators, which typically take hours or days to produce results. We have developed LiveSim, a novel microarchitectural simulation methodology that provides simulation results within seconds, making it suitable for interactive use. LiveSim works by creating in-memory checkpoints of application state, and then executing randomly selected samples from these checkpoints in parallel to produce simulation results. The initial results, which we call LiveSample, are reported less than one second after starting the simulation. As more samples are simulated the results become more accurate and are updated in real-time. Once enough samples are gathered, LiveSim provides confidence intervals for the reported values and continues simulation until it reaches the target confidence level, which we call LiveCI. We evaluated LiveSim using SPEC CPU 2006 benchmarks and found that within 5 seconds after starting simulation, LiveSample results reached an average error of 3.51% compared to full simulation, and the LiveCI results were available within 41 seconds on average.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446098","","Microarchitecture;Benchmark testing;Computational modeling;Simulation;Load modeling;Real-time systems;Computers","","15","","34","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"McVerSi: A test generation framework for fast memory consistency verification in simulation","M. Elver; V. Nagarajan",University of Edinburgh; University of Edinburgh,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","618","630","The memory consistency model (MCM), which formally specifies the behaviour of the memory system, is used by programmers to reason about parallel programs. It is imperative that hardware adheres to the promised MCM. For this reason, hardware designs must be verified against the specified MCM. One common way to do this is via executing tests, where specific threads of instruction sequences are generated and their executions are checked for adherence to the MCM. It would be extremely beneficial to execute such tests under simulation, i.e. when the functional design implementation of the hardware is being prototyped. Most prior verification methodologies, however, target post-silicon environments, which when applied under simulation would be too slow. We propose McVerSi, a test generation framework for fast MCM verification of a full-system design implementation under simulation. Our primary contribution is a Genetic Programming (GP) based approach to MCM test generation, which relies on a novel crossover function that prioritizes memory operations contributing to non-determinism, thereby increasing the probability of uncovering MCM bugs. To guide tests towards exercising as much logic as possible, the simulator's reported coverage is used as the fitness function. Furthermore, we increase test throughput by making the test workload simulation-aware. We evaluate our proposed framework using the Gem5 cycle accurate simulator in full-system mode with Ruby. We discover 2 new bugs due to the faulty interaction of the pipeline and the cache coherence protocol. Crucially, these bugs would not have been discovered through individual verification of the pipeline or the coherence protocol. We study 11 bugs in total. Our GP-based test generation approach finds all bugs consistently, therefore providing much higher guarantees compared to alternative approaches (pseudo-random test generation and litmus tests).","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446099","","Computer bugs;Hardware;Coherence;Protocols;Throughput;Pipelines;Measurement","","26","","57","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Energy-efficient address translation","V. Karakostas; J. Gandhi; A. Cristal; M. D. Hill; K. S. McKinley; M. Nemirovsky; M. M. Swift; O. S. Unsal","Universitat Politecnica de Catalunya; University of Wisconsin - Madison; Centro Nacional de Supercomputacion, Barcelona, ES; University of Wisconsin - Madison; Microsoft Research; ICREA Senior Research Professor at Barcelona Supercomputing Center; University of Wisconsin - Madison; Barcelona Supercomputing Center",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","631","643","Address translation is fundamental to processor performance. Prior work focused on reducing Translation Lookaside Buffer (TLB) misses to improve performance and energy, whereas we show that even TLB hits consume a significant amount of dynamic energy. To reduce the energy cost of address translation, we first propose Lite, a mechanism that monitors the performance and utility of L1 TLBs, and adaptively changes their sizes with way-disabling. The resulting TLBLite organization opportunistically reduces the dynamic energy spent in address translation by 23% on average with minimal impact on TLB miss cycles. To further reduce the energy and performance overheads of L1 TLBs, we also propose RMMLite that targets the recently proposed Redundant Memory Mappings (RMM) address-translation mechanism. RMM maps most of a process's address space with arbitrarily large ranges of contiguous pages in both virtual and physical address space using a modest number of entries in a range TLB. RMMLite adds to RMM an L1-range TLB and the Lite mechanism. The high hit ratio of the L1-range TLB allows Lite to downsize the L1-page TLBs more aggressively. RMMLite reduces the dynamic energy spent in address translation by 71% on average. Above the near-zero L2 TLB misses from RMM, RMMLite further reduces the overhead from L1 TLB misses by 99%. These proposed designs target current and future energy-efficient memory system design to meet the ever increasing memory demands of applications.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446100","","Organizations;Hardware;Memory management;Bridges;Monitoring;Complexity theory;Proposals","","48","1","53","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"RADAR: Runtime-assisted dead region management for last-level caches","M. Manivannan; V. Papaefstathiou; M. Pericas; P. Stenstrom",Chalmers University of Technology; Chalmers University of Technology; Chalmers University of Technology; Chalmers University of Technology,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","644","656","Last-level caches (LLCs) bridge the processor/memory speed gap and reduce energy consumed per access. Unfortunately, LLCs are poorly utilized because of the relatively large occurrence of dead blocks. We propose RADAR, a hybrid static/dynamic dead-block management technique that can accurately predict and evict dead blocks in LLCs. RADAR does dead-block prediction and eviction at the granularity of address regions supported in many of today's task-parallel programming models. The runtime system utilizes static control-flow information about future region accesses in conjunction with past region access patterns to make accurate predictions about dead regions. The runtime system instructs the cache to demote and eventually evict blocks belonging to such dead regions. This paper considers three RADAR schemes to predict dead regions: a scheme that uses control-flow information provided by the programming model (Look-ahead), a history-based scheme (Look-back) and a combined scheme (Look-ahead and Look-back). Our evaluation shows that, on average, all RADAR schemes outperform state-of-the-art hardware dead-block prediction techniques, whereas the combined scheme always performs best.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446101","","Runtime;Programming;Radar tracking;Niobium;Synchronization;Radar detection","","14","","46","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Cache QoS: From concept to reality in the Intel® Xeon® processor E5-2600 v3 product family","A. Herdrich; E. Verplanke; P. Autee; R. Illikkal; C. Gianos; R. Singhal; R. Iyer",Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","657","668","Over the last decade, addressing quality of service (QoS) in multi-core server platforms has been growing research topic. QoS techniques have been proposed to address the shared resource contention between co-running applications or virtual machines in servers and thereby provide better isolation, performance determinism and potentially improve overall throughput. One of the most important shared resources is cache space. Most proposals for addressing shared cache contention are based on simulations and analysis and no commercial platforms were available that integrated such techniques and provided a practical solution. In this paper, we will present the first set of shared cache QoS techniques designed and implemented in state-of-the-art commercial servers (the Intel® Xeon® processor E5-2600 v3 product family). We will describe two key technologies: (i) Cache Monitoring Technology (CMT) to enable monitoring of shared cache usage by different applications and (ii) Cache Allocation Technology (CAT) which enables redistribution of shared cache space between applications to address contention. This is the first paper to describing these techniques as they moved from concept to reality, starting from early research to product implementation. We will also present case studies highlighting the value of these techniques using example scenarios of multi-programmed workloads, virtualized platforms in datacenters and communications platforms. Finally, we will describe initial software infrastructure and enabling for industry practitioners and researchers to take advantage of these technologies for their QoS needs.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446102","","Monitoring;Quality of service;Resource management;Registers;Servers;System software","","94","3","33","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Symbiotic job scheduling on the IBM POWER8","J. Feliu; S. Eyerman; J. Sahuquillo; S. Petit","Dept. of Computer Engineering (DISCA), Universitat Politècnica de València, València, Spain; Dept. of Electronics and Information Systems (ELIS), Ghent University, Ghent, Belgium; Dept. of Computer Engineering (DISCA), Universitat Politècnica de València, València, Spain; Dept. of Computer Engineering (DISCA), Universitat Politècnica de València, València, Spain",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","669","680","Simultaneous multithreading (SMT) processors share most of the microarchitectural core components among the co-running applications. The competition for shared resources causes performance interference between applications. Therefore, the performance benefits of SMT processors heavily depend on the complementarity of the co-running applications. Symbiotic job scheduling, i.e., scheduling applications that co-run well together on a core, can have a considerable impact on the performance of a processor with SMT cores. Prior work uses sampling or novel hardware support to perform symbiotic job scheduling, which has either a non-negligible overhead or is impossible to use on existing hardware. This paper proposes a symbiotic job scheduler for the IBM POWER8 processor. We leverage the existing cycle accounting mechanism to predict symbiosis between applications, and use that information at run-time to decide which applications should run on the same core or on separate cores. We implement the scheduler in the Linux operating system and evaluate it on an IBM POWER8 server running multiprogrammed workloads. The symbiotic job scheduler significantly improves performance compared to both an agnostic random scheduler and the default Linux scheduler. With respect to Linux, it achieves an average speedup by 8.8% for workloads comprising 12 applications, and by 4.7% on average across all evaluated workloads.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446103","","Multicore processing;Symbiosis;Program processors;Interference;Predictive models;Optimal scheduling;Linux","","14","","23","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"ScalCore: Designing a core for voltage scalability","B. Gopireddy; C. Song; J. Torrellas; N. S. Kim; A. Agrawal; A. Mishra","University of Illinois at Urbana-Champaign; University of Wisconsin, Madison; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; NVIDIA Corp; Intel Corp",2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","681","693","Upcoming multicores need to provide increasingly stringent energy-efficient execution modes. Currently, energy efficiency is attained by lowering the voltage (Vdd) through DVFS. However, the effectiveness of DVFS is limited: designing cores for low Vdd results in energy inefficiency at nominal Vdd. Our goal is to design a core for Voltage Scalability, i.e., one that can work in high-performance mode (HPMode) at nominal Vdd, and in a very energy-efficient mode (EEMode) at low Vdd. We call this core ScalCore. To operate energy-efficiently in EEMode, ScalCore introduces two ideas. First, since logic and storage structures scale differently with Vdd, ScalCore applies two low Vdds to the pipeline: one to the logic stages (Vlogic) and a higher one to storage-intensive stages. Secondly, ScalCore further increases the low Vdd of the storage-intensive stages (Vop), so that they are substantially faster than the logic ones. Then, it exploits the speed differential by either fusing storage-intensive pipeline stages or increasing the size of storage structures in the pipeline. Our simulations of 16 cores show that a design with ScalCores in EEMode is much more energy-efficient than one with conventional cores and aggressive DVFS: for approximately the same power, ScalCores reduce the average execution time of programs by 31%, the energy (E) consumed by 48%, and the ED product by 60%. In addition, dynamically switching between EEMode and HPMode based on program phases is very effective: it reduces the average execution time and ED product by a further 28% and 15%, respectively.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446104","","Pipelines;Multicore processing;Scalability;Transistors;Delays;Program processors;Registers","","17","","49","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
"Cost effective physical register sharing","A. Perais; A. Seznec",IRISA/INRISA; IRISA/INRISA,2016 IEEE International Symposium on High Performance Computer Architecture (HPCA),"4 Apr 2016","2016","","","694","706","Sharing a physical register between several instructions is needed to implement several microarchitectural optimizations. However, register sharing requires modifications to the register reclaiming process: Committing a single instruction does not guarantee that the physical register allocated to the previous mapping of its architectural destination register is free-able anymore. Consequently, a form of register reference counting must be implemented. While such mechanisms (e.g., dependency matrix, per register counters) have been described in the literature, we argue that they either require too much storage, or that they lengthen branch misprediction recovery by requiring sequential rollback. As an alternative, we present the Inflight Shared Register Buffer (ISRB), a new structure for register reference counting. The ISRB has low storage overhead and lends itself to checkpoint-based recovery schemes, therefore allowing fast recovery on pipeline flushes. We illustrate our scheme with Move Elimination (short-circuiting moves) and an implementation of Speculative Memory Bypassing (short-circuiting store-load pairs) that makes use of a TAGE-like predictor to identify memory dependencies. We show that the whole potential of these two mechanisms can be achieved with a small register tracking structure.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446105","","Registers;Radiation detectors;Optimization;History;Pipelines;Tracking;Patents","","13","1","33","IEEE","4 Apr 2016","","","IEEE","IEEE Conferences"
