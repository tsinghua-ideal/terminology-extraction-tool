"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Learning Acceleration with Neuron-to-Memory Transformation","M. Imani; M. Samragh Razlighi; Y. Kim; S. Gupta; F. Koushanfar; T. Rosing","University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","1","14","Deep neural networks (DNN) have demonstrated effectiveness for various applications such as image processing, video segmentation, and speech recognition. Running state-of-theart DNNs on current systems mostly relies on either generalpurpose processors, ASIC designs, or FPGA accelerators, all of which suffer from data movements due to the limited on-chip memory and data transfer bandwidth. In this work, we propose a novel framework, called RAPIDNN, which performs neuron-to-memory transformation in order to accelerate DNNs in a highly parallel architecture. RAPIDNN reinterprets a DNN model and maps it into a specialized accelerator, which is designed using non-volatile memory blocks that model four fundamental DNN operations, i.e., multiplication, addition, activation functions, and pooling. The framework extracts representative operands of a DNN model, e.g., weights and input values, using clustering methods to optimize the model for in-memory processing. Then, it maps the extracted operands and their pre-computed results into the accelerator memory blocks. At runtime, the accelerator identifies computation results based on efficient in-memory search capability which also provides tunability of approximation to improve computation efficiency further. Our evaluation shows that RAPIDNN achieves 68.4×, 49.5× energy efficiency improvement and 48.1×, 10.9× speedup as compared to ISAAC and PipeLayer, the state-of-the-art DNN accelerators, while ensuring less than 0.5% quality loss.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065564","","Computational modeling;Neurons;Biological neural networks;Acceleration;RNA;Task analysis","","19","","90","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"HyGCN: A GCN Accelerator with Hybrid Architecture","M. Yan; L. Deng; X. Hu; L. Liang; Y. Feng; X. Ye; Z. Zhang; D. Fan; Y. Xie","State Key Laboratory of Computer Architecture, Chinese Academy of Sciences; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences; Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences; University of California, Santa Barbara",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","15","29","Inspired by the great success of neural networks, graph convolutional neural networks (GCNs) are proposed to analyze graph data. GCNs mainly include two phases with distinct execution patterns. The Aggregation phase, behaves as graph processing, showing a dynamic and irregular execution pattern. The Combination phase, acts more like the neural networks, presenting a static and regular execution pattern. The hybrid execution patterns of GCNs require a design that alleviates irregularity and exploits regularity. Moreover, to achieve higher performance and energy efficiency, the design needs to leverage the high intra-vertex parallelism in Aggregation phase, the highly reusable inter-vertex data in Combination phase, and the opportunity to fuse phase-by-phase execution introduced by the new features of GCNs. However, existing architectures fail to address these demands. In this work, we first characterize the hybrid execution patterns of GCNs on Intel Xeon CPU. Guided by the characterization, we design a GCN accelerator, HyGCN, using a hybrid architecture to efficiently perform GCNs. Specifically, first, we build a new programming model to exploit the fine-grained parallelism for our hardware design. Second, we propose a hardware design with two efficient processing engines to alleviate the irregularity of Aggregation phase and leverage the regularity of Combination phase. Besides, these engines can exploit various parallelism and reuse highly reusable data efficiently. Third, we optimize the overall system via inter-engine pipeline for inter-phase fusion and priority-based off-chip memory access coordination to improve off-chip bandwidth utilization. Compared to the state-of-the-art software framework running on Intel Xeon CPU and NVIDIA V100 GPU, our work achieves on average 1509× speedup with 2500× energy reduction and average 6.5× speedup with 10× energy reduction, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065592","Hardware Accelerator, Graph Convolution Neural Network, Graph Processing, Hybrid Execution Pattern","Computer architecture;Neural networks;Engines;Parallel processing;Aggregates;Fuses;Hardware","","265","","46","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"ACR: Amnesic Checkpointing and Recovery","I. Akturk; U. R. Karpuzcu","Electrical Engineering and Computer Science, University of Missouri, Columbia, Columbia, MO, USA; Electrical and Computer Engineering, University of Minnesota, Twin Cities, Minneapolis, MN, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","30","43","Systematic checkpointing of the machine state makes restart of execution from a safe state possible upon detection of an error. The time and energy overhead of checkpointing, however, grows with the frequency of checkpointing. Considering the growth of expected error rates, amortizing this overhead becomes especially challenging, as checkpointing frequency tends to increase with increasing error rates. Based on the observation that due to imbalanced technology scaling, recomputing a data value can be more energy efficient than retrieving (i.e., loading) a stored copy, this paper explores how recomputation of data values (which otherwise would be read from a checkpoint from memory or secondary storage) can reduce the machine state to be checkpointed, and thereby, the checkpointing overhead. Even in a relatively small scale system, recomputation-based checkpointing can reduce the storage overhead by up to 23.91%; time overhead, by 11.92%; and energy overhead, by 12.53%, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065585","checkpointing;recovery;recomputation","Checkpointing;Error analysis;Time-frequency analysis;Task analysis;Memory management;Hardware;Microsoft Windows","","4","","42","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Asymmetric Resilience: Exploiting Task-Level Idempotency for Transient Error Recovery in Accelerator-Based Systems","J. Leng; A. Buyuktosunoglu; R. Bertran; P. Bose; Q. Chen; M. Guo; V. Janapa Reddi",Shanghai Jiao Tong University; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Harvard University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","44","57","Accelerators make the task of building systems that are re-silient against transient errors like voltage noise and soft errors hard. Architects integrate accelerators into the system as black box third-party IP components. So a fault in one or more accelerators may threaten the system's reliability if there are no established failure semantics for how an error propagates from the accelerator to the main CPU. Existing solutions that assure system reliability come at the cost of sacrificing accelerator generality, efficiency, and incur significant overhead, even in the absence of errors. To over-come these drawbacks, we examine reliability management of accelerator systems via hardware-software co-design, coupling an efficient architecture design with compiler and run-time support, to cope with transient errors. We introduce asymmetric resilience that architects reliability at the system level, centered around a hardened CPU, rather than at the accelerator level. At runtime, the system exploits task-level idempotency to contain accelerator errors and use memory protection instead of taking checkpoints to mitigate over-heads. We also leverage the fact that errors rarely occur in systems, and exploit the trade-off between error recovery performance and improved error-free performance to enhance system efficiency. Using GPUs, which are at the fore-front of accelerator systems, we demonstrate how our system architecture manages reliability in both integrated and discrete systems, under voltage-noise and soft-error related faults, leading to extremely low overhead (less than 1%) and substantial gains (20% energy savings on average).","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065577","","Resilience;Task analysis;Reliability;Transient analysis;Acceleration;Computer architecture;Runtime","","13","","92","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training","E. Qin; A. Samajdar; H. Kwon; V. Nadella; S. Srinivasan; D. Das; B. Kaul; T. Krishna",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Intel; Intel; Intel; Georgia Institute of Technology,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","58","70","The advent of Deep Learning (DL) has radically transformed the computing industry across the entire spectrum from algorithms to circuits. As myriad application domains embrace DL, it has become synonymous with a genre of workloads across vision, speech, language, recommendations, robotics, and games. The key compute kernel within most DL workloads is general matrix-matrix multiplications (GEMMs), which appears frequently during both the forward pass (inference and training) and backward pass (training). GEMMs are a natural choice for hardware acceleration to speed up training, and have led to 2D systolic architectures like NVIDIA tensor cores and Google Tensor Processing Unit (TPU). Unfortunately, emerging GEMMs in DL are highly irregular and sparse, which lead to poor data mappings on systolic architectures. This paper proposes SIGMA, a flexible and scalable architecture that offers high utilization of all its processing elements (PEs) regardless of kernel shape and sparsity. Within SIGMA includes a novel reduction tree microarchitecture named Forwarding Adder Network (FAN). SIGMA performs 5.7x better than systolic array architectures for irregular sparse matrices, and roughly 3x better than state-of-the-art sparse accelerators. We demonstrate an instance of SIGMA operating at 10.8 TFLOPS efficiency across arbitrary levels of sparsity, with a 65.10 mm^2 and 22.33 W footprint on a 28 nm process.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065523","","Training;Sparse matrices;Arrays;Graphics processing units;Engines;Kernel;Tensile stress","","358","","48","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"EMSim: A Microarchitecture-Level Simulation Tool for Modeling Electromagnetic Side-Channel Signals","N. Sehatbakhsh; B. B. Yilmaz; A. Zajic; M. Prvulovic",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","71","85","Side-channel attacks have become a serious security concern for computing systems, especially for embedded devices, where the device is often located in, or in proximity to, a public place, and yet the system contains sensitive information. To design systems that are highly resilient to such attacks, an accurate and efficient design-stage quantitative analysis of side-channel leakage is needed. For many systems properties (e.g., performance, power, etc.), cycle-accurate simulation can provide such an efficient-yet-accurate design-stage estimate. Unfortunately, for an important class of side-channels, electromagnetic emanations, such a model does not exist, and there has not even been much quantitative evidence about what level of modeling detail (e.g., hardware, microarchitecture, etc.) would be needed for high accuracy. This paper presents EMSim, an approach that enables simulation of the electromagnetic (EM) side-channel signals cycle-by-cycle using a detailed micro-architectural model of the device. To evaluate EMSim, we compare its signals against actual EM signals emanated from real hardware (FPGA-based RISC-V processor), and find that they match very closely. To gain further insights, we also experimentally identify how the accuracy of the simulation degrades when key microarchitectural features (e.g., pipeline stall, cache-miss, etc.) and other hardware behaviors (e.g., data-dependent switching activity) are omitted from the simulation model. We further evaluate how robust the simulation-based results are, by comparing them to real signals collected in different conditions (manufacturing, distance, etc.). Finally, to show the applicability of EMSim, we demonstrate how it can be used to measure side-channel leakage through simulation at design-stage.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065440","Side-Channels;security;simulation;micro architecture","Hardware;Microarchitecture;Clocks;Computational modeling;Electromagnetics;Pipelines;Software","","13","","79","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Impala: Algorithm/Architecture Co-Design for In-Memory Multi-Stride Pattern Matching","E. Sadredini; R. Rahimi; M. Lenjani; M. Stan; K. Skadron","University of Virginia, Charlottesville, VA; University of Virginia, Charlottesville, VA; University of Virginia, Charlottesville, VA; University of Virginia, Charlottesville, VA; University of Virginia, Charlottesville, VA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","86","98","High-throughput and concurrent processing of thousands of patterns on each byte of an input stream is critical for many applications with real-time processing needs, such as network intrusion detection, spam filters, virus scanners, and many more. The demand for accelerated pattern matching has motivated several recent in-memory accelerator architectures for automata processing, which is an efficient computation model for pattern matching. Our key observations are: (1) all these architectures are based on 8-bit symbol processing (derived from ASCII), and our analysis on a large set of real-world automata benchmarks reveals that the 8-bit processing dramatically under-utilizes hardware resources, and (2) multi-stride symbol processing, a major source of throughput growth, is not explored in the existing in-memory solutions. This paper presents Impala, a multi-stride in-memory automata processing architecture by leveraging our observations. The key insight of our work is that transforming 8-bit processing to 4-bit processing exponentially reduces hardware resources for state-matching and improves resource utilization. This, in turn, brings the opportunity to have a denser design, and be able to utilize more memory columns to process multiple symbols per cycle with a linear increase in state-matching resources. Impala thus introduces threefold area, throughput, and energy benefits at the expense of increased offline compilation time. Our empirical evaluations on a wide range of automata benchmarks reveal that Impala has on average 2.7× (up to 3.7×) higher throughput per unit area and 1.22× lower power consumption than Cache Automaton, which is the best performing prior work.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065587","In memory processing;hardware/software co design;automata processing;multi stride pattern matching;accelerators","Automata;Field programmable gate arrays;Hardware;Pattern matching;Throughput;Memory management","","31","","65","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Framework for Architectural Exploration: A Routerless NoC Case Study","T. -R. Lin; D. Penney; M. Pedram; L. Chen","University of Southern California, Los Angeles, California, USA; Oregon State University, Corvallis, Oregon, USA; University of Southern California, Los Angeles, California, USA; Oregon State University, Corvallis, Oregon, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","99","110","Machine learning applied to architecture design presents a promising opportunity with broad applications. Recent deep reinforcement learning (DRL) techniques, in particular, enable efficient exploration in vast design spaces where conventional design strategies may be inadequate. This paper proposes a novel deep reinforcement framework, taking routerless networks-on-chip (NoC) as an evaluation case study. The new framework successfully resolves problems with prior design approaches, which are either unreliable due to random searches or inflexible due to severe design space restrictions. The framework learns (near-)optimal loop placement for routerless NoCs with various design constraints. A deep neural network is developed using parallel threads that efficiently explore the immense routerless NoC design space with a Monte Carlo search tree. Experimental results show that, compared with conventional mesh, the proposed deep reinforcement learning (DRL) routerless design achieves a 3.25x increase in throughput, 1.6x reduction in packet latency, and 5x reduction in power. Compared with the state-of-the-art routerless NoC, DRL achieves a 1.47x increase in throughput, 1.18x reduction in packet latency, 1.14x reduction in average hop count, and 6.3% lower power consumption.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065600","machine learning;network-on-chip;routerless","Machine learning;Learning (artificial intelligence);Throughput;Space exploration;Wiring;Complexity theory;Computer architecture","","23","","49","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"IRONHIDE: A Secure Multicore that Efficiently Mitigates Microarchitecture State Attacks for Interactive Applications","H. Omar; O. Khan","University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","111","122","Microprocessors enable aggressive hardware virtualization by means of which multiple processes temporally execute on the system. These security-critical and ordinary processes interact with each other to assure application progress. However, temporal sharing of hardware resources exposes the processor to various microarchitecture state attacks. State-of-the-art secure processors, such as MI6 adopt Intel's SGX enclave execution model. MI6 architects strong isolation by statically isolating shared memory state, and purging the microarchitecture state of private core, cache, and TLB resources on every enclave entry and exit. The purging overhead significantly impacts performance as the interactivity across the secure and insecure processes increases. This paper proposes IRONHIDE that implements strong isolation in the context of multicores to form spatially isolated secure and insecure clusters of cores. For an interactive application comprising of secure and insecure processes, IRONHIDE pins the secure process(es) to the secure cluster, where they execute and interact with the insecure process(es) without incurring the microarchitecture state purging overheads on every interaction event. IRONHIDE improves performance by 2.1x over the MI6 baseline for a set of user and OS interactive applications. Moreover, IRONHIDE improves performance by 20% over an SGX-like baseline, while also ensuring strong isolation guarantees against microarchitecture state attacks.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065326","","Multicore processing;Microarchitecture;Hardware;Security;Monitoring;Process control","","19","","50","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"A New Side-Channel Vulnerability on Modern Computers by Exploiting Electromagnetic Emanations from the Power Management Unit","N. Sehatbakhsh; B. B. Yilmaz; A. Zajic; M. Prvulovic",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","123","138","This paper presents a new micro-architectural vulnerability on the power management units of modern computers which creates an electromagnetic-based side-channel. The key observations that enable us to discover this sidechannel are: 1) in an effort to manage and minimize power consumption, modern microprocessors have a number of possible operating modes (power states) in which various sub-systems of the processor are powered down, 2) for some of the transitions between power states, the processor also changes the operating mode of the voltage regulator module (VRM) that supplies power to the affected sub-system, and 3) the electromagnetic (EM) emanations from the VRM are heavily dependent on its operating mode. As a result, these state-dependent EM emanations create a side-channel which can potentially reveal sensitive information about the current state of the processor and, more importantly, the programs currently being executed. To demonstrate the feasibility of exploiting this vulnerability, we create a covert channel by utilizing the changes in the processor's power states. We show how such a covert channel can be leveraged to exfiltrate sensitive information from a secured and completely isolated (air-gapped) laptop system by placing a compact, inexpensive receiver in proximity to that system. To further show the severity of this attack, we also demonstrate how such a covert channel can be established when the target and the receiver are several meters away from each other, including scenarios where the receiver and the target are separated by a wall. Compared to the state-of-the-art, the proposed covert channel has >3x higher bit-rate. Finally, to demonstrate that this new vulnerability is not limited to being used as a covert channel, we demonstrate how it can be used for attacks such as keystroke logging.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065580","Side-Channel;Microarchitecture Vulnerability;DVFS;Power Management.","Switches;Voltage control;Regulators;Capacitors;Power system management;Phasor measurement units;Receivers","","25","","101","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Leaking Information Through Cache LRU States","W. Xiong; J. Szefer","Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","139","152","The Least-Recently Used cache replacement policy and its variants are widely deployed in modern processors. This paper shows for the first time in detail that the LRU states of caches can be used to leak information: any access to a cache by a sender will modify the LRU state, and the receiver is able to observe this through a timing measurement. This paper presents LRU timing-based channels both when the sender and the receiver have shared memory, e.g., shared library data pages, and when they are separate processes without shared memory. In addition, the new LRU timing-based channels are demonstrated on both Intel and AMD processors in scenarios where the sender and the receiver are sharing the cache in both hyper-threaded setting and time-sliced setting. The transmission rate of the LRU channels can be up to 600Kbps per cache set in the hyper-threaded setting. Different from the majority of existing cache channels which require the sender to trigger cache misses, the new LRU channels work with the sender only having cache hits, making the channel faster and more stealthy. This paper also demonstrates that the new LRU channels can be used in transient execution attacks, e.g., Spectre. Further, this paper shows that the LRU channels pose threats to existing secure cache designs, and this work demonstrates the LRU channels affect the secure PL cache. The paper finishes by discussing and evaluating possible defenses.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065598","caches, side channels, covert channels, timing-based channels, replacement policy, LRU","Receivers;Timing;Program processors;Transient analysis;Encoding;History;Computer architecture","","43","","55","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Baldur: A Power-Efficient and Scalable Network Using All-Optical Switches","M. R. Jokar; J. Qiu; F. T. Chong; L. L. Goddard; J. M. Dallesasse; M. Feng; Y. Li","Department of Computer Science, University of Chicago; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Chicago; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Chicago",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","153","166","We present the first all-optical network, Baldur, to enable power-efficient and high-speed communications in future exascale computing systems. The essence of Baldur is its ability to perform packet routing on-the-fly in the optical domain using an emerging technology called the transistor laser (TL), which presents interesting opportunities and challenges at the system level. Optical packet switching readily eliminates many inefficiencies associated with the crossings between optical and electrical domains. However, TL gates consume high power at the current technology node, which makes TL-based buffering and optical clock recovery impractical. Consequently, we must adopt novel (bufferless and clock-less) architecture and design approaches that are substantially different from those used in current networks. At the architecture level, we support a bufferless design by turning to techniques that have fallen out of favor for current networks. Baldur uses a low-radix, multi-stage network with a simple routing algorithm that drops packets to handle congestion, and we further incorporate path multiplicity and randomness to minimize packet drops. This design also minimizes the number of TL gates needed in each switch. At the logic design level, a non-conventional, length-based data encoding scheme is used to eliminate the need for clock recovery. We thoroughly validate and evaluate Baldur using a circuit simulator and a network simulator. Our results show that Baldur achieves up to 3,000X lower average latency while consuming 3.2X-26.4X less power than various state-of-the art networks under a wide variety of traffic patterns and real workloads, for the scale of 1,024 server nodes. Baldur is also highly scalable, since its power per node stays relatively constant as we increase the network size to over 1 million server nodes, which corresponds to 14.6X-31.0X power improvements compared to state-of-the-art networks at this scale.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065595","optical computing;all-optical network;exascale computing;datacenter network","Optical switches;Optical buffering;Optical fiber networks;High-speed optical techniques;Optical packet switching;Logic gates;Servers","","6","","46","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Twig: Multi-Agent Task Management for Colocated Latency-Critical Cloud Services","R. Nishtala; V. Petrucci; P. Carpenter; M. Sjalander","Norwegian University of Science and Technology, Norway; Federal University of Bahia, Brazil; Barcelona Supercomputing Center, Spain; Norwegian University of Science and Technology, Trondheim, Norway",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","167","179","Many of the important services running on data centres are latency-critical, time-varying, and demand strict user satisfaction. Stringent tail-latency targets for colocated services and increasing system complexity make it challenging to reduce the power consumption of data centres. Data centres typically sacrifice server efficiency to maintain tail-latency targets resulting in an increased total cost of ownership. This paper introduces Twig, a scalable quality-of-service (QoS) aware task manager for latency-critical services co-located on a server system. Twig successfully leverages deep reinforcement learning to characterise tail latency using hardware performance counters and to drive energy-efficient task management decisions in data centres. We evaluate Twig on a typical data centre server managing four widely used latency-critical services. Our results show that Twig outperforms prior works in reducing energy usage by up to 38% while achieving up to 99% QoS guarantee for latency-critical services.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065442","","Task analysis;Quality of service;Data centers;Servers;Resource management;Standards;Explosions","","55","","72","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"QuickNN: Memory and Performance Optimization of k-d Tree Based Nearest Neighbor Search for 3D Point Clouds","R. Pinkham; S. Zeng; Z. Zhang",University of Michigan; General Motors; University of Michigan,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","180","192","The use of Light Detection And Ranging (LiDAR) has enabled the continued improvement in accuracy and performance of autonomous navigation. The latest applications require LiDAR's of the highest spatial resolution, which generate a massive amount of 3D point clouds that need to be processed in real time. In this work, we investigate the architecture design for k-Nearest Neighbor (kNN) search, an important processing kernel for 3D point clouds. An approximate kNN search based on a k-dimensional (k-d) tree is employed to improve performance. However, even for today's moderate-sized problems, this approximate kNN search is severely hindered by memory bandwidth due to numerous random accesses and minimal data reuse opportunities. We apply several memory optimization schemes to alleviate the bandwidth bottleneck: 1) the k-d tree data structure is partitioned to two sets: tree nodes and point buckets, based on their distinct characteristics - tree nodes that have high reuse are cached for their lifetime to facilitate search, while point buckets with low reuse are organized in regular contiguous segments in external memory to facilitate efficient burst access; 2) write and read caches are added to gather random accesses to transform them to sequential accesses; and 3) tree construction and tree search are interleaved to cut redundant access streams. With optimized memory bandwidth, the kNN search can be further accelerated by two new processing schemes: 1) parallel tree traversal that utilizes multiple workers with minimal tree duplication overhead, and 2) incremental tree building that minimizes the overhead of tree construction by dynamically updating the tree instead of building it from scratch every time. We demonstrate the performance and memory-optimized QuickNN architecture on FPGA and perform exhaustive benchmarking, showing that up to a 19× and 7.3× speedup over k-d tree searches performed on a modern CPU and GPU, respectively, and a 14.5× speedup over a comparable sized architecture performing an exact search. Finally, we show that QuickNN achieves two orders of magnitude performance per watt increase over CPU and GPU methods.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065602","autonomous driving;k nearest neighbor;fpga;architecture;lidar","Three-dimensional displays;Computer architecture;Search problems;Laser radar;Bandwidth;Buildings;Benchmark testing","","41","","49","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"CLITE: Efficient and QoS-Aware Co-Location of Multiple Latency-Critical Jobs for Warehouse Scale Computers","T. Patel; D. Tiwari",Northeastern University; Northeastern University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","193","206","Large-scale data centers run latency-critical jobs with quality-of-service (QoS) requirements, and throughput-oriented background jobs, which need to achieve high perfor-mance. Previous works have proposed methods which cannot co-locate multiple latency-critical jobs with multiple back-grounds jobs while: (1) meeting the QoS requirements of all latency-critical jobs, and (2) maximizing the performance of the background jobs. This paper proposes CLITE, a Bayesian Optimization-based, multi-resource partitioning technique which achieves these goals. CLITE is publicly available at https://github.com/GoodwillComputingLab/CLITE.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065583","QoS Delivery;Warehouse Scale Computing;Latency Critical Jobs;Multi Tenancy","Quality of service;Bandwidth;Sensitivity;Adaptation models;Bayes methods;Resource management;Data centers","","123","","72","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Q-Zilla: A Scheduling Framework and Core Microarchitecture for Tail-Tolerant Microservices","A. Mirhosseini; B. L. West; G. W. Blake; T. F. Wenisch",University of Michigan; University of Michigan; Amazon Web Services; University of Michigan,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","207","219","Managing tail latency is a primary challenge in designing large-scale Internet services. Queuing is a major contributor to end-to-end tail latency, wherein nominal tasks are enqueued behind rare, long ones, due to Head-of-Line (HoL) blocking. In this paper, we introduce Q-Zilla, a scheduling framework to tackle tail latency from a queuing perspective, and CoreZilla, a microarchitectural instantiation of our framework. On the algorthmic front, we first propose Server-Queue Decoupled Size-Interval Task Assignment (SQD-SITA), an efficient scheduling algorithm to minimize tail latency for high-disparity service distributions. SQD-SITA is inspired by an earlier algorithm, SITA, which explicitly seeks to address HoL blocking by providing an ""express-lane"" for short tasks, protecting them from queuing behind rare, long ones. But, SITA requires prior knowledge of task lengths to steer them into their corresponding lane, which is impractical. Furthermore, SITA may underperform an M/G/k system when some lanes become underutilized. In contrast, SQD-SITA uses incremental preemption to avoid the need for a priori task-size information, and dynamically reallocates servers to lanes to increase server utilization with no performance penalty. We then introduce Interruptible SQD-SITA, which further improves tail latency at the cost of additional preemptions. Finally, we describe and evaluate CoreZilla, wherein a multi-threaded core efficiently implements ISQD-SITA in a software-transparent manner at minimal cost. Our evaluation demonstrates that CoreZilla improves tail latency over a conventional SMT core with 2, 4, and 8 contexts by 2.25×, 3.23×, and 4.88×, on average, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065545","","Task analysis;Context;Servers;Organizations;Registers;Queueing analysis;Synchronization","","21","","80","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"PREMA: A Predictive Multi-Task Scheduling Algorithm For Preemptible Neural Processing Units","Y. Choi; M. Rhu","School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","220","233","To amortize cost, cloud vendors providing DNN acceleration as a service to end-users employ consolidation and virtualization to share the underlying resources among multiple DNN service requests. This paper makes a case for a ""preemptible"" neural processing unit (NPU) and a ""predictive"" multi-task scheduler to meet the latency demands of high-priority inference while maintaining high throughput. We evaluate both the mechanisms that enable NPUs to be preemptible and the policies that utilize them to meet scheduling objectives. We show that preemptive NPU multi-tasking can achieve an average 7.8×, 1.4×, and 4.8× improvement in latency, throughput, and SLA satisfaction, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065590","","Task analysis;Throughput;Graphics processing units;Servers;Google;Computer architecture;Random access memory","","88","","73","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Domain-Specialized Cache Management for Graph Analytics","P. Faldu; J. Diamond; B. Grot",The University of Edinburgh; Oracle Labs; The University of Edinburgh,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","234","248","Graph analytics power a range of applications in areas as diverse as finance, networking and business logistics. A common property of graphs used in the domain of graph analytics is a power-law distribution of vertex connectivity, wherein a small number of vertices are responsible for a high fraction of all connections in the graph. These richly-connected, hot, vertices inherently exhibit high reuse. However, this work finds that state-of-the-art hardware cache management schemes struggle in capitalizing on their reuse due to highly irregular access patterns of graph analytics. In response, we propose GRASP, domain-specialized cache management at the last-level cache for graph analytics. GRASP augments existing cache policies to maximize reuse of hot vertices by protecting them against cache thrashing, while maintaining sufficient flexibility to capture the reuse of other vertices as needed. GRASP keeps hardware cost negligible by leveraging lightweight software support to pinpoint hot vertices, thus eliding the need for storage-intensive prediction mechanisms employed by state-of-the-art cache management schemes. On a set of diverse graph-analytic applications with large high-skew graph datasets, GRASP outperforms prior domain-agnostic schemes on all datapoints, yielding an average speed-up of 4.2% (max 9.4%) over the best-performing prior scheme. GRASP remains robust on low-/no-skew datasets, whereas prior schemes consistently cause a slowdown.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065556","graph analytics;graph reordering;skew;last-level cache;cache management;domain-specialized","Hardware;Software;Task analysis;Computer architecture;Robustness;System-on-chip;Color","","44","","72","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"ALRESCHA: A Lightweight Reconfigurable Sparse-Computation Accelerator","B. Asgari; R. Hadidi; T. Krishna; H. Kim; S. Yalamanchili","Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","249","260","Sparse problems that dominate a wide range of applications fail to effectively benefit from high memory bandwidth and concurrent computations in modern high-performance computer systems. Therefore, hardware accelerators have been proposed to capture a high degree of parallelism in sparse problems. However, the unexplored challenge for sparse problems is the limited opportunity for parallelism because of data dependencies, a common computation pattern in scientific sparse problems. Our key insight is to extract parallelism by mathematically transforming the computations into equivalent forms. The transformation breaks down the sparse kernels into a majority of independent parts and a minority of data-dependent ones and reorders these parts to gain performance. To implement the key insight, we propose a lightweight reconfigurable sparse-computation accelerator (Alrescha). To efficiently run the data-dependent and parallel parts and to enable fast switching between them, Alrescha makes two contributions. First, it implements a compute engine with a fixed compute unit for the parallel parts and a lightweight reconfigurable engine for the execution of the data-dependent parts. Second, Alrescha benefits from a locally-dense storage format, with the right order of non-zero values to yield the order of computations dictated by the transformation. The combination of the lightweight reconfigurable hardware and the storage format enables uninterrupted streaming from memory. Our simulation results show that compared to GPU, Alrescha achieves an average speedup of 15.6x for scientific sparse problems, and 8x for graph algorithms. Moreover, compared to GPU, Alrescha consumes 14x less energy.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065572","Sparse;Reconfiguration;Accelerator;Scientific Problems;PDE solver","Sparse matrices;Hardware;Kernel;Parallel processing;US Department of Transportation;Mathematical model;Computational modeling","","49","","39","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"SpArch: Efficient Architecture for Sparse Matrix Multiplication","Z. Zhang; H. Wang; S. Han; W. J. Dally","EECS, Massachusetts Institute of Technology, Cambridge, MA, US; EECS, Massachusetts Institute of Technology, Cambridge, MA, US; EECS, Massachusetts Institute of Technology, Cambridge, MA, US; Electrical Engineering, Stanford University / NVIDIA, Stanford, CA, US",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","261","274","Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGEMM introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to extensive and expensive DRAM access. To address this problem, this paper proposes an efficient sparse matrix multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matrix representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4x. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8x. We also resolve the increased input matrix read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8x over previous state-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup and 6x, 164x, 435x, 307x, 62x energy savings over OuterSpace, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065428","Sparse Matrix Multiplication;Domain-Specific Architecture;Specialized Accelerators;Huffman-Tree;Data Reuse","Random access memory;Sparse matrices;Corporate acquisitions;Prefetching;Parallel processing;Indexes;Computer architecture","","205","","60","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Mitigating Voltage Drop in Resistive Memories by Dynamic RESET Voltage Regulation and Partition RESET","F. Zokaee; L. Jiang",Indiana University Bloomington; Indiana University Bloomington,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","275","286","The emerging resistive random access memory (ReRAM) technology has been deemed as one of the most promising alternatives to DRAM in main memories, due to its better scalability, zero cell leakage and short read latency. The cross-point (CP) array enables ReRAM to obtain the theoretical minimum 4F^2 cell size by placing a cell at the cross-point of a word-line and a bit-line. However, ReRAM CP arrays suffer from large sneak current resulting in significant voltage drop that greatly prolongs the array RESET latency. Although prior works reduce the voltage drop in CP arrays, they either substantially increase the array peripheral overhead or cannot work well with wear leveling schemes. In this paper, we propose two array micro-architecture level techniques, dynamic RESET voltage regulation (DRVR) and partition RESET (PR), to mitigate voltage drop on both bit-lines and word-lines in ReRAM CP arrays. DRVR dynamically provides higher RESET voltage to the cells far from the write driver and thus encountering larger voltage drop on a bit-line, so that all cells on a bit-line share approximately the same latency during RESETs. PR decides how many and which cells to reset online to partition the CP array into multiple equivalent circuits with smaller word-line resistance and voltage drop. Because DRVR and PR greatly reduce the array RESET latency, the ReRAM-based main memory lifetime under the worst case non-stop write traffic significantly decreases. To increase the CP array endurance, we further upgrade DRVR by providing lower RESET voltage to the cells suffering from less voltage drop on a word-line. Our experimental results show that, compared to the combination of prior voltage drop reduction techniques, our DRVR and PR improve the system performance by 11.7% and decrease the energy consumption by 46% averagely, while still maintaining >10-year main memory system lifetime.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065529","","Computer architecture;Microprocessors;Switches;Resistance;Voltage control;Random access memory;Equivalent circuits","","6","","37","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"DRAM-Less: Hardware Acceleration of Data Processing with New Memory","J. Zhang; G. Park; D. Donofrio; J. Shalf; M. Jung","Computer Architecture and Memory Systems Laboratory, Korea Advanced Institute of Science and Technology (KAIST); Computer Architecture and Memory Systems Laboratory, Korea Advanced Institute of Science and Technology (KAIST); Computer Architecture and Memory Systems Laboratory, Lawrence Berkeley National Laboratory; Computer Architecture and Memory Systems Laboratory, Lawrence Berkeley National Laboratory; Computer Architecture and Memory Systems Laboratory, Korea Advanced Institute of Science and Technology (KAIST)",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","287","302","General purpose hardware accelerators have become major data processing resources in many computing domains. However, the processing capability of hardware accelerations is often limited by costly software interventions and memory copies to support compulsory data movement between different processors and solid-state drives (SSDs). This in turn also wastes a significant amount of energy in modern accelerated systems. In this work, we propose, DRAM-less, a hardware automation approach that precisely integrates many state-of-the-art phase change memory (PRAM) modules into its data processing network to dramatically reduce unnecessary data copies with a minimum of software modifications. We implement a new memory controller that plugs a real 3x nm multi-partition PRAM to 28nm technology FPGA logic cells and interoperate its design into a real PCIe accelerator emulation platform. The evaluation results reveal that our DRAM-less achieves, on average, 47% better performance than advanced acceleration approaches that use a peer-to-peer DMA.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065582","near-data processing;accelerator;FPGA controller;data movement","Phase change random access memory;Data processing;Hardware;Acceleration;Kernel;Buffer storage","","4","","69","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"ELP2IM: Efficient and Low Power Bitwise Operation Processing in DRAM","X. Xin; Y. Zhang; J. Yang","Electrical and Computer Engineering Department, University of Pittsburgh; Department of Computer Science, University of Pittsburgh; Electrical and Computer Engineering Department, University of Pittsburgh",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","303","314","Recently proposed DRAM based memory-centric architectures have demonstrated their great potentials in addressing the memory wall challenge of modern computing systems. Such architectures exploit charge sharing of multiple rows to enable in-memory bitwise operations. However, existing designs rely heavily on reserved rows to implement computation, which introduces high data movement overhead, large operation latency, large energy consumption, and low operation reliability. In this paper, we propose ELP2IM, an efficient and low power processing in-memory architecture, to address the above issues. ELP2IM utilizes two stable states of sense amplifiers in DRAM subarrays so that it can effectively reduce the number of intra-subarray data movements as well as the number of concurrently opened DRAM rows, which exhibits great performance and energy consumption advantages over existing designs. Our experimental results show that the power efficiency of ELP2IM is more than 2X improvement over the state-of-the-art DRAM based memory-centric designs in real application.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065462","","Random access memory;Decoding;Logic gates;Logic arrays;Complexity theory;Voltage control;Computer architecture","","59","","60","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors","K. Qiu; N. Jao; M. Zhao; C. S. Mishra; G. Gudukbay; S. Jose; J. Sampson; M. T. Kandemir; V. Narayanan","Capital Normal University, Beijing, China; Pennsylvania State University, USA; Shandong University, Qingdao, China; Pennsylvania State University, USA; Pennsylvania State University, USA; Pennsylvania State University, USA; Pennsylvania State University, USA; Pennsylvania State University, USA; Pennsylvania State University, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","315","327","Many recent works have shown substantial efficiency boosts from performing inference tasks on Internet of Things (IoT) nodes rather than merely transmitting raw sensor data. However, such tasks, e.g., convolutional neural networks (CNNs), are very compute intensive. They are therefore challenging to complete at sensing-matched latencies in ultra-low-power and energy-harvesting IoT nodes. ReRAM crossbar-based accelerators (RCAs) are an ideal candidate to perform the dominant multiplication-and-accumulation (MAC) operations in CNNs efficiently, but conventional, performance-oriented RCAs, while energy-efficient, are power hungry and ill-optimized for the intermittent and unstable power supply of energy-harvesting IoT nodes. This paper presents the ResiRCA architecture that integrates a new, lightweight, and configurable RCA suitable for energy harvesting environments as an opportunistically executing augmentation to a baseline sense-and-transmit battery-powered IoT node. To maximize ResiRCA throughput under different power levels, we develop the ResiSchedule approach for dynamic RCA reconfiguration. The proposed approach uses loop tiling-based computation decomposition, model duplication within the RCA, and inter-layer pipelining to reduce RCA activation thresholds and more closely track execution costs with dynamic power income. Experimental results show that ResiRCA together with ResiSchedule achieve average speedups and energy efficiency improvements of 8× and 14× respectively compared to a baseline RCA with intermittency-unaware scheduling.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065347","Energy harvesting, ReRAM crossbar, CNN, Reconfigurable hardware, Loop tiling, Computation scheduling","Throughput;Power demand;Computer architecture;Task analysis;Hardware;Kernel","","27","","45","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation","T. J. Ham; S. J. Jung; S. Kim; Y. H. Oh; Y. Park; Y. Song; J. -H. Park; S. Lee; K. Park; J. W. Lee; D. -K. Jeong",Seoul National University; Seoul National University; Seoul National University; Sungkyunkwan University; Seoul National University; Seoul National University; Seoul National University; Seoul National University; SK Hynix; Seoul National University; Seoul National University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","328","341","With the increasing computational demands of the neural networks, many hardware accelerators for the neural networks have been proposed. Such existing neural network accelerators often focus on popular neural network types such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs); however, not much attention has been paid to attention mechanisms, an emerging neural network primitive that enables neural networks to retrieve most relevant information from a knowledge-base, external memory, or past states. The attention mechanism is widely adopted by many state-of-the-art neural networks for computer vision, natural language processing, and machine translation, and accounts for a large portion of total execution time. We observe today's practice of implementing this mechanism using matrix-vector multiplication is suboptimal as the attention mechanism is semantically a content-based search where a large portion of computations ends up not being used. Based on this observation, we design and architect A3, which accelerates attention mechanisms in neural networks with algorithmic approximation and hardware specialization. Our proposed accelerator achieves multiple orders of magnitude improvement in energy efficiency (performance/watt) as well as substantial speedup over the state-of-the-art conventional hardware.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065498","attention mechanism;approximation;neural network;accelerators;Domain Specific Architectures","Artificial neural networks;Hardware;Task analysis;Computational modeling;Bit error rate;Data models","","175","","79","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators","L. Song; F. Chen; Y. Zhuo; X. Qian; H. Li; Y. Chen",Duke University; Duke University; University of Southern California; University of Southern California; Duke University; Duke University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","342","355","Deep neural network (DNN) accelerators as an example of domain-specific architecture have demonstrated great success in DNN inference. However, the architecture acceleration for equally important DNN training has not yet been fully studied. With data forward, error backward and gradient calculation, DNN training is a more complicated process with higher computation and communication intensity. Because the recent research demonstrates a diminishing specialization return, namely, “accelerator wall”, we believe that a promising approach is to explore coarse-grained parallelism among multiple performance-bounded accelerators to support DNN training. Distributing computations on multiple heterogeneous accelerators to achieve high throughput and balanced execution, however, remaining challenging. We present ACCPAR, a principled and systematic method of determining the tensor partition among heterogeneous accelerator arrays. Compared to prior empirical or unsystematic methods, ACCPAR considers the complete tensor partition space and can reveal previously unknown new parallelism configurations. ACCPAR optimizes the performance based on a cost model that takes into account both computation and communication costs of a heterogeneous execution environment. Hence, our method can avoid the drawbacks of existing approaches that use communication as a proxy of the performance. The enhanced flexibility of tensor partitioning in ACCPAR allows the flexible ratio of computations to be distributed among accelerators with different performances. The proposed search algorithm is also applicable to the emerging multi-path patterns in modern DNNs such as ResNet. We simulate ACCPAR on a heterogeneous accelerator array composed of both TPU-v2 and TPU-v3 accelerators for the training of large-scale DNN models such as Alexnet, Vgg series and Resnet series. The average performance improvements of the state-of-the-art “one weird trick” (OWT) and HYPAR, and ACCPAR, normalized to the baseline data parallelism scheme where each accelerator replicates the model and processes different input data in parallel, are 2.98×, 3.78×, and 6.30×, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065574","Deep Learning Accelerator;Tensor Partitioning;Training Acceleration","Tensile stress;Computational modeling;Training;Parallel processing;Data models;Computer architecture;Bandwidth","","42","","166","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"FLOWER and FaME: A Low Overhead Bit-Level Fault-map and Fault-Tolerance Approach for Deeply Scaled Memories","D. Kline; J. Zhang; R. Melhem; A. K. Jones","Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; Computer Science, University of Pittsburgh, Pittsburgh, PA, USA; Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","356","368","To maintain appropriate yields in deeply scaled technologies requires fault-tolerance of increasingly high fault rates. These fault rates far exceed traditional general approaches such as ECC, particularly when faults accrue over time. Effective fault tolerance at such high fault rates requires detailed bit-level knowledge of the location of faulty cells. We provide a solution to this problem in the form of a space efficient, bit-level fault map called FLOWER. FLOWER utilizes Bloom filters to provide detailed fault characterization for a relatively small overhead. We demonstrate how FLOWER can enable improved fault tolerance at high fault rates by enhancing existing fault tolerance proposals and yielding 10–100x improvements. Using in-memory processing, FLOWER can maintain a less than 2% performance overhead at 10E-4 fault rates with less than 2% loss of memory density to report bit-level faults with high accuracy. Using a tuned novel hashing technique called MinCI, FLOWER for memory achieves considerably lower false positives than with disk-level hashing techniques at a fraction of the performance overhead. With a new technique to protect against errors during in-memory operations, PETAL bits, FLOWER can remain resilient against random errors while efficiently targeting predictable errors. Furthermore, we propose a new fault tolerance scheme called FaME, which provides ultra-efficient bit-level sparing by using the FLOWER fault map to identify the location of faults. FLOWER+FaME can achieve 14x longer PCM memory lifetime with half the area overhead versus SECDED ECC.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065456","Fault tolerance;Bloom filters;Hash functions;DRAM;Phase Change Memory;Processing in memory","Fault tolerance;Fault tolerant systems;Random access memory;Phase change materials;Error correction;Error correction codes;Crosstalk","","13","","53","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Multi-Range Supported Oblivious RAM for Efficient Block Data Retrieval","Y. Che; R. Wang","Department of Computer Science, Illinois Institute of Technology; Department of Computer Science, Illinois Institute of Technology",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","369","382","Data locality exists everywhere in the memory hierarchy. Most applications show temporal and spatial locality, and computer system and architecture designers utilize this property to improve the system performance with better data layout, prefetching, and scheduling. The locality property can be represented by memory access patterns, which records the time and frequency of accessed addresses. From the security perspective, if an attacker can trace the access pattern, sensitive information inside of the application could be observed and leaked. Oblivious RAM is one of the most effective solutions to mitigate the access pattern leakage on the system, which adds redundant data blocks in space and time. With ORAM protection, the intrinsic data locality is broken by the randomly stored data. Therefore, the application cannot gain any performance benefits from locality if the ORAM protocol is used. In this work, we would like to study the potential to support multi-range accesses with new storage and access efficient ORAM construction. Our proposed designs include two major schemes: Lite-rORAM, which minimize the storage overhead of existing rORAM; and Hybrid-rORAM, which support multiple ranges accesses with minimum storage overhead. We achieve the goal to preserve the locality for consecutive data blocks with different ranges in the application while obfuscates the access pattern as well. We tested our proposed schemes with different workloads on local and remote backends. The experimental results show that, in the best case, our proposed ORAM construction can reduce the data block retrieval time to 0.24x of the baseline Path ORAM, with 87.5% storage overhead reduction compared to rORAM.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065426","","Random access memory;Servers;Binary trees;Proposals;Hardware;Encryption","","12","","33","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows","I. Jeong; S. Park; C. Lee; W. W. Ro",Yonsei University; Yonsei University; Samsung Electronics; Yonsei University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","383","396","The performance gap between in-order (InO) and out-of-order (OoO) cores comes from the ability to dynamically create highly optimized instruction issue schedules. In this work, we observe that a significant amount of performance benefit of OoO scheduling can also be attained by supplementing a traditional InO core with a small and speculative instruction scheduling window, namely SpecInO. SpecInO monitors a small set of instructions ahead of a conventional InO scheduling window, aiming at issuing ready instructions behind long-latency stalls. Simulation results show that SpecInO captures and issues 62% of dynamic instructions out of program order. To this end, we propose a CASINO core microarchitecture that dynamically and speculatively generates OoO schedules with near-InO complexity, using CAScaded IN-Order scheduling windows. A Speculative IQ (S-IQ) issues an instruction if it is ready, or otherwise passes it to the next IQ. At the last IQ, instructions are scheduled in program order along serial dependence chains. The net effect is OoO scheduling via collaboration between cascaded InO IQs. To support speculative execution with minimal cost overhead, we propose a novel register renaming technique that allocates free physical registers only to instructions issued from the S-IQ. The proposed core performs dynamic memory disambiguation via an on-commit value check by extending the store buffer already existing in an InO core. We further optimize energy efficiency by filtering out redundant associative searches performed by speculated loads. In our analysis, CASINO core improves performance by 51% over an InO core (within 10 percentage points of an OoO core), which results in 25% and 42% improvements in energy efficiency over InO and OoO cores, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065575","Dynamic Scheduling;Register Renaming;Memory Disambiguation;Energy Efficiency","Schedules;Registers;Dynamic scheduling;Pipelines;Out of order;Engines;Microarchitecture","","10","","78","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Precise Runahead Execution","A. Naithani; J. Feliu; A. Adileh; L. Eeckhout","Ghent University, Belgium; Universitat Politecnica de València, Valencia, Spain; Ghent University, Belgium; Ghent University, Belgium",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","397","410","Runahead execution improves processor performance by accurately prefetching long-latency memory accesses. When a long-latency load causes the instruction window to fill up and halt the pipeline, the processor enters runahead mode and keeps speculatively executing code to trigger accurate prefetches. A recent improvement tracks the chain of instructions that leads to the long-latency load, stores it in a runahead buffer, and executes only this chain during runahead execution, with the purpose of generating more prefetch requests. Unfortunately, all prior runahead proposals have shortcomings that limit performance and energy efficiency because they release processor state when entering runahead mode and then need to refill the pipeline to restart normal operation. Moreover, runahead buffer limits prefetch coverage by tracking only a single chain of instructions that leads to the same long-latency load. We propose precise runahead execution (PRE) which builds on the key observation that when entering runahead mode, the processor has enough issue queue and physical register file resources to speculatively execute instructions. This mitigates the need to release and re-fill processor state in the ROB, issue queue, and physical register file. In addition, PRE pre-executes only those instructions in runahead mode that lead to full-window stalls, using a novel register renaming mechanism to quickly free physical registers in runahead mode, further improving efficiency and effectiveness. Finally, PRE optionally buffers decoded runahead micro-ops in the frontend to save energy. Our experimental evaluation using a set of memory-intensive applications shows that PRE achieves an additional 18.2% performance improvement over the recent runahead proposals while at the same time reducing energy consumption by 6.8%.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065552","Microarchitecture;single core performance;runahead execution","Prefetching;Registers;Pipelines;Buffer storage;Proposals;Out of order;Memory management","","22","","72","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"BBS: Micro-Architecture Benchmarking Blockchain Systems through Machine Learning and Fuzzy Set","L. Zhu; C. Chen; Z. Su; W. Chen; T. Li; Z. Yu","Shenzhen Institutes of Advanced Technology (SIAT), CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology (SIAT), CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology (SIAT), CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology (SIAT), CAS, Shenzhen, China; Dept. of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Shenzhen Institutes of Advanced Technology (SIAT), CAS, Shenzhen, China",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","411","423","Due to the decentralization, irreversibility, and traceability, blockchain has attracted significant attention and has been deployed in many critical industries such as banking and logistics. However, the micro-architecture characteristics of blockchain programs still remain unclear. What's worse, the large number of micro-architecture events make understanding the characteristics extremely difficult. We even lack a systematic approach to identify the important events to focus on. In this paper, we propose a novel benchmarking methodology dubbed BBS to characterize blockchain programs at micro-architecture level. The key is to leverage fuzzy set theory to identify important micro-architecture events after the significance of them is quantified by a machine learning based approach. The important events for single programs are employed to characterize the programs while the common important events for multiple programs form an importance vector which is used to measure the similarity between benchmarks. We leverage BBS to characterize seven and six benchmarks from Blockbench and Caliper, respectively. The results show that BBS can reveal interesting findings. Moreover, by leveraging the importance characterization results, we improve that the transaction throughput of Smallbank from Fabric by 70% while reduce the transaction latency by 55%. In addition, we find that three of seven and two of six benchmarks from Blockbench and Caliper are redundant, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065588","Micro architecture, Workload Characterization, Benchmarking, Blockchain, Performance","Benchmark testing;Measurement;Fabrics;Peer-to-peer computing;Computer architecture;Protocols","","10","","56","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Delay and Bypass: Ready and Criticality Aware Instruction Scheduling in Out-of-Order Processors","M. Alipour; S. Kaxiras; D. Black-Schaffer; R. Kumar","Uppsala Univeristy, Sweden; Uppsala Univeristy, Sweden; Uppsala Univeristy, Sweden; Norwegian University of Science and Technology, Norway",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","424","434","Flexible instruction scheduling is essential for performance in out-of-order processors. This is typically achieved by using CAM-based Instruction Queues (IQs) that provide complete flexibility in choosing ready instructions for execution, but at the cost of significant scheduling energy. In this work we seek to reduce the instruction scheduling energy by reducing the depth and width of the IQ. We do so by classifying instructions based on their readiness and criticality, and using this information to bypass the IQ for instructions that will not benefit from its expensive scheduling structures and delay instructions that will not harm performance. Combined, these approaches allow us to offload a significant portion of the instructions from the IQ to much cheaper FIFO-based scheduling structures without hurting performance. As a result we can reduce the IQ depth and width by half, thereby saving energy. Our design, Delay and Bypass (DNB), is the first design to explicitly address both readiness and criticality to reduce scheduling energy. By handling both classes we are able to achieve 95% of the baseline out-of-order performance while only using 33% of the scheduling energy. This represents a significant improvement over previous designs which addressed only criticality or readiness (91%/89% performance at 74%/53% energy).","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065303","out of order execution;instruction scheduling;energy efficiency;microarchitecture","Delays;Out of order;Processor scheduling;Resource management;Complexity theory","","12","","33","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"EquiNox: Equivalent NoC Injection Routers for Silicon Interposer-Based Throughput Processors","Y. Li; L. Chen","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","435","446","Throughput-oriented many-core processors demand highly efficient network-on-chip (NoC) architecture for data transferring. Recent advent of silicon interposer, stacked memory and 2.5D integration have further increased data transfer rate. This greatly intensifies traffic bottleneck in the NoC but, at the same time, also brings a significant new opportunity in utilizing wiring resources in the interposer. In this paper, we propose a novel concept called Equivalent Injection Routers (EIRs) which, together with interposer links, transform the few-to-many traffic pattern to many-to-many pattern, thus fundamentally solving the bottleneck problem. We have developed EquiNox as a design example. We utilize N-Queen and Monte Carlo Tree Search (MCTS) methods to help select EIRs by considering comprehensively from topological, architectural and physical aspects. Evaluation results show that, compared with prior work, the proposed EquiNox is able to reduce execution time by 23.5%, energy consumption by 18.9%, and EDP by 32.8%, under similar hardware cost.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065547","Networks on Chip;Throughput processors;Monte Carlo Tree Search;Interposer","Program processors;Micromechanical devices;Throughput;Silicon;System-on-chip;Wiring;Wires","","1","","47","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"DRAIN: Deadlock Removal for Arbitrary Irregular Networks","M. Parasar; H. Farrokhbakht; N. Enright Jerger; P. V. Gratz; T. Krishna; J. San Miguel",Georgia Institute of Technology; University of Toronto; University of Toronto; Texas A & M; Georgia Institute of Technology; University of Wisconsin-Madison,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","447","460","Correctness is a first-order concern in the design of computer systems. For multiprocessors, a primary correctness concern is the deadlock-free operation of the network and its coherence protocol; furthermore, we must guarantee the continued correctness of the network in the face of increasing faults. Designing for deadlock freedom is expensive. Prior solutions either sacrifice performance or power efficiency to proactively avoid deadlocks or impose high hardware complexity to reactively resolve deadlocks as they occur. However, the precise confluence of events that lead to deadlocks is so rare that minimal resources and time should be spent to ensure deadlock freedom. To that end, we propose DRAIN, a subactive approach to remove potential deadlocks without needing to explicitly detect or avoid them. We simply let deadlocks happen and periodically drain (i.e., force the movement of) packets in the network that may be involved in a cyclic dependency. As deadlocks are a rare occurrence, draining can be performed infrequently and at low cost. Unlike prior solutions, DRAIN eliminates not only routing-level but also protocol-level deadlocks without the need for expensive virtual networks. DRAIN dramatically simplifies deadlock freedom for irregular topologies and networks that are prone to wear-related faults. Our evaluations show that on an average, DRAIN can save 26.73% packet latency compared to proactive deadlock-freedom schemes in the presence of faults while saving 77.6% power compared to reactive schemes.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065569","networks;deadlock;routing deadlock;protocol deadlock;cache coherence;interconnection network;on chip network;subactive approach;loseless network;virtual channel;physical channel","System recovery;Routing;Hardware;Network topology;Routing protocols;Topology","","28","","44","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"SnackNoC: Processing in the Communication Layer","K. Sangaiah; M. Lui; R. Kuttappa; B. Taskin; M. Hempstead","Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Tufts University, Medford, MA, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","461","473","In this work, we propose and evaluate a Network-on-Chip (NoC) augmented with light-weight processing elements to provide a lean dataflow-style system. We show that contemporary NoC routers can frequently experience long periods of idle time, with less than 10% link utilization in HPC applications. By repurposing the temporal and spatial slack of the NoC, the proposed platform, SnackNoC, is able to compute linear algebra kernels efficiently within the communication layer with minimal additional resource costs. SnackNoC 'Snack' application kernels are programmed with a producer-consumer data model that uses the NoC slack to store and transmit intermediate data between processing elements. SnackNoC is demonstrated in a multi-program environment that continually executes linear algebra kernels on the NoC simultaneously with chip multiprocessor (CMP) applications on the processor cores. Linear algebra kernels are computed up to 14.2x faster on SnackNoC compared to an Intel Haswell EPx86 processing core. The cost of executing 'snack' kernels in parallel to the CMP applications is a minimal runtime impact of 0.01% to 0.83% due to higher link utilization, and an uncore area overhead of 1.1%.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065591","in network processing;opportunistic computing;Network on chip;computer architecture","Kernel;Linear algebra;Topology;Computational modeling;Benchmark testing;Runtime;Fabrics","","10","","46","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"PIXEL: Photonic Neural Network Accelerator","K. Shiflett; D. Wright; A. Karanth; A. Louri","School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio; Dept. of Electrical and Computer Engineering, George Washington University, Washington, DC",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","474","487","Machine learning (ML) architectures such as Deep Neural Networks (DNNs) have achieved unprecedented accuracy on modern applications such as image classification and speech recognition. With power dissipation becoming a major concern in ML architectures, computer architects have focused on designing both energy-efficient hardware platforms as well as optimizing ML algorithms. To dramatically reduce power consumption and increase parallelism in neural network accelerators, disruptive technology such as silicon photonics has been proposed which can improve the performance-per-Watt when compared to electrical implementation. In this paper, we propose PIXEL - Photonic Neural Network Accelerator that efficiently implements the fundamental operation in neural computation, namely the multiply and accumulate (MAC) functionality using photonic components such as microring resonators (MRRs) and Mach-Zehnder interferometer (MZI). We design two versions of PIXEL - a hybrid version that multiplies optically and accumulates electrically and a fully optical version that multiplies and accumulates optically. We perform a detailed power, area and timing analysis of the different versions of photonic and electronic accelerators for different convolution neural networks (AlexNet, VGG16, and others). Our results indicate a significant improvement in the energy-delay product for both PIXEL designs over traditional electrical designs (48.4% for OE and 73.9% for OO) while minimizing latency, at the cost of increased area over electrical designs.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065570","deep neural network;machine learning;silicon photonics;accelerator;microring resonator;Mach-Zehnder interferometer","Photonics;Optical resonators;Optical computing;Optical design;Optical network units;Biological neural networks;Optical coupling","","43","","62","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"The Architectural Implications of Facebook's DNN-Based Personalized Recommendation","U. Gupta; C. -J. Wu; X. Wang; M. Naumov; B. Reagen; D. Brooks; B. Cottel; K. Hazelwood; M. Hempstead; B. Jia; H. -H. S. Lee; A. Malevich; D. Mudigere; M. Smelyanskiy; L. Xiong; X. Zhang",Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","488","501","The widespread application of deep learning has changed the landscape of computation in data centers. In particular, personalized recommendation for content ranking is now largely accomplished using deep neural networks. However, despite their importance and the amount of compute cycles they consume, relatively little research attention has been devoted to recommendation systems. To facilitate research and advance the understanding of these workloads, this paper presents a set of real-world, production-scale DNNs for personalized recommendation coupled with relevant performance metrics for evaluation. In addition to releasing a set of open-source workloads, we conduct in-depth analysis that underpins future system design and optimization for at-scale recommendation: Inference latency varies by 60% across three Intel server generations, batching and co-location of inference jobs can drastically improve latency-bounded throughput, and diversity across recommendation models leads to different optimization strategies.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065589","","Computational modeling;Data centers;Computer architecture;Throughput;Optimization;Artificial intelligence;Videos","","175","","62","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"NVDIMM-C: A Byte-Addressable Non-Volatile Memory Module for Compatibility with Standard DDR Memory Interfaces","C. Lee; W. Shin; D. J. Kim; Y. Yu; S. -J. Kim; T. Ko; D. Seo; J. Park; K. Lee; S. Choi; N. Kim; V. G; A. George; V. V; D. Lee; K. Choi; C. Song; D. Kim; I. Choi; I. Jung; Y. H. Song; J. Han",Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; SAP Labs Korea; SAP Labs Korea; SAP Labs Korea; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","502","514","Currently, there are two representative non-volatile dual in-line memory module (NVDIMM) interfaces: a proprietary Intel DDR-T and the JEDEC NVDIMM-P, which are not supported by existing platforms. Adoption of new platform is costly and measuring its efficiency of migrating to the new platform is much more complex. This study is an alternative way of them—finding a new memory device that can be supported by all existing systems. In this paper, we propose an NVDIMM architecture with several system-wide mechanisms to allow the synchronous DDR4 memory interfaces to support non-deterministic (asynchronous) timing. The proposed memory architecture is implemented as a real device prototype, and also evaluated using synthetic and real workloads on an x86-64 server system.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065586","cache memory, nonvolatile memory, memory architecture, memory management, system software","Nonvolatile memory;Random access memory;Media;Timing;Memory management;Memory modules","","9","","75","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Missing the Forest for the Trees: End-to-End AI Application Performance in Edge Data Centers","D. Richins; D. Doshi; M. Blackmore; A. Thulaseedharan Nair; N. Pathapati; A. Patel; B. Daguman; D. Dobrijalowski; R. Illikkal; K. Long; D. Zimmerman; V. Janapa Reddi","The University of Texas at Austin; Intel; Intel; Intel; Intel; Intel; Intel; Intel; Intel; Intel; Intel; The University of Texas at Austin, Harvard University",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","515","528","Artificial intelligence and machine learning are experiencing widespread adoption in the industry, academia, and even public consciousness. This has been driven by the rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into developing specialized hardware AI accelerators. The rapid pace of the advances makes it easy to miss the forest for the trees: they are often developed and evaluated in a vacuum without considering the full application environment in which they must eventually operate. In this paper, we deploy and characterize Face Recognition, an AI-centric edge video analytics application built using open source and widely adopted infrastructure and ML tools. We evaluate its holistic, end-to-end behavior in a production-size edge data center and reveal the ""AI tax"" for all the processing that is involved. Even though the application is built around state-of-the-art AI and ML algorithms, it relies heavily on pre-and post-processing code which must be executed on a general-purpose CPU. As AI-centric applications start to reap the acceleration promised by so many accelerators, we find they impose stresses on the underlying software infrastructure and the data center's capabilities: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By not having to serve a wide variety of applications, we show that a purpose-built edge data center can be designed to accommodate the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure. We also discuss how our conclusions generalize beyond Face Recognition as many AI-centric applications at the edge rely upon the same underlying software and hardware infrastructure.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065599","artificial intelligence;machine learning;AI tax;edge data center;end to end;AI performance;total cost of ownership","Artificial intelligence;Data centers;Streaming media;Face;Face recognition;Acceleration;Containers","","17","","66","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Communication Lower Bound in Convolution Accelerators","X. Chen; Y. Han; Y. Wang","Center for Intelligent Computing Systems, State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Center for Intelligent Computing Systems, State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","529","541","In current convolutional neural network (CNN) accelerators, communication (i.e., memory access) dominates the energy consumption. This work provides comprehensive analysis and methodologies to minimize the communication for CNN accelerators. For the off-chip communication, we derive the theoretical lower bound for any convolutional layer and propose a dataflow to reach the lower bound. This fundamental problem has never been solved by prior studies. The on-chip communication is minimized based on an elaborate workload and storage mapping scheme. We in addition design a communication-optimal CNN accelerator architecture. Evaluations based on the 65nm technology demonstrate that the proposed architecture nearly reaches the theoretical minimum communication in a three-level memory hierarchy and it is computation dominant. The gap between the energy efficiency of our accelerator and the theoretical best value is only 37-87%.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065294","","System-on-chip;Convolution;Random access memory;Convolutional codes;Memory management;Microsoft Windows;Kernel","","28","","44","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Enabling Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design","X. Zhang; S. L. Song; C. Xie; J. Wang; W. Zhang; X. Fu","ECOMS Lab, ECE Department, University of Houston, Houston, USA; Future System Architecture (FSA) Lab, University of Sydney, Sydney, Australia; Pacific Northwest National Lab (PNNL), Richland, USA; College of Information Engineering, Capital Normal University, Beijing, China; Beijing Advanced Innovation Center for Imaging Theory and Technology, Beijing, China; ECOMS Lab, ECE Department, University of Houston, Houston, USA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","542","555","In recent years, the CNNs have achieved great successes in the image processing tasks, e.g., image recognition and object detection. Unfortunately, traditional CNN's classification is found to be easily misled by increasingly complex image features due to the usage of pooling operations, hence unable to preserve accurate position and pose information of the objects. To address this challenge, a novel neural network structure called Capsule Network has been proposed, which introduces equivariance through capsules to significantly enhance the learning ability for image segmentation and object detection. Due to its requirement of performing a high volume of matrix operations, CapsNets have been generally accelerated on modern GPU platforms that provide highly optimized software library for common deep learning tasks. However, based on our performance characterization on modern GPUs, CapsNets exhibit low efficiency due to the special program and execution features of their routing procedure, including massive unshareable intermediate variables and intensive synchronizations, which are very difficult to optimize at software level. To address these challenges, we propose a hybrid computing architecture design named PIM-CapsNet. It preserves GPU's on-chip computing capability for accelerating CNN types of layers in CapsNet, while pipelining with an off-chip in-memory acceleration solution that effectively tackles routing procedure's inefficiency by leveraging the processing-in-memory capability of today's 3D stacked memory. Using routing procedure's inherent parallellization feature, our design enables hierarchical improvements on CapsNet inference efficiency through minimizing data movement and maximizing parallel processing in memory. Evaluation results demonstrate that our proposed design can achieve substantial improvement on both performance and energy savings for CapsNet inference, with almost zero accuracy loss. The results also suggest good performance scalability in optimizing the routing procedure with increasing network size.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065441","Domain Specific Accelertors;Architectural Application of Machine Learning;Emerging Technologies","Routing;Acceleration;System-on-chip;Computer architecture;Task analysis;Heuristic algorithms;Object detection","","21","","73","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Fulcrum: A Simplified Control and Access Mechanism Toward Flexible and Practical In-Situ Accelerators","M. Lenjani; P. Gonzalez; E. Sadredini; S. Li; Y. Xie; A. Akel; S. Eilert; M. R. Stan; K. Skadron","University of Virginia; University of Virginia; University of Virginia; University of California, Santa Barbara; University of California, Santa Barbara; Micron Technology, Inc.; Micron Technology, Inc.; University of Virginia; University of Virginia",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","556","569","In-situ approaches process data very close to the memory cells, in the row buffer of each subarray. This minimizes data movement costs and affords parallelism across subarrays. However, current in-situ approaches are limited to only row-wide bitwise (or few-bit) operations applied uniformly across the row buffer. They impose a significant overhead of multiple row activations for emulating 32-bit addition and multiplications using bitwise operations and cannot support operations with data dependencies or based on predicates. Moreover, with current peripheral logic, communication among subarrays is inefficient, and with typical data layouts, bits in a word are not physically adjacent. The key insight of this work is that in-situ, single-word ALUs outperform in-situ, parallel, row-wide, bitwise ALUs by reducing the number of row activations and enabling new operations and optimizations. Our proposed lightweight access and control mechanism, Fulcrum, sequentially feeds data into the single-word ALU and enables operations with data dependencies and operations based on a predicate. For algorithms that require communication among subarrays, we augment the peripheral logic with broadcasting capabilities and a previously-proposed method for low-cost inter-subarray data movement. The sequential processor also enables overlapping of broadcasting and computation, and reuniting bits that are physically adjacent. In order to realize true subarray-level parallelism, we introduce a lightweight column-selection mechanism through shifting one-hot encoded values. This technique enables independent column selection in each subarray. We integrate Fulcrum with Compress Express Link (CXL), a new interconnect standard. Fulcrum with one memory stack delivers on average (up to) 23.4 (76) speedup over a server-class GPU, NVIDIA P100, with three stacks of HBM2 memory, (ii) 70 (228) times speedup per memory stack over the GPU, and (iii) 19 (178.9) times speedup per memory stack over an ideal model of the GPU, which only accounts for the overhead of data movement.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065566","processing in memory (PIM), in-situ computing, DRAM","Random access memory;Graphics processing units;Through-silicon vias;Three-dimensional displays;Parallel processing;Machine learning;Buffer storage","","26","1","63","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"BCoal: Bucketing-Based Memory Coalescing for Efficient and Secure GPUs","G. Kadam; D. Zhang; A. Jog","William & Mary, Williamsburg, VA; Pennsylvania State University, PA; William & Mary, Williamsburg, VA",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","570","581","Graphics Processing Units (GPUs) are becoming a de facto choice for accelerating applications from a wide range of domains ranging from graphics to high-performance computing. As a result, it is getting increasingly desirable to improve the cooperation between traditional CPUs and accelerators such as GPUs. However, given the growing security concerns in the CPU space, closer integration of GPUs has further expanded the attack surface. For example, several side-channel attacks have shown that sensitive information can be leaked from the CPU end. In the same vein, several side-channel attacks are also now being developed in the GPU world. Overall, it is challenging to keep emerging CPU-GPU heterogeneous systems secure while maintaining their performance and energy efficiency. In this paper, we focus on developing an efficient defense mechanism for a type of correlation timing attack on GPUs. Such an attack has been shown to recover AES private keys by exploiting the relationship between the number of coalesced memory accesses and total execution time. Prior state-of-the-art defense mechanisms use inefficient randomized coalescing techniques to defend against such GPU attacks and require turning-off bandwidth conserving techniques such as caches and miss-status holding registers (MSHRs) to ensure security. To address these limitations, we propose BCoal - a new bucketing-based coalescing mechanism. BCoal significantly reduces the information leakage by always issuing pre-determined numbers of coalesced accesses (called buckets). With the help of a detailed application-level analysis, BCoal determines the bucket sizes and pads, if necessary, the number of real accesses with additional (padded) accesses to meet the bucket sizes ensuring the security against the correlation timing attack. Furthermore, BCoal generates the padded accesses such that the security is ensured even in the presence of MSHRs and caches. In effect, BCoal significantly improves GPU security at a modest performance loss.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065581","GPUs;Hardware Security;Coalescing","Security;Graphics processing units;Instruction sets;Bandwidth;Correlation;Timing;Merging","","15","","55","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"HMG: Extending Cache Coherence Protocols Across Modern Hierarchical Multi-GPU Systems","X. Ren; D. Lustig; E. Bolotin; A. Jaleel; O. Villa; D. Nellans",The University of British Columbia; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","582","595","Prior work on GPU cache coherence has shown that simple hardware-or software-based protocols can be more than sufficient. However, in recent years, features such as multi-chip modules have added deeper hierarchy and non-uniformity into GPU memory systems. GPU programming models have chosen to expose this non-uniformity directly to the end user through scoped memory consistency models. As a result, there is room to improve upon earlier coherence protocols that were designed only for flat single-GPU hierarchies and/or simpler memory consistency models. In this paper, we propose HMG, a cache coherence protocol designed for forward-looking multi-GPU systems. HMG strikes a balance between simplicity and performance: it uses a readily-implementable VI-like protocol to track coherence states, but it tracks sharers using a hierarchical scheme optimized for mitigating the bandwidth limitations of inter-GPU links. HMG leverages the novel scoped, non-multi-copy-atomic properties of modern GPU memory models, and it avoids the overheads of invalidation acknowledgments and transient states that were needed to support prior GPU memory models. On a 4-GPU system, HMG improves performance over a software-controlled, bulk invalidation-based coherence mechanism by 26% and over a non-hierarchical hardware cache coherence protocol by 18%, thereby achieving 97% of the performance of an idealized caching system.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065597","","Graphics processing units;Coherence;Protocols;Synchronization;Hardware;Kernel","","24","","63","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Griffin: Hardware-Software Support for Efficient Page Migration in Multi-GPU Systems","T. Baruah; Y. Sun; A. T. Dinçer; S. A. Mojumder; J. L. Abellán; Y. Ukidave; A. Joshi; N. Rubin; J. Kim; D. Kaeli",Northeastern University; Northeastern University; Istanbul Technical University; Boston University; Universidad Católica San Antonio de Murcia; Millennium USA; Boston University; Northeastern University; KAIST; Northeastern University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","596","609","As transistor scaling becomes increasingly more difficult to achieve, scaling the core count on a single GPU chip has also become extremely challenging. As the volume of data to process in today's increasingly parallel workloads continues to grow unbounded, we need to find scalable solutions that can keep up with this increasing demand. To meet the need of modern-day parallel applications, multi-GPU systems offer a promising path to deliver high performance and large memory capacity. However, multi-GPU systems suffer from performance issues associated with GPU-to-GPU communication and data sharing, which severely impact the benefits of multi-GPU systems. Programming multi-GPU systems has been made considerably simpler with the advent of Unified Memory which enables runtime migration of pages to the GPU on demand. Current multi-GPU systems rely on a first-touch Demand Paging scheme, where memory pages are migrated from the CPU to the GPU on the first GPU access to a page. The data sharing nature of GPU applications makes deploying an efficient programmer-transparent mechanism for inter-GPU page migration challenging. Therefore following the initial CPU-to-GPU page migration, the page is pinned on that GPU. Future accesses to this page from other GPUs happen at a cache-line granularity - pages are not transferred between GPUs without significant programmer intervention. We observe that this mechanism suffers from two major drawbacks: 1) imbalance in the page distribution across multiple GPUs, and 2) inability to move the page to the GPU that uses it most frequently. Both of these problems lead to load imbalance across GPUs, degrading the performance of the multi-GPU system. To address these problems, we propose Griffin, a holistic hardware-software solution to improve the performance of NUMA multi-GPU systems. Griffin introduces programmer-transparent modifications to both the IOMMU and GPU architecture, supporting efficient runtime page migration based on locality information. In particular, Griffin employs a novel mechanism to detect and move pages at runtime between GPUs, increasing the frequency of resolving accesses locally, which in turn improves the performance. To ensure better load balancing across GPUs, Griffin employs a Delayed First-Touch Migration policy that ensures pages are evenly distributed across multiple GPUs. Our results on a diverse set of multi-GPU workloads show that Griffin can achieve up to a 2.9× speedup on a multi-GPU system, while incurring low implementation overhead.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065453","GPU;Multi GPU;NUMA;Page Migration","Graphics processing units;Programming;Memory management;Hardware;Runtime;Pipelines;Fabrics","","31","","44","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"EFLOPS: Algorithm and System Co-Design for a High Performance Distributed Training Platform","J. Dong; Z. Cao; T. Zhang; J. Ye; S. Wang; F. Feng; L. Zhao; X. Liu; L. Song; L. Peng; Y. Guo; X. Jiang; L. Tang; Y. Du; Y. Zhang; P. Pan; Y. Xie",Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","610","622","Deep neural networks (DNNs) have gained tremendous attractions as compelling solutions for applications such as image classification, object detection, speech recognition, and so forth. Its great success comes with excessive trainings to make sure the model accuracy is good enough for those applications. Nowadays, it becomes challenging to train a DNN model because of 1) the model size and data size keep increasing, which usually needs more iterations to train; 2) DNN algorithms evolve rapidly, which requires the training phase to be short for a quick deployment. To address those challenges, distributed training platforms have been proposed to leverage massive server nodes for training, with the hope of significant training time reduction. Therefore, scalability is a critical performance metric to evaluate a distributed training platform. Nevertheless, our analysis reveals that traditional server clusters have poor scalability for training due to the traffic congestions within the server and beyond. The intra-server traffic on the I/O fabric can result in severe congestions and skewed quality of service as high performance devices are competing with each other. Moreover, the traffic congestions on the Ethernet for inter-server communication could also incur significant performance degradation. In this work, we devise a novel distributed training platform, EFLOPS, that adopts an algorithm and system co-design methodology to achieve good scalability. A new server architecture is proposed to alleviate the intra-server congestions. Moreover, a new network topology, BiGraph, is proposed to divide the network into two separate parts, so that there is always a direct connection between any nodes from different parts. Finally, accompany with BiGraph, a topology-aware allreduce algorithm is proposed to eliminate the traffic congestion on the direct connection. The experimental results show that eliminating the congestions on network interface can gain up to 11.3xcommunication speedup. The proposed algorithm and topology can provide further improvement up to 6.08x. The overall performance of ResNet-50 training achieves near-linear scalability, and is competitive to the top-rankings of MLPerf results.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065603","","Training;Servers;Scalability;Fabrics;Performance evaluation;Computer architecture;Bandwidth","","45","","67","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Techniques for Reducing the Connected-Standby Energy Consumption of Mobile Devices","J. Haj-Yahya; Y. Sazeides; M. Alser; E. Rotem; O. Mutlu",ETH Zürich; University of Cyprus; ETH Zürich; Intel Corporation; ETH Zürich,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","623","636","Modern mobile devices, such as smartphones, tablets, and laptops, are idle most of the time but they remain connected to communication channels even when idle. This operation mode is called connected-standby. To increase battery life in the connected-standby mode, a mobile device enters the deepest-runtime-idle-power state (DRIPS), which minimizes power consumption and retains fast wake-up capability. In this work, we identify three sources of energy inefficiency in modern DRIPS designs and introduce three techniques to reduce the power consumption of mobile devices in connected-standby. To our knowledge, this is the first work to explicitly focus on and improve the connected-standby power management of high-performance mobile devices, with evaluations on a real system. We propose the optimized-deepest-runtime-idle-power state (ODRIPS), a mechanism that dynamically: 1) offloads the monitoring of wake-up events to low-power off-chip circuitry, which enables turning off all of the processor's clock sources, 2) offloads all of the processor's input/output functionality off-chip and power-gates the corresponding on-chip input/output functions, and 3) transfers the processor's context to a secure memory region inside DRAM, which eliminates the need to store the context using on-chip high-leakage SRAMs, thereby reducing leakage power. We implement ODRIPS in Intel's Skylake client processor and its associated Sunrise-Point chipset. An analysis of ODRIPS on a real system reveals that it reduces the platform average power consumption in connected-standby mode by 22%. We also identify an opportunity to further reduce ODRIPS power by using emerging low-power non-volatile memory (instead of DRAM) to store the processor context.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065420","Power-management;connected-standby;Energy-efficiency","Power demand;Mobile handsets;Context;Phasor measurement units;Clocks;Random access memory;Hardware","","17","","95","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Experiences with ML-Driven Design: A NoC Case Study","J. Yin; S. Sethumurugan; Y. Eckert; C. Patel; A. Smith; E. Morton; M. Oskin; N. Enright Jerger; G. H. Loh","Advanced Micro Devices, Inc.; University of Minnesota, Twin Cities; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; University of Toronto, Toronto, ON, Canada; Advanced Micro Devices, Inc.",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","637","648","There has been a lot of recent interest in applying machine learning (ML) to the design of systems, which purports to aid human experts in extracting new insights leading to better systems. In this work, we share our experiences with applying ML to improve one aspect of networks-on-chips (NoC) to uncover new ideas and approaches, which eventually led us to a new arbitration scheme that is effective for NoCs under heavy contention. However, a significant amount of human effort and creativity was still needed to optimize just one aspect (arbitration) of what is only one component (the NoC) of the overall processor. This leads us to conclude that much work (and opportunity!) remains to be done in the area of ML-driven architecture design.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065601","","Computer architecture;Learning (artificial intelligence);Artificial neural networks;Training;System analysis and design;Tuning","","18","","47","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Hybrid2: Combining Caching and Migration in Hybrid Memory Systems","E. Vasilakis; V. Papaefstathiou; P. Trancoso; I. Sourdis","CSE Dept, Chalmers University of Technology, Gothenburg, Sweden; Foundation for Research and Technology – Hellas (FORTH), Heraklion, Greece; CSE Dept, Chalmers University of Technology, Gothenburg, Sweden; CSE Dept, Chalmers University of Technology, Gothenburg, Sweden",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","649","662","This paper considers a hybrid memory system composed of memory technologies with different characteristics; in particular a small, near memory exhibiting high bandwidth, i.e., 3D-stacked DRAM, and a larger, far memory offering capacity at lower bandwidth, i.e., off-chip DRAM. In the past, the near memory of such a system has been used either as a DRAM cache or as part of a flat address space combined with a migration mechanism. Caches and migration offer different tradeoffs (between performance, main memory capacity, data transfer costs, etc.) and share similar challenges related to data-transfer granularity and metadata management. This paper proposes Hybrid2, a new hybrid memory system architecture that combines a DRAM cache with a migration scheme. Hybrid2 does not deny valuable capacity from the memory system because it uses only a small fraction of the near memory as a DRAM cache; 64MB in our experiments. It further leverages the DRAM cache as a staging area to select the data most suitable for migration. Finally, Hybrid2 alleviates the metadata overheads of both DRAM caches and migration using a common mechanism. Using near to far memory ratios of 1:16, 1:8 and 1:4 in our experiments, Hybrid2 on average outperforms current state-of-the-art migration schemes by 7.9%, 9.1% and 6.4%, respectively. In the same system configurations, compared to DRAM caches Hybrid2 gives away on average only 0.3%, 1.2%, and 5.3% of performance offering 5.9%, 12.1%, and 24.6% more main memory capacity, respectively.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065506","DRAM Cache;Data Migration;Hybrid Memory System;3D stacked DRAM;Memory","Random access memory;Bandwidth;Frequency modulation;Metadata;Three-dimensional displays;System-on-chip;Hardware","","26","","49","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Charge-Aware DRAM Refresh Reduction with Value Transformation","S. Kim; W. Kwak; C. Kim; D. Baek; J. Huh","Samsung Research, Samsung Electronics; School of Computing, KAIST; ETRI; School of Computing, KAIST; School of Computing, KAIST",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","663","676","As the memory capacity in a system has been growing, refresh operations consume increasing ratios of the total DRAM power. To reduce the power consumption of such refresh operations, this paper proposes a novel value-aware refresh reduction technique called ZERO - REFRESH which exploits zero values in memory contents. A DRAM cell can retain the discharged state without refresh operations, and ZERO - REFRESH skips refresh operations on rows with all discharged cells. For abundant unallocated memory pages in typical systems, the operating system fills them with zeros to clean the contents. For those idle pages, ZERO - REFRESH can eliminate refresh operations in an OS-transparent way without any new interface to DRAM. However, for allocated memory pages, memory contents may not have many consecutive zero values to match the refresh granularity of DRAM. To increase the frequency of zero values and to arrange them to match the refresh granularity, ZERO - REFRESH transforms the value of memory blocks to the base and delta values, inspired by the prior BDI (Base-Delta-Immediate) compression technique. Once values are converted, bits are transposed to be stored as consecutive discharged bits at the refresh granularity. Such value transformation and rearrangement can make the memory contents friendly to refresh reduction based on discharged cells. The experimental results based on simulation show that the DRAM refresh operations are reduced by 37% on average for a set of benchmark applications, if the entire memory is allocated for the applications. If the memory usage statistics collected from three data center traces are applied, the DRAM refresh operations can be reduced by 46%, 57%, and 83% respectively for the three scenarios.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065416","","Wires;Memory management;Microprocessors;DRAM chips;High-voltage techniques","","10","","44","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"DWT: Decoupled Workload Tracing for Data Centers","J. Chen; Y. Zhang; X. Jiang; L. Zhao; Z. Cao; Q. Liu","Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","677","688","Workload tracing is the foundational technology that many applications hinge upon. However, recent paradigm shift to-ward cloud computing has caused tremendous challenges to traditional workload tracing. Existing solutions either require a dedicated offline cluster or fail to capture the full-spectrum workload characteristics. This paper proposes DWT, a novel framework that leverages fast online instruction tracing, and uses synthetic data offline for memory access pattern reconstruction, thereby capturing the full workload characteristics while obviating the need of dedicated clusters. Experiment results show that the stack distance profiles generated from synthetic address traces match well with the original ones across all SPEC CPU 2017 programs and representative cloud applications, with correlation coefficient R^2 no less than 0.9. The page-level access frequencies also match well with those of the original programs. This decoupled tracing approach not only removes the roadblocks on workload characterization for data centers, but also enables new applications such as efficient online resource management.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065578","datacenter workload tracing;performance modeling;memory access patterns;synthetic data","Cloud computing;Discrete wavelet transforms;Data centers;Resource management;Tools;Computer architecture;Benchmark testing","","2","","39","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations","N. Srivastava; H. Jin; S. Smith; H. Rong; D. Albonesi; Z. Zhang",Cornell University; Cornell University; Microsoft AI & Research; Intel Parallel Computing Lab; Cornell University; Cornell University,2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","689","702","Tensor factorizations are powerful tools in many machine learning and data analytics applications. Tensors are often sparse, which makes sparse tensor factorizations memory bound. In this work, we propose a hardware accelerator that can accelerate both dense and sparse tensor factorizations. We co-design the hardware and a sparse storage format, which allows accessing the sparse data in vectorized and streaming fashion and maximizes the utilization of the memory bandwidth. We extract a common computation pattern that is found in numerous matrix and tensor operations and implement it in the hardware. By designing the hardware based on this common compute pattern, we can not only accelerate tensor factorizations but also mixed sparse-dense matrix operations. We show significant speedup and energy benefit over the state-of-the-art CPU and GPU implementations of tensor factorizations and over CPU, GPU and accelerators for matrix operations.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065579","tensor factorizations;hardware accelerators;tensor algebra;sparse tensors;sparse formats;MTTKRP;TTMc","Tensile stress;Sparse matrices;Hardware;Kernel;Acceleration;Matrix decomposition;Bandwidth","","90","","65","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"A Hybrid Systolic-Dataflow Architecture for Inductive Matrix Algorithms","J. Weng; S. Liu; Z. Wang; V. Dadu; T. Nowatzki","University of California, Los Angeles, Los Angeles, California, US; University of California, Los Angeles, Los Angeles, California, US; University of California, Los Angeles, Los Angeles, California, US; University of California, Los Angeles, Los Angeles, California, US; University of California, Los Angeles, Los Angeles, California, US",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","703","716","Dense linear algebra kernels are critical for wireless, and the oncoming proliferation of 5G only amplifies their importance. Due to the inductive nature of many such algorithms, parallelism is difficult to exploit: parallel regions have fine-grain producer/consumer interaction with iteratively changing depen-dence distance, reuse rate, and memory access patterns. This makes multi-threading impractical due to fine-grain synchronization, and vectorization ineffective due to the non-rectangular iteration domain. CPUs, DSPs, and GPUs perform order-of-magnitude below peak. Our insight is that if the nature of inductive dependences and memory accesses were explicit in the hardware/software interface, then a spatial architecture could efficiently execute parallel code regions. To this end, we first develop a novel execution model, inductive dataflow, where inductive dependence patterns and memory access patterns (streams) are first-order primitives. Second, we develop a hybrid spatial architecture combining systolic and tagged dataflow execution to attain high utilization at low energy and area cost. Finally, we create a scalable design through a novel vector-stream control model which amortizes control overhead both in time and spatially across architecture lanes. We evaluate our design, REVEL, with a full stack (compiler, ISA, simulator, RTL). Across a suite of linear algebra kernels, REVEL outperforms equally-provisioned DSPs by 4.6×-37×. Compared to state-of-the-art spatial architectures, REVEL is mean 3× faster. Compared to a set of ASICs, REVEL is only 2× the power and half the area.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065593","Spatial Architecture;Reconfigurable Accelerator;Software/Hardware Codesign;Digital Signal Processor","Computer architecture;Parallel processing;Kernel;Linear algebra;Schedules;Wireless communication;Signal processing algorithms","","56","","74","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Improving Predication Efficiency through Compaction/Restoration of SIMD Instructions","A. Barredo; J. M. Cebrian; M. Moretó; M. Casas; M. Valero","Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center; Barcelona Supercomputing Center",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),"16 Apr 2020","2020","","","717","728","Vector processors offer a wide range of unexplored opportunities to improve performance and energy efficiency. However, despite its potential, vector code generation and execution have significant challenges, the most relevant ones being control flow divergence. Most modern processors including SIMD extensions (such as AVX) rely on predication to support divergence control. In predicated codes, performance and energy consumption are usually insensitive to the number of true values in a predicated mask. This implies that the system efficiency becomes sub-optimal as vector length increases. In this paper we focus on SIMD extensions and propose a novel approach to improve execution efficiency in predicated SIMD instructions, the Compaction/Restoration (CR) technique. CR delays predicated SIMD instructions with inactive elements and compacts them with instances of the same instruction from different loop iterations to form an equivalent dense vector instruction, where, in the best case, all the elements are active. After executing such dense instructions, their results are restored to the original instructions. Our evaluation shows that CR improves performance by up to 25% and reduces dynamic energy consumption by up to 43% on real unmodified applications with predicated execution. Moreover, CR allows executing unmodified legacy code with short vector instructions (AVX-2) on newer architectures with wider vectors (AVX-512), achieving up to 56% performance benefits.","2378-203X","978-1-7281-6149-5","10.1109/HPCA47549.2020.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065430","SIMD;Energy effiency;Predication","Registers;Computer architecture;Program processors;Hardware;Benchmark testing;Compaction;Parallel processing","","4","","49","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
