"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"The Accelerator Wall: Limits of Chip Specialization","A. Fuchs; D. Wentzlaff","Department of Electrical Engineering, Princeton University; Department of Electrical Engineering, Princeton University",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","1","14","Specializing chips using hardware accelerators has become the prime means to alleviate the gap between the growing computational demands and the stagnating transistor budgets caused by the slowdown of CMOS scaling. Much of the benefits of chip specialization stems from optimizing a computational problem within a given chip's transistor budget. Unfortunately, the stagnation of the number of transistors available on a chip will limit the accelerator design optimization space, leading to diminishing specialization returns, ultimately hitting an accelerator wall. In this work, we tackle the question of what are the limits of future accelerators and chip specialization? We do this by characterizing how current accelerators depend on CMOS scaling, based on a physical modeling tool that we constructed using datasheets of thousands of chips. We identify key concepts used in chip specialization, and explore case studies to understand how specialization has progressed over time in different applications and chip platforms (e.g., GPUs, FPGAs, ASICs)1. Utilizing these insights, we build a model which projects forward to see what future gains can and cannot be enabled from chip specialization. A quantitative analysis of specialization returns and technological boundaries is critical to help researchers understand the limits of accelerators and develop methods to surmount them.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675237","Accelerator Wall;Moore's Law;CMOS Scaling","Transistors;Semiconductor device modeling;Mathematical model;Computer architecture;Field programmable gate arrays;Measurement;Throughput","","45","","80","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Stretch: Balancing QoS and Throughput for Colocated Server Workloads on SMT Cores","A. Margaritov; S. Gupta; R. Gonzalez-Alberquilla; B. Grot",University of Edinburgh; EPFL; Arm Ltd.; University of Edinburgh,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","15","27","In a drive to maximize resource utilization, today's datacenters are moving to colocation of latency-sensitive and batch workloads on the same server. State-of-the-art deployments, such as those at Google, colocate such diverse workloads even on a single SMT core. This form of aggressive colocation is afforded by virtue of the fact that a latency-sensitive service operating below its peak load has significant slack in its response latency with respect to the QoS target. The slack affords a degradation in single-thread performance, which is inevitable under SMT colocation, without compromising QoS targets. This work makes the observation that many batch applications can greatly benefit from a large instruction window to uncover ILP and MLP. Under SMT colocation, conventional wisdom holds that individual hardware threads should be limited in their ability to acquire and hold a disproportionately large share of microarchitectural resources so as not to compromise the performance of a co-running thread. We show that the performance slack inherent in latency-sensitive workloads operating at low to moderate load makes it safe to shift microarchitectural resources to a co-running batch thread without compromising QoS targets. Based on this insight, we introduce Stretch, a simple ROB partitioning scheme that is invoked by system software to provide one hardware thread with a much larger ROB partition at the expense of another thread. When Stretch is enabled for latency-sensitive workloads operating below their peak load on an SMT core, co-running batch applications gain 13% of performance on average (30% max) over a baseline SMT colocation and without compromising QoS constraints.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675191","Quality of service;Datacenter;Simultaneous multithreading;Latency-sensitive applications;Microarchitecture","Quality of service;Servers;Hardware;Web search;Google;Microarchitecture;Degradation","","19","","37","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"CIDR: A Cost-Effective In-Line Data Reduction System for Terabit-Per-Second Scale SSD Arrays","M. Ajdari; P. Park; J. Kim; D. Kwon; J. Kim","Department of Computer Science and Engineering, POSTECH, Pohang, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","28","41","An SSD array, a storage system consisting of multiple SSDs per node, has become a design choice to implement a fast primary storage system, and modern storage architects now aim to achieve terabit-per-second scale performance with the next-generation SSD array. To reduce the storage cost and improve the device endurability, such SSD array must employ data reduction schemes (i.e., deduplication, compression), which provide high data reduction capability at minimum costs. However, existing data reduction schemes do not scale with the fast increasing performance of an SSD array, due to inhibitive amount of CPU resources (e.g., in software-based schemes) or low data reduction ratio (e.g., in SSD device wide deduplication) or being cost ineffective to address workload changes in datacenters (e.g., in ASIC-based acceleration). In this paper, we propose CIDR, a novel FPGA-based, cost-effective data reduction system for an SSD array to achieve the terabit-per-second scale storage performance. Our key ideas are as follows. First, we decouple data reduction related computing tasks from the unscalable host CPUs by offloading them to a scalable array of FPGA boards. Second, we employ a centralized, node-wide metadata management scheme to achieve an SSD array-wide, high data reduction. Third, our FPGA-based reconfiguration adapts to different workload patterns by dynamically balancing the amount of software and hardware tasks running on CPUs and FPGAs, respectively. For evaluation, we built our example CIDR prototype achieving up to 12.8 GB/s (0.1 Tbps) on one FPGA. CIDR outperforms the baseline for a write-only workload by up to 2.47x and a mixed read-write workload by an expected 3.2x, respectively. We showed CIDR's scalability to achieve Tbps-scale performance by measuring a two-FPGA CIDR and projecting the performance impacts for more FPGAs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675249","deduplication;compression;FPGA;SSD array","Arrays;Servers;Throughput;Field programmable gate arrays;Nonvolatile memory;Hardware;Acceleration","","14","","62","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Composite-ISA Cores: Enabling Multi-ISA Heterogeneity Using a Single ISA","A. Venkat; H. Basavaraj; D. M. Tullsen","University of Virginia; University of California, San Diego; University of California, San Diego",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","42","55","Heterogeneous multicore architectures are comprised of multiple cores of different sizes, organizations, and capabilities. These architectures maximize both performance and energy efficiency by allowing applications to adapt to phase changes by migrating execution to the most efficient core. Heterogeneous-ISA architectures further take advantage of the inherent ISA preferences of different application phases to provide additional performance and efficiency gains. This work proposes composite-ISA cores that implement composite feature sets made available from a single large superset ISA. This architecture has the potential to recreate, and in many cases supersede, the gains of multi-ISA heterogeneity, by leveraging a single composite-ISA, exploiting greater flexibility in ISA choice. Composite-ISA CMPs enhance existing performance gains due to hardware heterogeneity by an average of 19%, and have the potential to achieve an additional 31% energy savings and 35% reduction in Energy Delay Product, with no loss in performance.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675215","heterogeneous architectures;multicore processors;isa;risc;cisc;energy efficiency;single-thread performance","Registers;Multicore processing;Hardware;Program processors;Microarchitecture;Complexity theory","","19","","115","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array","L. Song; J. Mao; Y. Zhuo; X. Qian; H. Li; Y. Chen",Duke University; Duke University; University of Southern California; University of Southern California; Duke University; Duke University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","56","68","With the rise of artificial intelligence in recent years, Deep Neural Networks (DNNs) have been widely used in many domains. To achieve high performance and energy efficiency, hardware acceleration (especially inference) of DNNs is intensively studied both in academia and industry. However, we still face two challenges: large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. It poses the key research question to seek the best organization of computation and dataflow among accelerators. In this paper, we propose a solution HYPAR to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. HYPAR partitions the feature map tensors (input and output), the kernel tensors, the gradient tensors, and the error tensors for the DNN accelerators. A partition constitutes the choice of parallelism for weighted layers. The optimization target is to search a partition that minimizes the total communication during training a complete DNN. To solve this problem, we propose a communication model to explain the source and amount of communications. Then, we use a hierarchical layer-wise dynamic programming method to search for the partition for each layer. HYPAR is practical: the time complexity for the partition search in HYPAR is linear. We apply this method in an HMC-based DNN training architecture to minimize data movement. We evaluate HYPAR with ten DNN models from classic Lenet to large-size model VGGs, and the number of weighted layers of these models range from four to nineteen. Our evaluation finds that: the default Model Parallelism is indeed the worst; the default Data Parallelism is not the best; but hybrid parallelism can be better than either the default Data Parallelism or Model Parallelism in DNN training with an array of accelerators. Our evaluation shows that HYPAR achieves a performance gain of 3.39× and an energy efficiency gain of 1.51× compared to Data Parallelism on average, and HYPAR performs up to 2.40× better than “one weird trick”.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675232","Accelerator Architecture;Accelerator Array;Coarse Grain Parallelism;Communication in DNN Training;Deep Learning;DNN Training;Domain Specific Architecture;Hybrid Parallelism;Intra and Inter Layer Communication;Tensor Partition","Parallel processing;Training;Data models;Neural networks;Computer architecture;Acceleration","","88","1","118","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"[Title page iii]","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","3","3","Presents the title page of the proceedings record.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675184","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs","Z. Li; C. Ding; S. Wang; W. Wen; Y. Zhuo; C. Liu; Q. Qiu; W. Xu; X. Lin; X. Qian; Y. Wang",Syracuse University; Northeastern University; Northeastern University; Florida International University; University of Southern California; Carnegie Mellon University; Syracuse University; SUNY University at Buffalo; Northeastern University; University of Southern California; Northeastern University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","69","80","Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The two major types are Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. It is a challenging task to have real-time, efficient, and accurate hardware RNN implementations because of the high sensitivity to imprecision accumulation and the requirement of special activation function implementations. Recently two works have focused on FPGA implementation of inference phase of LSTM RNNs with model compression. First, ESE uses a weight pruning based compressed RNN model but suffers from irregular network structure after pruning. The second work C-LSTM mitigates the irregular network limitation by incorporating block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration. A key limitation of the prior works is the lack of a systematic design optimization framework of RNN model and hardware implementations, especially when the block size (or compression ratio) should be jointly optimized with RNN type, layer size, etc. In this paper, we adopt the block-circulant matrixbased framework, and present the Efficient RNN (E-RNN) framework for FPGA implementations of the Automatic Speech Recognition (ASR) application. The overall goal is to improve performance/energy efficiency under accuracy requirement. We use the alternating direction method of multipliers (ADMM) technique for more accurate block-circulant training, and present two design explorations providing guidance on block size and reducing RNN training trials. Based on the two observations, we decompose E-RNN in two phases: Phase I on determining RNN model to reduce computation and storage subject to accuracy requirement, and Phase II on hardware implementations given RNN model, including processing element design/optimization, quantization, activation implementation, etc. 1 Experimental results on actual FPGA deployments show that E-RNN achieves a maximum energy efficiency improvement of 37.4× compared with ESE, and more than 2× compared with C-LSTM, under the same accuracy.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675229","RNN;design optimization;FPGAs;block-circulant matrix","Logic gates;Field programmable gate arrays;Hardware;Computer architecture;Training;Design optimization","","55","","43","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks","X. Wang; J. Yu; C. Augustine; R. Iyer; R. Das",University of Michigan; University of Michigan; Intel Corporation; Intel Corporation; University of Michigan,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","81","93","We propose Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks - an in-SRAM architecture for accelerating Convolutional Neural Network (CNN) inference by leveraging network redundancy and massive parallelism. The network redundancy is exploited in two ways. First, we prune and fine-tune the trained network model and develop two distinct methods - coalescing and overlapping - to run inferences efficiently with sparse models. Second, we propose an architecture for network models with a reduced bit width by leveraging bit-serial computation. Our proposed architecture achieves a 17.7×/3.7× speedup over server class CPU/GPU, and a 1.6× speedup compared to the relevant in-cache accelerator, with 2% area overhead each processor die, and no loss on top-1 accuracy for AlexNet. With a relaxed accuracy limit, our tunable architecture achieves higher speedups.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675204","In-Memory Computing;Cache;Neural Network Pruning;Low Precision Neural Network","Computational modeling;Neural networks;Random access memory;Redundancy;Arrays;Acceleration","","34","","33","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators","A. Azizimazreah; L. Chen","School of Electrical Engineering and Computer Science, Oregon State University, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","94","105","Off-chip memory traffic has been a major performance bottleneck in deep learning accelerators. While reusing on-chip data is a promising way to reduce off-chip traffic, the opportunity on reusing shortcut connection data in deep networks (e.g., residual networks) have been largely neglected. Those shortcut data accounts for nearly 40% of the total feature map data. In this paper, we propose Shortcut Mining, a novel approach that “mines” the unexploited opportunity of on-chip data reusing. We introduce the abstraction of logical buffers to address the lack of flexibility in existing buffer architecture, and then propose a sequence of procedures which, collectively, can effectively reuse both shortcut and non-shortcut feature maps. The proposed procedures are also able to reuse shortcut data across any number of intermediate layers without using additional buffer resources. Experiment results from prototyping on FPGAs show that, the proposed Shortcut Mining achieves 53.3%, 58%, and 43% reduction in off-chip feature map traffic for SqueezeNet, ResNet-34, and ResNet152, respectively and a 1.93X increase in throughput compared with a state-of-the-art accelerator.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675236","deep learning;Convolutional Neural Network;Hardware Accelerator","System-on-chip;Field programmable gate arrays;Tin;Deep learning;Memory management;Data models","","42","2","44","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Fine-Tuning the Active Timing Margin (ATM) Control Loop for Maximizing Multi-core Efficiency on an IBM POWER Server","Y. Zu; D. Richins; C. Lefurgy; V. Reddi",The University of Texas at Austin; The University of Texas at Austin; IBM Research; Harvard University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","106","119","Active Timing Margin (ATM) is a technology that improves processor efficiency by reducing the pipeline timing margin with a control loop that adjusts voltage and frequency based on real-time chip environment monitoring. Although ATM has already been shown to yield substantial performance benefits, its full potential has yet to be unlocked. In this paper, we investigate how to maximize ATM's efficiency gain with a new means of exposing the inter-core speed variation: finetuning the ATM control loop. We conduct our analysis and evaluation on a production-grade POWER7+ system. On the POWER7+ server platform, we fine-tune the ATM control loop by programming its Critical Path Monitors, a key component of its ATM design that measures the cores' timing margins. With a robust stress-test procedure, we expose over 200 MHz of inherent inter-core speed differential by fine-tuning the percore ATM control loop. Exploiting this differential, we manage to double the ATM frequency gain over the static timing margin; this is not possible using conventional means, i.e. by setting fixed <;v, f> points for each core, because the corelevel <;v, f> must account for chip-wide worst-case voltage variation. To manage the significant performance heterogeneity of fine-tuned systems, we propose application scheduling and throttling to manage the chip's process and voltage variation. Our proposal improves application performance by more than 10% over the static margin, almost doubling the 6% improvement of the default, unmanaged ATM system. Our technique is general enough that it can be adopted by any system that employs an active timing margin control loop.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675227","Active timing margin, Performance, Power effi--ciency, Reliability, Critical path monitors","Monitoring;Clocks;Temperature measurement;Temperature sensors;Voltage control;Delays","","3","","49","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"μDPM: Dynamic Power Management for the Microsecond Era","C. -H. Chou; L. N. Bhuyan; D. Wong","Ph.D. studies at UCR, Xilinx, Inc.; Department of Computer Science & Engineering, University of California, Riverside; Department of Electrical & Computer Engineering, University of California, Riverside",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","120","132","The complex, distributed nature of data centers have spawned the adoption of distributed, multi-tiered software architectures, consisting of many inter-connected microservices. These microservices exhibit extremely short request service times, often less than 250μs. We show that these “killer microsecond” service times can cause state-of-the-art dynamic power management techniques to break down, due to short idle period length and low power state transition overheads. In this paper, we propose μDPM, a dynamic power management scheme for the microsecond era that coordinates request delaying, per-core sleep states, and voltage frequency scaling. The idea is to postpone the wake up of a CPU as long as possible and then adjust the frequency so that the tail latency constraint of requests are satisfied just-in-time. μDPM reduces processor energy consumption by up to 32% and consistently outperforms state-of-the-art techniques by 2×.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675218","Dynamic power management;DVFS;Sleep states","Power system management;Servers;Energy consumption;Power demand;Data centers;Market research;Linux","","36","","57","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Adaptive Voltage/Frequency Scaling and Core Allocation for Balanced Energy and Performance on Multicore CPUs","G. Papadimitriou; A. Chatzidimitriou; D. Gizopoulos","Dept. of Informatics and Telecommunications, University of Athens, Greece; Dept. of Informatics and Telecommunications, University of Athens, Greece; Dept. of Informatics and Telecommunications, University of Athens, Greece",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","133","146","Energy efficiency is a known major concern for computing system designers. Significant effort is devoted to power optimization of modern systems, especially in large-scale installations such as data centers, in which both high performance and energy efficiency are important. Power optimization can be achieved through different approaches, several of which focus on adaptive voltage regulation. In this paper, we present a comprehensive exploration of how two server-grade systems behave in different frequency and core allocation configurations beyond nominal voltage operation. Our analysis, which is built on top of two state-of-the-art ARMv8 microprocessor chips (Applied Micro's X-Gene 2 and X-Gene 3) aims (1) to identify the best performance per watt operation points when the servers are operating in various voltage/frequency combinations, (2) to reveal how and why the different core allocation options on the available cores of the microprocessor affect the energy consumption, and (3) to enhance the default Linux scheduler to take task allocation decisions for balanced performance and energy efficiency. Our findings, on actual servers' hardware, have been integrated into a lightweight online monitoring daemon which decides the optimal combination of voltage, core allocation, and clock frequency to achieve higher energy efficiency. Our approach reduces on average the energy by 25.2% on X-Gene 2, and 22.3% on X-Gene 3, with a minimal performance penalty of 3.2% on X-Gene 2 and 2.5% on X-Gene 3, compared to the default system configuration.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675246","Energy efficiency, voltage and frequency scaling, power consumption, multicore characterization, micro-servers","Clocks;Microprocessors;Multicore processing;Benchmark testing;Resource management;Radio spectrum management;Linux","","39","","59","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Resilient Low Voltage Accelerators for High Energy Efficiency","N. Chandramoorthy; K. Swaminathan; M. Cochet; A. Paidimarri; S. Eldridge; R. V. Joshi; M. M. Ziegler; A. Buyuktosunoglu; P. Bose",IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","147","158","Low voltage architecture and design are key enablers of high throughput per watt in heterogeneous, accelerator-rich many-core designs. However, such low voltage operation poses significant challenges because of difficulties in achieving reliable functionality of on-chip memories, particularly SRAMs at these design points. In this paper, we present a technique of low-voltage neural network acceleration, where the embedded SRAM architecture is equipped with a novel application-aware supply voltage boosting capability. This technique mitigates low-voltage induced failures, while enabling Very low voltage (VLV)1 operation during most of the application run, resulting in substantial improvement in net energy efficiency. We present a framework to evaluate the impact of low-voltage SRAM errors on machine learning applications and characterize trade-offs between output inference accuracy and energy efficiency in our application-programmable supply boosted SRAM architecture. Using the proposed technique we push the limits on the minimum operable voltage (Vmin) for the desired output quality. As a proof of concept, we demonstrate these techniques on Dante, a Deep Neural Network (DNN) accelerator chip taped out in state-of-the art 14nm technology.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675205","Low voltage operation;Neural Networks;Supply Voltage Boosting;SRAM","Random access memory;Voltage control;Boosting;Computer architecture;Neural networks;Low voltage;System-on-chip","","29","","43","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Pliant: Leveraging Approximation to Improve Datacenter Resource Efficiency","N. Kulkarni; F. Qi; C. Delimitrou",Cornell University; Cornell University; Cornell University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","159","171","Cloud multi-tenancy is typically constrained to a single interactive service colocated with one or more batch, low-priority services, whose performance can be sacrificed when deemed necessary. Approximate computing applications offer the opportunity to enable tighter colocation among multiple applications whose performance is important. We present Pliant, a lightweight cloud runtime that leverages the ability of approximate computing applications to tolerate some loss in their output quality to boost the utilization of shared servers. During periods of high resource contention, Pliant employs incremental and interference-aware approximation to reduce contention in shared resources, and prevent QoS violations for co-scheduled interactive, latency-critical services. We evaluate Pliant across different interactive and approximate computing applications, and show that it preserves QoS for all co-scheduled workloads, while incurring a 2.1% loss in output quality, on average.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675193","cloud computing;datacenter;approximate computing;QoS;latency critical;dynamic recompilation;interference;runtime","Quality of service;Approximate computing;Runtime;Cloud computing;Interference;Optimization;Synchronization","","11","","64","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"[Copyright notice]","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","4","4","Presents the copyright information for the conference. May include reprint permission information.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675203","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Kelp: QoS for Accelerated Machine Learning Systems","H. Zhu; D. Lo; L. Cheng; R. Govindaraju; P. Ranganathan; M. Erez","Microsoft; Google; Google; Google; Google; The University of Texas, Austin",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","172","184","Development and deployment of machine learning (ML) accelerators in Warehouse Scale Computers (WSCs) demand significant capital investments and engineering efforts. However, even though heavy computation can be offloaded to the accelerators, applications often depend on the host system for various supporting tasks. As a result, contention on host resources, such as memory bandwidth, can significantly discount the performance and efficiency gains of accelerators. The impact of performance interference is further amplified in distributed learning, which has become increasingly common as model sizes continue to grow. In this work, we study the performance of four production machine learning workloads on three accelerator platforms. Our experiments show that these workloads are highly sensitive to host memory bandwidth contention, which can cause 40% average performance degradation when left unmanaged. To tackle this problem, we design and implement Kelp, a software runtime that isolates high priority accelerated ML tasks from memory resource interference. We evaluate Kelp with both production and artificial aggressor workloads, and compare its effectiveness with previously proposed solutions. Our evaluation shows that Kelp is effective in mitigating performance degradation of the accelerated tasks, and improves performance by 24% on average. Compared to previous work, Kelp reduces performance degradation of ML tasks by 7% and improves system efficiency by 17%. Our results further expose opportunities in future architecture designs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675247","Quality of Service;Machine Learning;Memory Bandwidth;Warehouse Scale Computers;Accelerator","Task analysis;Acceleration;Servers;Interference;Degradation;Machine learning","","14","","67","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Enhancing Server Efficiency in the Face of Killer Microseconds","A. Mirhosseini; A. Sriraman; T. F. Wenisch",University of Michigan; University of Michigan; University of Michigan,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","185","198","We are entering an era of “killer microseconds” in data center applications. Killer microseconds refer to μs-scale “holes” in CPU schedules caused by stalls to access fast I/O devices or brief idle times between requests in high throughput microservices. Whereas modern computing platforms can efficiently hide ns-scale and ms-scale stalls through micro-architectural techniques and OS context switching, they lack efficient support to hide the latency of μs-scale stalls. Simultaneous Multithreading (SMT) is an efficient way to improve core utilization and increase server performance density. Unfortunately, scaling SMT to provision enough threads to hide frequent μs-scale stalls is prohibitive and SMT co-location can often drastically increase the tail latency of cloud microservices. In this paper, we propose Duplexity, a heterogeneous server architecture that employs aggressive multithreading to hide the latency of killer microseconds, without sacrificing the Quality-of-Service (QoS) of latency-sensitive microservices. Duplexity provisions dyads (pairs) of two kinds of cores: master-cores, which each primarily executes a single latency-critical master-thread, and lender-cores, which multiplex latency-insensitive throughput threads. When the master-thread stalls, the master-core borrows filler-threads from the lender-core, filling μs-scale utilization holes of the microservice. We propose critical mechanisms, including separate memory paths for the master-thread and filler-threads, to enable master-cores to borrow filler-threads while protecting master-threads' state from disruption. Duplexity facilitates fast master-thread restart when stalls resolve and minimizes the microservice's QoS violation. Our evaluation demonstrates that Duplexity is able to achieve 1.9× higher core utilization and 2.7× lower iso-throughput 99th-percentile tail latency over an SMT-based server design, on average.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675221","Killer Microseconds;Server Architecture;Data centers;Microarchitecture","Servers;Instruction sets;Data centers;Context;Throughput;Switches;Quality of service","","25","","155","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Poly: Efficient Heterogeneous System and Application Management for Interactive Applications","S. Wang; Y. Liang; W. Zhang","CECA, Peking University, China; CECA, Peking University, China; Hong Kong University of Science and Technology",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","199","210","QoS-sensitive workloads, common in warehouse-scale datacenters, require a guaranteed stable tail latency percentile response latency) of the service. Unfortunately, the system load (e.g., BPS) fluctuates drastically during daily datacenter operations. In order to meet the maximum system BPS requirement, datacenter tends to overprovision the hardware accelerators, which makes the datacenter underutilized.Therefore, the throughput and energy efficiency scaling of the current accelerator-outfitted datacenter are very expensive for QoS-sensitive workloads. To overcome this challenge, this work introduces Poly, an OpenCL based heterogeneous system optimization framework that targets to improve the overall throughput scalability and energy proportionality while guaranteeing the QoS by efficiently utilizing GPUs and FPGAs based accelerators within datacenter. Poly is mainly composed of two phases. At compile-time, Poly automatically captures the parallel patterns in the applications and explores a comprehensive design space within and across parallel patterns. At runtime, Poly relies on a runtime kernel scheduler to judiciously make the scheduling decisions to accommodate the dynamic latency and throughput requirements. Experiments using a variety of cloud QoS-sensitive applications show that Poly improves the energy proportionality by 23%(17%) without sacrificing the QoS compared to the state-of-the-art GPU (FPGA) solution, respectively.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675214","Heterogeneous;GPU;FPGA;Performance Optimization","Kernel;Field programmable gate arrays;Throughput;Optimization;Quality of service;Cloud computing;Runtime","","7","","61","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"The What's Next Intermittent Computing Architecture","K. Ganesan; J. San Miguel; N. Enright Jerger",University of Toronto; University of Toronto; University of Toronto,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","211","223","Energy-harvesting devices operate under extremely tight energy constraints. Ensuring forward progress under frequent power outages is paramount. Applications running on these devices are typically amenable to approximation, offering new opportunities to provide better forward progress between power outages. We propose What's Next (WN), a set of anytime approximation techniques for energy harvesting: subword pipelining, subword vectorization and skim points. Skim points fundamentally decouple the checkpoint location from the recovery location upon a power outage. Ultimately, WN transforms processing on energy-harvesting devices from all-or-nothing to as-is computing. We enable an approximate (yet acceptable) result sooner and proceed to the next task when power is restored rather than resume processing from a checkpoint to yield the perfect output. WN yields speedups of 2.26x and 3.02x on non-volatile and checkpoint-based volatile processors, while still producing high-quality outputs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675223","energy harvesting,intermittent computing,approximate computing","Power system reliability;Pipeline processing;Program processors;Sugar;Blood;Computer architecture;Monitoring","","17","","53","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"eQASM: An Executable Quantum Instruction Set Architecture","X. Fu; L. Riesebos; M. A. Rol; J. van Straten; J. van Someren; N. Khammassi; I. Ashraf; R. F. L. Vermeulen; V. Newsum; K. K. L. Loh; J. C. de Sterke; W. J. Vlothuizen; R. N. Schouten; C. G. Almudever; L. DiCarlo; K. Bertels","Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Kavli Institute of Nanoscience, Delft University of Technology, Delft, GA, the Netherlands; Computer Engineering Lab, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Technische Universiteit Delft, Delft, Zuid-Holland, NL; QuTech, Delft University of Technology, Delft, GA, Netherlands; TNO, Delft, South Holland, NL; QuTech, Delft University of Technology, Delft, GA, Netherlands; QuTech, Delft University of Technology, Delft, GA, Netherlands; Kavli Institute of Nanoscience, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands; Kavli Institute of Nanoscience, Delft University of Technology, Delft, GA, the Netherlands; Quantum Computer Architecture Lab, Delft University of Technology, Delft, GA, the Netherlands",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","224","237","Bridging the gap between quantum software and hardware, recent research proposed a quantum control microarchitecture QuMA which implements the quantum microinstruction set QuMIS. However, QuMIS does not offer feedback control, and is tightly bound to the hardware implementation. Also, as the number of qubits grows, QuMA cannot fetch and execute instructions fast enough to apply all operations on qubits in time. Known as the quantum operation issue rate problem, this limitation is aggravated by the low information density of QuMIS instructions. In this paper, we propose an executable quantum instruction set architecture (QISA), called eQASM, that can be translated from quantum assembly language (QASM), supports comprehensive quantum program flow control, and is executed on a quantum control microarchitecture. eQASM alleviates the quantum operation issue rate problem by efficient timing specification, single-operation-multiple-qubit execution, and a very-long-instruction-word architecture. The definition of eQASM focuses on the assembly level to be expressive. Quantum operations are configured at compile time instead of being defined at QISA design time. We instantiate eQASM into a 32-bit instruction set targeting a seven-qubit superconducting quantum processor. We validate our design by performing several experiments on a two-qubit quantum processor.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675197","Quantum instruction set architecture (QISA);eQASM;Quantum control microarchitecture;QuMA;superconducting quantum processor","Hardware;Qubit;Microarchitecture;Computer architecture;Timing;Instruction sets","","50","1","48","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Reliability Evaluation of Mixed-Precision Architectures","F. Fernandes dos Santos; C. Lunardi; D. Oliveira; F. Libano; P. Rech","Institute of Informatics, Universidade Federal do Rio Grande do Sul (UFRGS); Institute of Informatics, Universidade Federal do Rio Grande do Sul (UFRGS); Institute of Informatics, Universidade Federal do Rio Grande do Sul (UFRGS); Institute of Informatics, Universidade Federal do Rio Grande do Sul (UFRGS); Institute of Informatics, Universidade Federal do Rio Grande do Sul (UFRGS)",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","238","249","Novel computing architectures offer the possibility to execute float point operations with different precisions. The execution of reduced precision operations, when acceptable for certain applications, is likely to reduce both the execution time and the power consumption. However, the application's error rate and the device's reliability can also be impacted by these precision changes. In this paper, we study the impact of data and operation precision changes on the reliability of modern architectures. We consider Xilinx Field-Programmable Gate-Arrays (FPGA), Intel Xeon Phis, and NVIDIA Graphics Processing Units (GPUs) executing a set of codes implemented in double, single, and half-precision IEEE754-compliant float point data. On FPGAs, the reduced area and performance improvements brought by reduced precision operations increase reliability. On Xeon Phis the compiler biases significantly double and single-precision instructions execution. This raises the drawback of increasing single-precision error rates when compared to double-precision operations. NVIDIA GPUs make use of dedicated mixed-precision cores, which draw nontrivial effects on the device reliability. Generically speaking, on GPUs half-precision allows a higher number of executions to be correctly completed before experimenting a failure. Finally, we also evaluate how transient faults impact the output correctness. Our study shows that for most applications faults in a single or half-precision data or operation are more likely to significantly modify the output value than errors in double-precision data.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675194","reliability;mixed precision;GPU;FPGA;Xeon","Computer architecture","","33","","45","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Architecting Waferscale Processors - A GPU Case Study","S. Pal; D. Petrisko; M. Tomei; P. Gupta; S. S. Iyer; R. Kumar","Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","250","263","Increasing communication overheads are already threatening computer system scaling. One approach to dramatically reduce communication overheads is waferscale processing. However, waferscale processors [1], [2], [3] have been historically deemed impractical due to yield issues [1], [4] inherent to conventional integration technology. Emerging integration technologies such as Silicon-Interconnection Fabric (Si-IF) [5], [6], [7], where pre-manufactured dies are directly bonded on to a silicon wafer, may enable one to build a waferscale system without the corresponding yield issues. As such, waferscalar architectures need to be revisited. In this paper, we study if it is feasible and useful to build today's architectures at waferscale. Using a waferscale GPU as a case study, we show that while a 300 mm wafer can house about 100 GPU modules (GPM), only a much scaled down GPU architecture with about 40 GPMs can be built when physical concerns are considered. We also study the performance and energy implications of waferscale architectures. We show that waferscale GPUs can provide significant performance and energy efficiency advantages (up to 18.9x speedup and 143x EDP benefit compared against equivalent MCM-GPU based implementation on PCB) without any change in the programming model. We also develop thread scheduling and data placement policies for waferscale GPU architectures. Our policies outperform state-of-art scheduling and data placement policies by up to 2.88x (average 1.4x) and 1.62x (average 1.11x) for 24 GPM and 40 GPM cases respectively. Finally, we build the first Si-IF prototype with interconnected dies. We observe 100% of the inter-die interconnects to be successfully connected in our prototype. Coupled with the high yield reported previously for bonding of dies on Si-IF, this demonstrates the technological readiness for building a waferscale GPU architecture.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675211","Waferscale Processors;GPU;Silicon Interconnect Fabric","Graphics processing units;Silicon;Integrated circuit interconnections;Copper;Substrates;Computer architecture","","41","1","82","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Conditional Speculation: An Effective Approach to Safeguard Out-of-Order Execution Against Spectre Attacks","P. Li; L. Zhao; R. Hou; L. Zhang; D. Meng","State Key Laboratory of Information Security, CAS and University of Chinese Academy of Sciences; State Key Laboratory of Information Security, CAS and University of Chinese Academy of Sciences; State Key Laboratory of Information Security, CAS and University of Chinese Academy of Sciences; CAS, Institute of Computing Technology; State Key Laboratory of Information Security, CAS and University of Chinese Academy of Sciences",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","264","276","Speculative execution side-channel vulnerabilities such as Spectre reveal that conventional architecture designs lack security consideration. This paper proposes a software transparent defense mechanism, named as Conditional Speculation, against Spectre vulnerabilities found on traditional out-of-order microprocessors. It introduces the concept of security dependence to mark speculative memory instructions which could leak information with potential security risk. More specifically, security-dependent instructions are detected and marked with suspect speculation flags in the Issue Queue. All the instructions can be speculatively issued for execution in accordance with the classic out-of-order pipeline. For those instructions with suspect speculation flags, they are considered as safe instructions if their speculative execution will not refill new cache lines with unauthorized privilege data. Otherwise, they are considered as unsafe instructions and thus not allowed to execute speculatively. To reduce the performance impact from not executing unsafe instructions speculatively, we investigate two filtering mechanisms, Cachehit based Hazard Filter and Trusted Page Buffer based Hazard Filter to filter out false security hazards. Our design philosophy is to speculatively execute safe instructions to maintain the performance benefits of out-of-order execution while blocking the speculative execution of unsafe instructions for security consideration. We evaluate Conditional Speculation in terms of performance, security and area. The experimental results show that the hardware overhead is marginal and the performance overhead is minimal.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675250","Spectre vulnerabilities defense;Security dependence;Speculative execution side-channel vulnerabilities","Security;Hazards;Out of order;Microprocessors;Registers","","54","3","45","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Table of contents","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","5","12","Presents the table of contents/splash page of the proceedings record.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675202","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"FPGA Accelerated INDEL Realignment in the Cloud","L. Wu; D. Bruns-Smith; F. A. Nothaft; Q. Huang; S. Karandikar; J. Le; A. Lin; H. Mao; B. Sweeney; K. Asanović; D. A. Patterson; A. D. Joseph","Computer Science Division, University of California, Berkeley, CA; Computer Science Division, University of California, Berkeley, CA; Databricks; Computer Science Division, University of California, Berkeley, CA; Computer Science Division, University of California, Berkeley, CA; Computer Science Division, University of California, Berkeley, CA; Computer Science Division, University of California, Berkeley, CA; Computer Science Division, University of California, Berkeley, CA; University of California Berkeley, Berkeley, CA, US; UC Berkeley and SiFive, Inc.; UC Berkeley and Google, Inc.; Computer Science Division, University of California, Berkeley, CA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","277","290","The amount of data being generated in genomics is predicted to be between 2 and 40 exabytes per year for the next decade, making genomic analysis the new frontier and the new challenge for precision medicine. This paper explores targeted deployment of hardware accelerators in the cloud to improve the runtime and throughput of immense-scale genomic data analyses. In particular, INDEL (INsertion/DELetion) realignment is a critical operation that enables diagnostic testings of cancer through error correction prior to variant calling. It is the slowest part of the somatic (cancer) genomic analysis pipeline, the alignment refinement pipeline, and represents roughly one-third of the execution time of time-sensitive diagnostics for acute cancer patients. To accelerate genomic analysis, this paper describes a hardware accelerator for INDEL realignment (IR), and a hardware-software framework leveraging FPGAs-as-a-service in the cloud. We chose to implement genomics analytics on FPGAs because genomic algorithms are still rapidly evolving (e.g. the de facto standard """"GATK Best Practices"""" has had five releases since January of this year). We chose to deploy genomics accelerators in the cloud to reduce capital expenditure and to provide a more quantitative performance and cost analysis. We built and deployed a sea of IR accelerators using our hardware-software accelerator development framework on AWS EC2 F1 instances. We show that our IR accelerator system performed 81x better than multi-threaded genomic analysis software while being 32x more cost efficient.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675252","Computer Architecture, Microarchitecture, Accelerator Architecture, Hardware Specialization, Genomic Analytics, INDEL Realignment, FPGA Acceleration, FPGAs-as-a-service, Cloud FPGAs","Genomics;Bioinformatics;Pipelines;Acceleration;Field programmable gate arrays;Cancer;Hardware","","29","","70","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"POWERT Channels: A Novel Class of Covert CommunicationExploiting Power Management Vulnerabilities","S. K. Khatamifard; L. Wang; A. Das; S. Kose; U. R. Karpuzcu","Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, Minnesota, USA; Department of Electrical and Computer Engineering, University of South Florida, Tampa, Florida, USA; Austin, Texas, USA; Department of Electrical and Computer Engineering, University of Rochester, Rochester, New York, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, Minnesota, USA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","291","303","To be able to meet demanding application performance requirements within a tight power budget, runtime power management must track hardware activity at a very fine granularity in both space and time. This gives rise to sophisticated power management algorithms, which need the underlying system to be both highly observable (to be able to sense changes in instantaneous power demand timely) and controllable (to be able to react to changes in instantaneous power demand timely). The end goal is allocating the power budget, which itself represents a very critical shared resource, in a fair way among active tasks of execution. Fundamentally, if not carefully managed, any system-wide shared resource can give rise to covert communication. Power budget does not represent an exception, particularly as systems are becoming more and more observable and controllable. In this paper, we demonstrate how power management vulnerabilities can enable covert communication over a previously unexplored, novel class of covert channels which we will refer to as POWERT channels. We also provide a comprehensive characterization of the POWERT channel capacity under various sharing and activity scenarios. Our analysis based on experiments on representative commercial systems reveal a peak channel capacity of 121.6 bits per second (bps).","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675190","covert channels;power management;power headroom modulation","Power system management;Power demand;Monitoring;Hardware;Software;Control systems;Runtime","","22","","28","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Killi: Runtime Fault Classification to Deploy Low Voltage Caches without MBIST","S. Ganapathy; J. Kalamatianos; B. M. Beckmann; S. Raasch; L. G. Szafaryn","AMD Research, Advanced Micro Devices, Inc.; AMD Research, Advanced Micro Devices, Inc.; AMD Research, Advanced Micro Devices, Inc.; AMD Research, Advanced Micro Devices, Inc.; Intel",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","304","316","Supply voltage (VDD) scaling is one of the most effective mechanisms to reduce energy consumption in high-performance microprocessors. However, VDD scaling is challenging for SRAM-based on-chip memories such as caches due to persistent failures at low voltage (LV). Previously designed LV-enabling mechanisms require additional Memory Built-in Self-Test (MBIST) steps, employed either offline or online to identify persistent failures for every LV operating mode. However, these additional MBIST steps are time consuming, resulting in extended boot time or delayed power state transitions. Furthermore, most prior techniques combine MBIST-based solutions with customized Error Correction Codes (ECC), which suffer from non-trivial area or performance overheads. In this paper, we highlight the practical challenges for deploying LV techniques and propose a new low-cost error protection scheme, called Killi, which leverages conventional ECC and parity to enable LV operation. Foremost, the failing lines are discovered dynamically at runtime using both parity and ECC, negating the need for extra MBIST testing. Killi then provides on demand error protection by decoupling cheap error detection from expensive error correction. Killi provides error detection capability to all lines using parity but employs Single Error Correction, Double Error Detection (SECDED) ECC for a subset of the lines with a single LV fault. All lines with more than one fault are disabled. We evaluate this completely hardware enclosed solution on a GPU write-through L2 cache and show that the Vmin (minimum reliable VDD) can be reduced to 62.5% of nominal VDD when operating at 1GHz with only a maximum of 0.8% performance degradation. As a result, an 8CU GPU with Killi can reduce the power consumption of the L2 cache by 59.3% compared to the baseline L2 cache running at nominal VDD. In addition, Killi reduces the error protection area overhead by 50% compared to SECDED ECC.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675224","cache;energy-efficiency;GPU;low voltage;reliability","Error correction codes;Circuit faults;Graphics processing units;Random access memory;Computer architecture;Microprocessors;Error correction","","5","","28","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Gables: A Roofline Model for Mobile SoCs","M. Hill; V. Janapa Reddi","Computer Sciences Department, University of Wisconsin, Madison; School Of Engineering And Applied Sciences, Harvard University",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","317","330","Over a billion mobile consumer system-on-chip (SoC) chipsets ship each year. Of these, the mobile consumer market undoubtedly involving smartphones has a significant market share. Most modern smartphones comprise of advanced SoC architectures that are made up of multiple cores, GPS, and many different programmable and fixed-function accelerators connected via a complex hierarchy of interconnects with the goal of running a dozen or more critical software usecases under strict power, thermal and energy constraints. The steadily growing complexity of a modern SoC challenges hardware computer architects on how best to do early stage ideation. Late SoC design typically relies on detailed full-system simulation once the hardware is specified and accelerator software is written or ported. However, early-stage SoC design must often select accelerators before a single line of software is written. To help frame SoC thinking and guide early stage mobile SoC design, in this paper we contribute the Gables model that refines and retargets the Roofline model---designed originally for the performance and bandwidth limits of a multicore chip---to model each accelerator on a SoC, to apportion work concurrently among different accelerators (justified by our usecase analysis), and calculate a SoC performance upper bound. We evaluate the Gables model with an existing SoC and develop several extensions that allow Gables to inform early stage mobile SoC design.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675198","Accelerator architectures;Mobile computing;System-on-Chip;Processor Architecture","Computer architecture;Computational modeling;Smart phones;Bandwidth;Fabrics;Software;Hardware","","41","","34","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Machine Learning at Facebook: Understanding Inference at the Edge","C. -J. Wu; D. Brooks; K. Chen; D. Chen; S. Choudhury; M. Dukhan; K. Hazelwood; E. Isaac; Y. Jia; B. Jia; T. Leyvand; H. Lu; Y. Lu; L. Qiao; B. Reagen; J. Spisak; F. Sun; A. Tulloch; P. Vajda; X. Wang; Y. Wang; B. Wasti; Y. Wu; R. Xian; S. Yoo; P. Zhang","Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook Inc., Menlo Park, CA, USA; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Seoul National University; Facebook, Inc.",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","331","344","At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized data-center infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a data-driven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675201","Machine learning;Edge Inference","Facebook;Smart phones;Performance evaluation;Graphics processing units;Optimization;Hardware;Machine learning","","278","2","66","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"VIP: A Versatile Inference Processor","S. Hurkat; J. F. Martínez","Cornell University; Cornell University, Ithaca, NY, USA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","345","358","We present Versatile Inference Processor (VIP), a highly programmable architecture for machine learning inference. VIP consists of 128 lightweight processing engines employing a vector processing paradigm, with a simple ISA and carefully chosen microarchitecture features. It is coupled with a modern, lightly customized, 3D-stacked memory system. Through detailed execution-driven simulations backed by RTL synthesis, we show that we can achieve online, real-time vision throughput (24 fps), at low power consumption, for both full-HD depth-from-stereo using belief propagation, and VGG-16 and VGG-19 deep neural networks (batch size of 1). Our RTL synthesis of a VIP processing engine in TSMC 28 nm technology, using a commercial standard-cell library supplied by ARM, results in 18 mm2 of silicon area and 3.5W to 4.8W of power consumption for all 128 VIP processing engines combined.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675242","accelerators;microarchitecture","Engines;Optical imaging;Field programmable gate arrays;Convolution;Task analysis;Computer architecture;Microarchitecture","","10","","62","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Darwin-WGA: A Co-processor Provides Increased Sensitivity in Whole Genome Alignments with High Speedup","Y. Turakhia; S. D. Goenka; G. Bejerano; W. J. Dally","Stanford University; Stanford University; Stanford University; NVIDIA Research, Stanford University",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","359","372","Whole genome alignment (WGA) is an indispensable tool in comparative genomics to study how different life-forms have been shaped by evolution at the molecular level. Existing software whole genome aligners require several CPU weeks to compare a pair of mammalian genomes and still miss several biologically-meaningful, high-scoring alignment regions. These aligners are based on the seed-filter-and-extend paradigm with an ungapped filtering stage. Ungapped filtering is responsible for the low sensitivity of these aligners but is used because it is 200× faster than performing gapped alignment, using dynamic programming, in software. In this paper, we show that both performance and sensitivity can be greatly improved by using a hardware accelerator for WGA. Using the genomes of two roundworms (C. elegans and C. Briggsae) and four fruit flies (D. melanogaster, D. simulans, D. yakuba, and D. pseudoobscura), we show that replacing ungapped filtering with gapped filtering increases the number of matching base-pairs in alignments by up to 3×. Our accelerator, Darwin-WGA, is the first hardware accelerator for whole genome alignment and accelerates the gapped filtering stage. Darwin-WGA also employs GACT-X, a novel algorithm used in the extension stage to align arbitrarily long genome sequences using a small on-chip memory, that provides better quality alignments at 2× improvement in memory and speed over the previously published GACT algorithm. Implemented on an FPGA, Darwin-WGA provides up to 24× improvement (performance/$) in WGA over iso-sensitive software. An ASIC implementation of the proposed architecture on TSMC 40nm technology takes around 43W power with 36mm2 area. It achieves up to 10× performance/watt improvement on whole genome alignments over state-of-the-art software at higher sensitivity, and up to 1,500× performance/watt improvement compared to iso-sensitive software. Darwin-WGA is released under open-source MIT license and is available from https://github.com/gsneha26/Darwin-WGA.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675186","Co-processor;Comparative Genomics;Whole Genome Alignment;Gapped Filtering","Genomics;Bioinformatics;Sensitivity;Software;Hardware;Acceleration","","21","","71","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads","A. Basak; S. Li; X. Hu; S. M. Oh; X. Xie; L. Zhao; X. Jiang; Y. Xie","University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; Alibaba, Inc.; Alibaba, Inc.; University of California, Santa Barbara",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","373","386","Graph processing is an important analysis technique for a wide range of big data applications. The ability to explicitly represent relationships between entities gives graph analytics a significant performance advantage over traditional relational databases. However, at the microarchitecture level, performance is bounded by the inefficiencies in the memory subsystem for single-machine in-memory graph analytics. This paper consists of two contributions in which we analyze and optimize the memory hierarchy for graph processing workloads. First, we perform an in-depth data-type-aware characterization of graph processing workloads on a simulated multi-core architecture. We analyze 1) the memory-level parallelism in an out-of-order core and 2) the request reuse distance in the cache hierarchy. We find that the load-load dependency chains involving different application data types form the primary bottleneck in achieving a high memory-level parallelism. We also observe that different graph data types exhibit heterogeneous reuse distances. As a result, the private L2 cache has negligible contribution to performance, whereas the shared L3 cache shows higher performance sensitivity.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675225","Graph Processing;Workload Characterization;Prefetching","Prefetching;Random access memory;Arrays;Parallel processing;Hardware;Benchmark testing","","62","","72","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Message from the General Chairs","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","13","13","Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675206","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"FPGA-Based High-Performance Parallel Architecture for Homomorphic Computing on Encrypted Data","S. Sinha Roy; F. Turan; K. Jarvinen; F. Vercauteren; I. Verbauwhede","KU Leuven, imec-COSIC, Belgium; KU Leuven, imec-COSIC, Belgium; Department of Computer Science, University of Helsinki, Finland; KU Leuven, imec-COSIC, Belgium; KU Leuven, imec-COSIC, Belgium",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","387","398","Homomorphic encryption is a tool that enables computation on encrypted data and thus has applications in privacy-preserving cloud computing. Though conceptually amazing, implementation of homomorphic encryption is very challenging and typically software implementations on general purpose computers are extremely slow. In this paper we present our year long effort to design a domain specific architecture in a heterogeneous Arm+FPGA platform to accelerate homomorphic computing on encrypted data. We design a custom co-processor for the computationally expensive operations of the well-known Fan-Vercauteren (FV) homomorphic encryption scheme on the FPGA, and make the Arm processor a server for executing different homomorphic applications in the cloud, using this FPGA-based co-processor. We use the most recent arithmetic and algorithmic optimization techniques and perform designspace exploration on different levels of the implementation hierarchy. In particular we apply circuit-level and block-level pipeline strategies to boost the clock frequency and increase the throughput respectively. To reduce computation latency, we use parallel processing at all levels. Starting from the highly optimized building blocks, we gradually build our multi-core multi-processor architecture for computing. We implemented and tested our optimized domain specific programmable architecture on a single Xilinx Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit. At 200 MHz FPGA-clock, our implementation achieves over 13x speedup with respect to a highly optimized software implementation of the FV homomorphic encryption scheme on an Intel i5 processor running at 1.8 GHz.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675244","Cloud computing, privacy in cloud computing, homomorphic encryption, FV homomorphic encryption, latticebased cryptography, polynomial multiplication, number theoretic transform, domain specific accelerator, hardware accelerator","Encryption;Cloud computing;Computer architecture;Hardware;Parallel processing;Software","","103","","36","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Bingo Spatial Data Prefetcher","M. Bakhshalipour; M. Shakerinava; P. Lotfi-Kamran; H. Sarbazi-Azad","Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology; School of Computer Science, Institute for Research in Fundamental Sciences (IPM); Department of Computer Engineering, Sharif University of Technology",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","399","411","Applications extensively use data objects with a regular and fixed layout, which leads to the recurrence of access patterns over memory regions. Spatial data prefetching techniques exploit this phenomenon to prefetch future memory references and hide the long latency of DRAM accesses. While state-of-the-art spatial data prefetchers are effective at reducing the number of data misses, we observe that there is still significant room for improvement. To select an access pattern for prefetching, existing spatial prefetchers associate observed access patterns to either a short event with a high probability of recurrence or a long event with a low probability of recurrence. Consequently, the prefetchers either offer low accuracy or lose significant prediction opportunities. We identify that associating the observed spatial patterns to just a single event significantly limits the effectiveness of spatial data prefetchers. In this paper, we make a case for associating the observed spatial patterns to both short and long events to achieve high accuracy while not losing prediction opportunities. We propose Bingo spatial data prefetcher in which short and long events are used to select the best access pattern for prefetching. We propose a storage-efficient design for Bingo in such a way that just one history table is needed to maintain the association between the access patterns and the long and short events. Through a detailed evaluation of a set of big-data applications, we show that Bingo improves system performance by 60% over a baseline with no data prefetcher and 11% over the best-performing prior spatial data prefetcher.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675188","Big-Data Applications;Memory System;Data Prefetching;Spatial Correlation","Prefetching;Spatial databases;History;Random access memory;Metadata;Hardware;System performance","","103","","89","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"NoMap: Speeding-Up JavaScript Using Hardware Transactional Memory","T. Shull; J. Choi; M. J. Garzaran; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Intel Corp.; University of Illinois at Urbana-Champaign,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","412","425","Scripting languages' inferior performance stems from compilers lacking enough static information. To address this limitation, they use JIT compilers organized into multiple tiers, with higher tiers using profiling information to generate high-performance code. Checks are inserted to detect incorrect assumptions and, when a check fails, execution transfers to a lower tier. The points of potential transfer between tiers are called Stack Map Points (SMPs). They require a consistent state in both tiers and, hence, limit code optimization across SMPs in the higher tier. This paper examines the code generated by a state-of-theart JavaScript compiler and finds that the code has a high frequency of SMPs. These SMPs rarely cause execution to transfer to lower tiers. However, both the optimization-limiting effect of the SMPs, and the overhead of the SMP-guarding checks contribute to scripting languages' low performance. To tackle this problem, we extend the compiler to generate hardware transactions around SMPs, and perform simple within-transaction optimizations enabled by transactions. We target emerging lightweight HTM systems and call our changes NoMap. We evaluate NoMap on the SunSpider and Kraken suites. We find that NoMap lowers the instruction count by an average of 14.2% and 11.5%, and the execution time by an average of 16.7% and 8.9%, for SunSpider and Kraken, respectively.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675185","JavaScript;TransactionalMemory;Compiler Optimizations;JIT Compilation","Optimization;Benchmark testing;Hardware;Python;Steady-state;Program processors;Arrays","","1","","59","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"FUSE: Fusing STT-MRAM into GPUs to Alleviate Off-Chip Memory Access Overheads","J. Zhang; M. Jung; M. Kandemir","Computer Architecture and Memory Systems Laboratory, Yonsei University; Computer Architecture and Memory Systems Laboratory, Yonsei University; Pennsylvania State University",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","426","439","In this work, we propose FUSE, a novel GPU cache system that integrates spin-transfer torque magnetic random-access memory (STT-MRAM) into the on-chip L1D cache. FUSE can minimize the number of outgoing memory accesses over the interconnection network of GPU's multiprocessors, which in turn can considerably improve the level of massive computing parallelism in GPUs. Specifically, FUSE predicts a read-level of GPU memory accesses by extracting GPU runtime information and places write-once-read-multiple (WORM) data blocks into the STT-MRAM, while accommodating write-multiple data blocks over a small portion of SRAM in the L1D cache. To further reduce the off-chip memory accesses, FUSE also allows WORM data blocks to be allocated anywhere in the STT-MRAM by approximating the associativity with the limited number of tag comparators and I/O peripherals. Our evaluation results show that, in comparison to a traditional GPU cache, our proposed heterogeneous cache reduces the number of outgoing memory references by 32% across the interconnection network, thereby improving the overall performance by 217% and reducing energy cost by 53%.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675216","GPU;L1D cache;STT-MRAM;NVM;heterogeneous cache system;associativity approximation;read-level predictor","Graphics processing units;Random access memory;System-on-chip;Registers;Fuses;Context;Grippers","","10","","59","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Featherlight Reuse-Distance Measurement","Q. Wang; X. Liu; M. Chabbi",College of William & Mary; College of William & Mary; Scalable Machines Research,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","440","453","Data locality has a profound impact on program performance. Reuse distance-the number of distinct memory locations accessed between two consecutive accesses to the same location-is the de facto, machine-independent metric of data locality in a program. Reuse distance measurement, typically, requires exhaustive instrumentation (code or binary) to log every memory access, which results in orders of magnitude runtime slowdown and memory bloat. Such high overheads impede reuse distance tools from adoption in long-running, production applications despite their usefulness. We develop RDX, a lightweight profiling tool for characterizing reuse distance in an execution; RDX typically incurs negligible time (5%) and memory (7%) overheads. RDX performs no instrumentation whatsoever but uniquely combines hardware performance counter sampling with hardware debug registers, both available in commodity CPU processors, to produce reuse-distance histograms. RDX typically has more than 90% accuracy compared to the ground truth. With the help of RDX, we are the first to characterize memory performance of long-running SPEC CPU2017 benchmarks.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675243","reuse distance;locality;hardware performance counters;debug registers;profiling","Phasor measurement units;Registers;Histograms;Hardware;Instruments;Monitoring;Tools","","22","","76","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Efficient Load Value Prediction Using Multiple Predictors and Filters","R. Sheikh; D. Hower","Qualcomm Technologies, Inc.; Qualcomm Technologies, Inc.",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","454","465","Value prediction [1], [2] has the potential to break through the performance limitations imposed by true data dependencies. Aggressive value predictors can deliver significant performance improvements, but usually require large hardware budgets. While predicting values of all instruction types is possible, prior work has shown that predicting just load values is most effective with a modest hardware budget (e.g., 8KB of prediction state [3], [4]). However, with hardware budget constraints and high prediction accuracy requirements (99%), prior work has struggled to increase the fraction of predicted loads (coverage) beyond the low 30s. In this paper, we analyzed four state-of-the-art load value predictors, and found that they complement each other. Based on that finding, we evaluated a new composite predictor that combines all four component predictors. Our results show that the composite predictor, combined with several optimizations we proposed, improve the benefit of load value prediction by 54%-74% depending on the total predictor budget. Moreover, our composite predictor delivers more than twice the coverage of first championship value prediction winner predictor (EVES [4]), and substantially increases the delivered speedup by more than 50%.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675251","Microarchitecture;Value Prediction;Address Prediction;Accuracy Monitors;Smart Training;Predictor Fusion","Benchmark testing;Hardware;Optimization;Microarchitecture;Prefetching;Load modeling;Pipelines","","11","","33","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"BRB: Mitigating Branch Predictor Side-Channels","I. Vougioukas; N. Nikoleris; A. Sandberg; S. Diestelhorst; B. M. Al-Hashimi; G. V. Merrett",Arm Research; Arm Research; Arm Research; Arm Research; University of Southampton; University of Southampton,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","466","477","Modern processors use branch prediction as an optimization to improve processor performance. Predictors have become larger and increasingly more sophisticated in order to achieve higher accuracies which are needed in high performance cores. However, branch prediction can also be a source of side channel exploits, as one context can deliberately change the branch predictor state and alter the instruction flow of another context. Current mitigation techniques either sacrifice performance for security, or fail to guarantee isolation when retaining the accuracy. Achieving both has proven to be challenging. In this work we address this by, (1) introducing the notions of steady-state and transient branch predictor accuracy, and (2) showing that current predictors increase their misprediction rate by as much as 90% on average when forced to flush branch prediction state to remain secure. To solve this, (3) we introduce the branch retention buffer, a novel mechanism that partitions only the most useful branch predictor components to isolate separate contexts. Our mechanism makes thread isolation practical, as it stops the predictor from executing cold with little if any added area and no warm-up overheads. At the same time our results show that, compared to the state-of-the-art, average misprediction rates are reduced by 15-20% without increasing area, leading to a 2% performance increase.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675222","Branch retention buffer;Branch prediction;TAGE;Perceptron;microarchitecture;context switching","Steady-state;Transient analysis;Switches;History;Context;Program processors;Security","","24","1","30","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Elastic Instruction Fetching","A. Perais; R. Sheikh; L. Yen; M. McIlvaine; R. D. Clancy","Qualcomm Datacenter Technologies, Inc.; Qualcomm Technologies, Inc.; Qualcomm Datacenter Technologies, Inc.; Qualcomm Datacenter Technologies, Inc.; Qualcomm Datacenter Technologies, Inc.",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","478","490","Branch prediction (i.e., the generation of fetch addresses) and instruction cache accesses need not be tightly coupled. As the instruction fetch stage stalls because of an ICache miss or back-pressure, the branch predictor may run ahead and generate future fetch addresses that can be used for different optimizations, such as instruction prefetching but more importantly hiding taken branch fetch bubbles. This approach is used in many commercially available high-performance design. However, decoupling branch prediction from instruction retrieval has several drawbacks. First, it can increase the pipeline depth, leading to more expensive pipeline flushes. Second, it requires a large Branch Target Buffer (BTB) to store branch targets, allowing the branch predictor to follow taken branches without decoding instruction bytes. Missing the BTB will also cause additional bubbles. In some classes of workloads, those drawbacks may significantly offset the benefits of decoupling. In this paper, we present ELastic Fetching (ELF), a hybrid mechanism that decouples branch prediction from instruction retrieval while minimizing additional bubbles on pipeline flushes and BTB misses. We present two different implementations that trade off complexity for additional performance. Both variants outperform a baseline decoupled fetcher design by up to 3.7% and 5.2%, respectively.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675212","Microarchitecture;instruction fetch;decoupled fetcher","Pipelines;Prefetching;Target tracking;Timing;Decoding;Ground penetrating radar;Geophysical measurement techniques","","10","","21","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Message from the Program Chairs","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","14","15","Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675183","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"The Best of CAL - IEEE Computer Architecture Letters in 2018","P. Gratz",NA,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","491","491","Three papers are noted: ""Amoeba: An Autonomous Backup and Recovery SSD for Ransomware Attack Defense"", by Donghyun Min, Donggyu Park, Jinwoo Ahn, Ryan Walker, Junghee Lee, Sungyong Park, Youngjae Kim, Sogang (University and University of Texas at San Antonio, USA); ""The Architectural Implications of Cloud Microservices"", by Yu Gan and Christina Delimitrou, (Cornell University, USA); and ""An Alternative Analytical Approach to Associative Processing"", Soroosh Khoram, Yue Zha, and Jing Li, (University of Wisconsin-Madison, USA).","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675240","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Poise: Balancing Thread-Level Parallelism and Memory System Performance in GPUs Using Machine Learning","S. Dublish; V. Nagarajan; N. Topham",University of Edinburgh; University of Edinburgh; University of Edinburgh,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","492","505","GPUs employ a high degree of thread-level parallelism (TLP) to hide the long latency of memory operations. However, the consequent increase in demand on the memory system causes pathological effects such as cache thrashing and bandwidth bottlenecks. As a result, high degrees of TLP can adversely affect system throughput. In this paper, we present Poise, a novel approach for balancing TLP and memory system performance in GPUs. Poise has two major components: a machine learning framework and a hardware inference engine. The machine learning framework comprises a regression model that is trained offline on a set of profiled kernels to learn best warp scheduling decisions. At runtime, the hardware inference engine uses the previously learned model to dynamically predict best warp scheduling decisions for unseen applications. Therefore, Poise helps in optimizing entirely new applications without posing any profiling, training or programming burden on the end-user. Across a set of benchmarks that were unseen during training, Poise achieves a speedup of up to 2.94× and a harmonic mean speedup of 46.6%, over the baseline greedythen-oldest warp scheduler. Poise is extremely lightweight and incurs a minimal hardware overhead of around 41 bytes per SM. It also reduces the overall energy consumption by an average of 51.6%. Furthermore, Poise outperforms the prior state-ofthe-art warp scheduler by an average of 15.1%. In effect, Poise solves a complex hardware optimization problem with considerable accuracy and efficiency.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675219","GPU;warp scheduling;cache thrashing;machine learning","Hardware;Kernel;System performance;Machine learning;Instruction sets;Training;Engines","","18","","51","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"A Hybrid Framework for Fast and Accurate GPU Performance Estimation through Source-Level Analysis and Trace-Based Simulation","X. Wang; K. Huang; A. Knoll; X. Qian","Department of Informatics, Technical University of Munich, Munich, Germany; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Department of Informatics, Technical University of Munich, Munich, Germany; Department of Computer S-cience, University of Southern California, Los Angles, USA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","506","518","This paper proposes a hybrid framework for fast and accurate performance estimation of OpenCL kernels running on GPUs. The kernel execution flow is statically analyzed and thereupon the execution trace is generated via a loop-based bidirectional branch search. Then the trace is dynamically simulated to perform a dummy execution of the kernel to obtain the estimated time. The framework does not rely on profiling or measurement results which are used in conventional performance estimation techniques. Moreover, the lightweight trace-based simulation consumes much less time than a fine-grained GPU simulator. Our framework can accurately grasp the variation trend of the execution time in the design space and robustly predict the performance of the kernels across two generations of recent Nvidia GPU architectures. Experiments on four Commercial Off-The-Shelf (COTS) GPUs show that our framework can predict the runtime performance with average Mean Absolute Percentage Error (MAPE) of 17.04% and time consumption of a few seconds. We also demonstrate the practicability of our framework with a real-world application.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675207","GPU;performance;OpenCL","Kernel;Graphics processing units;Estimation;Runtime;Hardware;Measurement;Analytical models","","17","","54","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Understanding the Future of Energy Efficiency in Multi-Module GPUs","A. Arunkumar; E. Bolotin; D. Nellans; C. -J. Wu",Arizona State University; NVIDIA; NVIDIA; Arizona State University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","519","532","As Moore's law slows down, GPUs must pivot towards multi-module designs to continue scaling performance at historical rates. Prior work on multi-module GPUs has focused on performance, while largely ignoring the issue of energy efficiency. In this work, we propose a new metric for GPU efficiency called EDP Scaling Efficiency that quantifies the effects of both strong performance scaling and overall energy efficiency in these designs. To enable this analysis, we develop a novel top-down GPU energy estimation framework that is accurate within 10% of a recent GPU design. Being decoupled from granular GPU microarchitectural details, the framework is appropriate for energy efficiency studies in future GPUs. Using this model in conjunction with performance simulation, we show that the dominating factor influencing the energy efficiency of GPUs over the next decade is GPUmodule (GPM) idle time. Furthermore, neither inter-module interconnect energy, nor GPM microarchitectural design is expected to play a key role in this regard. We demonstrate that multi-module GPUs are on a trajectory to become 2× less energy efficient than current monolithic designs; a significant issue for data centers which are already energy constrained. Finally, we show that architects must be willing to spend more (not less) energy to enable higher bandwidth inter-GPM connections, because counter-intuitively, this additional energy expenditure can reduce total GPU energy consumption by as much as 45%, providing a path to energy efficient strong scaling in the future.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675192","GPU;Moore's Law;Energy Efficiency;Energy Model;Multi Chip Module;NUMA","Graphics processing units;Measurement;Microarchitecture;Bandwidth;Hardware;Estimation;Computational modeling","","31","","66","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled Look-Ahead Architectures","S. Kondguli; M. Huang","Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","533","544","Modern societies have developed insatiable demands for more computation capabilities. Exploiting implicit parallelism to provide automatic performance improvement remains a central goal in engineering future general-purpose computing systems. One approach is to use a separate thread context to perform continuous look-ahead to improve the data and instruction supply to the main pipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective in accelerating a broad range of applications in a relatively straightforward implementation. It also has broad design flexibility as the look-ahead agent need not be concerned with correctness constraints. In this paper, we explore a number of optimizations that make the look-ahead agent more efficient and yet extract more utility from it. With these optimizations, a DLA architecture can achieve an average speedup of 1.4 over a state-of-the-art microarchitecture for a broad set of benchmark suites, making it a powerful tool to enhance single-thread performance.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675226","single thread performance;decoupled lookahead architectures","Prefetching;Optimization;Parallel processing;Skeleton;Multicore processing","","9","","42","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Recycling Data Slack in Out-of-Order Cores","G. S. Ravi; M. Lipasti","ECE Department, University of Wisconsin - Madison; ECE Department, University of Wisconsin - Madison",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","545","557","In order to operate reliably and produce expected outputs, modern processors set timing margins conservatively at design time to support extreme variations in workload and environment, imposing a high cost in performance and energy efficiency. The relentless pressure to improve execution bandwidth has exacerbated this problem, requiring instructions with increasingly diverse semantics, leading to datapaths with a large gap between best-case and worst-case timing. In practice, data slack, the unutilized portion of the clock period due to inactive critical paths in a circuit, can often be as high as half of the clock period. In this paper we propose ReDSOC, which dynamically identifies data slack and aggressively recycles it, to improve performance on Out-Of-Order (OOO) cores. It is implemented via a transparent-flow based data bypass network between the execution units of the core. Further, ReDSOC performs slack-aware OOO instruction scheduling aided by optimizations to the wakeup and select logic, to support this aggressive operation execution mechanism. ReDSOC is implemented atop OOO cores of different sizes and tested on a variety of general purpose and machine learning applications. The implementation achieves average speedups in the range of 5% to 25% across the different cores and application categories. Further, it is shown to be more efficient at improving performance in comparison to prior proposals.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675213","clock cycle slack;out-of-order core;transparent dataflow;scheduling logic","Timing;Out of order;Recycling;Clocks;Proposals;Semantics","","5","","42","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Freeway: Maximizing MLP for Slice-Out-of-Order Execution","R. Kumar; M. Alipour; D. Black-Schaffer",Norwegian University of Science and Technology (NTNU); Uppsala University; Uppsala University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","558","569","Exploiting memory level parallelism (MLP) is crucial to hide long memory and last level cache access latencies. While out-of-order (OoO) cores, and techniques building on them, are effective at exploiting MLP, they deliver poor energy efficiency due to their complex hardware and the resulting energy overheads. As energy efficiency becomes the prime design constraint, we investigate low complexity/energy mechanisms to exploit MLP. This work revisits slice-out-of-order (sOoO) cores as an energy efficient alternative to OoO cores for MLP exploitation. These cores construct slices of MLP generating instructions and execute them out-of-order with respect to the rest of instructions. However, the slices and the remaining instructions, by themselves, execute in-order. Though their energy overhead is low compared to full OoO cores, sOoO cores fall considerably behind in terms of MLP extraction. We observe that their dependence-oblivious inorder slice execution causes dependent slices to frequently block MLP generation. To boost MLP generation in sOoO cores, we introduce Freeway, a sOoO core based on a new dependence-aware slice execution policy that tracks dependent slices and keeps them out of the way of MLP extraction. The proposed core incurs minimal area and power overheads, yet approaches the MLP benefits of fully OoO cores. Our evaluation shows that Freeway outperforms the state-of-the-art sOoO core by 12% and is within 7% of the MLP limits of full OoO execution.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675234","Memory Level Parallelism;Energy efficiency;Out of order execution;Memory wall","Out of order;Traffic control;Hardware;Complexity theory;Memory management;Performance gain;Parallel processing","","20","","40","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Organizing Committee","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","16","17","Provides a listing of current committee members and society officers.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675235","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Enabling Transparent Memory-Compression for Commodity Memory Systems","V. Young; S. Kariyappa; M. K. Qureshi","Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology, Atlanta, GA, US",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","570","581","Transparent Memory-Compression (TMC) allows the system to obtain the bandwidth benefits of memory compression in an OS-transparent manner. Unfortunately, prior designs for TMC (MemZip) rely on using non-commodity memory modules, which can limit their adoption. We show that TMC can be implemented with commodity memories by storing multiple compressed lines in a single memory location and retrieving all these lines in a single memory access, thereby increasing the effective memory bandwidth. TMC requires metadata to specify the compressibility and location of the line. Unfortunately, even with dedicated metadata caches, maintaining and accessing this metadata incurs significant bandwidth overheads and causes slowdown. Our goal is to enable TMC for commodity memories by eliminating the bandwidth overheads of metadata accesses. This paper proposes PTMC (Practical and Transparent Memory-Compression), a simple design for obtaining bandwidth benefits of memory compression while relying only on commodity (nonECC) memory modules and avoiding any OS support. Our design uses a novel inline-metadata mechanism, whereby the compressibility of the line can be determined by scanning the line for a special marker word, eliminating the overheads of metadata access. We also develop a low-cost Line Location Predictor (LLP) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads. Our evaluations show that PTMC provides a speedup of up to 73%, is robust (no slowdown for any workload), and can be implemented with a total storage overhead of less than 300 bytes.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675200","Memory;Compression;Bandwidth;DRAM","Metadata;Bandwidth;Memory management;Memory modules;Hardware;Error correction codes;Organizations","","14","","48","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers with Low Latency and High Throughput","J. S. Kim; M. Patel; H. Hassan; L. Orosa; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","582","595","We propose a new DRAM-based true random number generator (TRNG) that leverages DRAM cells as an entropy source. The key idea is to intentionally violate the DRAM access timing parameters and use the resulting errors as the source of randomness. Our technique specifically decreases the DRAM row activation latency (timing parameter t R a D ) below manufacturer recommended specifications, to induce read errors, or activation failures, that exhibit true random behavior. We then aggregate the resulting data from multiple cells to obtain a TRNG capable of providing a high throughput of random numbers at low latency.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675245","","DRAM chips;Throughput;Timing;Entropy;Generators;Testing","","72","","163","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"PageSeer: Using Page Walks to Trigger Page Swaps in Hybrid Memory Systems","A. Kokolis; D. Skarlatos; J. Torrellas",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","596","608","Hybrid main memories composed of DRAM and NonVolatile Memory (NVM) combine the capacity benefits of NVM with the low-latency properties of DRAM. For highest performance, data segments should be exchanged between the two types of memories dynamically-a process known as segment swapping-based on the access patterns to the segments in the program. The key difficulty in hardware-managed swapping is to identify the appropriate segments to swap between the memories at the right time in the execution. To perform hardware-managed segment swapping both accurately and with substantial lead time, this paper proposes to use hints from the page walk in a TLB miss. We call the scheme PageSeer. During the generation of the physical address for a page in a TLB miss, the memory controller is informed. The controller uses historic data on the accesses to that page and to a subsequently-referenced page (i.e., its follower page), to potentially initiate swaps for the page and for its follower. We call these actions MMU-Triggered Prefetch Swaps. PageSeer also initiates other types of page swaps, building a complete solution for hybrid memory. Our evaluation of PageSeer with simulations of 26 workloads shows that PageSeer effectively hides the swap overhead and services many requests from the DRAM. Compared to a state-of-the-art hardware-only scheme for hybrid memory management, PageSeer on average improves performance by 19% and reduces the average main memory access time by 29%.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675233","Hybrid Memory Systems, Non-Volatile Memory, Virtual Memory, Page Walks, Page Swapping","Random access memory;Nonvolatile memory;Hardware;Memory management;Prefetching;Bandwidth","","32","","44","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube","C. Xie; X. Zhang; A. Li; X. Fu; S. Song","ECE Department., University of Houston; ECE Department., University of Houston; HPC group, Pacific Northwest National Lab (PNNL); ECE Department., University of Houston; HPC group, Pacific Northwest National Lab (PNNL)",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","609","622","With the revolutionary innovations emerging in the computer graphics domain, virtual reality (VR) has become increasingly popular and mainstream for entertainment, medical simulation and education. In the highly-interactive VR world, the motion-to-photon delay (MPD) which represents the delay from users' head motion to the responded image displayed on their head devices, is the most critical factor for a successful VR experience. Long MPD may cause user to experience significant motion anomalies: judder, lagging and sickness. In order to alleviate this negative effect, asynchronous time warp (ATW) has been proposed by VR vendors to map the rendered stereoscopic frame in the correct position using the latest head-motion information. However, after a careful investigation on the efficiency of the current GPU-accelerated ATW through executing real VR applications on modern VR hardware, we observe that the current ATW design on commercial hardware cannot achieve the ideal MPD and often cause ATW to miss the refresh deadline, resulting in motion anomalies and dropped frame rate. This is caused by two major challenges: inefficient VR execution model and intensive off-chip memory accesses. To tackle these, we propose a preemption-free Processing-In-Memory based ATW design which asynchronously executes ATW within a 3D-stacked memory, without interrupting the rendering tasks on the host GPU. We also identify a redundancy reduction mechanism to further simplify and accelerate the ATW operation. A comprehensive evaluation on our proposed design demonstrates that our PIM-based ATW can achieve the ideal MPD and provide superior user experience. Finally, we provide a design space exploration to showcase different design choices for the PIM-based ATW design.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675230","Accelerators;Hardware/software interactions;Emerging technologies;Virtual Reality;Processing in Memory;Off chip memory systems;GPUs","Rendering (computer graphics);Graphics processing units;Resists;Head;Hardware;Photonics;Task analysis","","16","","68","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Rendering Elimination: Early Discard of Redundant Tiles in the Graphics Pipeline","M. Anglada; E. de Lucas; J. -M. Parcerisa; J. L. Aragón; P. Marcuello; A. González","Universitat Politècnica de Catalunya; Semidynamics Technology Services, Barcelona; Universitat Politècnica de Catalunya; Universidad de Murcia; Universidad de Murcia; Universitat Politècnica de Catalunya",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","623","634","GPUs are one of the most energy-consuming components for real-time rendering applications, since a large number of fragment shading computations and memory accesses are involved. Main memory bandwidth is especially taxing battery-operated devices such as smart-phones. TileBased Rendering GPUs divide the screen space into multiple tiles that are independently rendered in on-chip buffers, thus reducing memory bandwidth and energy consumption. We have observed that, in many animated graphics workloads, a large number of screen tiles have the same color across adjacent frames. In this paper, we propose Rendering Elimination (RE), a novel micro-architectural technique that accurately determines if a tile will be identical to the same tile in the preceding frame before rasterization by means of comparing signatures. Since RE identifies redundant tiles early in the graphics pipeline, it completely avoids the computation and memory accesses of the most power consuming stages of the pipeline, which substantially reduces the execution time and the energy consumption of the GPU. For widely used Android applications, we show that RE achieves an average speedup of 1.74x and energy reduction of 43% for the GPU/Memory system, surpassing by far the benefits of Transaction Elimination, a state-of-the-art memory bandwidth reduction technique available in some commercial Tile-Based Rendering GPUs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675248","Graphics Pipeline;Energy Efficiency;Tile Based Rendering","Pipelines;Graphics processing units;Image color analysis;Rendering (computer graphics);Games;Redundancy;Geometry","","12","1","36","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Early Visibility Resolution for Removing Ineffectual Computations in the Graphics Pipeline","M. Anglada; E. de Lucas; J. -M. Parcerisa; J. L. Aragón; A. González","Universitat Politecnica de Catalunya; Semidynamics Technology Services, Barcelona; Universitat Politecnica de Catalunya; Universidad de Murcia; Universitat Politecnica de Catalunya",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","635","646","GPUs' main workload is real-time image rendering. These applications take a description of a (animated) scene and produce the corresponding image(s). An image is rendered by computing the colors of all its pixels. It is normal that multiple objects overlap at each pixel. Consequently, a significant amount of processing is devoted to objects that will not be visible in the final image, in spite of the widespread use of the Early Depth Test in modern GPUs, which attempts to discard computations related to occluded objects. Since animations are created by a sequence of similar images, visibility usually does not change much across consecutive frames. Based on this observation, we present Early Visibility Resolution (EVR), a mechanism that leverages the visibility information obtained in a frame to predict the visibility in the following one. Our proposal speculatively determines visibility much earlier in the pipeline than the Early Depth Test. We leverage this early visibility estimation to remove ineffectual computations at two different granularities: pixel-level and tile-level. Results show that such optimizations lead to 39% performance improvement and 43% energy savings for a set of commercial Android graphics applications running on stateof-the-art mobile GPUs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675195","Graphics Pipeline;Tile-Based Rendering;Energy Efficiency;Visibility","Color;Pipelines;Rendering (computer graphics);Image color analysis;Buffer storage;Geometry","","10","","24","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"String Figure: A Scalable and Elastic Memory Network Architecture","M. Ogleari; Y. Yu; C. Qian; E. Miller; J. Zhao","University of California, San DieRO; University of Kentucky; University of California, Santa Cruz; Pure Storage; University of California, San DieRO",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","647","660","Demand for server memory capacity and performance is rapidly increasing due to expanding working set sizes of modern applications, such as big data analytics, inmemory computing, deep learning, and server virtualization. One promising techniques to tackle this requirements is memory networking, whereby a server memory system consists of multiple 3D die-stacked memory nodes interconnected by a high-speed network. However, current memory network designs face substantial scalability and flexibility challenges. This includes (1) maintaining high throughput and low latency in large-scale memory networks at low hardware cost, (2) efficiently interconnecting an arbitrary number of memory nodes, and (3) supporting flexible memory network scale expansion and reduction without major modification of the memory network design or physical implementation. To address the challenges, we propose String Figure1, a highthroughput, elastic, and scalable memory network architecture. String Figure consists of (1) an algorithm to generate random topologies that achieve high network throughput and nearoptimal path lengths in large-scale memory networks, (2) a hybrid routing protocol that employs a mix of computation and look up tables to reduce the overhead of both in routing, (3) a set of network reconfiguration mechanisms that allow both static and dynamic network expansion and reduction. Our experiments using RTL simulation demonstrate that String Figure can interconnect over one thousand memory nodes with a shortest path length within five hops across various traffic patterns and real workloads.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675231","memory fabric;3D die-stacked DRAM;memory network;scalability;reconfiguration;routing;memory centric","Servers;Memory management;Network topology;Routing;Topology;Scalability;Sockets","","13","","62","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Program Committee","",,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","18","19","Provides a listing of current committee members and society officers.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675189","","","","","","","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"NAND-Net: Minimizing Computational Complexity of In-Memory Processing for Binary Neural Networks","H. Kim; J. Sim; Y. Choi; L. -S. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST); School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST)",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","661","673","Popular deep learning technologies suffer from memory bottlenecks, which significantly degrade the energy-efficiency, especially in mobile environments. In-memory processing for binary neural networks (BNNs) has emerged as a promising solution to mitigate such bottlenecks, and various relevant works have been presented accordingly. However, their performances are severely limited by the overheads induced by the modification of the conventional memory architectures. To alleviate the performance degradation, we propose NAND-Net, an efficient architecture to minimize the computational complexity of in-memory processing for BNNs. Based on the observation that BNNs contain many redundancies, we decomposed each convolution into sub-convolutions and eliminated the unnecessary operations. In the remaining convolution, each binary multiplication (bitwise XNOR) is replaced by a bitwise NAND operation, which can be implemented without any bit cell modifications. This NAND operation further brings an opportunity to simplify the subsequent binary accumulations (popcounts). We reduced the operation cost of those popcounts by exploiting the data patterns of the NAND outputs. Compared to the prior state-of-the-art designs, NAND-Net achieves 1.04-2.4x speedup and 34-59% energy saving, thus making it a suitable solution to implement efficient in-memory processing for BNNs.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675209","BNN;In-memory processing;Convolution","Computer architecture;Random access memory;Microprocessors;Convolution;Kernel;Neural networks;Computational complexity","","37","","39","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Active-Routing: Compute on the Way for Near-Data Processing","J. Huang; R. Reddy Puli; P. Majumder; S. Kim; R. Boyapati; K. H. Yum; E. J. Kim",Texas A&M University; NVIDIA; Texas A&M University; Texas A&M University; Intel Corporation; Texas A&M University; Texas A&M University,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","674","686","The explosion of data availability and the demand for faster data analysis have led to the emergence of applications exhibiting large memory footprint and low data reuse rate. These workloads, ranging from neural networks to graph processing, expose compute kernels that operate over myriads of data. Significant data movement requirements of these kernels impose heavy stress on modern memory subsystems and communication fabrics. To mitigate the worsening gap between high CPU computation density and deficient memory bandwidth, solutions like memory networks and near-data processing designs are being architected to improve system performance substantially. In this work, we examine the idea of mapping compute kernels to the memory network so as to leverage in-network computing in data-flow style, by means of near-data processing. We propose Active-Routing, an in-network compute architecture that enables computation on the way for near-data processing by exploiting patterns of aggregation over intermediate results of arithmetic operators. The proposed architecture leverages the massive memory-level parallelism and network concurrency to optimize the aggregation operations along a dynamically built Active-Routing Tree. Our evaluations show that Active-Routing can achieve upto 7X speedup with an average of 60% performance improvement, and reduce the energy-delay product by 80% across various benchmarks compared to the state-of-the-art processing-in-memory architecture.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675217","memory network, data-flow, in-network computing, near-data processing, processing-in-memory","Bandwidth;Computer architecture;Routing;Program processors;Kernel;Fabrics;Random access memory","","23","","40","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Understanding the Impact of Socket Density in Density Optimized Servers","M. Arora; M. Skach; W. Huang; X. An; J. Mars; L. Tang; D. M. Tullsen","Advanced Micro Devices, Inc., AMD Research; University of Michigan; Advanced Micro Devices, Inc., AMD Research; Advanced Micro Devices, Inc., AMD Research; University of Michigan; University of Michigan; University of California, San Diego",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","687","700","The increasing demand for computational power has led to the creation and deployment of large-scale data centers. During the last few years, data centers have seen improvements aimed at increasing computational density - the amount of throughput that can be achieved within the allocated physical footprint. This need to pack more compute in the same physical space has led to density optimized server designs. Density optimized servers push compute density significantly beyond what can be achieved by blade servers by using innovative modular chassis based designs. This paper presents a comprehensive analysis of the impact of socket density on intra-server thermals and demonstrates that increased socket density inside the server leads to large temperature variations among sockets due to inter-socket thermal coupling. The paper shows that traditional chip-level and data center-level temperature-aware scheduling techniques do not work well for thermally-coupled sockets. The paper proposes new scheduling techniques that account for the thermals of the socket a task is scheduled on, as well as thermally coupled nearby sockets. The proposed mechanisms provide 2.5% to 6.5% performance improvements across various workloads and as much as 17% over traditional temperature-aware schedulers for computation-heavy workloads.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675196","","Sockets;Servers;Couplings;Scheduling;Cooling;Organizations;Blades","","1","","82","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"A Scalable Priority-Aware Approach to Managing Data Center Server Power","Y. Li; C. R. Lefurgy; K. Rajamani; M. S. Allen-Ware; G. J. Silva; D. D. Heimsoth; S. Ghose; O. Mutlu",IBM; IBM; IBM; IBM; IBM; IBM; Carnegie Mellon University; ETH Zürich,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","701","714","Power management is a key component of modern data center design. Power managers must (1) ensure the costand energy-efficient utilization of the data center infrastructure, (2) maintain availability of the services provided by the center, and (3) address environmental concerns associated with the center's power consumption. While several power management techniques have been proposed and deployed in production data centers, there are still many challenges to comprehensive data center power management. This is particularly true in public cloud environments, where different jobs have different priority levels, and where high availability is critical. One example of the challenges facing public cloud data centers involves power capping. As power delivery must be highly reliable and tolerate wide variation in the load drawn by the data center components, the power infrastructure (e.g., power supplies, circuit breakers, UPS) has high redundancy and overprovisioning. During normal operation (i.e., typical server power demands, and no failures in the center), the power infrastructure is significantly underutilized. Power capping is a common solution to reduce this underutilization, by allowing more servers to be added safely (i.e., without power shortfalls) to the existing power infrastructure, and throttling power consumption in the infrequent cases where the demanded power exceeds the provisioned power capacity to avoid shortfalls. However, state-of-the-art power capping solutions are (1) not directly applicable to the redundant power infrastructure used in highly-available data centers; and (2) oblivious to differing workload priorities across the entire center when power consumption needs to be throttled, which can unnecessarily slow down high-priority work. To address this need, we develop CapMaestro, a new power management architecture with three key features for public cloud data centers. First, CapMaestro is designed to work with multiple power feeds (i.e., sources), and exploits server-level power capping to independently cap the load on each feed of a server. Second, CapMaestro uses a scalable, global priority-aware power capping approach, which accounts for power capacity at each level of the power distribution hierarchy. It exploits the underutilization of commonly-employed redundant power infrastructure at each level of the hierarchy to safely accommodate a much greater number of servers. Third, CapMaestro exploits stranded power (i.e., power budgets that are not utilized) in redundant power infrastructure to boost the performance of workloads in the data center. We add CapMaestro to a real cloud data center control plane, and demonstrate the effectiveness of all three key features. Using a large-scale data center simulation, we demonstrate that CapMaestro significantly and safely increases the number of servers for existing infrastructure. We also call out other key technical challenges the industry faces in data center power management.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675238","data center;public cloud;power capping;priority aware resource allocation","Servers;Power demand;Cloud computing;Power system management;Data center power;Feeds","","17","1","48","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Power Aware Heterogeneous Node Assembly","B. Acun; A. Buyuktosunoglu; E. K. Lee; Y. Park","IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA",2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),"28 Mar 2019","2019","","","715","727","To meet ever increasing computational requirements, supercomputers and data centers are beginning to utilize fat compute nodes with multiple hardware components such as manycore CPUs and accelerators. These components have intrinsic power variations even among same model components from same manufacturer. In this paper, we argue that node assembly techniques that consider these intrinsic power variations can achieve better power efficiency without any performance trade off on large scale supercomputing facilities and data centers. We propose three different node assembly techniques: (1) Sorted Assembly, (2) Balanced Power Assembly, and (3) Application-Aware Assembly. In Sorted Assembly, node components are categorized (or sorted) into groups according to their power efficiency, and components from the same group are assembled into a node. In Balanced Power Assembly, components are assembled to minimize node-to-node power variations. In Application-Aware Assembly, the most heavily used components by the application are selected based on the highest power efficiency. We evaluate the effectiveness and cost savings of the three techniques compared to the standard random assembly under different node counts and variability scenarios.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675210","variability;process variation;manufacturing variability;power variation;power aware;energy efficient;energy;power;node assembly;assembly;heterogeneous;GPUs;supercomputing;data center","Program processors;Temperature measurement;Data centers;Power measurement;Semiconductor device measurement;Supercomputers;Voltage measurement","","4","","38","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
