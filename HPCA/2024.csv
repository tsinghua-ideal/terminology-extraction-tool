"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Exploitation of Security Vulnerability on Retirement","K. Xu; M. Tang; Q. Wang; H. Wang","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","14","The backend of the processor executes the μops decoded from the frontend out of order, while the retirement is responsible for retiring completed μops in the Reorder Buffer in order. Consequently, the retirement may stall differently depending on the execution time of the first instruction in the Reorder Buffer. Moreover, since retirement is shared between two logical cores on the same physical core, an attacker can deduce the instructions executed on the other logical core by observing the availability of its own retirement. Based on this finding, we introduce two novel covert channels: the Different Instructions covert channel and the Same Instructions covert channel, which can transmit information across logical cores and possess the ability to bypass the existing protection strategies. Furthermore, this paper explores additional applications of retirement. On the one hand, we propose a new variant of Spectre v1 by applying the retirement to the Spectre attack using the principle that the fallback penalty of misprediction is related to the instructions speculated to be executed. On the other hand, based on the principle that different programs result in varied usage patterns of retirement, we propose an attack method that leverages the retirement to infer the program run by the victim. Finally, we discuss possible mitigations against new covert channels.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476429","","Out of order;Bandwidth;Computer architecture;Security;Retirement","","2","","44","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"GADGETSPINNER: A New Transient Execution Primitive Using the Loop Stream Detector","Y. Chen; A. Hajiabadi; T. E. Carlson","School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","15","30","Transient execution attacks constitute a major class of attacks affecting all modern out-of-order CPUs. These attacks exploit transient execution windows (i.e., the instructions that execute but never commit) to leak confidential information from victims. Existing attacks either rely on branch mispredictions, incorrect memory speculation, or deferred exception handling to create transient windows. In this work, we introduce a new transient execution primitive, called GADGETSPINNER. We exploit the Loop Stream Detector (LSD) in Intel processors to perform out-of-loop-bounds execution and perform illegal operations. Our key observation is that the LSD holds on to an old copy of branch predictions from the first iteration of the loop and keeps using this copy until a branch misprediction occurs, i.e., advances beyond the loop bound. We exploit the delay between the speculative iteration of the loop and when the branch misprediction is resolved. In this paper, we analyze the transient execution of the LSD and perform end-to-end attacks to (1) perform illegal reads from protected memory regions, (2) bypass Intel SGX and extract the weights of a trained CNN model in DNNL library, (3) break Kernel ASLR (KASLR), and finally (4) perform cross-core/cross-process attacks. We also show that many defenses for prior transient execution attacks, like secure Branch Prediction Unit (BPU) designs, fail to protect against GADGETSPINNER.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476459","Side-Channel Attacks;Loop Stream Detector;Intel CPUs;Transient Attacks","Out of order;Program processors;Detectors;Computer architecture;Programming;Libraries;Delays","","1","","72","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Uncovering and Exploiting AMD Speculative Memory Access Predictors for Fun and Profit","C. Liu; D. Wang; Y. Lyu; P. Qiu; Y. Jin; Z. Lu; Y. Zhang; G. Qu",Tsinghua University; Tsinghua University; Tsinghua University; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Southern University of Science and Technology; University of Maryland,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","31","45","This paper presents a comprehensive investigation into the security vulnerabilities associated with speculative memory access on AMD processors. Firstly, employing novel reverse engineering techniques, our study uncovers two key predictors, namely the Predictive Store Forwarding Predictor (PSFP) and the Speculative Store Bypass Predictor (SSBP), along with elucidating their internal structures and state machine designs. Secondly, our research empirically confirms that these predictors can be deliberately manipulated and altered during transient execution, resulting in secret leakage across security domains. Leveraging these discoveries, we propose innovative attacks targeting these predictors, including an out-of-place variant of Spectre-STL and an entirely new form of Spectre attack named Spectre-CTL. Finally, we establish experimentally that enabling Speculative Store Bypass Disable alleviates the vulnerabilities. However, this comes at the expense of significant performance degradation.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00014","National Key Research and Development Program of China(grant numbers:2021YFB3100902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476467","","Degradation;Program processors;Reverse engineering;Organizations;Computer architecture;Security;Transient analysis","","3","","52","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"E2EMap: End-to-End Reinforcement Learning for CGRA Compilation via Reverse Mapping","D. Liu; Y. Xia; J. Shang; J. Zhong; P. Ouyang; S. Yin","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; Beijing TsingMicro Co. Ltd., Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","46","60","Coarse-Grained Reconfigurable Arrays (CGRAs) are a promising architecture to cope with the challenges of increasing demand for high performance and high energy efficiency. However, the actual achieved performance of CGRA is highly dependent on the mappers. Traditional mappers using heuristics or combinatorial optimization can hardly learn from past experience, suffering from poor quality and portability. Recently, machine learning has been introduced to partial components in CGRA compilers, leaving other components to traditional heuristics, which is also prone to a sub-optimum, To this end, this paper proposes an end-to-end learning framework, E2EMap, for CGRA mapping that can cover the full mapping process. To reduce the complexity of the learning model, a reverse mapping problem is formulated, where various routing strategies can be thoroughly explored. To solve the problem, policy gradient reinforcement learning is introduced to learn from scratch. Experimental results demonstrate that E2EMap can achieve up to 2.23 x mapping quality across different CGRA settings while consuming even less compilation time as compared to state-of-the-art works.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00015","National Natural Science Foundation of China(grant numbers:62274019); National Key Research and Development Project of China(grant numbers:2020AAA0104603); Natural Science Foundation of Chongqing, China(grant numbers:STB2022NSCQ-MSX1017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476475","CGRA;Compilation;Reinforcement Learning","Reinforcement learning;Computer architecture;Routing;Energy efficiency;Complexity theory;Optimization","","6","","65","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Revet: A Language and Compiler for Dataflow Threads","A. C. Rucker; S. Sundram; C. Smith; M. Vilim; R. Prabhakar; F. Kjølstad; K. Olukotun","Stanford University; Stanford University; Stanford University; SambaNova Systems, Inc.; SambaNova Systems, Inc.; Stanford University; Stanford University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","14","Spatial dataflow architectures such as reconfigurable dataflow accelerators (RDA) can provide much higher performance and efficiency than CPUs and GPUs. In particular, vectorized reconfigurable dataflow accelerators (vRDA) in recent literature represent a design point that enhances the efficiency of dataflow architectures with vectorization. Today, vRDAs can be exploited using either hard-coded kernels or MapReduce languages like Spatial, which cannot vectorize data-dependent control flow. In contrast, CPUs and GPUs can be programmed using general-purpose threaded abstractions. The ideal combination would be the generality of a threaded programming model coupled with the efficient execution model of a vRDA. We introduce Revet: a programming model, compiler, and execution model that lets threaded applications run efficiently on vRDAs. The Revet programming language uses threads to support a broader range of applications than prior parallel-patterns approaches, and our MLIR-based compiler lowers this language to a generic dataflow backend that operates on streaming tensors. Finally, we show that mapping threads to dataflow out-performs GPUs, the current state-of-the-art for threaded accelerators, by 3.8×.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476453","Reconfigurable Architectures;Dataflow;RDA;vRDA;CGRA;Programming Models;Languages;Dataflow Threads","Computer languages;Tensors;Instruction sets;Graphics processing units;Computer architecture;Programming;Parallel processing","","1","","45","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator Generation","W. Zhang; J. Zhao; G. Shen; Q. Chen; C. Chen; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","75","90","With the increasing demand for computing capability given limited resource and power budgets, it is prominent to deploy applications to customized accelerators like FPGAs. However, FPGA programming is non-trivial. Although existing high-level synthesis (HLS) tools improve productivity to a certain extent, they are limited in scope and capability to support sufficient FPGA-oriented transformations and optimizations. This paper focuses on FPGA-based accelerators and proposes POM, an end-to-end optimizing framework built on multi-level intermediate representation (MLIR). POM has several features which demonstrate its scope and capability of performance optimization. First, most HLS tools depend exclusively on a single-level IR like LLVM IR to perform all the optimizations, introducing excessive information into the IR and making debugging an arduous task. In contrast, POM explicitly introduces three layers of IR to perform operations at suitable abstraction levels, streamlining the implementation and debugging process and exhibiting better flexibility, extensibility, and systematicness. Second, POM integrates the polyhedral model into MLIR and hence enables advanced dependence analysis and a wide range of FPGA-oriented loop transformations. By representing nested loops with integer sets and maps at suitable IR, loop transformations can be conducted conveniently through a series of manipulations on polyhedral semantics. Finally, to further relieve design effort, POM is equipped with a user-friendly programming interface (DSL) that allows a concise description of computation and includes a rich collection of scheduling primitives. An automatic design space exploration (DSE) engine is also provided to search for high-performance optimization schemes efficiently and generate optimized accelerators automatically. Experimental results show that POM achieves a 6.46× average speedup on typical benchmark suites and a 6.06 ×average speedup on real-world applications compared to the state-of-the-art.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476481","","Productivity;Processor scheduling;Semantics;Debugging;Programming;Extensibility;Space exploration","","1","","41","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Celeritas: Out-of-Core Based Unsupervised Graph Neural Network via Cross-Layer Computing","Y. Li; T. -Y. Yang; M. -C. Yang; Z. Shen; B. Li","Department of Computer Science, University of Texas at Dallas; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science, University of Texas at Dallas; Department of Computer Science, Shandong University; Department of Computer Science, University of Texas at Dallas",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","91","107","Graph neural networks (GNN) one of the most popular neural network models, are extensively applied in graph-related fields, including drug discovery, recommendation systems, etc. Unsupervised graph learning as one type of GNN plays a crucial role in various graph-related missions like node classification and edge prediction. However, with the increasing size of real-world graph datasets, processing such massive graphs in host memory becomes impractical, and GNN training demands a substantial storage volume to accommodate the vast amount of graph data. Consequently, GNN training results in significant I/O migration between the host and storage. Although state-of-the-art frameworks have made strides in mitigating I/O overhead by considering embedding locality, their GNN frameworks still suffer from long training times. In this paper, we propose a fully out-of-core framework, called Celeritas, which speeds up the unsupervised GNN training on a single machine by co-designing the GNN algorithm and storage systems. First, based on the theoretical analysis, we propose a new partial combination operation to enable the embedding updates across GNN layers. This cross-layer computing achieves future computation for the embedding stored in memory to save data migration. Second, due to the dependency between embedding and edges, we consider their data locality together. Based on the cross-layer computing property, we propose a new loading order to fully utilize the data stored in the main memory to save I/O. Finally, a new sampling scheme called two-level sampling is proposed associated with a new partition algorithm to further reduce data migration and computation overhead while maintaining similar training accuracy. The real system experiments indicate that the proposed Celeritas can reduce the total training time of different G NN models from 44.76 % to 73.85 % compared to state-of-art schemes for different graph datasets.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476451","","Training;Cross layer design;Accuracy;Computational modeling;Loading;Memory management;Predictive models;Graph neural networks;Partitioning algorithms;Recommender systems","","2","","70","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"PruneGNN: Algorithm-Architecture Pruning Framework for Graph Neural Network Acceleration","D. Gurevin; M. Shan; S. Huang; M. A. Hasan; C. Ding; O. Khan","University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA; University of Connecticut, Storrs, CT, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","108","123","Performing training and inference for Graph Neural Networks (GNNs) under tight latency constraints has become increasingly difficult as real-world input graphs continue to grow. Compared to traditional DNNs, GNNs present unique computational challenges due to their massive, unstructured, and sparse input graphs. Prior works have applied irregular and structured model pruning techniques to reduce the complexity of GNNs to accelerate GNN performance. However, irregular pruning techniques presented in the literature use floating point operations to estimate G NN performance, which does not reveal the true performance implications of model sparsity caused by the diminished parallelism of sparse matrix multiplication kernels. This paper quantitatively shows that irregular sparsity in G NN models is unable to be exploited to improve performance in parallel architectures that employ highly vectorized hardware. While structured pruning can overcome these issues, the existing structured pruning work for GNNs introduces performance scalability challenges as low-dimensional mapping of the pruned model is unable to exploit the full parallelism potential of the GPU's vectorized hardware. We propose PruneGNN, an optimized algorithm-architecture framework for structured GNN pruning. At the algorithm level, a dimension-pruning-aware sparse training method is proposed that achieves high sparsity while maintaining accuracy. At the architecture level, novel SIMD-aware kernels are proposed that exploit matrix-operator-level parallelism and unlock performance gains with reduced-dimension GNN models. The efficacy of the proposed framework is evaluated for end-to-end inference as well as training performance using real-world dynamic and static graphs on representative GNN models. Experimental results using an NVIDIA A100 GPU show that PruneGNN achieves an average of 2 x speedup over the prior structured pruning work for state-of-the-art GNN models.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476445","","Training;Heuristic algorithms;Scalability;Graphics processing units;Artificial neural networks;Parallel processing;Graph neural networks","","11","","57","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware Mixed-Precision Quantization","Z. Zhu; F. Li; G. Li; Z. Liu; Z. Mo; Q. Hu; X. Liang; J. Cheng","Chinese Academy of Sciences, Institute of Automation; Chinese Academy of Sciences, Institute of Automation; Shanghai Jiao Tong University; Chinese Academy of Sciences, Institute of Automation; Chinese Academy of Sciences, Institute of Automation; Chinese Academy of Sciences, Institute of Automation; Shanghai Jiao Tong University; Chinese Academy of Sciences, Institute of Automation",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","124","138","Graph Neural Networks (GNNs) are becoming a promising technique in various domains due to their excellent capabilities in modeling non-Euclidean data. Although a spectrum of accelerators has been proposed to accelerate the inference of GNNs, our analysis demonstrates that the latency and energy consumption induced by DRAM access still significantly impedes the improvement of performance and energy efficiency. To address this issue, we propose a Memory - Efficient GNN Accelerator (MEGA) through algorithm and hardware co-design in this work. Specifically, at the algorithm level, through an in-depth analysis of the node property, we observe that the data-independent quantization in previous works is not optimal in terms of accuracy and memory efficiency. This motivates us to propose the Degree-Aware mixed-precision quantization method, in which a proper bitwidth is learned and allocated to a node according to its in-degree to compress GNNs as much as possible while maintaining accuracy. At the hardware level, we employ a heterogeneous architecture design in which the aggregation and combination phases are implemented separately with different dataflows. In order to boost the performance and energy efficiency, we also present an Adaptive-Package format to alleviate the storage overhead caused by the fine-grained bitwidth and diverse sparsity, and a Condense-Edge scheduling method to enhance the data locality and further alleviate the access irregularity induced by the extremely sparse adjacency matrix in the graph. We implement our MEGA accelerator in a 28nm technology node. Extensive experiments demonstrate that MEGA can achieve an average speedup of 38.3 ×, 7.1 ×, 4.0 ×, 3.6× and 47.6 ×, 7.2 ×, 5.4 ×, 4.5 × energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW, and SGCN, respectively, while retaining task accuracy.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476431","GNN;Mixed-Precision Quantization;Accelerator","Quantization (signal);Processor scheduling;Memory management;Random access memory;Hardware;Energy efficiency;Inference algorithms","","15","","65","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Bandwidth-Effective DRAM Cache for GPU s with Storage-Class Memory","J. Hong; S. Cho; G. Park; W. Yang; Y. -H. Gong; G. Kim","Department of Computer Science and Engineering, POSTECH; Department of Computer Science and Engineering, POSTECH; Department of Computer Science and Engineering, POSTECH; Department of Computer Science and Engineering, POSTECH; Soongsil University, School of Software; Department of Computer Science and Engineering, POSTECH",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","139","155","We propose overcoming the memory capacity limitation of GPUs with high-capacity Storage-Class Memory (SCM) and DRAM cache. By significantly increasing the memory capacity with SCM, the GPU can capture a larger fraction of the memory footprint than HBM for workloads that mandate memory oversubscription, resulting in substantial speedups. However, the DRAM cache needs to be carefully designed to address the latency and bandwidth limitations of the SCM while minimizing cost overhead and considering GPU's characteristics. Because the massive number of GPU threads can easily thrash the DRAM cache and degrade performance, we first propose an SCM-aware DRAM cache bypass policy for GPUs that considers the multi-dimensional characteristics of memory accesses by G PU s with SCM to bypass DRAM for data with low performance utility. In addition, to reduce DRAM cache probe traffic and increase effective DRAM BW with minimal cost overhead, we propose a Configurable Tag Cache (CTC) that repurposes part of the L2 cache to cache DRAM cacheline tags. The L2 capacity used for the CTC can be adjusted by users for adaptability. Furthermore, to minimize DRAM cache probe traffic from CTC misses, our Aggregated Metadata-In-Last-column (AMIL) DRAM cache organization co-locates all DRAM cacheline tags in a single column within a row. The AMIL also retains the full ECC protection, unlike prior DRAM cache implementation with Tag-And-Data (TAD) organization. Additionally, we propose SCM throttling to curtail power consumption and exploiting SCM's SLC/MLC modes to adapt to workload's memory footprint. While our techniques can be used for different DRAM and SCM devices, we focus on a Heterogeneous Memory Stack (HMS) organization that stacks SCM dies on top of DRAM dies for high performance. Compared to HBM, the HMS improves performance by up to 12.5× (2.9× overall) and reduces energy by up to 89.3% (48.1 % overall). Compared to prior works, we reduce DRAM cache probe and SCM write traffic by 91–93 % and 57–75 %, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00021","National Research Foundation of Korea (NRF)(grant numbers:RS-2023-00277080,RS-2023-00212711); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476449","Graphics processing unit (GPU);Storage-Class Memory (SCM);DRAM cache;3D-stacked memory;Cache bypassing","Performance evaluation;Costs;Power demand;Instruction sets;Memory management;Random access memory;Graphics processing units","","10","","156","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Gemini: Mapping and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators","J. Cai; Z. Wu; S. Peng; Y. Wei; Z. Tan; G. Shi; M. Gao; K. Ma",Tsinghua University; Xi'an Jiaotong University; Xi'an Jiaotong University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","156","171","Chiplet technology enables the integration of an increasing number of transistors on a single accelerator with higher yield in the post-Moore era, addressing the immense computational demands arising from rapid AI advancements. However, it also introduces more expensive packaging costs and costly Die-to-Die (D2D) interfaces, which require more area, consume higher power, and offer lower bandwidth than onchip interconnects. Maximizing the benefits and minimizing the drawbacks of chiplet technology is crucial for developing largescale DNN chiplet accelerators, which poses challenges to both architecture and mapping. Despite its importance in the post-Moore era, methods to address these challenges remain scarce. To bridge the gap, we first propose a layer-centric encoding method to encode Layer-Pipeline (LP) spatial mapping for largescale DNN inference accelerators and depict the optimization space of it. Based on it, we analyze the unexplored optimization opportunities within this space, which play a more crucial role in chiplet scenarios. Based on the encoding method and a highly configurable and universal hardware template, we propose an architecture and mapping co-exploration framework, Gemini, to explore the design and mapping space of large-scale DNN chiplet accelerators while taking monetary cost (MC), performance, and energy efficiency into account. Compared to the state-of-the-art (SOTA) Simba architecture with SOTA Tangram LP Mapping, Gemini's co-optimized architecture and mapping achieve, on average, 1.98 × performance improvement and 1.41 × energy efficiency improvement simultaneously across various DNNs and batch sizes, with only a 14.3% increase in monetary cost. Moreover, we leverage Gemini to uncover intriguing insights into the methods for utilizing chiplet technology in architecture design and mapping DNN workloads under chiplet scenarios. The Gemini framework is open-sourced at https://github.com/SETScheduling-Project/GEMINI-HPCA2024.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476408","Chiplet;Deep Neural Network;Accelerator;Mapping;Design Space Exploration","Costs;Navigation;Computer architecture;Packaging;Energy efficiency;Encoding;Hardware","","25","","68","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Stellar: Energy-Efficient and Low-Latency SNN Algorithm and Hardware Co-Design with Spatiotemporal Computation","R. Mao; L. Tang; X. Yuan; Y. Liu; J. Zhou","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","172","185","The brain-inspired Spiking Neural Network (SNN) has great potential to reduce energy consumption in AI applications. However, the state-of-the-art SNN algorithms focus on high accuracy and large sparsity by constructing complex neuron models with sparse spike generation, leading to low energy efficiency and high latency. The state-of-the-art SNN hardware designs are hard to exploit high data reuse and parallel processing dataflows due to the irregularity and time-dependency of the spikes. To address the above issues, in this work we propose STELLAR, an algorithm-hardware co-design framework exploiting rich spatiotemporal dynamics of the SNN for high energy efficiency and low latency while maintaining high accuracy. Firstly, based on the Few Spikes (FS) neuron, we propose few spikes backpropagation (FSBP) and its training flow with strong hardware awareness to adaptively train the deep SNN for a short time window and few spikes. The resulting sparse SNN enjoys rapid inference with few synaptic operations and competitive accuracy. Secondly, we propose a dedicated SNN architecture and spatiotemporal Row Stationary (stRS) dataflow to exploit large sparsity brought by the proposed algorithm for highly parallel and energy -efficient computation. Several techniques have been proposed to boost energy efficiency and speedup while maintaining accuracy, including the window-based parallel processing technique and the spatiotemporal encoding-based computation architecture. The experimental results show that 1) on the algorithm level, STELLAR outperforms the state-of-the-art SNN models with significantly fewer spikes and shorter time window on both static and neuromorphic datasets with higher or comparable accuracy; 2) on the architecture level, compared with several SOTA SNN hardware designs, STELLAR achieves up to 8.1 × energy efficiency and 7.1 × speedup.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00023","National Natural Science Foundation of China(grant numbers:U203020462074026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476421","Spatiotemporal computation;FSBP-based training flow;exploit sparsity;stRS dataflow","Training;Neurons;Computer architecture;Parallel processing;Energy efficiency;Hardware;Inference algorithms","","9","","53","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Computing","G. F. Oliveira; A. Olgun; A. G. Yağlıkçı; F. N. Bostancı; J. Gómez-Luna; S. Ghose; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; Univ. of Illinois Urbana-Champaign; ETH Zürich,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","186","203","Processing-using-DRAM (PUD) is a processing-in-memory (PIM) approach that uses a DRAM array's massive internal parallelism to execute very-wide (e.g., 16,384-262,144-bit-wide) data-parallel operations, in a single-instruction multiple-data (SIMD) fashion. However, DRAM rows' large and rigid granularity limit the effectiveness and applicability of PUD in three ways. First, since applications have varying degrees of SIMD parallelism (which is often smaller than the DRAM row granularity), PUD execution often leads to underutilization, through-put loss, and energy waste. Second, due to the high area cost of implementing interconnects that connect columns in a wide DRAM row, most PUD architectures are limited to the execution of parallel map operations, where a single operation is performed over equally-sized input and output arrays. Third, the need to feed the wide DRAM row with tens of thousands of data elements combined with the lack of adequate compiler support for PUD systems create a programmability barrier, since programmers need to manually extract SIMD parallelism from an application and map computation to the PUD hardware. Our goal is to design a flexible PUD system that overcomes the limitations caused by the large and rigid granularity of PUD. To this end, we propose MIMDRAM, a hardware/software co-designed PUD system that introduces new mechanisms to allocate and control only the necessary resources for a given PUD operation. The key idea of MIMDRAM is to leverage fine-grained DRAM (i.e., the ability to independently access smaller segments of a large DRAM row) for PUD computation. MIMDRAM exploits this key idea to enable a multiple-instruction multiple-data (MIMD) execution model in each DRAM subarray (and SIMD execution within each DRAM row segment). We evaluate MIMDRAM using twelve real-world applications and 495 multi-programmed application mixes. Our evaluation shows that MIMDRAM provides 34 × the performance, 14.3 × the energy efficiency, 1.7 × the throughput, and 1.3 × the fairness of a state-of-the-art PUD framework, along with 30.6 × and 6.8 × the energy efficiency of a high-end CPU and GPU, respectively. MIMDRAM adds small area cost to a DRAM chip (1.11%) and CPU die (0.6%). We hope and believe that MIMDRAM's ideas and results will help to enable more efficient and easy-to-program PUD systems. To this end, we open source MIMDRAM at https://glthub.com/CMU-SAFARI/MIMDRAM.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476478","processing-in-memory;DRAM;energy-efficiency;hardware/software co-design;memory-centric computing","Costs;Source coding;Random access memory;Process control;Graphics processing units;Computer architecture;Throughput","","16","","252","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Supporting Secure Multi-GPU Computing with Dynamic and Batched Metadata Management","S. Na; J. Kim; S. Lee; J. Huh",Georgia Institute of Technology; KAIST; KAIST; KAIST,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","204","217","With growing problem sizes for GPU computing, multi-GPU systems with fine-grained memory sharing have emerged to improve the current coarse-grained unified memory support based on page migration. Such multi-GPU systems with shared memory pose a new challenge in securing CPU-GPU and inter-GPU communications, as the cost of secure data transfers adds a significant performance overhead. There are two overheads of secure communication in multi-GPU systems: First, extra overhead is added to generate one-time pads (OTPs) for authenticated encryption. Second, the security metadata such as MACs and counters passed along with encrypted data consume precious network bandwidth. This study investigates the performance impact of secure communication in multi-GPU systems and evaluates the prior CPU-oriented OTP precomputation schemes adapted for multi-GPU systems. Our investigation identifies the challenge with the limited OTP buffers for inter-GPU communication and the opportunity to reduce traffic for security meta-data with bursty communications in GPUs. Based on the analysis, this paper proposes a new dynamic OTP buffer allocation technique, which adjusts the buffer assignment for each source-destination pair to reflect the communication patterns. To address the bandwidth problem by extra security metadata, the study employs a dynamic batching scheme to transfer only a single set of metadata for each batched group of data responses. The proposed design constantly tracks the communication pattern from each GPU, periodically adjusts the allocated buffer size, and dynamically forms batches of data transfers. Our evaluation shows that in a 16-GPU system, the proposed scheme can improve the performance by 13.2 % and 17.5 % on average from the prior cached and private schemes, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00025","National Research Foundation of Korea(grant numbers:NRF-2022R1A2B5BOI002133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476487","","Costs;Memory management;Graphics processing units;Bandwidth;Metadata;Traffic control;Dynamic scheduling","","5","","51","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Data Enclave: A Data-Centric Trusted Execution Environment","Y. Xu; J. Pangia; C. Ye; Y. Solihin; X. Shen","University of California, Santa Cruz; North Carolina State University; Huazhong University of Science and Technology; University of Central Florida; North Carolina State University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","218","232","Trusted Execution Environments (TEEs) protect sensitive applications in the cloud with the minimal trust in the cloud provider. Existing TEEs with integrity protection however lack support for data management primitives, causing data sharing between enclaves either insecure or cumbersome. This paper proposes a new data abstraction for TEEs, data enclave. As a data-centric abstraction, data enclave is decoupled from an enclave's existence, is equipped with flexible secure permission controls, and crytographically isolated. It eliminates the hurdles for enclaves to cooperate efficiently, and at the same time, enables dynamic shrinking of the height of integrity tree for performance. This paper presents this new abstraction, its properties, and the architecture support. Experiments on synthetic benchmarks and three real-world applications all show that data enclave can help improve the efficiency of enclaves and inter-enclave cooperations significantly while enhancing the security protection.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00026","Office of Naval Research(grant numbers:N00014-23-1-2136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476466","Memory Security;Memory Architecture;Trusted;Execution Environments","Computer architecture;Vegetation;Benchmark testing;Hardware;Security","","2","","81","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Salus: Efficient Security Support for CXL-Expanded GPU Memory","R. Abdullah; H. Lee; H. Zhou; A. Awad","North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","15","GPUs have become indispensable accelerators for many data-intensive applications such as scientific workloads, deep learning models, and graph analytics; these applications share a common demand for increasingly large memory. As the memory capacity connected through traditional memory interfaces is reaching limits, heterogeneous memory systems have gained traction in expanding the memory pool. These systems involve dynamic data movement between different memory locations for efficient utilization, which poses challenges for existing security implementations, whose metadata are tied to the physical location of data. In this work, we propose a new security model specifically designed for systems with dynamic page migration. Our model minimizes the need for security recalculations due to data movement, optimizes security structures for efficient bandwidth utilization, and reduces the overall traffic caused by security operations. Based on our evaluation, our proposed security support improves the GPU throughput by a geometric mean of 29.94% (up to 190.43%) over the conventional security model, and it reduces the security traffic in the memory subsystem to 47.79% on average (as low as 17.71% overhead).","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476444","GPUs;Memory Expansion;Memory Security;CXL","Deep learning;Analytical models;Protocols;Memory management;Graphics processing units;Bandwidth;Metadata","","7","","67","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Morphling: A Throughput-Maximized TFHE-based Accelerator using Transform-domain Reuse","Prasetiyo; A. Putra; J. -Y. Kim","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","249","262","Fully Homomorphic Encryption (FHE) has become an increasingly important aspect in modern computing, particularly in preserving privacy in cloud computing by enabling computation directly on encrypted data. Despite its potential, FHE generally poses major computational challenges, including huge computational and memory requirements. The bootstrapping operation, which is essential particularly in Torus-Fhe(tfhe) scheme, involves intensive computations characterized by an enormous number of polynomial multiplications. For instance, performing a single bootstrapping at the 128-bit security level requires more than 10,000 polynomial multiplications. Our in-depth analysis reveals that domain-transform operations, i.e., Fast Fourier Transform (FFT), contribute up to 88% of these operations, which is the bottleneck of the TFHE system. To address these challenges, we propose Morphling, an accelerator architecture that combines the 2D systolic array and strategic use of transform-domain reuse in order to reduce the overhead of domain-transform in TFHE. This novel approach effectively reduces the number of required domain-transform operations by up to 83.3 %, allowing more computational cores in a given die area. In addition, we optimize its micro architecture design for end-to-end TFHE operation, such as merge-split pipelined-FFT for efficient domain-transform operation, double-pointer method for high-throughput polynomial rotation, and specialized buffer design. Furthermore, we introduce custom instructions for tiling, batching, and scheduling of multiple ciphertext operations. This facilitates software-hardware co-optimization, effectively mapping high-level applications such as XG-Boost classifier, Neural-Network, and VGG-9. As a result, Morphling, with four 2D systolic arrays and four vector units with domain-transform reuse, takes 74.79 mm2 die area and 53.00 W power consumption in 28nm process. It achieves a throughput of up to 147,615 bootstrappings per second, demonstrating improvements of 3440x over the CPU, 143x over the GPU, and 14.7x over the state-of-the-art TFHE accelerator. It can run various deep learning models with sub-second latency.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476414","Fully Homomorphic Enryption;TFHE;Transform-domain Reuse;TFHE Accelerator;Bootstrapping","Power demand;Processor scheduling;Fast Fourier transforms;Memory management;Graphics processing units;Throughput;Systolic arrays","","5","","60","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology","B. Hyun; T. Kim; D. Lee; M. Rhu",KAIST; KAIST; KAIST; KAIST,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","263","279","Processing-in-memory (PIM) has been explored for decades by computer architects, yet it has never seen the light of day in real-world products due to its high design overheads and lack of a killer application. With the advent of critical memoryintensive workloads, several commercial PIM technologies have been introduced to the market, ranging from domain-specific PIM architectures to more general-purpose PIM architectures. In this work, we deepdive into UPMEM's commercial PIM technology, a general-purpose PIM-enabled parallel computing architecture that is highly programmable. Our first key contribution is the development of a flexible simulation framework for PIM. The simulator we developed (aka uPIMulator) enables the compilation of UPMEM-PIM source codes into its compiled machine-level instructions, which are subsequently consumed by our cycle-level performance simulator. Using uPIMulator, we demystify UPMEM's PIM design through a detailed characterization study. Finally, we identify some key limitations of the current UPMEM-PIM system through our case studies and present some important architectural features that will become critical for future PIM architectures to support.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00029","National Research Foundation of Korea (NRF); Korea government (MSIT)(grant numbers:NRF-2021R1A2C2091753); Institute of Information & communications Technology Planning & Evaluation (IITP); Artificial Intelligence Graduate School Program (KAIST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476411","Processing-In-Memory (PIM);Near-Memory Processing;Parallel Architecture","Microarchitecture;Source coding;Computer architecture;Parallel processing;Vectors;Distance measurement","","28","","136","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Functionally-Complete Boolean Logic in Real DRAM Chips: Experimental Characterization and Analysis","İ. E. Yüksel; Y. C. Tuğrul; A. Olgun; F. N. Bostancı; A. G. Yağlıkçı; G. F. Oliveira; H. Luo; J. Gómez-Luna; M. Sadrosadati; O. Mutlu","ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich; ETH, Zürich",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","280","296","Processing-using-DRAM (PuD) is an emerging paradigm that leverages the analog operational properties of DRAM circuitry to enable massively parallel in-DRAM computation. PuD has the potential to significantly reduce or eliminate costly data movement between processing elements and main memory. A common approach for PuD architectures is to make use of bulk bitwise computation (e.g., AND, OR, NOT). Prior works experimentally demonstrate three-input MAJ (i.e., MAJ3) and two-input AND and OR operations in commercial off-the-shelf (COTS) DRAM chips. Yet, demonstrations on COTS DRAM chips do not provide a functionally complete set of operations (e.g., NAND or AND and NOT). We experimentally df performing 1) functionally-complete Boolean operations: NOT, NAND, and NOR and 2) many-input (i.e., more than two-input) AND and OR operations. We present an extensive characterization of new bulk bitwise operations in 256 off-theshelf modern DDR4 DRAM chips. We evaluate the reliability of these operations using a metric called success rate: the fraction of correctly performed bitwise operations. Among our 19 new observations, we highlight four major results. First, we can perform the NOT operation on COTS DRAM chips with a 98.37% success rate on average. Second, we can perform up to 16-input NAND, NOR, AND, and OR operations on COTS DRAM chips with high reliability (e.g., 16-input NAND, NOR, AND, and OR with an average success rate of 94.94%, 95.87%, 94.94%, and 95.85%, respectively). Third, data pattern only slightly affects NAND, NOR, AND, and OR operations. Our results show that executing NAND, NOR, AND, and OR operations with random data patterns decreases the success rate compared to all logic-1/logic-0 patterns by 1.39%, 1.97%, 1.43%, and 1.98%, respectively. Fourth, NOT, NAND, NOR, AND, and OR operations are highly resilient to temperature changes, with small success rate fluctuations of at most 1.66% among all the tested operations when the temperature is increased from 50°C to 95°C. We believe these empirical results demonstrate the promising potential of using DRAM as a computation substrate.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476435","DRAM;Processing-using-DRAM;Processing-using-Memory;Real Chip Characterization","Measurement;Temperature distribution;Fluctuations;Computer architecture;DRAM chips;Substrates","","11","","128","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"StreamPIM: Streaming Matrix Computation in Racetrack Memory","Y. An; Y. Tang; S. Yi; L. Peng; X. Pan; G. Sun; Z. Luo; Q. Li; J. Zhang","Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Peking University; Computer Hardware and System Evolution Laboratory, Xiamen University; Computer Hardware and System Evolution Laboratory, Peking University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","297","311","Racetrack memory (RM) techniques have become promising solutions to resolve the memory wall issue as they increase memory density, reduce energy consumption and are capable of building processing-in-memory (PIM) architectures. RM can place arithmetic logic units in or near its memory arrays to process tasks offloaded by the host. While there already exist multiple studies of processing in RM, these solutions, unfortunately, suffer from data transfer overheads imposed by the loose coupling of the memory core and the computation units. To address this issue, we propose StreamPIM, a new processing-in-RM architecture, which tightly couples the memory core and the computation units. Specifically, StreamPIM directly constructs a matrix processor from domain-wall nanowires without the usage of CMOS-based computation units. It also designs a domainwall nanowire-based bus, which can eliminate electromagnetic conversion. StreamPIM further optimizes the performance by leveraging RM internal parallelism. Our evaluation results show that StreamPIM achieves 39.1 × higher performance and saves 58.4 × energy consumption, compared with the traditional computing platform.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00031","National Key Research and Development Program of China(grant numbers:2023YFB4502702); Natural Science Foundation of China(grant numbers:62332021); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476415","Processing-in-memory;Racetrack memory","Energy consumption;Memory management;Energy resolution;Energy conservation;Parallel processing;Energy efficiency;Nanowires","","3","","83","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SmartDIMM: In-Memory Acceleration of Upper Layer Protocols","N. Patel; A. Mamandipoor; M. Nouri; M. Alian",University of Kansas; University of Kansas; University of Kansas; University of Kansas,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","312","329","There has been significant focus on offloading upperlayer network protocols (ULPs) to accelerators located on CPUs and SmartNICs. However, restricting accelerator placement to these locations limits both the variety of ULPs that can be accelerated and the overall performance. In particular, it overlooks the opportunity to accelerate ULPs running atop a stateful transport protocol in the face of high cache contention. That is, at high network rates, the frequent DRAM accesses and SmartNIC-CPU synchronizations outweigh the benefits of hardware acceleration. This work introduces SmartDIMM, which unlocks the opportunity for accelerating ULPs running atop stateful transport protocols that primarily operate on data stored in DRAM. We prototyped SmartDIMM using Samsung's AxDIMM and implemented endto-end offloading of (de/en)cryption and (de)compression– two ULPs widely employed in datacenters. We then compared the performance of SmartDIMM with accelerator placements on the CPU, SmartNIC, and PCIe cards. Our results demonstrate that ULP offloading on SmartDIMM outperforms CPU, SmartNIC and PCIe-based offload configurations. In comparison to a server executing (de/en)cryption and (de)compression on the CPU, SmartDIMM achieves 21.0% to 10.28 × higher requests per second and 36.3% to 88.9% lower memory bandwidth utilization.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476416","Processing near-memory;Datacenter-scale systems and architectures;End-host Networking","Transport protocols;Performance evaluation;Random access memory;Computer architecture;Bandwidth;Programming;Servers","","2","","97","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing","Y. Wang; X. Pan; Y. An; J. Zhang; G. Reinman",UCLA; Peking University; Peking University; UCLA; UCLA,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","330","344","Prior in-storage computing (ISC) solutions show fundamental drawbacks when applied to GNN acceleration. First, they obey a strict ordering of GNN neighbor sampling. Such serialization fails to utilize flash internal parallelism. Second, the I/Osizes generated by GNN are much smaller than the minimum flash access granularity. The limited channel bandwidth is wasted when serving the requests. Third, the prior solutions rely on firmware-based request processing, making the backend I/O throughput constrained by the embedded core processing power. To address these challenges, we propose BeaconGNN, an in-storage computing (ISC) design for GNN that supports both large-scale graph structures and feature tables. First, it utilizes a novel graph format to enable out-of-order GNN neighbor sampling, improving flash resource utilization. Second, it deploys near-data processing engines across multiple levels of the flash hierarchy (i.e., controller, channel, and die). Specifically, flash-die-level samplers perform neighbor samplings while reducing channel transfer simultaneously. Flash-channel-level command routers communicate with backend dies without the involvement of flash firmware. Lastly, a spatial accelerator is attached to the device bus to accelerate GNN computation. With our software and hardware co-design, BeaconGNN achieves up to 11.6x higher throughput and 4 x better energy efficiency than the state-of-the-art ISC design.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00033","National Key Research and Development Program of China(grant numbers:2023YFB4502702); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476427","computational storage;in-storage computing;GNN acceleration;Flash SSD;storage system","Out of order;Performance evaluation;Process control;Parallel processing;Throughput;Routing;Energy efficiency","","7","","83","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System","H. Jang; J. Song; J. Jung; J. Park; Y. Kim; J. Lee","Department of Electrical and Computer Engineering, Seoul National University; Department of Electrical and Computer Engineering, Seoul National University; Department of Electrical and Computer Engineering, Seoul National University; Department of Electrical and Computer Engineering, University of Texas at Austin; Department of Computer Science, Yonsei University; Department of Electrical and Computer Engineering, Seoul National University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","345","360","The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity. The handler allows overlapping data transfers with fixed memory consumption by reusing the device buffer. Lastly, we propose accelerator-assisted gradient compression/decompression to enhance the scalability of Smart-Infinity. When scaling to multiple near-storage processing devices, the write traffic on the shared channel becomes the bottleneck. To alleviate this, we compress the gradients on the GPU and decompress them on the accelerators. It provides further acceleration from reduced traffic. As a result, Smart-Infinity achieves a significant speedup compared to the baseline. Notably, SmartInfinity is a ready-to-use approach that is fully integrated into PyTorch on a real system. The implementation of Smart-Infinity is available at https://github.com/AIS-SNU/smart-infinity.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00034","National Research Foundation of Korea (NRF)(grant numbers:2022R1C1C1011307,2022R1C1C1008131); Institute of Information & communications Technology Planning & Evaluation (IITP)(grant numbers:IITP2023-RS-2023-00256081); MSIT; Ministry of Education (MOE); National Research Foundation (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476401","Processing in-memory/near-memory/in-cache;FPGA: Architectures and accelerators;Large Language Models (LLMs)","Training;Performance evaluation;Scalability;Memory management;Graphics processing units;Bandwidth;System integration","","10","","130","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"FlashGNN: An In-SSD Accelerator for GNN Training","F. Niu; J. Yue; J. Shen; X. Liao; H. Jin","Huazhong University of Science and Technology, China; Michigan Technological University, USA; Michigan Technological University, USA; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","361","378","Recently, Graph Neural Networks (GNNs) have emerged as powerful tools for data analysis, surpassing traditional algorithms in various applications. However, the growing size of real-world datasets has outpaced the capabilities of centralized CPU or G PU - based systems. To address this challenge, numerous distributed systems have been proposed. However, these systems suffer from low hardware utilization due to slow network data exchange. While SSDs provide a promising alternative with large capacity and improved access latency, SSD-based G NN training on a single computer is bottlenecked by slow PCIe bus data transfer. This bottleneck leads to low CPU and G PU utilization, as confirmed by our experiments. Moreover, the design of in-SSD GNN training is hindered by slow access to flash memory. FlashGNN is a proposed solution that overcomes the PCIe bottleneck, fully utilizes I/O parallelism in flash chips, and maximizes data reuse from fetched flash memory chunks for efficient GNN training. We achieve this by designing the SSD firmware to coordinate data movements and hardware unit access. To address design challenges arising from slow flash memory and limited resources, we propose a novel node-wise GNN training method, an efficient scheduling algorithm for flash requests, and a high-performance subgraph generation method. Experimental results demonstrate that FlashGNN outperforms Ginex, a state-of-the-art SSD-based GNN training system, with a speed-up ratio ranging from 4.89× to 11.83 × and achieves energy savings of 57.14 × to 192.66 × for four typical real-world graph datasets. Additionally, FlashGNN is up to 23.17 × more efficient than the enhanced state-of-the-art in-storage accelerator, SmartSAGE+.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00035","National Natural Science Foundation of China(grant numbers:61825202,62072198,61732010); National Key Research and Development Program of China(grant numbers:61825202,62072198,61732010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476462","In-SSD accelerator;Near-data accelerator;GNN training","Training;Scheduling algorithms;Energy conservation;Parallel processing;Data transfer;Graph neural networks;Hardware","","4","","54","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"DockerSSD: Containerized In-Storage Processing and Hardware Acceleration for Computational SSDs","D. Gouk; M. Kwon; H. Bae; M. Jung","Computer Architecture and Memory Systems Laboratory, KAIST; Computer Architecture and Memory Systems Laboratory, KAIST; Computer Architecture and Memory Systems Laboratory, KAIST; Computer Architecture and Memory Systems Laboratory, KAIST",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","379","394","Processing data in storage is an energy-efficient solution to examine massive datasets. However, a general incarnation of such well-known task-offloading model in a real system is unfortunately unsuccessful due to not only poor performance but also many practical challenges, such as limited processing capabilities and high vulnerabilities at the storage-level. We propose DockerSSD, a fully flexible in-storage processing (ISP) model that can run a variety of applications near flash without their source-level modification. Specifically, it enables lightweight OS-level virtualization in modern SSDs, which allows the storage intelligence to be well harmonized with existing computing environment and makes ISP even faster. Instead of developing a vendor-specific ISP to offload, DockerSSD can reuse existing Docker images, create containers as a self-governing execution object in storage, and process data directly where they are in real-time. To this end, we design a new communication method and virtual firmware that operate together to download Docker images and manage their container execution without a change of the existing storage interface and runtime. We further accelerate ISP and reduce the execution latency by automating container-related network and I/O handling data paths over hardware. Our evaluation shows that DockerSSD is 2.0 × faster than state-of-the-art ISP models for workloads with a high volume of system calls or file accesses. Moreover, it demonstrates a reduction in power and energy consumption by 1.6 × and 2.3 × respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476461","OS virtualization;storage system;container;in-storage processing;processing-in-memory;near-data processing","Solid modeling;Runtime;Computational modeling;Containers;Real-time systems;Virtualization;Task analysis","","","","82","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"PREFETCHX: Cross-Core Cache-Agnostic Prefetcher-based Side-Channel Attacks","Y. Chen; A. Hajiabadi; L. Pei; T. E. Carlson","School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","395","408","In this paper, we reveal the existence of a new class of prefetcher, the XPT prefetcher, in modern Intel processors which has never been officially detailed. It speculatively issues a load, bypassing last-level cache (LLC) lookups, when it predicts that a load request will result in an LLC miss. We demonstrate that XPT prefetcher is shared among different cores, which enables an attacker to build cross-core side-channel and covertchannel attacks. We propose PREFETCHX, a cross-core attack mechanism, to leak users’ sensitive data and activities. We empirically demonstrate that PREFETCHX can be used to extract private keys of real-world RSA applications. Furthermore, we show that PREFETCHX can enable side-channel attacks that can monitor keystrokes and network traffic patterns of users. Our two cross-core covert-channel attacks also see a low error rate and a 122KiB/s maximum channel capacity. Due to the cache-independent feature of PREFETCHX, current cache-based mitigations are not effective against our attacks. Overall, our work uncovers a significant vulnerability in the XPT prefetcher, which can be exploited to compromise the confidentiality of sensitive information in both cryptography and non-cryptography-related applications among processor cores.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476423","Side-Channel Attacks;Prefetcher;Intel CPU;Computer Security","Program processors;Error analysis;Prefetching;Side-channel attacks;Telecommunication traffic;Computer architecture;Feature extraction","","7","","48","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Modeling, Derivation, and Automated Analysis of Branch Predictor Security Vulnerabilities","Q. Wang; M. Tang; K. Xu; H. Wang","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","409","423","With the intensification of microarchitectural side-channel attacks targeting branch predictors, the security boundary of computer systems and users' security-critical data are under serious threat. Since the root cause of these attacks is the neglect of security issues in the microarchitecture design of branch predictors, an analysis framework that can exhaustively and automatically explore these concerns in the design phase is imminent. In this paper, we propose a comprehensive and automated evaluation framework for inspecting the security guarantees of branch predictors at the microarchitecture design stage. Our technique involves a three-step modeling approach that abstractly characterizes 19 branch predictor states and 53 operations that could affect these states. Subsequently, we develop a symbolic execution-based framework to investigate all three-step combinations and derive 156 valid attack patterns against branch predictors, including 89 novel attacks never considered in the previous work. Finally, we apply our framework to 8 secure branch predictor designs and four typical hardware-based countermeasures against speculative execution attacks to evaluate their security capabilities. The result demonstrates that these security branch predictors provide efficient security guarantees and outperform those hardware-based alleviations against speculative execution attacks, indicating that the security branch predictors are promising in mitigating branch predictor security vulnerabilities.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00038","National Key R&D Program of China(grant numbers:2022YFB3103800); National Natural Science Foundation of China(grant numbers:61972295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476476","","Analytical models;Microarchitecture;Program processors;Side-channel attacks;Computer architecture;Predictive models;Security","","1","","66","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SegScope: Probing Fine-grained Interrupts via Architectural Footprints","X. Zhang; Z. Zhang; Q. Shen; W. Wang; Y. Gao; Z. Yang; J. Zhang","School of Software and Microelectronics, Peking University; The University of Western Australia; School of Software and Microelectronics, Peking University; CAS, Institute of Information Engineering; Data61, CSIRO; School of Software and Microelectronics, Peking University; College of Semiconductors, (College of Integrated Circuits), Hunan University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","424","438","Interrupts are critical hardware resources for OS kernels to schedule processes. As they are related to system activities, interrupts can be used to mount various side-channel attacks (i.e., monitoring keystrokes, inferring website visits, detecting GPU activities, and fingerprinting processes). Given that all these attacks rely on system file interfaces or architectural timers to probe interrupts, various countermeasures have been proposed to either remove the unprivileged access to the file interfaces or detect/cripple architectural timers. In this work, we propose SegScope, a new technique that abuses segment protection to provision fine-grained interrupt observations without any timer. As segment protection is widely used on x86, SegScope works across a wide range of Intel-and AMD-based CPUs. Particularly, we observe that while segment protection preserves the confidentiality of high privileged domain, it leaves a footprint via the data segment registers values when an interrupt occurs. With this key observation, SegScope is crafted by capturing the footprints. To show its security implications, we evaluate it in four case studies. First, SegScope has inferred website visits with a respective success rate of 92.4% on Chrome and 87.4% on Tor Browser in default system settings. Second, SegScope successfully extracts the keys from Cloudflare's Interoperable Reusable Cryptographic Library (CIRCL) vl.l. Third, SegScope steals DNN model architectures with an accuracy of over 80%. Last, SegScope effectively reduces the noise of interrupts to improve the performance of other side channels. As an example, SegScope reduces the error rate of Spectral side channel by 56×. Compared with existing timer-based interrupt-probing techniques, SegScope is fine-grained without introducing false-positives. Further, we leverage SegScope to craft a fine-grained timer, as regular timer interrupts as clock edges contain timestamps. Our evaluation shows that it achieves the same level of timing granularity as the high-resolution timer, i.e., rdtsc and rdpru. We then leverage the timer to break KASLR in about 10 seconds and mount a Flush+Reload based Spectre attack.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00039","National Natural Science Foundation of China(grant numbers:61672062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476480","Micro-architectural Attacks;Side Channel Attacks;System Security","Schedules;Side-channel attacks;Computer architecture;Libraries;Timing;Registers;Probes","","4","","79","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Differential-Matching Prefetcher for Indirect Memory Access","G. Fu; T. Xia; Z. Luo; R. Chen; W. Zhao; P. Ren",Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","439","453","Indirect memory access is a critical bottleneck for modern CPUs, especially for graph analysis and sparse linear algebra applications, where the values of one data array are used to generate the fetching addresses of another array. It often causes irregular data accesses with poor temporal and spatial locality that are difficult to be captured by conventional hardware prefetchers. For many complex workloads, such indirect access patterns may have different types and are nested in a multiplelevel form. Moreover, branch mispredictions would further disturb their patterns, making them even harder to detect. As a result, existing hardware prefetchers are unable to fully prefetch complex indirect patterns. This paper proposes DMP, a low-cost hardware prefetcher to improve the memory latency in several representative irregular workloads. DMP targets four types of indirect memory access patterns including single, range, multi-level, and multi-way indirect access. DMP uses differential matching to identify an indirect access pattern in pair with its corresponding index stream. Then DMP uses a flexible prefetching mechanism to dynamically adapt the prefetching degree to maintain prefetching coverage. We evaluate the performance, energy consumption, and transistor cost of DMP among various algorithms from GAP, NAS, and HPCG benchmarks. DMP improves performance by 1.8 × (up to 5.6 ×) on average against state-of-the-art hardware prefetchers and 1.2 × (up to 2.3 ×) speedup against state-of-the-art compiler-based prefetcher Prodigy. Besides, the proposed design is optimized to take only 0.9KB of storage, making it feasible to be integrated into current CPU designs.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476460","","Energy consumption;Prefetching;Heuristic algorithms;Linear algebra;Hardware;Indexes;Arrays","","4","","76","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SPADE: Sparse Pillar-based 3D Object Detection Accelerator for Autonomous Driving","M. Lee; S. Park; H. Kim; M. Yoon; J. Lee; J. W. Choi; N. S. Kim; M. Kang; J. Choi","Hanyang University; Hanyang University; Hanyang University; Hanyang University; Hanyang University; Hanyang University; University of Illinois at Urbana-Champaign; University of California, San Diego; Hanyang University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","454","467","3D object detection using point cloud (PC) data is essential for perception pipelines of autonomous driving, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for fast and accurate 3D object detection. However, the stateof-the-art methods employing PointPillars overlook the inherent sparsity of pillar encoding where only a valid pillar is encoded with a vector of channel elements, missing opportunities for significant computational reduction. Meanwhile, current sparse convolution accelerators are designed to handle only elementwise activation sparsity and do not effectively address the vector sparsity imposed by pillar encoding. In this paper, we propose SPADE, an algorithm-hardware codesign strategy to maximize vector sparsity in pillar-based 3D object detection and accelerate vector-sparse convolution commensurate with the improved sparsity. SPADE consists of three components: (1) a dynamic vector pruning algorithm balancing accuracy and computation savings from vector sparsity, (2) a sparse coordinate management hardware transforming 2D systolic array into a vector-sparse convolution accelerator, and (3) sparsityaware dataflow optimization tailoring sparse convolution schedules for hardware efficiency. Taped-out with a commercial technology, SPADE saves the amount of computation by 36.3–89.2% for representative 3D object detection networks and benchmarks, leading to 1.3–10.9 × speedup and 1.5–12.6 × energy savings compared to the ideal dense accelerator design. These sparsityproportional performance gains equate to 4.1–28.8 × speedup and 90.2–372.3 × energy savings compared to the counterpart server and edge platforms.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00041","Institute of Information & communications Technology Planning & Evaluation (IITP)(grant numbers:IITP2024-RS-2023-00253914); Korea government (MSIT)(grant numbers:2022-0-00957); National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476405","","Three-dimensional displays;Convolution;Heuristic algorithms;Energy conservation;Object detection;Vectors;Encoding","","9","","59","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Rapper: A Parameter-Aware Repair-in-Memory Accelerator for Blockchain Storage Platform","C. Ma; Y. Wang; F. Chen; J. Liao; Y. Wang; R. Mao","College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","468","482","Blockchain storage platforms reward storage nodes for keeping user-uploaded data for a certain amount of time. These storage nodes are unstable and can go online or offline unpredictably at any time, leading to potential data loss. To prevent data loss, blockchain storage platforms adopt erasure codes on user-uploaded encrypted data. Data repair processes will be performed to recover the lost data. However, the data repair processes heavily rely on time-consuming erasure coding algorithms, mainly consisting of vector-matrix multiplications. The emerging processing-in-memory technique can efficiently speed up the processing of vector-matrix multiplications. It can be integrated into blockchain storage platforms to solve the data repair issue. This paper presents Rapper, a parameter-aware repair-inmemory accelerator for blockchain storage platforms. Rapper utilizes the computing power of emerging processing-in-memory architecture so that data repair processes can be processed in a parallel manner and the overall efficiency can be improved significantly. Specifically, at the hardware level, the ReRAM memory is reorganized into our proposed double bank, XRU, XGroup, and ReRAM crossbars structure. At the software level, a parallel decoding/encoding strategy is proposed to fully exploit the internal parallelism of ReRAM. We also propose an adaptive parameter-aware mapping to handle various sizes of stripes. To demonstrate the viability of the proposed technique, a representative blockchain storage project Storj is adopted as the default storage infrastructure. Experimental results show that Rapper can achieve a 1.96 × speedup on average compared to the representative scheme.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476464","Data Repair;Accelerator;Processing-in-memory;Blockchain Storage;Erasure Coding","Computer architecture;Maintenance engineering;Parallel processing;Throughput;Encoding;Software;Hardware","","","","54","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MOPED: Efficient Motion Planning Engine with Flexible Dimension Support","L. Huang; Y. Gong; Y. Sui; X. Zang; B. Yuan","Rutgers University, New Jersey, USA; Rutgers University, New Jersey, USA; Rutgers University, New Jersey, USA; Rutgers University, New Jersey, USA; Rutgers University, New Jersey, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","483","497","Motion planning aims to compute the high-quality and collision-free robotic trajectory. To solve the planning problems defined in varying dimensional sizes, motion planners, especially sampling-based, are typically computation intensive because of the costly kernel operations, and computation inefficient due to the inherent sequential processing scheme, hindering their efficient deployment. To address these challenges and enable real-time highly efficient motion planning, this paper proposes MOPED, an algorithm and hardware co-design for sampling-based motion planning engine with flexible dimension support. At the algorithm level, MOPED proposes a two-stage processing scheme to reduce the frequency and unit cost of collision check. It also fully leverages the spatial information and unique property of planning process to enable low-cost approximated neighbor search. At the hardware level, MOPED proposes a correctness-ensured speculative processing scheme to overcome the serialization problem. It also develop a multi-level caching strategy to reduce data movement and resolve resource conflict. We demonstrate the effectiveness of MOPED via implementing a design example with CMOS 28nm technology via synthesizing. Compared with the baseline motion planning processors, MOPED brings significant improvement on throughput, energy efficiency and area efficiency.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00043","National Science Foundation(grant numbers:CCF-2239945); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476403","VLSI;Hardware architecture;Motion planning","Motorcycles;Approximation algorithms;Throughput;Hardware;Planning;Computational efficiency;Trajectory","","1","","89","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"TALCO: Tiling Genome Sequence Alignment Using Convergence of Traceback Pointers","S. Walia; C. Ye; A. Bera; D. Lodhavia; Y. Turakhia","Department of Electrical and Computer, Engineering University of California, San Diego; Department of Electrical and Computer, Engineering University of California, San Diego; Department of Electrical and Computer, Engineering University of California, San Diego; Department of Electrical and Computer, Engineering University of California, San Diego; Department of Electrical and Computer, Engineering University of California, San Diego",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","91","107","Pairwise sequence alignment is one of the most fundamental and computationally intensive steps in genome analysis. With the improving costs and throughput of third-generation sequencing technologies and the growing availability of whole-genome datasets, longer alignments are becoming more common in the field of bioinformatics. However, the high memory demands of long alignments create significant obstacles to hardware acceleration. Banding techniques allow recovering high-quality alignments with lower memory, but they also require more memory for long alignments than what is typically available on-chip in hardware accelerators. Recently, tiling-based hardware accelerators have made remarkable strides in accelerating sequence alignment, achieving three to four orders of magnitude improvement in alignment throughput over software tools without any restrictions on alignment length. However, it is crucial to note that existing tiling heuristics can cause the alignment quality to degrade, which is a critical concern for the wider adoption of accelerators in the field of bioinformatics. To address this issue, this paper describes TALCO - a novel method for tiling long sequence alignments, that, similar to prior tiling techniques, maintains a constant memory footprint during the acceleration step independent of alignment length. However, unlike previous techniques, TALCO also ensures optimal alignments under banding constraints. TALCO does this by leveraging the convergence of traceback paths beyond a tile to a single point on the boundary of that tile - a strategy that generalizes well to a broad set of sequence alignment algorithms. We demonstrate the advantages of TALCO by applying it to two different and widely-used banded sequence alignment algorithms, X-Drop and WFA-Adapt. To the best of our knowledge, this is the first time that a tiling technique is being applied to a non-classical algorithm for sequence alignment, such as WFA-Adapt. The TALCO tiling strategy is beneficial to both software and hardware. When implemented in software, the TALCO strategy reduces the memory requirements for X-Drop and WFA-Adapt algorithms by up to 39 × and 57 ×, respectively, and when implemented as ASIC accelerator, it provides up to 1,900 × and 2,000 × improvement in alignment throughput/watt over CPU baselines implementing the same algorithms. Compared to state-of-the-art GPU and ASIC baselines implementing tiling heuristics, TALCO provides up to 50 × and 1.1 × improvement in alignment throughput, respectively, while also maintaining a higher alignment quality. Code availability: https://github.com/TurakhiaLab/TALCO.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476438","Genome Sequence Alignment;Hardware Accelerator;Banded Alignment;Tiling","Sequential analysis;Software algorithms;Memory management;Genomics;Throughput;System-on-chip;Bioinformatics","","6","","115","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Effective Context-Sensitive Memory Dependence Prediction","S. S. Kim; A. Ros",University of Murcia; University of Murcia,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","515","527","Memory dependence prediction is a fundamental technique to increase instruction- and memory-level parallelism in out-of-order processors, which are crucial for high performance. However, over the years, the performance gap of state-of-the-art memory dependence predictors with respect to an ideal predictor has grown due to the increase of the pipeline width, reaching up to 6% for modern architectures. State-of-the-art predictors brace context sensitivity, however, not-well-adjusted history lengths lead to loss of accuracy and high storage requirements. This work proposes PHAST, a novel context-sensitive memory dependence predictor that identifies for each load the minimum history length necessary to provide precise predictions. Our key observation is that for each load, it suffices to identify the youngest conflicting store and the path between them. This observation is proven empirically using an unlimited budget version of PHAST, which performs close to an ideal predictor with a 0.47% gap. Through cycle-accurate simulation of the SPEC CPU 2017 suite, we show that a 14.5KB implementation of PHAST falls 1.50% behind an ideal predictor. Compared to the top-performing state-of-the-art predictors, PHAST achieves average speedups of 5.05% (up to 39.7%), 1.29% (up to 22.0%), and 3.04% (up to 38.2%) with respect to an 18.5KB StoreSets, a 19KB NoSQ, and a 38.6 MDP-TAGE, respectively. This stems from a considerable misprediction reduction, ranging between 62.5% and 70.0%, on average.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00045","European Research Council (ERC)(grant numbers:819134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476400","Memory Dependence Prediction;Microarchitecture","Out of order;Sensitivity;Program processors;Pipelines;Memory management;Parallel processing;Distance measurement","","3","","45","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering","A. V. Jamet; G. Vavouliotis; D. A. Jiménez; L. Alvarez; M. Casas","Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC); Huawei Zurich Research Center; Texas A&M University; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC); Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC)",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","528","542","To alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D). TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP). FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component. The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features. TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multilevel perceptron hardware approach. TLP only requires 7KB of storage. To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads. Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions. As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively. In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00046","National Science Foundation(grant numbers:CCF-1912617); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476485","Hardware Prefetching;Off-Chip Prediction;Prefetch Filtering;micro-architecture;Graph-Processing","Pollution;Microarchitecture;Filtering;Prefetching;Memory management;Random access memory;Bandwidth","","7","","58","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Gem5-MARVEL: Microarchitecture-Level Resilience Analysis of Heterogeneous SoC Architectures","O. Chatzopoulos; G. Papadimitriou; V. Karakostas; D. Gizopoulos","Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","543","559","In this paper, we present gem5-MARVEL, the first consolidated microarchitecture-level fault injection infrastructure for heterogeneous System-on-Chip architectures comprising CPUs of all major Instruction Set Architectures (ISAs) and different types of domain-specific accelerators. The proposed framework is based on a modular design that facilitates flexible fault injection scenarios that correspond to different fault models and system configurations. gem5-MARVEL includes a set of libraries for the automation of fault injection and the analysis of the effects of hardware faults at full system execution. We evaluate the proposed framework on several 64-bit CPU ISAs: x86, Arm, and RISC-V, as well as on different designs of domain-specific accelerators. The case studies we present unveil important insights and demonstrate the effectiveness of the proposed infrastructure in the analysis of the impact of faults on different types of heterogeneous computing systems. gem5-MARVEL facilitates broad design space exploration for entire heterogeneous computing systems at the microarchitecture level, where resilience under realistic fault scenarios can be simultaneously analyzed with performance (the typical use of microarchitectural simulators).","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476486","Reliability;CPUs;accelerators;heterogeneous architectures;transient faults;permanent faults;silent data corruptions;microarchitecture-level fault injection","Microarchitecture;Systems architecture;Computer architecture;Heterogeneous networks;Hardware;Libraries;System-on-chip","","9","","86","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Spatial Variation-Aware Read Disturbance Defenses: Experimental Analysis of Real DRAM Chips and Implications on Future Solutions","A. G. Yağlıkçı; Y. C. Tuğrul; G. F. Oliveira; İ. E. Yüksel; A. Olgun; H. Luo; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","560","577","Read disturbance in modern DRAM chips is a widespread phenomenon and is reliably used for breaking memory isolation, a fundamental building block for building robust systems. RowHammer and RowPress are two examples of read disturbance in DRAM where repeatedly accessing (hammering) or keeping active (pressing) a memory location induces bitflips in other memory locations. Unfortunately, shrinking technology node size exacerbates read disturbance in DRAM chips over generations. As a result, existing defense mechanisms suffer from significant performance and energy overheads, limited effectiveness, or prohibitively high hardware complexity. In this paper, we tackle these shortcomings by leveraging the spatial variation in read disturbance across different memory locations in real DRAM chips. To do so, we 1) present the first rigorous real DRAM chip characterization study of spatial variation of read disturbance and 2) propose Svärd, a new mechanism that dynamically adapts the aggressiveness of existing solutions based on the row-level read disturbance profile. Our experimental characterization on 144 real DDR4 DRAM chips representing 11 die revisions demonstrates a large variation in read disturbance vulnerability across different memory locations: in the part of memory with the worst read disturbance vulnerability, 1) up to 2 × the number of bitflips can occur and 2) bitflips can occur at an order of magnitude fewer accesses, compared to the memory locations with the least vulnerability to read disturbance. Svärd leverages this variation to reduce the overheads of five state-of-the-art read disturbance solutions, and thus significantly increases system performance.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476430","DRAM;Memory;Security;Reliability;Read Disturbance;RowHammer;RowPress;Real Chip;Characterization;Spatial Variation","System performance;Buildings;Pressing;Memory modules;Computer architecture;DRAM chips;Reliability engineering","","9","","286","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"START: Scalable Tracking for any Rowhammer Threshold","A. Saxena; M. Qureshi","Georgia Tech, Atlanta, USA; Georgia Tech, Atlanta, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","578","592","The Rowhammer vulnerability is worsening, with the Rowhammer Threshold (TRH) reducing from 139K to 4.8K activations over the last decade. As thresholds reduce further, the number of possible aggressor rows increases inversely, making it difficult to reliably track such rows in a storage-efficient manner for typical Rowhammer defenses. To be secure at lower thresholds, academic trackers like Graphene must dedicate prohibitively high storage (hundreds of KBs to MBs) at the chip's design time. Recent in-DRAM trackers from the industry, such as DSAC-TRR, perform approximate tracking and sacrifice guaranteed protection for reduced storage overheads, leaving DRAM vulnerable to Rowhammer attacks. Ideally, we seek a configurable tracker that is secure and precise, incurs negligible dedicated storage and performance overheads, and scales at deployment to track arbitrarily low thresholds. To that end, we propose START - a Scalable Tracker for Any Rowhammer Threshold. Rather than relying on dedicated SRAM structures, START dynamically repurposes a small fraction of the Last-Level Cache (LLC) to store tracking metadata. START leverages the observation that while the memory contains millions of rows, typical workloads touch only a small subset of rows within a refresh period of 64ms. Thus, allocating tracking entries on demand reduces storage significantly. If the application does not access many rows in memory, START does not reserve any LLC capacity. Otherwise, START dynamically uses 1-way, 2-way, or 8-way of the cache set based on demand. START consumes, on average, 9.4% of the LLC capacity to store metadata, which is 5 × lower compared to dedicating a counter in LLC for each row in memory. We also propose START-M, a memory-mapped START for large-memory systems. Our designs require only 4KB SRAM for newly added structures and perform within 1% of idealized tracking even at TRH of less than 100.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00049","Intel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476473","Memory Systems;Rowhammer;Security;Caches","Industries;Graphene;Random access memory;Computer architecture;Metadata;Reliability;Chip scale packaging","","8","","46","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"CoMeT: Count-Min-Sketch-based Row Tracking to Mitigate RowHammer at Low Cost","F. N. Bostanci; I. E. Yüksel; A. Olgun; K. Kanellopoulos; Y. C. Tuğrul; A. G. Yağliçi; M. Sadrosadati; O. Mutlu",ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","593","612","DRAM chips are increasingly more vulnerable to read-disturbance phenomena (e.g., RowHammer and RowPress), where repeatedly accessing DRAM rows causes bitflips in nearby rows due to DRAM density scaling. Under low RowHammer thresholds, existing RowHammer mitigations either incur high area overheads or degrade performance significantly. We propose a new RowHammer mitigation mechanism, CoMeT, that prevents RowHammer bitflips with low area, performance, and energy costs in DRAM-based systems at very low RowHammer thresholds. The key idea of CoMeT is to use low-cost and scalable hash-based counters to track DRAM row activations. CoMeT uses the Count-Min Sketch technique that maps each DRAM row to a group of counters, as uniquely as possible, using multiple hash functions. When a DRAM row is activated, CoMeT increments the counters mapped to that DRAM row. Because the mapping from DRAM rows to counters is not completely unique, activating one row can increment one or more counters mapped to another row. Thus, CoMeT may overestimate, but never underestimates, a DRAM row's activation count. This property of CoMeT allows it to securely prevent RowHammer bitflips while properly configuring its hash functions reduces overestimations. As a result, CoMeT 1) implements substantially fewer counters (e.g., thousands of counters) than the number of DRAM rows in a DRAM bank (e.g., 128K rows) and 2) does not significantly overestimate a DRAM row's activation count. We demonstrate that CoMeT securely prevents RowHammer bitflips at low area, performance, and energy cost. Our comprehensive evaluations show that CoMeT prevents RowHammer bitflips with an average performance overhead of only 0.19% and 4.01 % across 61 benign single-core workloads for a RowHammer threshold of 1K and a very low RowHammer threshold of 125, respectively, normalized to a system with no RowHammer mitigation. CoMeT achieves a good trade-off between performance, energy, and area overheads. Compared to the best prior performance- and energy-efficient RowHammer mitigation mechanism, CoMeT requires 5.4x and 74.2x less area overhead at RowHammer thresholds of 1K and 125, respectively, and incurs a small (≤ 1.75%) performance overhead on average, for all RowHammer thresholds. Compared to the best prior low-area-cost mitigation mechanism, at a very low RowHammer threshold of 125, CoMeT improves performance by up to 39.1% while incurring a similar area overhead. CoMeT is openly and freely available at https://github.com/CMU-SAFARI/CoMeT.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00050","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476483","","Comets;Hash functions;Costs;Random access memory;Computer architecture;DRAM chips;Energy efficiency","","12","","168","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"A Quantum Computer Trusted Execution Environment","T. Trochatos; C. Xu; S. Deshpande; Y. Lu; Y. Ding; J. Szefer","Yale University, USA; Yale University, USA; Yale University, USA; Yale University, USA; Yale University, USA; Yale University, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","613","613","We present the first architecture for a trusted execution environment for quantum computers. In the architecture, to protect the user's circuits, they are obfuscated with decoy control pulses added during circuit transpilation by the user. The decoy pulses are removed, i.e. attenuated, by the trusted hardware inside the superconducting quantum computer's fridge before they reach the qubits. This preliminary work demonstrates that protection from possibly malicious cloud providers is feasible with minimal hardware cost.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476482","","Computers;Costs;Qubit;Computer architecture;Hardware;Computer security","","3","","0","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models","J. Choi; J. Park; K. Kyung; N. S. Kim; J. Ho Ahn","Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; University of Illinois at Urbana-Champaign, USA; Seoul National University, South Korea",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","614","614","Transformer-based generative models, such as GPT, summarize an input sequence by generating key/value (KV) matrices through attention and generate the corresponding output sequence by utilizing these matrices once per token of the sequence. Both input and output sequences tend to get longer, which improves the understanding of contexts and conversation quality. These models are also typically batched for inference to improve the serving throughput. All these trends enable the models' weights to be reused effectively, increasing the relative importance of sequence generation, especially in processing KV matrices through attention. We identify that the conventional computing platforms (e.g., GPUs) are not efficient at handling this attention part for inference because each request generates different KV matrices, it has a low operation per byte ratio regardless of the batch size, and the aggregate size of the KV matrices can even surpass that of the entire model weights. This motivates us to propose AttAcc, which exploits the fact that the KV matrices are written once during summarization but used many times (proportional to the output sequence length), each multiplied by the embedding vector corresponding to an output token. The volume of data entering/leaving AttAcc could be more than orders of magnitude smaller than what should be read internally for attention. We design AttAcc with multiple processing-in-memory devices, each multiplying the embedding vector with the portion of the KV matrices within the devices, saving external (inter-device) bandwidth and energy consumption.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476442","","Energy consumption;Computational modeling;Aggregates;Oral communication;Computer architecture;Bandwidth;Transformers","","","","0","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Computational CXL-Memory Solution for Accelerating Memory-Intensive Applications","J. Sim; S. Ahn; T. Ahn; S. Lee; M. Rhee; J. Kim; K. Shin; D. Moon; E. Kim; K. Park","SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea; SK Hynix, South Korea",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","615","615","CXL interface is the up-to-date technology that enables effective memory expansion by providing a memory-sharing protocol in configuring heterogeneous devices. However, its limited physical bandwidth can be a significant bottleneck for emerging data-intensive applications. In this work, we propose a novel CXL-based memory disaggregation architecture with a real-world prototype demonstration, which overcomes the bandwidth limitation of the CXL interface using near-data processing. The experimental results demonstrate that our design achieves up to 1.9× better performance/power efficiency than the existing CPU system.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476436","","Protocols;Memory architecture;Prototypes;Bandwidth","","","","0","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"LearnedFTL: A Learning-Based Page-Level FTL for Reducing Double Reads in Flash-Based SSDs","S. Wang; Z. Lin; S. Wu; H. Jiang; J. Zhang; B. Mao","Xiamen Key Laboratory of Intelligent Storage and Computing, School of Informatics, Xiamen University, Xiamen, Fujian, China; Xiamen Key Laboratory of Intelligent Storage and Computing, School of Informatics, Xiamen University, Xiamen, Fujian, China; Xiamen Key Laboratory of Intelligent Storage and Computing, School of Informatics, Xiamen University, Xiamen, Fujian, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; School of Computer Science, Peking University, Beijing, China; Xiamen Key Laboratory of Intelligent Storage and Computing, School of Informatics, Xiamen University, Xiamen, Fujian, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","616","629","We present LearnedFTL, a new on-demand pagelevel flash translation layer (FTL) design, which employs learned indexes to improve the address translation efficiency of flashbased SSDs. The first of its kind, it reduces the number of double reads induced by address translation in random read accesses. LearnedFTL proposes three key techniques: an in-place-update linear model to build learned indexes efficiently, a virtual PPN representation to obtain contiguous PPNs for sorted LPNs, and a group-based allocation and model training via GC/rewrite strategy to reduce the training overhead. By tightly integrating the aforementioned key techniques, LearnedFTL considerably speeds up address translation while reducing the number of flash read accesses caused by the address translation. Our extensive experiments on a FEMU-based prototype show that LearnedFTL can reduce up to 55.5% address translation-induced double reads. As a result, LearnedFTL reduces the P99 tail latency by 2.9× ∼ 12.2× with an average of 5.5× and 8.2× compared to the state-of-the-art TPFTL and LeaFTL schemes, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476463","Learned Index;Flash Translation Layer;Solid-State Drive","Training;Computational modeling;Prototypes;Tail;Computer architecture;Indexes;Resource management","","8","","42","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Are Superpages Super-fast? Distilling Flash Blocks to Unify Flash Pages of a Superpage in an SSD","S. -H. Tseng; T. -Y. Chen; M. -C. Yang","Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Engineering, The Chinese University of Hong Kong",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","630","642","This work discovers a flash memory performance issue resulting from flash superpages organization. Because of process variation, each flash page has its own read/write performance. If a slow page is grouped with a fast page in a superpage unit, computer systems with solid-state drives (SSD) receive a poor performance result. In this work, we prove the existence of this issue by conducting a series of experiments on a real SSD platform. To resolve this issue, we characterize flash memory chips to find hints to organize super-fast superpages in SSDs. By the tips, this work develops a process-variation check scheme (PV Check) that can group a superpage with an optimized performance at runtime with low overheads. According to our experiments, the PV Check scheme has encouraged results in performance improvement. Compared with a traditional method, our work can decrease the extra program and erase latency by 16.61% and 34.55%, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476406","super page;flash memory;solid-state drive;process variation","Runtime;Solid state drives;Organizations;Computer architecture;Flash memories","","2","","36","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"RiF: Improving Read Performance of Modern SSDs Using an On-Die Early-Retry Engine","M. Chun; J. Lee; M. Kim; J. Park; J. Kim",Seoul National University; Seoul National University; Kyungpook National University; POSTECH; Seoul National University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","643","656","Modern high-performance SSDs have multiple flash channels operating in parallel to achieve their high I/O bandwidth. However, when the effective bandwidth of these flash channels declines, the SSD's overall bandwidth is substantially impacted. In contemporary SSDs featuring high-density 3D NAND flash memory, frequent invocations of a read-retry procedure pose a significant challenge to fully utilizing the maximum I/O bandwidth of a flash channel. In this paper, we propose a novel read-retry optimization scheme, Retry-in-Flash (RiF), which proactively minimizes the amount of time wasted in conventional read-retry procedures. Unlike existing read-retry solutions that focus on identifying an optimal read-reference voltage for a sensed page, the RiF scheme focuses on determining early on whether a read-retry will be required for the sensed data. To know if a read-retry is needed or not at the earliest possible time, we propose a RiF-enabled flash chip with an on-die early-retry (ODEAR) engine. When the ODEAR engine determines that a sensed page requires a read-retry, a readreference voltage is immediately adjusted and the same page is re-read while ignoring the previously sensed page. By performing the key steps of a read-retry procedure inside a RiF flash chip without transferring the sensed uncorrectable page to an offchip controller, the RiF scheme prevents the read bandwidth of a flash channel from being wasted due to failed read data. To evaluate the RiF scheme, we developed a prototype RiF-enabled flash chip and constructed a RiF-aware SSD simulator using RiF flash chips. Our evaluation results show that the proposed RiF scheme improves the effective SSD bandwidth by 72.1% on average over a state-of-the-art read-retry solution at 2K P/E cycles with negligible power and area overheads.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00056","MOTIE (Ministry of Trade, Industry & Energy)(grant numbers:1415181081); KSRC (Korea Semiconductor Research Consortium)(grant numbers:20019402); National Research Foundation of Korea(grant numbers:RS-202300283799); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476412","","Three-dimensional displays;Prototypes;Bandwidth;Voltage;Computer architecture;Data transfer;Decoding","","4","","80","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Midas Touch: Invalid-Data Assisted Reliability and Performance Boost for 3d High-Density Flash","Q. Li; H. Dang; Z. Wan; C. Gao; M. Ye; J. Zhang; T. -W. Kuo; C. J. Xue","Xiamen Key Laboratory of Intelligent Storage and Computing, Xiamen University; Xiamen Key Laboratory of Intelligent Storage and Computing, Xiamen University; Xiamen Key Laboratory of Intelligent Storage and Computing, Xiamen University; Xiamen Key Laboratory of Intelligent Storage and Computing, Xiamen University; City University of Hong Kong; Peking University; National Taiwan University; Mohamed bin Zayed University of Artificial Intelligence",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","657","670","High-density 3D NAND flash like QLC (Quad-Level Cell) is prevailing in providing large capacities for data-intensive applications. Because of the structure limitation, a two-step programming with a specific sequence is adopted in 3D QLC flash, where data could become invalid between the two programming steps. This is called invalid programming, as the second-step programming is conducted on partially-invalid wordlines (WLs). By exploiting this phenomenon, this work proposes invalid-data assisted strategies for performance and reliability boosting of valid data in 3D QLC-based flash storage systems. We first propose a high-efficiency re-programming (RP) scheme to reprogram the valid data and a high-reliability not-programming (NP) scheme to program data on the partially-invalid WLs. An adaptive data allocation (ADA) strategy for data management between the SLC and QLC regions is further introduced to reduce the occurrence of invalid programming. The simulator-based experiments show the proposed RP scheme combined with ADA can reduce the execution time for programming by 13.51%, on average. Through real-device evaluations, we present that the NP scheme can reduce the bit error rate of NP-programmed data by 32.8% of the worst page type, thus improving overall reliability, which translates to 12% reduction in refreshing overheads and 30% lifetime extension, on average. Besides, the NP scheme with ADA averagely reduces energy consumption by 4.8%.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00057","National Key Research and Development Program of China(grant numbers:2023YFB4502703); National Natural Science Foundation of China(grant numbers:62202396,62332021,62102219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476428","3D QLC flash;invalid programming;reliability enhancement","Energy consumption;Three-dimensional displays;Microprocessors;Bit error rate;Computer architecture;Programming;Boosting","","5","","53","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"ECO-CHIP: Estimation of Carbon Footprint of Chiplet-based Architectures for Sustainable VLSI","C. C. Sudarshan; N. Matkar; S. Vrudhula; S. S. Sapatnekar; V. A. Chhabria",Arizona State University;; Arizona State University;; Arizona State University;; University of Minnesota; Arizona State University;,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","671","685","Decades of progress in energy-efficient and low-power design have successfully reduced the operational carbon footprint in the semiconductor industry. However, this has led to increased embodied emissions, arising from design, manufacturing, and packaging. While existing research has developed tools to analyze embodied carbon for traditional monolithic systems, these tools do not apply to near-mainstream heterogeneous integration (HI) technologies. HI systems offer significant potential for sustainable computing by minimizing carbon emissions through two key strategies: “reducing” computation by “reusing” pre-designed chiplet IP blocks and adopting hierarchical approaches to system design. The reuse of chiplets across multiple designs, even spanning multiple generations of ICs, can substantially reduce carbon emissions throughout the lifespan. This paper introduces ECO-CHIP, a carbon analysis tool designed to assess the potential of HI systems toward sustainable computing by considering scaling, chip let, and packaging yields, design complexity, and even overheads associated with advanced packaging techniques. Experimental results from ECO-CHIP demonstrate that HI can reduce embodied carbon emissions by up to 30% compared to traditional monolithic systems. ECO-CHIP is integrated with other chiplet simulators and is applied to chiplet disaggregation considering other metrics such as power, area, and cost. ECO-CHIP suggests that HI can pave the way for sustainable computing practices.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476457","Computer architecture;sustainable computing;heterogeneous integration;chiplets;embodied carbon;operational carbon;silicon lifecycle;carbon footprint;manufacturing","Costs;Estimation;Carbon dioxide;Computer architecture;Green computing;Very large scale integration;Manufacturing","","23","","65","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Lightening-Transformer: A Dynamically-Operated Optically-Interconnected Photonic Transformer Accelerator","H. Zhu; J. Gu; H. Wang; Z. Jiang; Z. Zhang; R. Tang; C. Feng; S. Han; R. T. Chen; D. Z. Pan",The University of Texas at Austin; The University of Texas at Austin; Massachusetts Institute of Technology; The University of Texas at Austin; Massachusetts Institute of Technology; The University of Texas at Austin; The University of Texas at Austin; Massachusetts Institute of Technology; The University of Texas at Austin; The University of Texas at Austin,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","686","703","The wide adoption and significant computing resource cost of attention-based transformers, e.g., Vision Transformers and large language models, have driven the demand for efficient hardware accelerators. While electronic accelerators have been commonly used, there is a growing interest in exploring photonics as an alternative technology due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have demonstrated promising results for convolutional neural networks (CNNs) workloads, which predominantly rely on weight-static linear operations. However, they encounter challenges when it comes to efficiently supporting attention-based Transformer architectures, raising questions about the applicability of photonics to advanced machine-learning tasks. The primary hurdle lies in their inefficiency in handling the unique workloads inherent to Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome the fundamental limitation of existing photonic tensor core designs, we introduce a novel dynamically-operated photonic tensor core, DPTC, consisting of a crossbar array of interference-based optical vector dot-product engines, supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. The comprehensive evaluation demonstrates that Lightening-Transformer achieves >2.6x energy and > 12 x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to the electronic Transformer accelerator, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for efficient hardware accelerators, particularly for advanced machine-learning workloads, such as Transformer-backboned large language models (LLM). Our implementation is available at https://github.com/zhuhanqing/Lightening-Transformer.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476418","Algorithm-Architecture Co-design;Transformer;Attention;Domain-Specific Accelerator;Photonic Accelerator;Optical Neural Network","Tensors;Electron accelerators;Machine learning;Transformer cores;Transformers;Energy efficiency;Vectors","","12","","70","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MIRAGE: Quantum Circuit Decomposition and Routing Collaborative Design Using Mirror Gates","E. McKinney; M. Hatridge; A. K. Jones","Department of Electrical and Computer Engineering, University of Pittsburgh; Department of Physics and Astronomy, University of Pittsburgh; Department of Electrical and Computer Engineering, University of Pittsburgh",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","704","718","Building efficient large-scale quantum computers is a significant challenge due to limited qubit connectivities and noisy hardware operations. Transpilation is critical to ensure that quantum gates are on physically linked qubits, while minimizing SWAP gates and simultaneously finding efficient decomposition into native basis gates. The goal of this multifaceted optimization step is typically to minimize circuit depth and to achieve the best possible execution fidelity. In this work, we propose MIRAGE, a collaborative design and transpilation approach to minimize SWAP gates while improving decomposition using mirror gates. Mirror gates utilize the same underlying physical interactions, but when their outputs are reversed, they realize a different or mirrored quantum operation. Given the recent attention to √iSWAP as a powerful basis gate with decomposition advantages over CNOT, we show how systems that implement the iSWAP family of gates can particularly benefit from mirror gates. Further, MIRAGE uses mirror gates to reduce routing pressure and reduce true circuit depth instead of just minimizing SWAPs. We explore the benefits of decomposition for √iSWAP and 4√SWAP using mirror gates, including both expanding Haar coverage and conducting a detailed fault rate analysis trading off circuit depth against approximate gate decomposition. We also describe a novel greedy approach accepting mirror substitution at different aggression levels within MIRAGE. For iSWAP systems that use square-lattice topologies, MIRAGE provides an average of 29.6% reduction in circuit depth by eliminating an average of 59.9% SWAP gates, with a relative decrease in infidelity of 28%. MIRAGE also improves circuit depth and decreases relative infidelity by 25% and 21 % for CNOT-based and 23% and 19% SYC-based machines, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476425","quantum computing;quantum circuit optimization;quantum circuit routing","Program processors;Qubit;Pipelines;Collaboration;Logic gates;Routing;Topology","","2","","58","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SACHI: A Stationarity-Aware, All-Digital, Near-Memory, Ising Architecture","S. R. Sundara Raman; L. K. John; J. P. Kulkarni",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","719","731","Recently there have been efforts to solve difficult computation problems harnessing or drawing inspiration from nature. A prominent example is the use of Ising machines for solving NP-complete problems [1], [23]. Ising machines have evolved from quantum/optical annealers and oscillator-based designs [1] to the recent CMOS-based Von-Neumann [36]/in-memory designs [35]. While prior works have demonstrated the power of Ising machines to solve complex real-world problems, the state-of-the-art Ising accelerators are dedicated accelerators that are useful only for a class of problems, involve complex data converter circuits (ADCs/DACs), are unreliable compared to the rest of the CMOS SoC due to the use of process-variation sensitive/specific embedded memory technologies. In this paper, we present an all-digital Ising architecture realized using repurposing of L1 cache of a CPU. It relies on processing in-memory technology implemented in SRAM. SACHI solves the reliability problems of prior works such as BRIM, eliminates the need for ADCs/DACs, and provides Ising compute acceleration with minor hardware overhead over a CPU pipeline. The novelty of the proposed approach consists of (i) tightly coupled interfacing of the accelerator to the CPU, (ii) reuse/ repurposing of existing hardware to provide acceleration, (iii) ability to achieve higher parallelism than earlier Ising designs due to reuse-aware compute, and (iv) improved performance/energy for a wide variety of large-sized high precision real-life optimization problems using novel compute/mapping strategies. In comparison to BRIM, the proposed all-digital Ising accelerator achieves (i) 36x, 160x, 286x, 300x better performance, (ii) 72x, 79x, 80x, and 75x improved energy, (iii) reuse of 4x, 32x, 200x, and 4000x is observed for asset allocation, molecular dynamics, image segmentation, and traveling salesman respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476402","","Scalability;Pipelines;Random access memory;Computer architecture;Traveling salesman problems;Parallel processing;Hardware","","6","","36","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"BitWave: Exploiting Column-Based Bit-Level Sparsity for Deep Learning Acceleration","M. Shi; V. Jain; A. Joseph; M. Meijer; M. Verhelst","Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium; Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium; NXP Semiconductor, Belgium; NXP Semiconductor, Belgium; Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","732","746","Bit-serial computation facilitates bit-wise sequential data processing, offering numerous benefits, such as a reduced area footprint and dynamically-adaptive computational precision. It has emerged as a prominent approach, particularly in leveraging bit-level sparsity in Deep Neural Networks (DNNs). However, existing bit-serial accelerators exploit bit-level sparsity to reduce computations by skipping zero bits, but they suffer from inefficient memory accesses due to the irregular indices of the non-zero bits. As memory accesses typically are the dominant contributor to DNN accelerator performance, this paper introduces a novel computing approach called “bit-column-serial” and a compatible architecture design named “BitWave.” BitWave harnesses the advantages of the “bit-column-serial” approach, leveraging structured bit-level sparsity in combination with dynamic dataflow techniques. This achieves a reduction in computations and memory footprints through redundant computation skipping and weight compression. BitWave is able to mitigate the performance drop or the need for retraining that is typically associated with sparsity-enhancing techniques using a post-training optimization involving selected weight bit-flips. Empirical studies conducted on four deep-learning benchmarks demonstrate the achievements of BitWave: (1) Maximally realize 13.25x higher speedup, 7.71 x efficiency compared to state-of-the-art sparsity-aware accelerators. (2) Occupying 1.138 mm2 area and consuming 17.56 mW power in 16nm FinFet process node.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476419","DNN acceleration;bit-serial computation;sparsity;compression;dynamic dataflow","Deep learning;Memory management;Artificial neural networks;Benchmark testing;FinFETs;Energy efficiency;Hardware","","17","","45","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"LUTein: Dense-Sparse Bit-Slice Architecture With Radix-4 LUT-Based Slice-Tensor Processing Units","D. Im; H. -J. Yoo",Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST),2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","747","759","Bit-slice architectures have been developed to support various bit-precision and data sparsity of deep neural networks (DNNs). However, because of low-bit precision and a wide range of sparsity of bit-slice computations, bit-slice architectures are challenging in multiplier-, processing element (PE)-, core-, and software (SW)-level designs. First, data sparsity causes a power trade-off between the Radix numbers of a multiplier, and the previous multipliers cannot take advantage of all sparsity ranges. Second, a bit-slice PE which integrated massive lowbit multiplier-and-accumulate (MAC) units brings about large data transactions compared to a fixed bit-width PE. Third, bitslice core architectures only focus on either dense or sparse data computations, limiting the overall performance of bit-slice computations with a wide sparsity range. Lastly, low-bit bit-slice computations cause massive repetitive instruction fetches across hardware units. To solve the challenges, LUTein is proposed. It exploits the new lookup table (LUT)-based computing method to support the Radix-4 Modified Booth algorithm, achieving low power consumption in all sparsity ranges. Moreover, the slice-tensor PE efficiently processes slice-tensor data by sharing hardware units across the Radix-4 LUT-based MAC units. In addition, the LUTein architecture adopts a systolic datapath with a multi-port buffer to exploit both inter-PE data reuse and slice-level sparsity. Lastly, LUTein's instruction set architecture (ISA) and the hierarchical instruction decoder are introduced to alleviate repetitive instruction fetches. As a result, LUTein outperforms the state-of-the-art bit-slice architecture, Sibia, over 1.34× higher energy-efficiency and 1.78× higher area-efficiency.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476468","Bit-scalable architecture;deep neural network;Modified Booth algorithm;bit-slice;sparsity;LUT-based computing;slice-tensor;dense-sparse core architecture","Energy consumption;Power demand;Limiting;Instruction sets;Computer architecture;Hardware;Software","","4","","42","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"FIGNA: Integer Unit-Based Accelerator Design for FP-INT GEMM Preserving Numerical Accuracy","J. Jang; Y. Kim; J. Lee; J. -J. Kim",Seoul National University; Sungkyunkwan University; Seoul National University; Seoul National University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","760","773","The weight-only quantization has emerged as a promising technique for alleviating the computational burden of large language models (LLMs) by employing low-precision integer (INT) weights, while retaining full-precision floating point (FP) activations to ensure inference quality. Despite the memory footprint reduction achieved through decreased bit-precision of weight parameters, the actual computing performance is often not improved significantly due to FP-INT multiply-accumulation (MAC) operations being performed on the floating point unit (FPU) after de quantizing the INT weight values to FP values, owing to the lack of dedicated FP- INT arithmetic units. In this study, we investigate the impact of introducing a dedicated FP-INT unit on overall performance and find that such specialization does not yield substantial improvements. As an alternative approach, we propose FIGNA, an accelerator based on INT units designed specifically for FP- INT MAC operations. A key feature of FIGNA is its ability to achieve the same numerical accuracy as the FPU while relying solely on the integer-unit, a departure from prior methods that relied on integer-units with numerical approximations for FP arithmetic results, albeit claiming similar inference accuracy through dedicated network training. Through comprehensive experiments on FP- INT quantized networks for LLMs, including OPT and BLOOM, we demonstrate the superior performance of FIGNA compared to conventional FPUs in terms of performance per area ($TOPS/mm^{2}$) and energy efficiency (TOPS/W) across various input and weight precision combinations. For instance, in the FP16-INT4 case, FIGNA shows 6.34x higher $TOPS/ mm^{2}$ and 2.19x higher TOPS/W compared to the baseline.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476470","FP-INT GEMM;Numerical Accuracy;Weight Only Quantization;NPU","Training;Quantization (signal);Computational modeling;Neural networks;Computer architecture;Hardware;Energy efficiency","","15","","57","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"ASADI: Accelerating Sparse Attention Using Diagonal-based In-Situ Computing","H. Li; Z. Li; Z. Bai; T. Mitra","School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","774","787","The self-attention mechanism is the performance bottleneck of Transformer-based language models, particularly for long sequences. Researchers have proposed using sparse attention to speed up the Transformer. However, sparse attention introduces significant random access overhead, limiting computational efficiency. To mitigate this issue, researchers attempt to improve data reuse by utilizing row/column locality. Unfortunately, we find that sparse attention does not naturally exhibit strong row/column locality, but instead has excellent diagonal locality. Thus, it is worthwhile to use diagonal compression (DIA) format. However, existing sparse matrix computation paradigms struggle to efficiently support DIA format in attention computation. To address this problem, we propose ASADI, a novel software-hardware co-designed sparse attention accelerator. In the soft-ware side, we propose a new sparse matrix computation paradigm that directly supports the DIA format in self-attention computation. In the hardware side, we present a novel sparse attention accelerator that efficiently implements our computation paradigm using highly parallel in-situ computing. We thoroughly evaluate ASADI across various models and datasets. Our experimental results demonstrate an average performance improvement of 18.6 × and energy savings of 2.9× compared to a PIM-based baseline.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476432","processing-in-memory;sparse attention;diagonal compression format","Limiting;Computational modeling;Neural networks;Energy conservation;Transformers;Hardware;Energy efficiency","","12","","47","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Enabling Large Dynamic Neural Network Training with Learning-based Memory Management","J. Ren; D. Xu; S. Yang; J. Zhao; Z. Li; C. Navasca; C. Wang; H. Xu; D. Li","William & Mary; University of California, Merced; University of California, Merced; University of Chinese Academy of Sciences; University of Chinese Academy of Sciences; University of California, Los Angeles; University of Chinese Academy of Sciences; University of California, Los Angeles; University of California, Merced",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","788","802","Dynamic neural network (DyNN) enables high computational efficiency and strong representation capability. However, training DyNN can face a memory capacity problem because of increasing model size or limited GPU memory capacity. Managing tensors to save GPU memory is challenging, because of the dynamic structure of DyNN. We present DyNN-Offload, a memory management system to train DyNN. DyNN-Offload uses a learned approach (using a neural network called the pilot model) to increase predictability of tensor accesses to facilitate memory management. The key of DyNN-Offload is to enable fast inference of the pilot model in order to reduce its performance overhead, while providing high inference (or prediction) accuracy. DyNNOffload reduces input feature space and model complexity of the pilot model based on a new representation of DyNN; DyNNOffload converts the hard problem of making prediction for individual operators into a simpler problem of making prediction for a group of operators in DyNN. DyNN-Offload enables 8 × larger DyNN training on a single GPU compared with using PyTorch alone (unprecedented with any existing solution). Evaluating with AlphaFold (a production-level, large-scale DyNN), we show that DyNN-Offload outperforms unified virtual memory (UVM) and dynamic tensor rematerialization (DTR), the most advanced solutions to save GPU memory for DyNN, by 3 × and 2.1 × respectively in terms of maximum batch size.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476398","Memory Management;Dynamic Neural Network;Neural Network Training","Training;Tensors;Computational modeling;Memory management;Neural networks;Graphics processing units;Predictive models","","7","","87","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Tessel: Boosting Distributed Execution of Large DNN Models via Flexible Schedule Search","Z. Lin; Y. Miao; G. Xu; C. Li; O. Saarikivi; S. Maleki; F. Yang",University of Science and Technology of China; Microsoft Research; University of Science and Technology of China; University of Science and Technology of China; Microsoft Research; University of Science and Technology of China; University of Science and Technology of China,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","803","816","Increasingly complex and diverse deep neural network (DNN) models necessitate distributing the execution across multiple devices for training and inference tasks, and also require carefully planned schedules for performance. However, existing practices often rely on predefined schedules that may not fully exploit the benefits of emerging diverse model-aware operator placement strategies. Handcrafting high-efficiency schedules can be challenging due to the large and varying schedule space. This paper presents Tessel, an automated system that searches for efficient schedules for distributed DNN training and inference for diverse operator placement strategies. To reduce search costs, Tessel leverages the insight that the most efficient schedules often exhibit repetitive pattern (repetend) across different data inputs. This leads to a two-phase approach: repetend construction and schedule completion. By exploring schedules for various operator placement strategies, Tessel significantly improves both training and inference performance. Experiments with representative DNN models demonstrate that Tessel achieves up to 5.5 x training performance speedup and up to 38 % inference latency reduction.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476399","Distributed systems;Large Language Models (LLM);Machine learning Systems","Training;Performance evaluation;Schedules;Costs;Artificial neural networks;Computer architecture;Boosting","","1","","46","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SpecFL: An Efficient Speculative Federated Learning System for Tree-based Model Training","Y. Zhang; L. Zhao; C. Che; X. Wang; D. Meng; R. Hou","Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, CAS and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, CAS and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, CAS and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Indiana University Bloomington, Bloomington, America; Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, CAS and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, CAS and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","817","831","Federated tree-based models are popular in many real-world applications owing to their high accuracy and good interpretability. However, the classical synchronous method causes inefficient federated tree model training due to tree node dependencies. Inspired by speculative execution techniques in modern high-performance processors, this paper proposes SpecFL, a novel and efficient speculative federated learning system. Instead of simply waiting, SpecFL optimistically predicts the outcome of the prior tree node. By resolving tree node dependencies with a split point predictor, the training tasks of child tree nodes can be executed speculatively in advance via separate threads. This speculation enables cross-layer concurrent training, thus significantly reducing the waiting time. Furthermore, we propose a greedy speculation policy to exploit speculative training for deeper inter-layer concurrent training and an eager rollback mechanism for lossless model quality. We implement SpecFL and evaluate its efficiency in a real-world federated learning setting with six public datasets. The evaluation results demonstrate that SpecFL can be 2.08-3.33x and 2.14-3.44x faster than the state-of-the-art GBDT and RF implementations, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476417","Federated Learning;Tree-based Model;Speculative Training","Training;Radio frequency;Cross layer design;Program processors;Machine learning algorithms;Federated learning;Computational modeling","","2","","115","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Enhancing Collective Communication in MCM Accelerators for Deep Learning Training","S. Laskar; P. Majhi; S. Kim; F. Mahmud; A. Muzahid; E. J. Kim",Texas A&M University; Texas A&M University; Texas A&M University; Texas A&M University; Texas A&M University; Texas A&M University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","16","With the widespread adoption of Deep Learning (DL) models, the demand for DL accelerator hardware has risen. On top of that, DL models are becoming massive in size. To accommodate those models, multi-chip-module (MCM) emerges as an effective approach for implementing large-scale DL accelerators. While MCMs have shown promising results for DL inference, its potential for Deep Learning Training remains largely unexplored. Current approaches fail to fully utilize available links in a mesh interconnection network of an MCM accelerator. To address this issue, we propose two novel AllReduce algorithms for mesh-based MCM accelerators - RingBiOdd and Three Tree Overlap (TTO). RingBiOdd is a ring-based algorithm that enhances the bandwidth of AllReduce by creating two unidirectional rings using bidirectional interconnects. On the other hand, TTO is a tree-based algorithm that improves AllReduce performance by overlapping data chunks. TTO constructs three topology-aware disjoint trees and runs different steps of the AllReduce operation in parallel. We present a detailed design and implementation of the proposed approaches. Our experimental results over seven DL models indicate that RingBiOdd achieves 50% and 8% training time reduction over unidirectional Ring AllReduce and MultiTree. Furthermore, TTO demonstrates 33% and 29% training time reduction over state-ofthe-art MultiTree and Bidirectional Ring AllReduce, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476450","MCMs;mesh;distributed deep learning;data parallel training;allreduce","Training;Deep learning;Network topology;Multiprocessor interconnection;Computational modeling;Bandwidth;Inference algorithms","","6","","89","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"TinyTS: Memory-Efficient TinyML Model Compiler Framework on Microcontrollers","Y. -Y. Liu; H. -S. Zheng; Y. Fang Hu; C. -F. Hsu; T. T. Yeh","Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","848","860","Deploying deep neural network (DNN) models on Microcontroller Units (MCUs) is typically limited by the tightness of the SRAM memory budget. Previously, machine learning system frameworks often allocated tensor memory layer-wise, but this will result in out-of-memory exceptions when a DNN model includes a large tensor. Patch-based inference, another past solution, reduces peak SRAM memory usage by dividing a tensor into small patches and storing one small patch at a time. However, executing these overlapping small patches requires significantly more time to complete the inference and is undesirable for MCUs. We resolve these problems by developing a novel DNN model compiler: TinyTS. In the TinyTS, our tensor partition method creates a tensor-splitting model that eliminates the redundant computation observed in the patch-based inference. Furthermore, the TinyTS memory planner significantly reduces peak SRAM memory usage by releasing the memory space of unused split tensors for other ready split tensors early before the completion of the entire tensor. Finally, TinyTS presents different optimization techniques to eliminate the metadata storage and runtime overhead when executing multiple fine-grained split tensors. Using the TensorFlow Lite for Microcontroller (TFLM) framework as a baseline, we tested the effectiveness of TinyTS. We found that TinyTS reduces the peak SRAM memory usage of 9 TinyML models up to 5.92X over the baseline. TinyTS also achieves a geometric mean of 8.83X speedup over the patch-based inference. In resolving the two key issues when deploying DNN models on MCUs, TinyTS substantially boosts memory usage efficiency for TinyML applications. The source code of TinyTS can be obtained from https://github.com/nycu-caslab/TinyTS","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476479","TinyML;Deep Neural Network;Compiler;AIoT","Schedules;Tensors;Runtime;Microcontrollers;Computational modeling;Source coding;Random access memory","","2","","42","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"CAMEL: Co-Designing AI Models and eDRAMs for Efficient On-Device Learning","S. Q. Zhang; T. Tambe; N. Cuevas; G. -Y. Wei; D. Brooks","Department of Electrical Engineering and Department of Computer Science, New York University; John A. Paulson School of Engineering and Applied Sciences, Harvard University; John A. Paulson School of Engineering and Applied Sciences, Harvard University; John A. Paulson School of Engineering and Applied Sciences, Harvard University; John A. Paulson School of Engineering and Applied Sciences, Harvard University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","861","875","On-device learning allows AI models to adapt to user data, thereby enhancing service quality on edge platforms. However, training AI on resource-limited devices poses significant challenges due to the demanding computing workload and the substantial memory consumption and data access required by deep neural networks (DNNs). To address these issues, we propose utilizing embedded dynamic random-access memory (eDRAM) as the primary storage medium for transient training data. In comparison to static random-access memory (SRAM), eDRAM provides higher storage density and lower leakage power, resulting in reduced access cost and power leakage. Nevertheless, to maintain the integrity of the stored data, periodic power-hungry refresh operations could potentially degrade system performance. To minimize the occurrence of expensive eDRAM refresh operations, it is beneficial to shorten the lifetime of stored data during the training process. To achieve this, we adopt the principles of algorithm and hardware co-design, introducing a family of reversible DNN architectures that effectively decrease data lifetime and storage costs throughout training. Additionally, we present a highly efficient on-device training engine named CAMEL, which leverages eDRAM as the primary on-chip memory. This engine enables efficient on-device training with significantly reduced memory usage and off-chip DRAM traffic while maintaining superior training accuracy. We evaluate our CAMEL system on multiple DNNs with different datasets, demonstrating a 2.5× speedup of the training process and 2.8× training energy savings than the other baseline hardware platforms.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476409","","Training;Costs;Computational modeling;Memory management;Random access memory;Artificial neural networks;Hardware","","4","","65","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"FlipBit: Approximate Flash Memory for IoT Devices","A. Buck; K. Ganesan; N. E. Jerger",University of Toronto; University of Toronto; University of Toronto,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","876","890","IoT devices commonly use flash memory for both data and code storage. Flash memory consumes a significant portion of the overall energy of such devices. This is problematic because IoT devices are energy constrained due to their reliance on batteries or energy harvesting. To save energy, we leverage a unique property of flash memory; write operations take unequal amounts of energy depending on if we are flipping a 1 → 0 versus a 0 → 1. We exploit this asymmetry to reduce energy consumption with FLIPBIT, a hardware-software approximation approach that limits costly 0→1 transitions in flash. Instead of performing an exact write, we write an approximated value that avoids any costly 0→1 bit flips. Using FLIPBIT, we reduce the mean energy used by flash by 68% on video streaming applications while maintaining 42 dB PSNR. On machine learning models, we reduce energy by an average of 39% and up to 71% with only a 1% accuracy loss. Additionally, by reducing the number of program-erase cycles, we increase the flash lifetime by 68%.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00072","Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2020-04179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476404","Approximate Computing;Non-volatile memory;Internet-of-Things","Performance evaluation;Energy consumption;Codes;Machine learning;Computer architecture;Streaming media;Hardware","","1","","101","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Usas: A Sustainable Continuous-Learning´ Framework for Edge Servers","C. S. Mishra; J. Sampson; M. T. Kandemir; V. Narayanan; C. R. Das","Department of Computer Science and Engineering, The Pennsylvania State University, University Park, USA; Department of Computer Science and Engineering, The Pennsylvania State University, University Park, USA; Department of Computer Science and Engineering, The Pennsylvania State University, University Park, USA; Department of Computer Science and Engineering, The Pennsylvania State University, University Park, USA; Department of Computer Science and Engineering, The Pennsylvania State University, University Park, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","891","907","Edge servers have recently become very popular for performing localized analytics, especially on video, as they reduce data traffic and protect privacy. However, due to their resource constraints, these servers often employ compressed models, which are typically prone to data drift. Consequently, for edge servers to provide cloud-comparable quality, they must also perform continuous learning to mitigate this drift. However, at expected deployment scales, performing continuous training on every edge server is not sustainable due to their aggregate power demands on grid supply and associated sustainability footprints. To address these challenges, we propose Us.as,´ an approach combining algorithmic adjustments, hardware-software co-design, and morphable acceleration hardware to enable the training of workloads on these edge servers to be powered by renewable, but intermittent, solar power that can sustainably scale alongside data sources. Our evaluation of Us.as on a real-world´ traffic dataset indicates that our continuous learning approach simultaneously improves both accuracy and efficiency: Us.as´ offers a 4.96% greater mean accuracy than prior approaches while our morphable accelerator that adapts to solar variance can save up to {234.95kWH, 2.63MWH}/year/edge-server compared to a {DNN accelerator, data center scale GPU}, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476448","Intermittent Computing;loT and edge computing;Machine Learning Tranining;Sustainability;Edge Servers","Training;Renewable energy sources;Data privacy;Power demand;Soft sensors;Graphics processing units;Hardware","","3","","109","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Cepheus: Accelerating Datacenter Applications with High-Performance RoCE-Capable Multicast","W. Li; J. Zhang; Y. Liu; G. Zeng; Z. Wang; C. Zeng; P. Zhou; Q. Wang; K. Chen","iSING Lab, Hong Kong University of Science and Technology; University of Science and Technology of China; NA; NA; iSING Lab, Hong Kong University of Science and Technology; iSING Lab, Hong Kong University of Science and Technology; NA; NA; iSING Lab, Hong Kong University of Science and Technology",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","908","921","Modern datacenter applications widely exhibit multicast communication patterns. Meanwhile, RDMA is emerging as the de-facto networking architecture to meet the stringent performance requirement of applications. However, existing multicast approaches fail to efficiently collaborate multicast with commodity RDMA transport, either causing inefficient multicast traffic transmission or being trapped in the insufficient end-host transport protocol. In this paper, we propose Cepheus, which delivers performance gains from both multicast (i.e., traffic reduction and transmission hop minimization) and RDMA transport (i.e., ultra-low latency, high throughput and low CPU overhead). Cepheus reuses RoCE as its transport layer and provides a RoCE-capable multicast primitive via in-network assistance. At its core, Cepheus builds on and goes beyond the native multicast architecture by exploiting more switch functionalities to tackle the incompatibilities between multicast flow structure and RoCE processing logic. We prototype Cepheus on an FPGA board, as a building block attached to an Ethernet switch. Extensive experiments demonstrate Cepheus inter-operates with commodity RoCE protocol and outperforms existing RDMA multicast schemes, e.g., 5.2 × faster multicast communication and 2.7 × higher replication throughput for distributed storage.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476407","RDMA;RoCE;Multicast","Protocols;Prototypes;Computer architecture;Switches;Ethernet;Performance gain;Multicast communication","","5","","77","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"LibPreemptible: Enabling Fast, Adaptive, and Hardware-Assisted User-Space Scheduling","Y. Li; N. Lazarev; D. Koufaty; T. Yin; A. Anderson; Z. Zhang; G. E. Suh; K. Kaffes; C. Delimitrou","Cornell University, Ithaca, NY; Massachusetts Institute of Technology, Cambridge, MA; Intel Labs, Hillsboro, OR; Cornell University, Ithaca, NY; Intel Labs, Hillsboro, OR; Cornell University, Ithaca, NY; Cornell University, Ithaca, NY; Columbia University, New York, NY; Massachusetts Institute of Technology, Cambridge, MA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","922","936","Modern cloud applications are prone to high tail latencies since their requests typically follow highly-dispersive distributions. Prior work has proposed both OS- and systemlevel solutions to reduce tail latencies for microsecond-scale workloads through better scheduling. Unfortunately, existing approaches like customized dataplane OSes, require significant OS changes, experience scalability limitations, or do not reach the full performance capabilities hardware offers. We propose LibPreemptible, a preemptive user-level threading library that is flexible, lightweight, and scalable. LibPreemptible is based on three key techniques: 1) a fast and lightweight hardware mechanism for delivery of timed interrupts, 2) a general-purpose user-level scheduling interface, and 3) an API for users to express adaptive scheduling policies tailored to the needs of their applications. Compared to the prior state-of-the-art scheduling system Shinjuku, our system achieves significant tail latency and throughput improvements for various workloads without the need to modify the kernel. We also demonstrate the flexibility of LibPreemptible across scheduling policies for real applications experiencing varying load levels and characteristics.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476424","datacenter;scheduling;cloud computing;Performance and quality of service","Adaptive scheduling;Processor scheduling;Scalability;Tail;Computer architecture;Dynamic scheduling;Throughput","","4","","69","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MINOS: Distributed Consistency and Persistency Protocol Implementation & Offloading to SmartNICs","A. Psistakis; F. Chaix; J. Torrellas","University of Illinois Urbana-Champaign, USA; FORTH, Greece; University of Illinois Urbana-Champaign, USA",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","17","To enable high-performance, programmable, and resilient distributed systems, Distributed Data Persistency (DDP) models provide specific data consistency and persistency guarantees. Since these models target leaderless systems (i.e., systems where any node can initiate requests), they deliver high performance and are scalable. However, they are also more complex. In this paper, we develop detailed distributed algorithms for DDP models. They support Linearizable consistency with five different types of persistency. We call these algorithms MINOSBaseline (MINOS-B) and evaluate them on a 5-node distributed machine. Additionally, to improve performance, we also redesign the algorithms to offload them to a new SmartNIC architecture. The resulting system is called MINOS-Offload (MINOS-O). The MINOS-O SmartNIC introduces optimizations such as selective data coherence in hardware between host and SmartNIC, message batching, and message broadcasting. Our evaluation shows that offloading is very beneficial. It substantially reduces request latency and increases request throughput for various workloads and number of nodes. For example, compared to MINOS-B, MINOS-O reduces the average end-to-end latency of two microservice functions by 35%.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476396","","Protocols;Microservice architectures;Distributed databases;Computer architecture;Coherence;Broadcasting;Throughput","","2","","65","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Ursa: Lightweight Resource Management for Cloud-Native Microservices","Y. Zhang; Z. Zhou; S. Elnikety; C. Delimitrou",Cornell University; Cornell University; Microsoft Research; MIT,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","954","969","Resource management for cloud-native microservices has attracted a lot of recent attention. Previous work has shown that machine learning (ML)-driven approaches out-perform traditional techniques, such as autoscaling, in terms of both SLA maintenance and resource efficiency. However, ML-driven approaches also face challenges including lengthy data collection processes and limited scalability. We present Ursa, a lightweight resource management system for cloud-native microservices that addresses these challenges. Ursa uses an analytical model that decomposes the end-to-end SLA into per-service SLA, and maps per-service SLA to individual resource allocations per microservice tier. To speed up the exploration process and avoid prolonged SLA violations, Ursa explores each microservice individually, and swiftly stops exploration if latency exceeds its SLA. We evaluate Ursa on a set of representative and end-to-end microservice topologies, including a social network, media service and video processing pipeline, each consisting of multiple classes and priorities of requests with different SLAs, and compare it against two representative ML-driven systems, Sinan and Firm. Compared to these ML-driven approaches, Ursa provides significant advantages: It shortens the data collection process by more than 128 ×, and its control plane is 43 × faster than ML-driven approaches. At the same time, Ursa does not sacrifice resource efficiency or SLAs. During online deployment, Ursa reduces the SLA violation rate by 9.0% up to 49.9%, and reduces CPU allocation by up to 86.2% compared to ML-driven approaches","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476471","Microservices;Resource management;SLA","Analytical models;Social networking (online);Scalability;Pipelines;Microservice architectures;Process control;Maintenance engineering","","3","","62","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models","S. -S. Park; K. Kim; J. So; J. Jung; J. Lee; K. Woo; N. Kim; Y. Lee; H. Kim; Y. Kwon; J. Kim; J. Lee; Y. Cho; Y. Tai; J. Cho; H. Song; J. H. Ahn; N. S. Kim",Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Seoul National University; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Seoul National University; University of Illinois Urbana-Champaign,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","970","982","Transformer-based large language models (LLMs) such as Generative Pre-trained Transformer (GPT) have become popular due to their remarkable performance across diverse applications, including text generation and translation. For LLM training and inference, the GPU has been the predominant accelerator with its pervasive software development ecosystem and powerful computing capability. However, as the size of LLMs keeps increasing for higher performance and/or more complex applications, a single GPU cannot efficiently accelerate LLM training and inference due to its limited memory capacity, which demands frequent transfers of the model parameters needed by the GPU to compute the current layer(s) from the host CPU memory/storage. A GPU appliance may provide enough aggregated memory capacity with multiple GPUs, but it suffers from frequent transfers of intermediate values among GPU devices, each accelerating specific layers of a given LLM. As the frequent transfers of these model parameters and intermediate values are performed over relatively slow device-to-device interconnects such as PCIe or NVLink, they become the key bottleneck for efficient acceleration of LLMs. Focusing on accelerating LLM inference, which is essential for many commercial services, we develop CXL-PNM, a processing near memory (PNM) platform based on the emerging interconnect technology, Compute eXpress Link (CXL). Specifically, we first devise an LPDDR5X-based CXL memory architecture with 512GB of capacity and 1.1TB/s of bandwidth, which boasts 16× larger capacity and 10× higher bandwidth than GDDR6and DDR5-based CXL memory architectures, respectively, under a module form-factor constraint. Second, we design a CXLPNM controller architecture integrated with an LLM inference accelerator, exploiting the unique capabilities of such CXL memory to overcome the disadvantages of competing technologies such as HBM-PIM and AxDIMM. Lastly, we implement a CXLPNM software stack that supports seamless and transparent use of CXL-PNM for Python-based LLM programs. Our evaluation shows that a CXL-PNM appliance with 8 CXL-PNM devices offers 23% lower latency, 31% higher throughput, and 2.8× higher energy efficiency at 30% lower hardware cost than a GPU appliance with 8 GPU devices for an LLM inference service.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00078","Semiconductor Research Corporation (SRC); DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476443","CXL-PNM;LLM;LPDDR;CXL","Training;Memory management;Memory architecture;Graphics processing units;Full stack;Bandwidth;Transformers","","22","","52","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"LightPool: A NVMe-oF-based High-performance and Lightweight Storage Pool Architecture for Cloud-Native Distributed Database","J. Xu; Y. Chen; Y. Wang; W. Shi; G. Fang; Y. Chen; H. Liao; Y. Wang; H. Lin; Z. Jin; Q. Liu; W. Chen",Zhejiang University; Zhejiang University; Alibaba Group; AntGroup; Alibaba Group; Zhejiang University; AntGroup; AntGroup; AntGroup; Zhejiang University; Alibaba Group; Zhejiang University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","983","995","Emerging cloud-native distributed databases rely on local NVMe SSDs to provide high-performance and highavailable data services to many cloud applications. However, the database clusters suffer from low utilization of local storage because of the imbalance between CPU and storage capacities within each node. For instance, the OceanBase distributed database cluster, with hundreds of PB local storage capacity, only utilizes around 40% of its local storage. Although disaggregated storage (EBS) can enhance storage utilization by provisioning the CPU and storage independently on demand, they suffer from performance bottlenecks and high costs. In this paper, we propose LightPool, a high-performance and lightweight storage pool architecture large-scale deployed in the OceanBase clusters, enhancing storage resource utilization. The key idea of LightPool is aggregating cluster storage into a storage pool and enabling unified management. In particular, LightPool adopts NVMe-oF to enable high-performance storage resource sharing among cluster nodes and integrate the storage pool with Kubernetes to achieve flexible management and allocation of storage resources. Furthermore, we design the hot-upgrade and hot-migration mechanisms to enhance the availability of LightPool. We have deployed LightPool on over 8500 nodes in production clusters. Statistics show that LightPool can improve storage resource utilization from about 40% to 65%. Experimental results show that the extra latency from LightPool is only about 2.1 μs compared to local storage. Compared to OpenEBS, LightPool enhances bandwidth up to 190.9% in microbenchmarks and throughput up to 6.9% in real-world applications. LightPool is the best practice to deploy NVMe-oF (NVMe/TCP) in the production environment. We also discuss important lessons and experiences learned from the development of LightPool.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476433","NVMe-oF;NVMe;High-performance Storage;Cloud-native Storage","Distributed databases;Computer architecture;Production;Bandwidth;Throughput;Resource management;Best practices","","2","","41","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Enterprise-Class Cache Compression Design","A. Buyuktosunoglu; D. Trilla; B. Abali; D. Berger; C. Walters; J. -S. Lee",IBM Research; IBM Research; IBM Research; IBM Infrastructure; IBM Infrastructure; IBM Infrastructure,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","996","1011","Larger cache sizes closer to processor cores increase processing efficiency, but physical limitations restrict cache sizes at a given latency. Effective cache capacity can be expanded via the inline compression of data as it enters a lower level cache. Using the IBM Telum® processor cache hierarchy as a comparative baseline, this paper presents a custom compression scheme designed for small, line-sized data blocks, examines op-timal compressor/decompressor placement, solutions to common compression drawbacks, and proposes a tiered design blueprint to facilitate product integration. The impact of compression and prediction-assisted adaptive compression on effective cache capacity, hit rate and access latency across several typical industry workloads is explored.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476477","","Industries;Adaptation models;Additives;Computational modeling;Computer architecture;Performance gain;Complexity theory","","2","","97","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"HotTiles: Accelerating SpMM with Heterogeneous Accelerator Architectures","G. Gerogiannis; S. Aananthakrishnan; J. Torrellas; I. Hur",Intel Corporation; Intel Corporation; University of Illinois at Urbana-Champaign; Intel Corporation,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1012","1028","Sparse Matrix Dense Matrix Multiplication (SpMM) is an important kernel with application across a wide range of domains, including machine learning and linear algebra solvers. In many sparse matrices, the pattern of nonzeros is nonuniform: nonzeros form dense and sparse regions, rather than being uniformly distributed across the whole matrix. We refer to this property as Intra-Matrix Heterogeneity (IMH). Currently, SpMM accelerator designs do not leverage this heterogeneity. They employ the same processing elements (PEs) for all the regions of a sparse matrix, resulting in suboptimal acceleration. To address this limitation, we utilize heterogeneous SpMM accelerator architectures, which include different types of PEs to exploit IMH. We develop an analytical modeling framework to predict the performance of different types of accelerator PEs taking into account IMH. Furthermore, we present a heuristic for partitioning sparse matrices among heterogeneous PEs. We call our matrix modeling and partitioning method HotTiles. To evaluate HotTiles, we simulate three different heterogeneous architectures. Each one consists of two types of workers (i.e., PEs): one suited for compute-bound denser regions (Hot Worker) and one for memory-bound sparser regions (Cold Worker). Our results show that exploiting IMH with HotTiles is very effective. Depending on the architecture, heterogeneous execution with HotTiles outperforms homogeneous execution using only hot or only cold workers by 9.2-16.8x and 1.4-3.7x, respectively. In addition, HotTiles outperforms the best worker type used on a per-matrix basis by 1.3-2.5 x. Finally, HotTiles outperforms an IMH-unaware heterogeneous execution strategy by 1.4-2.2x.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476437","hardware accelerators;heterogeneous computing;sparse computations;SpMM","Analytical models;Computational modeling;Accelerator architectures;Machine learning;Linear algebra;Predictive models;Hardware","","5","","70","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding","F. Liu; N. Yang; H. Li; Z. Wang; Z. Song; S. Pei; L. Jiang",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of Shanghai for Science and Technology; Shanghai Jiao Tong University,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1029","1042","Deep Neural Networks (DNNs) have demonstrated remarkable success; however, their increasing model size poses a challenge due to the widening gap between model size and hardware capacity. To address this, model compression techniques have been proposed, but existing compression methods struggle to effectively handle the significant parameter variations (activations and weights) within the model. Moreover, current variance-aware encoding solutions for compression introduce complex logic, leading to limited compression benefits and hardware efficiency. In this context, we present SPARK, a novel algorithm/architecture co-designed solution that utilizes variable-length data representation for local parameter value processing, offering low hardware overhead and high-performance gains. Our key insight is that the high-order part in quantized values are often sparse, allowing us to employ an identity bit to assign the appropriate encoding length, thereby eliminating redundant bit-length footprints. This reduction in data representation based on data characteristics enables a serialized structured data encoding scheme that seamlessly integrates with existing hardware accelerators, such as systolic arrays. We evaluate SPARK-based accelerators against some existing encoding-based accelerator, and our results demonstrate significant improvements. The SPARK-based accelerator achieves up to 4.65 × speedup and 74.7% energy reduction, while maintaining superior model accuracy.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00082","National Natural Science Foundation of China(grant numbers:61834006,61975124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476472","","Quantization (signal);Computational modeling;Computer architecture;Artificial neural networks;Performance gain;Encoding;Systolic arrays","","25","","54","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Data Motion Acceleration: Chaining Cross-Domain Multi Accelerators","S. -T. Wang; H. Xu; A. Mamandipoor; R. Mahapatra; B. H. Ahn; S. Ghodrati; K. Kailas; M. Alian; H. Esmaeilzadeh",University of California San Diego; University of California San Diego; University of Kansas; University of California San Diego; University of California San Diego; University of California San Diego; IBM Research; University of Kansas; University of California San Diego,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1043","1062","There has been an arms race for devising accelerators for deep learning in recent years. However, real-world applications are not only neural networks but often span across multiple domains, e.g., database queries, compression, encryption, video coding, signal processing, and traditional machine learning, which may or may not contain deep learning. The sole focus on this single domain is sub-optimal as it misses the potential to proliferate and promote cross-domain multi-acceleration as there is an opportunity to harness the power of chaining heterogeneous Domain-Specific Architectures (DSAs) in modern datacenter applications. However, there is a catch as the data motion overhead can outweigh the benefits from all these chained heterogeneous accelerators. We dub the data restructuring and communication overhead of executing a single application using a chain of accelerators [1] as the data motion overhead. In a stark contrast with most works on DSAs that deal with accelerating compute kernels, this work focuses on accelerating data motion within a chain of heterogeneous DSAs in a multi-accelerator datacenter. To that end, this paper introduces Data Motion Acceleration (DMX) for (1) reducing data movement, (2) accelerating data restructuring, and (3) enabling interoperability between heterogeneous accelerators from different domains through a cross-stack hardware-software solution. The results with five end-to-end applications show that utilizing DMX offers up to 8.2 ×, 13.6 ×, and 5.2 × improvement in latency, throughput, and energy efficiency in a multi-accelerator system, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00083","National Science Foundation (NSF)(grant numbers:CCF#2107598,CNS#1822273); National Institute of Health (NIH)(grant numbers:#R01EB028350); Defense Advanced Research Project Agency (DARPA)(grant numbers:HR001118-C-0020); Semiconductor Research Corporation (SRC)(grant numbers:2021-AH-3039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476422","","Deep learning;Video coding;Databases;Weapons;Neural networks;Computer architecture;Signal processing","","3","","155","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"RELIEF: Relieving Memory Pressure In SoCs Via Data Movement-Aware Accelerator Scheduling","S. Gupta; S. Dwarkadas",University of Rochester; University of Virginia,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1063","1079","Data movement latency when using on-chip accelerators in emerging heterogeneous architectures is a serious performance bottleneck. While hardware/software mechanisms such as peer-to-peer DMA between producer/consumer accelerators allow bypassing main memory and significantly reduce main memory contention, schedulers in both the hardware and software domains remain oblivious to their presence. Instead, most contemporary schedulers tend to be deadline-driven, with improved utilization and/or throughput serving as secondary or co-primary goals. This lack of focus on data communication will only worsen execution times as accelerator latencies reduce. In this paper, we present RELIEF (RElaxing Least-laxIty to Enable Forwarding), an online least laxity-driven accelerator scheduling policy that relieves memory pressure in accelerator-rich architectures via data movement-aware scheduling. RELIEF leverages laxity (time margin to a deadline) to opportunistically utilize available hardware data forwarding mechanisms while minimizing quality-of-service (QoS) degradation and unfairness. RELIEF achieves up to 50 % more forwards compared to state-of-the-art policies, reducing main memory traffic and energy consumption by up to 32 % and 18 %, respectively. At the same time, RELIEF meets 14% more task deadlines on average and reduces worst-case deadline violation by 14%, highlighting QoS and fairness improvements.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00084","U.S. National Science Foundation(grant numbers:CNS-1900803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476426","Scheduling;Mobile systems;Data movement;Accelerators","Energy consumption;Processor scheduling;Memory management;Quality of service;Throughput;Hardware;Software","","2","","60","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"GRIT: Enhancing Multi-GPU Performance with Fine-Grained Dynamic Page Placement","Y. Wang; B. Li; A. Jaleel; J. Yang; X. Tang",University of Pittsburgh; University of Pittsburgh; NVIDIA; University of Pittsburgh; University of Pittsburgh,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1080","1094","Multi-GPU systems have become popular to cater to the growing demands for high parallelism and large memory capacity. However, the delivered performance is constrained by the non-uniform memory access (NUMA) overhead arising from data sharing and communication across multiple GPUs. Recent multi-GPUs employ unified virtual memory (UVM) to simplify the programming effort. In UVM-enabled multi-GPUs, three popular page placement schemes are adopted to mitigate the NUMA overheads: i) on-touch page migration, ii) access counter-based migration, and iii) page duplication. However, we observe that the preferred page placement scheme varies across i) different applications, ii) different pages of the same application, and iii) even different execution phases of a single page, making it challenging to find a “one-size-fits-all” page placement scheme. To this end, we propose GRIT, which dynamically and automatically determines the appropriate page placement schemes at runtime in a fine-grained manner to enhance multi-GPU performance and scalability. Experimental results indicate that GRIT achieves an average of 60%, 49%, and 29% performance improvements over uniformly adopting on-touch migration, access counter-based migration, and page duplication, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00085","NSF(grant numbers:2011146,2154973,1725657,1910413,2312157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476474","Multi-GPUs;NUMA;Page placement","Runtime;Scalability;Computer architecture;Programming;Parallel processing","","7","","64","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"WASP: Exploiting GPU Pipeline Parallelism with Hardware-Accelerated Automatic Warp Specialization","N. C. Crago; S. Damani; K. Sankaralingam; S. W. Keckler",NVIDIA; NVIDIA; NVIDIA; NVIDIA,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1","16","Graphics processing units (GPUs) are an important class of parallel processors that offer high compute throughput and memory bandwidth. GPUs are used in a variety of important computing domains, such as machine learning, high performance computing, sparse linear algebra, autonomous vehicles, and robotics. However, some applications from these domains can underperform due to sensitivity to memory latency and bandwidth. Some of this sensitivity can be reduced by better overlapping memory access with compute. Current GPUs often leverage pipeline parallelism in the form of warp specialization to enable better overlap. However, current warp specialization support on GPUs is limited in three ways. First, warp specialization is a complex and manual program transformation that is out of reach for many applications and developers. Second, it is limited to coarse-grained transfers between global memory and the shared memory scratchpad (SMEM); fine-grained memory access patterns are not well supported. Finally, the GPU hardware is unaware of the pipeline parallelism expressed by the programmer, and is unable to take advantage of this information to make better decisions at runtime. In this paper we introduce WASP, hardware and compiler support for warp specialization that addresses these limitations. WASP enables fine-grained streaming and gather memory access patterns through the use of warp-level register file queues and hardware-accelerated address generation. Explicit warp to pipeline stage naming enables the GPU to be aware of pipeline parallelism, which WASP capitalizes on by designing pipeline-aware warp mapping, register allocation, and scheduling. Finally, we design and implement a compiler that can automatically generate warp specialized kernels, reducing programmer burden. Overall, we find that runtime performance can be improved on a variety of important applications by an average of 47% over a modern GPU baseline.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476447","","Sensitivity;Runtime;Pipelines;Graphics processing units;Bandwidth;Parallel processing;Throughput","","4","","50","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Guser: A GPGPU Power Stressmark Generator","Y. Shan; Y. Yang; X. Qian; Z. Yu","Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology; Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology; Purdue University; Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1111","1124","Power stress mark is crucial for estimating Thermal Design Power (TDP) of GPGPUs to ensure efficient power control. This paper proposes Guser, the first systematic methodology to generate GPGPU power stressmarks. It features three ideas: Instruction Power Analysis (IPA) to analyze the power behavior of PTX instructions; Pipeline-based Instruction Grouping (PIG) to classify all PTX instructions into a number of groups; and Quantifying the Importance of power impact factors (QIF) to select a small number but important adjustable knobs. We adopt Optuna, an advanced BO algorithm, to generate stressmarks that maximize power consumption. We evaluate Guser on two real GPGPUs Tesla T4 (Turing) and Tesla A10 (Ampere). The experimental results show that the Guser-generated stressmark for T4 (T4-stresser) consumes 109.3 watts and the one for A10 (A10-stresser) consumes 238.7 watts, which are 48.7% and 73% higher than those consumed by the stressmarks generated by the state-of-the-art approach for CPUs, respectively. Moreover, T4-stresser and A10-stresser consume significantly more power than that consumed by any benchmarks in three benchmark suites: Cactus, Rodinia, and Parboil.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476420","Power stressmark;Thermal Design Power;GPGPU","Systematics;Power demand;Power control;Computer architecture;Benchmark testing;Generators;Classification algorithms","","1","","55","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"GPU Scale-Model Simulation","H. SeyyedAghaei; M. Naderan-Tahan; L. Eeckhout","Ghent University, Belgium; Ghent University, Belgium; Ghent University, Belgium",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1125","1140","The continuously increasing GPU system scale and compute capabilities, i.e., increasing number of streaming multiprocessors (SMs), caches, on-chip and off-chip memory bandwidth, pose a major challenge for performance evaluation methodologies. Architectural simulation is time-consuming and resource-intensive, and because of simulator and/or simulation host infrastructure limitations, it might not even be possible to simulate large-scale systems. Scale-model simulation is a recently proposed performance prediction methodology to predict large-scale system performance based on (much smaller) scale models. Prior work in scale-model simulation for general-purpose multicore CPUs and specialized graph analytics accelerators, unfortunately, cannot be readily applied to GPUs because different GPU applications exhibit vastly different scaling behavior with system size, thereby breaking the one-size-fits-all regression models deployed in prior work. This paper proposes a GPU scale-model simulation methodology that leverages performance measurements of two scale models alongside a miss rate curve to predict GPU target system performance. A key asset of GPU scale-model simulation is that it does not require access to a simulation model of the target system, unlike prior work in simulation acceleration. Our experimental evaluation demonstrates the accuracy of GPU scale-model simulation for both strong-scaling and weak-scaling workload scenarios. Under strong scaling, the performance of a 128-SM target system is predicted within 4% error on average, and at most 17%, using 8-SM and 16-SM scale models. Under weak scaling, the performance of a 128-SM target system is estimated with an average error of 1.7%, and at most 4.5%, while yielding a 9.3× simulation time speedup. We furthermore demonstrate how scale-model simulation predicts multi-chiplet GPU performance with an average error of 2.5% (and at most 4.3%). Alternate solutions are substantially less accurate.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00088","Research Foundation Flanders (FWO)(grant numbers:G018722N); European Research Council (ERC) Advanced(grant numbers:741097); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476465","GPU;performance modeling;scale-model simulation","Performance evaluation;Analytical models;Multicore processing;System performance;Graphics processing units;Computer architecture;Predictive models","","3","","66","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Agile-DRAM: Agile Trade-Offs in Memory Capacity, Latency, and Energy for Data Centers","J. Lee; W. Jung; D. Kim; D. Kim; J. Lee; J. Kim","Sungkyunkwan University, Republic of Korea; Sungkyunkwan University, Republic of Korea; Sungkyunkwan University, Republic of Korea; Samsung Electronics, Republic of Korea; Sungkyunkwan University, Republic of Korea; Sungkyunkwan University, Republic of Korea",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1141","1153","Data centers frequently face significant memory under-utilization due to factors such as infrastructure overprovisioning, inefficient workload scheduling, and limited server configurations. This paper introduces Agile-DRAM, a novel DRAM architecture that addresses this issue by flexibly converting the under-utilized memory capacity into enhanced latency performance and reduced power consumption. Through minor modifications to the conventional DRAM architecture, Agile-DRAM supports multiple operational modes: low-latency, lowpower, and the default max-capacity mode. Notably, Agile-DRAM facilitates agile transitions between these modes in response to workload fluctuations in data centers at runtime. Evaluation results demonstrate that the low-latency mode can boost singlecore execution speed by up to 25.8% and diminish energy usage by up to 22.4%. Similarly, the low-power mode can reduce DRAM standby and self-refresh power by 31.6% and 85.7%, respectively.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476434","DRAM;Memory;Refresh;Access Latency;Power Consumption","Data centers;Power demand;Sensitivity;Runtime;System performance;Memory management;Random access memory","","","","45","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"CHROME: Concurrency-Aware Holistic Cache Management Framework with Online Reinforcement Learning","X. Lu; H. Najafi; J. Liu; X. -H. Sun",Illinois Institute of Technology; Florida International University; Florida International University; Illinois Institute of Technology,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1154","1167","Cache management is a critical aspect of computer architecture, encompassing techniques such as cache replacement, bypassing, and prefetching. Existing research has often focused on individual techniques, overlooking the potential benefits of joint optimization. Moreover, many of these approaches rely on static and intuition-driven policies, limiting their performance under complex and dynamic workloads. To address these challenges, this paper introduces CHROME, a novel concurrencyaware cache management framework. CHROME takes a holistic approach by seamlessly integrating intelligent cache replacement and bypassing with pattern-based prefetching. By leveraging online reinforcement learning, CHROME dynamically adapts cache decisions based on multiple program features and applies a reward for each decision that considers the accuracy of the action and the system-level feedback information. Our performance evaluation demonstrates that CHROME outperforms current state-of-the-art schemes, exhibiting significant improvements in cache management. Notably, CHROME achieves a remarkable performance boost of up to 13.7% over the traditional LRU method in multi-core systems with only modest overhead.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00090","National Science Foundation(grant numbers:CCF-2008000,CNS-1956229,IIS-2331908,CNS-2310422,CNS-2152497); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476439","Microarchitecture (CPU);Cache;Reinforcement Learning;Concurrency","Performance evaluation;Limiting;Prefetching;Reinforcement learning;Computer architecture;Optimization","","7","","62","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Prosper: Program Stack Persistence in Hybrid Memory Systems","A. KP; D. Mishra; B. Panda","Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Bombay, India",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1168","1183","A persistent and crash-consistent execution state is essential for systems to guarantee resilience against power failures and abrupt system crashes. The availability of nonvolatile memory (NVM) with read/write latency comparable to DRAM allows designing efficient checkpoint mechanisms for process persistence. Operating system (OS) level checkpoint solutions require capturing the change in the execution state of a process in an efficient manner. One of the crucial components of the execution state of any process is its memory state consisting of mutable stack and heap segments. Tracking modifications to the program stack is interesting because of its unique grow/shrink usage pattern and activation record write characteristics. Moreover, the stack is used in a programmer-agnostic manner where the compiler makes use of the support provided by the underlying ISA to use the stack and the OS manages the memory used by the stack region in an on-demand fashion. In this paper, we show the benefit of a checkpoint-based mechanism for stack persistence and the inefficiency of adapting existing generic memory persistence mechanisms for the stack region. We propose Prosper, a hardware-software (OS) codesigned checkpoint approach for stack persistence. Prosper tracks stack changes at sub-page byte granularity in hardware, allowing symbiosis with OS to realize efficient checkpoints of the stack region. Prosper significantly reduces (on average ∼4 ×) the amount of data copied during checkpoint and improves the overall checkpoint time with minimum overhead (less than 1% on average). Integration of Prosper with existing state-of-theart memory persistence mechanisms (such as SSP) for heap provides 2.6 × improvement over solely using the state-of-the-art mechanism for the entire memory area persistence.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476455","Process Persistence;Non-Volatile Memory;Stack Persistence","Symbiosis;Program processors;Nonvolatile memory;Operating systems;Memory management;Random access memory;Computer crashes","","1","","58","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Mitigating Write Disturbance in Non-Volatile Memory via Coupling Machine Learning with Out-of-Place Updates","R. Wu; Z. Shen; Z. Yang; J. Shu","School of Informatics, Xiamen University; School of Informatics, Xiamen University; School of Informatics, Xiamen University; School of Informatics, Xiamen University",2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),"2 Apr 2024","2024","","","1184","1198","Non-volatile memory (NVM) opens up new opportunities to resolve scaling restrictions of main memory, yet it is still hindered by the write disturbance (WD) problem. The WD problem mistakenly transforms the values of NVM cells, hence seriously deteriorating memory reliability and downgrading access performance. Existing studies mainly mitigate the WD problem via encoding WD-prone data patterns under in-place updates, yet we find that when turning to out-of-place updates, they can gain the potential to reduce more WD errors. We present LearnWD, an approach that mitigates the WD problem in NVM via coupling machine learning with out-of-place updates. LearnWD first employs clustering algorithms to classify the stale data based on the error proneness. To perform a write operation, LearnWD carefully examines the aggressivity of new data and the error proneness of stale data, so as to speculatively minimize the resulting WD errors. We conduct extensive experiments using 15 real-world data sets with different data types, showing that LearnWD can assist a variety of data encoding schemes to further reduce 20.1% of WD errors, shorten 11.0% of write latency, and extend 21.9% of write endurance.","2378-203X","979-8-3503-9313-2","10.1109/HPCA57654.2024.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476469","Non-Volatile Memory;Reliability;Write Disturbance","Couplings;Energy consumption;Machine learning algorithms;Nonvolatile memory;Machine learning;Transforms;Turning","","2","","112","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
