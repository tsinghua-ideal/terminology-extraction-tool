@inproceedings{10.1145/3579371.3589036,
author = {Min, Dongmoon and Kim, Junpyo and Choi, Junhyuk and Byun, Ilkwon and Tanaka, Masamitsu and Inoue, Koji and Kim, Jangwoo},
title = {QIsim: Architecting 10+K Qubit QC Interfaces Toward Quantum Supremacy},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589036},
doi = {10.1145/3579371.3589036},
abstract = {A 10+K qubit Quantum-Classical Interface (QCI) is essential to realize the quantum supremacy. However, it is extremely challenging to architect scalable QCIs due to the complex scalability trade-offs regarding operating temperatures, device and wire technologies, and microarchitecture designs. Therefore, architects need a modeling tool to evaluate various QCI design choices and lead to an optimal scalable QCI architecture.In this paper, we propose (1) QIsim, an open-source QCI simulation framework, and (2) novel architectural optimizations for designing 10+K qubit QCIs toward quantum supremacy. To achieve the goal, we first implement detailed QCI microarchitectures to model all the existing temperature and technology candidates. Next, based on the microarchitectures, we develop our scalability-analysis tool (QIsim) and thoroughly validate it using previous works, post-layout analyses, and real quantum-machine experiments. Finally, we successfully develop our 60,000+ qubit-scale QCI designs by applying eight architectural optimizations driven by QIsim.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {1},
numpages = {16},
keywords = {quantum-classical interface, simulation, modeling, single flux quantum (SFQ), cryogenic computing, quantum computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589037,
author = {Vittal, Suhas and Das, Poulami and Qureshi, Moinuddin},
title = {Astrea: Accurate Quantum Error-Decoding via Practical Minimum-Weight Perfect-Matching},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589037},
doi = {10.1145/3579371.3589037},
abstract = {Quantum devices suffer from high error rates, which makes them ineffective for running practical applications. Quantum computers can be made fault tolerant using Quantum Error Correction (QEC), which protects quantum information by encoding logical qubits using data qubits and parity qubits. The data qubits collectively store the quantum information and the parity qubits are measured periodically to produce a syndrome, which is decoded by a classical decoder to identify the location and type of errors. To prevent errors from accumulating and causing a logical error, decoders must accurately identify errors in real-time, necessitating the use of hardware solutions because software decoders are slow. Ideally, a real-time decoder must match the performance of the Minimum-Weight Perfect Matching (MWPM) decoder. However, due to the complexity of the underlying Blossom algorithm, state-of-the-art real-time decoders either use lookup tables, which are not scalable, or use approximate decoding, which significantly increases logical error rates.In this paper, we leverage two key insights to enable practical real-time MWPM decoding. First, for near-term implementations (with redundancies up to distance d = 7) of surface codes, the Hamming weight of the syndromes tends to be quite small (less than or equal to 10). For this regime, we propose Astrea, which simply performs a brute-force search for the few hundred possible options to perform accurate decoding within a few nanoseconds (1ns average, 456ns worst-case), thus representing the first decoder to be able to do MWPM in real-time up-to distance 7. Second, even for codes that produce syndromes with higher Hamming weights (e.g. d = 9) the search for optimal pairings can be made more efficient by simply discarding the weights that denote significantly lower probability than the logical error-rate of the code. For this regime, we propose a greedy design called Astrea-G, which filters high-cost weights and reorders the search from high-likelihood pairings to low-likelihood pairings to produce the most likely decoding within 1Î¼s (average 450ns). Our evaluations show that Astrea-G provides similar logical error-rates as the software-based MWPM for up-to d = 9 codes while meeting the real-time decoding latency constraints.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {2},
numpages = {16},
keywords = {real-time decoding, error decoding, quantum error correction},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589038,
author = {Guo, Cong and Tang, Jiaming and Hu, Weiming and Leng, Jingwen and Zhang, Chen and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao},
title = {OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589038},
doi = {10.1145/3579371.3589038},
abstract = {Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs' size grows by 240\texttimes{} every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5\texttimes{} speedup and 4.0\texttimes{} energy reduction, respectively, with a superior model accuracy.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {3},
numpages = {15},
keywords = {quantization, outlier-victim pair, large language model},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589039,
author = {Ha, Dongho and Oh, Yunho and Ro, Won Woo},
title = {R2D2: Removing ReDunDancy Utilizing Linearity of Address Generation in GPUs},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589039},
doi = {10.1145/3579371.3589039},
abstract = {A generally used GPU programming methodology is that adjacent threads access data in neighbor or specific-stride memory addresses and perform computations with the fetched data. This paper demonstrates that the memory addresses often exhibit a simple linear value pattern across GPU threads, as each thread uses built-in variables and constant values to compute the memory addresses. However, since the threads compute their context data individually, GPUs incur a heavy instruction overhead to calculate the memory addresses, even though they exhibit a simple pattern. We propose a GPU architecture called Removing ReDunDancy Utilizing Linearity of Address Generation (R2D2), reducing a large amount of the dynamic instruction count by detecting such linear patterns in the memory addresses and exploiting them for kernel computations. R2D2 detects linearities of the memory addresses with software support and pre-computes them before the threads execute the instructions. With the proposed scheme, each thread is able to compute its memory addresses with fewer dynamic instructions than conventional GPUs. In our evaluation, R2D2 achieves dynamic instruction reduction by 28\%, 1.25x speedup, and energy consumption reduction by 17\% over baseline GPU.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {4},
numpages = {14},
keywords = {redundant instruction, single instruction multiple thread, GPU},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589040,
author = {Fan, Zichen and Zhang, Qirui and Abillama, Pierre and Shoouri, Sara and Lee, Changwoo and Blaauw, David and Kim, Hun-Seok and Sylvester, Dennis},
title = {TaskFusion: An Efficient Transfer Learning Architecture with Dual Delta Sparsity for Multi-Task Natural Language Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589040},
doi = {10.1145/3579371.3589040},
abstract = {The combination of pre-trained models and task-specific fine-tuning schemes, such as BERT, has achieved great success in various natural language processing (NLP) tasks. However, the large memory and computation costs of such models make it challenging to deploy them in edge devices. Moreover, in real-world applications like chatbots, multiple NLP tasks need to be processed together to achieve higher response credibility. Running multiple NLP tasks with specialized models for each task increases the latency and memory cost latency linearly with the number of tasks. Though there have been recent works on parameter-shared tuning that aim to reduce the total parameter size by partially sharing weights among multiple tasks, computation remains intensive and redundant despite different tasks using the same input. In this work, we identify that a significant portion of activations and weights can be reused among different tasks, to reduce cost and latency for efficient multi-task NLP. Specifically, we propose TaskFusion, an efficient transfer learning software-hardware co-design that exploits delta sparsity in both weights and activations to boost data sharing among tasks. For training, TaskFusion uses â1 regularization on delta activation to learn inter-task data redundancies. A novel hardware-aware sub-task inference algorithm is proposed to exploit the dual delta sparsity. We then designed a dedicated heterogeneous architecture to accelerate multi-task inference with an optimized scheduling to increase hardware utilization and reduce off-chip memory access. Extensive experiments demonstrate that TaskFusion can reduce the number of floating point operations (FLOPs) by over 73\% in multi-task NLP with negligible accuracy loss, while adding a new task at the cost of only &lt; 2\% parameter size increase. With the proposed architecture and optimized scheduling, Task-Fusion can achieve 1.48--2.43\texttimes{} performance and 1.62--3.77\texttimes{} energy efficiency than those using state-of-the-art single-task accelerators for multi-task NLP applications.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {5},
numpages = {14},
keywords = {deep learning, accelerator, natural language processing, heterogeneous architecture, transformer, sparsity, multi-task, transfer learning},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589115,
author = {Li, Sixu and Li, Chaojian and Zhu, Wenbo and Yu, Boyang (Tony) and Zhao, Yang (Katie) and Wan, Cheng and You, Haoran and Shi, Huihong and Lin, Yingyan (Celine)},
title = {Instant-3D: Instant Neural Radiance Field Training Towards On-Device AR/VR 3D Reconstruction},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589115},
doi = {10.1145/3579371.3589115},
abstract = {Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for immersive Augmented and Virtual Reality (AR/VR) applications, but achieving instant (i.e., &lt; 5 seconds) on-device NeRF training remains a challenge. In this work, we first identify the inefficiency bottleneck: the need to interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during each training iteration. To alleviate this, we propose Instant-3D, an algorithm-hardware co-design acceleration framework that achieves instant on-device NeRF training. Our algorithm decomposes the embedding grid representation in terms of color and density, enabling computational redundancy to be squeezed out by adopting different (1) grid sizes and (2) update frequencies for the color and density branches. Our hardware accelerator further reduces the dominant memory accesses for embedding grid interpolation by (1) mapping multiple nearby points' memory read requests into one during the feed-forward process, (2) merging embedding grid updates from the same sliding time window during back-propagation, and (3) fusing different computation cores to support the different grid sizes needed by the color and density branches of Instant-3D algorithm. Extensive experiments validate the effectiveness of Instant-3D, achieving a large training time reduction of 41\texttimes{} - 248\texttimes{} while maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled instant 3D reconstruction for AR/VR, requiring a reconstruction time of only 1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9 W.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {6},
numpages = {13},
keywords = {hardware accelerator, neural radiance field (NeRF)},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589042,
author = {Maurya, Satvik and Mude, Chaithanya Naik and Oliver, William D. and Lienhard, Benjamin and Tannu, Swamit},
title = {Scaling Qubit Readout with Hardware Efficient Machine Learning Architectures},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589042},
doi = {10.1145/3579371.3589042},
abstract = {Reading a qubit is a fundamental operation in quantum computing. It translates quantum information into classical information enabling subsequent classification to assign the qubit states '0' or '1'. Unfortunately, qubit readout is one of the most error-prone and slowest operations on a superconducting quantum processor. On state-of-the-art superconducting quantum processors, readout errors can range from 1--10\%. These errors occur for various reasons - crosstalk, spontaneous state transitions, and excitation caused by the readout pulse. The error-prone nature of readout has resulted in significant research to design better discriminators to achieve higher qubit-readout accuracies. High readout accuracy is essential for enabling high fidelity for near-term noisy quantum computers and error-corrected quantum computers of the future.Prior works have used machine-learning-assisted single-shot qubit-state classification, where a deep neural network was used for more robust discrimination by compensating for crosstalk errors. However, the neural network size can limit the scalability of systems, especially if fast hardware discrimination is required. This state-of-the-art baseline design cannot be implemented on of-the-shelf FPGAs used for the control and readout of superconducting qubits in most systems, which increases the overall readout latency as discrimination has to be performed in software.In this work, we propose herqles, a scalable approach to improve qubit-state discrimination by using a hierarchy of matched filters in conjunction with a significantly smaller and scalable neural network for qubit-state discrimination. We achieve substantially higher readout accuracies (16.4\% relative improvement) than the baseline with a scalable design that can be readily implemented on off-the-shelf FPGAs. We also show that herqles is more versatile and can support shorter readout durations than the baseline design without additional training overheads.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {7},
numpages = {13},
keywords = {quantum control hardware, quantum computer architecture, qubit readout},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589043,
author = {Stein, Samuel and Wiebe, Nathan and Ding, Yufei and Ang, James and Li, Ang},
title = {Q-BEEP: Quantum Bayesian Error Mitigation Employing Poisson Modeling over the Hamming Spectrum},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589043},
doi = {10.1145/3579371.3589043},
abstract = {Quantum computing technology has grown rapidly in recent years, with new technologies being explored, error rates being reduced, and quantum processors' qubit capacity growing. However, near-term quantum algorithms are still unable to be induced without compounding consequential levels of noise, leading to non-trivial erroneous results. Quantum Error Correction (in-situ error mitigation) and Quantum Error Mitigation (post-induction error mitigation) are promising fields of research within the quantum algorithm scene, aiming to alleviate quantum errors. IBM recently published an article stating that Quantum Error Mitigation is the path to quantum computing usefulness. A recent work, namely HAMMER, demonstrated the existence of a latent structure regarding post-circuit induction errors when mapping to the Hamming spectrum. However, they assumed that errors occur solely in local clusters, whereas we observe that at higher average Hamming distances this structure falls away. In this work, we show that such a correlated structure is not only local but extends certain non-local clustering patterns which can be precisely described by a Poisson distribution model taking the input circuit, the device run time status (i.e., calibration statistics) and qubit topology into consideration. Using this quantum error characterizing model, we developed an iterative algorithm over the generated Bayesian network state-graph for post-induction error mitigation. Thanks to more precise modeling of the error distribution latent structure and the proposed iterative method, our Q-Beep approach provides state of the art performance and can boost circuit execution fidelity by up to 234.6\% on Bernstein-Vazirani circuits and on average 71.0\% on QAOA solution quality, using 16 practical IBMQ quantum processors. For other benchmarks such as those in QASMBench, a fidelity improvement of up to 17.8\% is attained. Q-Beep is a light-weight post-processing technique that can be performed offline and remotely, making it a useful tool for quantum vendors to adopt and provide more reliable circuit induction results. Q-Beep is maintained at github.com/pnnl/qbeep},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {8},
numpages = {13},
keywords = {quantum algorithms, noisy intermediate scale quantum computing, state graphs, bayesian error mitigation, quantum error mitigation, quantum computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589044,
author = {Hao, Tianyi and Liu, Kun and Tannu, Swamit},
title = {Enabling High Performance Debugging for Variational Quantum Algorithms using Compressed Sensing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589044},
doi = {10.1145/3579371.3589044},
abstract = {Variational quantum algorithms (VQAs) can potentially solve practical problems using contemporary Noisy Intermediate Scale Quantum (NISQ) computers. VQAs find near-optimal solutions in the presence of qubit errors by classically optimizing a loss function computed by parameterized quantum circuits. However, developing and testing VQAs is challenging due to the limited availability of quantum hardware, their high error rates, and the significant overhead of classical simulations. Furthermore, VQA researchers must pick the right initialization for circuit parameters, utilize suitable classical optimizer configurations, and deploy appropriate error mitigation methods. Unfortunately, these tasks are done in an ad-hoc manner today, as there are no software tools to configure and tune the VQA hyperparameters.In this paper, we present OSCAR (cOmpressed Sensing based Cost lAndscape Reconstruction) to help configure: 1) correct initialization, 2) noise mitigation techniques, and 3) classical optimizers to maximize the quality of the solution on NISQ hardware. OSCAR enables efficient debugging and performance tuning by providing users with the loss function landscape without running thousands of quantum circuits as required by the grid search. Using OSCAR, we can accurately reconstruct the complete cost landscape with up to 100X speedup. Furthermore, OSCAR can compute an optimizer function query in an instant by interpolating a computed landscape, thus enabling the trial run of a VQA configuration with considerably reduced overhead.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {9},
numpages = {13},
keywords = {debugging, variational quantum algorithms, quantum computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589045,
author = {Mo, Jianqiao and Gopinath, Jayanth and Reagen, Brandon},
title = {HAAC: A Hardware-Software Co-Design to Accelerate Garbled Circuits},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589045},
doi = {10.1145/3579371.3589045},
abstract = {Privacy and security have rapidly emerged as priorities in system design. One powerful solution for providing both is privacy-preserving computation, where functions are computed directly on encrypted data and control can be provided over how data is used. Garbled circuits (GCs) are a PPC technology that provide both confidential computing and control over how data is used. The challenge is that they incur significant performance overheads compared to plaintext. This paper proposes a novel garbled circuits accelerator and compiler, named HAAC, to mitigate performance overheads and make privacy-preserving computation more practical. HAAC is a hardware-software co-design. GCs are exemplars of co-design as programs are completely known at compile time, i.e., all dependence, memory accesses, and control flow are fixed. The design philosophy of HAAC is to keep hardware simple and efficient, maximizing area devoted to our proposed custom execution units and other circuits essential for high performance (e.g., on-chip storage). The compiler can leverage its program understanding to realize hardware's performance potential by generating effective instruction schedules, data layouts, and orchestrating off-chip events. In taking this approach we can achieve ASIC performance/efficiency without sacrificing generality. Insights of our approach include how co-design enables expressing arbitrary GCs programs as streams, which simplifies hardware and enables complete memory-compute decoupling, and the development of a scratchpad that captures data reuse by tracking program execution, eliminating the need for costly hardware managed caches and tagging logic. We evaluate HAAC with VIP-Bench and achieve an average speedup of 589\texttimes{} with DDR4 (2,627\texttimes{} with HBM2) in 4.3mm2 of area.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {10},
numpages = {13},
keywords = {hardware acceleration, cryptography},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589046,
author = {Chen, Dibei and Zhang, Tairan and Huang, Yi and Zhu, Jianfeng and Liu, Yang and Gou, Pengfei and Feng, Chunyang and Li, Binghua and Wei, Shaojun and Liu, Leibo},
title = {Orinoco: Ordered Issue and Unordered Commit with Non-Collapsible Queues},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589046},
doi = {10.1145/3579371.3589046},
abstract = {Modern out-of-order processors call for more aggressive scheduling techniques such as priority scheduling and out-of-order commit to make use of increasing core resources. Since these approaches prioritize the issue or commit of certain instructions, they face the conundrum of providing the capacity efficiency of scheduling structures while preserving the ideal ordering of instructions. Traditional collapsible queues are too expensive for today's processors, while state-of-the-art queue designs compromise with the pseudo-ordering of instructions, leading to performance degradation as well as other limitations.In this paper, we present Orinoco, a microarchitecture/circuit co-design that supports ordered issue and unordered commit with non-collapsible queues. We decouple the temporal ordering of instructions from their queue positions by introducing an age matrix with the bit count encoding, along with a commit dependency matrix and a memory disambiguation matrix to determine instructions to prioritize issue or commit. We leverage the Processing-in-Memory (PIM) approach and efficiently implement the matrix schedulers as 8T SRAM arrays. Orinoco achieves an average IPC improvement of 14.8\% over the baseline in-order commit core with the state-of-the-art scheduler while incurring overhead equivalent to a few kilobytes of SRAM.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {11},
numpages = {14},
keywords = {microarchitecture, out-of-order execution, instruction scheduling, out-of-order commit, processing-in-memory},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589047,
author = {Zhang, Hezi and Wu, Anbang and Wang, Yuke and Li, Gushu and Shapourian, Hassan and Shabani, Alireza and Ding, Yufei},
title = {OneQ: A Compilation Framework for Photonic One-Way Quantum Computation},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589047},
doi = {10.1145/3579371.3589047},
abstract = {In this paper, we propose OneQ, the first optimizing compilation framework for one-way quantum computation towards realistic photonic quantum architectures. Unlike previous compilation efforts for solid-state qubit technologies, our innovative framework addresses a unique set of challenges in photonic quantum computing. Specifically, this includes the dynamic generation of qubits over time, the need to perform all computation through measurements instead of relying on 1-qubit and 2-qubit gates, and the fact that photons are instantaneously destroyed after measurements. As pioneers in this field, we demonstrate the vast optimization potential of photonic one-way quantum computing, showcasing the remarkable ability of OneQ to reduce computing resource requirements by orders of magnitude.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {12},
numpages = {14},
keywords = {compiler, photonics, measurement-based quantum computing (MBQC), one-way quantum computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589048,
author = {Cai, Jingwei and Wei, Yuchen and Wu, Zuotong and Peng, Sen and Ma, Kaisheng},
title = {Inter-layer Scheduling Space Definition and Exploration for Tiled Accelerators},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589048},
doi = {10.1145/3579371.3589048},
abstract = {With the continuous expansion of the DNN accelerator scale, inter-layer scheduling, which studies the allocation of computing resources to each layer and the computing order of all layers in a DNN, plays an increasingly important role in maintaining a high utilization rate and energy efficiency of DNN inference accelerators. However, current inter-layer scheduling is mainly conducted based on some heuristic patterns. The space of inter-layer scheduling has not been clearly defined, resulting in significantly limited optimization opportunities and a lack of understanding on different inter-layer scheduling choices and their consequences.To bridge the gaps, we first propose a uniform and systematic notation, the Resource Allocation Tree (RA Tree), to represent different inter-layer scheduling schemes and depict the overall space of inter-layer scheduling. Based on the notation, we then thoroughly analyze how different inter-layer scheduling choices influence the performance and energy efficiency of an accelerator step by step. Moreover, we show how to represent existing patterns in our notation and analyze their features. To thoroughly explore the space of the inter-layer scheduling for diverse tiled accelerators and workloads, we develop an end-to-end and highly-portable scheduling framework, SET. Compared with the state-of-the-art (SOTA) open-source Tangram framework, SET can, on average, achieves 1.78\texttimes{} performance improvement and 13.2\% energy cost reduction simultaneously. Moreover, the SET framework will be open-sourced.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {13},
numpages = {17},
keywords = {tiled accelerators, neural networks, inter-layer scheduling, scheduling},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589049,
author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
title = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589049},
doi = {10.1145/3579371.3589049},
abstract = {Machine learning (ML) has become a prevalent approach to tame the complexity of design space exploration for domain-specific architectures. While appealing, using ML for design space exploration poses several challenges. First, it is not straightforward to identify the most suitable algorithm from an ever-increasing pool of ML methods. Second, assessing the trade-offs between performance and sample efficiency across these methods is inconclusive. Finally, the lack of a holistic framework for fair, reproducible, and objective comparison across these methods hinders the progress of adopting ML-aided architecture design space exploration and impedes creating repeatable artifacts. To mitigate these challenges, we introduce ArchGym, an open-source gymnasium and easy-to-extend framework that connects a diverse range of search algorithms to architecture simulators. To demonstrate its utility, we evaluate ArchGym across multiple vanilla and domain-specific search algorithms in the design of a custom memory controller, deep neural network accelerators, and a custom SoC for AR/VR workloads, collectively encompassing over 21K experiments. The results suggest that with an unlimited number of samples, ML algorithms are equally favorable to meet the user-defined target specification if its hyperparameters are tuned thoroughly; no one solution is necessarily better than another (e.g., reinforcement learning vs. Bayesian methods). We coin the term "hyperparameter lottery" to describe the relatively probable chance for a search algorithm to find an optimal design provided meticulously selected hyperparameters. Additionally, the ease of data collection and aggregation in ArchGym facilitates research in ML-aided architecture design space exploration. As a case study, we show this advantage by developing a proxy cost model with an RMSE of 0.61\% that offers a 2,000-fold reduction in simulation time. Code and data for ArchGym is available at https://bit.ly/ArchGym.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {14},
numpages = {16},
keywords = {reproducibility, baselines, open source, bayesian optimization, reinforcement learning, machine learning for system, machine learning for computer architecture, machine learning},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589050,
author = {Fan, Shulin and Hua, Zhichao and Xia, Yubin and Chen, Haibo and Zang, Binyu},
title = {ISA-Grid: Architecture of Fine-grained Privilege Control for Instructions and Registers},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589050},
doi = {10.1145/3579371.3589050},
abstract = {Isolation is a critical mechanism for enhancing the security of computer systems. By controlling the access privileges of software and hardware resources, isolation mechanisms can decouple software into multiple isolated components and enforce the principle of least privilege. While existing isolation systems primarily focus on memory isolation, they overlook the isolation of instruction and register resources, which we refer to as ISA (Instruction Set Architecture) resources. However, previous works have shown that exploiting ISA resources can lead to serious security problems, such as breaking the system's memory isolation property by abusing x86's CR3 register. Furthermore, existing hardware only provides privilege-level-based access control for ISA resources, which is too coarse-grained for software decoupling. For example, ARM Cortex A53 has several hundred system instructions/registers, but only four exception levels (EL0 to EL3) are provided. Additionally, more than 100 instructions/registers for system control are available in only EL1 (the kernel mode). To address this problem, this paper proposes ISA-Grid, an architecture of fine-grained privilege control for instructions and registers. ISA-Grid is a hardware extension that enables the creation of multiple ISA domains, with each domain having different privileges to access instructions and registers. The ISA domain can provide bit-level fine-grained privilege control for registers. We implemented prototypes of ISA-Grid based on two different CPU cores: 1) a RISC-V CPU core on an FPGA board and 2) an x86 CPU core on a simulator. We applied ISA-Grid to different cases, including Linux kernel decomposition and enhancing existing security systems, to demonstrate how ISA-Grid can isolate ISA resources and mitigate attacks based on abusing them. The performance evaluation results on both x86 and RISC-V platforms with real-world applications showed that ISA-Grid has negligible runtime overhead (less than 1\%).},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {15},
numpages = {15},
keywords = {software isolation, instruction set architecture, privilege control},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589051,
author = {Jin, Wenjing and Jang, Wonsuk and Park, Haneul and Lee, Jongsung and Kim, Soosung and Lee, Jae W.},
title = {DRAM Translation Layer: Software-Transparent DRAM Power Savings for Disaggregated Memory},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589051},
doi = {10.1145/3579371.3589051},
abstract = {Memory disaggregation is a promising solution to scale memory capacity and bandwidth shared by multiple server nodes in a flexible and cost-effective manner. DRAM power consumption, which is reported to be around 40\% of the total system power in the datacenter server, will become an even more serious concern in this high-capacity environment. Exploiting the low average utilization of DRAM capacity in today's datacenters, it is appealing to put unallocated/cold DRAM ranks into a power-saving mode. However, the conventional DRAM address mapping with fine-grained interleaving to maximize rank-level parallelism is incompatible with such rank-level DRAM power management techniques. Furthermore, existing DRAM power-saving techniques often require intrusive changes to the system stack, including OS, memory controller (MC), or even DRAM devices, to pose additional challenges for deployment. Thus, we propose DRAM Translation Layer (DTL) for host software/MC-transparent DRAM power management with commodity DRAM devices. Inspired by Flash Translation Layer (FTL) in modern SSDs, DTL is placed in the CXL memory controller to provide (i) flexible address mappings between host physical address and DRAM device physical address and (ii) host-transparent memory page migration. Leveraging DTL, we propose two DRAM power-saving techniques with different temporal granularities to maximize the number of DRAM ranks that can enter low-power states while provisioning sufficient DRAM bandwidth: rank-level power-down and hotness-aware self-refresh. The first technique consolidates unallocated memory pages into a subset of ranks at deallocation of a virtual machine (VM) and turns them off transparently to both OS and host MC. Our evaluation with CloudSuite benchmarks demonstrates that this technique saves DRAM power by 31.6\% on average at a 1.6\% performance cost. The hotness-aware self-refresh scheme further reduces DRAM energy consumption by up to 14.9\% with negligible performance loss via opportunistically migrating cold pages into a rank and making it enter self-refresh mode.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {16},
numpages = {13},
keywords = {address translation, pooled memory, CXL, disaggregated memory, datacenters, power management, DRAM},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589052,
author = {Ning, August and Tziantzioulis, Georgios and Wentzlaff, David},
title = {Supply Chain Aware Computer Architecture},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589052},
doi = {10.1145/3579371.3589052},
abstract = {Progressively and increasingly, our society has become more and more dependent on semiconductors and semiconductor-enabled products and services. The importance of chips and their supply chains has been highlighted during the 2020-present chip shortage caused by manufacturing disruptions and increased demand due to the COVID-19 pandemic. However, semiconductor supply chains are inherently vulnerable to disruptions and chip crises can easily recur in the future.We present the first work that elevates supply chain conditions to be a first-class design constraint for future computer architectures. We characterize and model the chip creation process from standard tapeout to packaging to provide a framework for architects to quickly assess the time-to-market of their chips depending on their architecture and the current market conditions. In addition, we propose a novel metric, the Chip Agility Score (CAS) - a way to quantify a chip architecture's resilience against production-side supply changes.We utilize our proposed time-to-market model, CAS, and chip design/manufacturing economic models to evaluate prominent architectures in the context of current and speculative supply chain changes. We find that using an older process node to re-release chips can decrease time-to-market by 73\%-116\% compared to using the most advanced processes. Also, mixed-process chiplet architectures can be 24\%-51\% more agile compared to equivalent single-process chiplet and monolithic designs respectively. Guided by our framework, we present an architectural design methodology that minimizes time-to-market and chip creation costs while maximizing agility for mass-produced legacy node chips.Our modeling framework and data sets are open-sourced to advance supply chain aware computer architecture research. https://github.com/PrincetonUniversity/ttm-cas},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {17},
numpages = {15},
keywords = {economics, modeling, chip shortage, semiconductor supply chain},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589053,
author = {Kim, Jongmin and Kim, Sangpyo and Choi, Jaewan and Park, Jaiyoung and Kim, Donghwan and Ahn, Jung Ho},
title = {SHARP: A Short-Word Hierarchical Accelerator for Robust and Practical Fully Homomorphic Encryption},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589053},
doi = {10.1145/3579371.3589053},
abstract = {Fully homomorphic encryption (FHE) is an emerging cryptographic technology that guarantees the privacy of sensitive user data by enabling direct computations on encrypted data. Despite the security benefits of this approach, FHE is associated with prohibitively high levels of computational and memory overhead, preventing its widespread use in real-world services. Numerous domain-specific hardware designs have been proposed to address this issue, but most of them use excessive amounts of chip area and power, leaving room for further improvements in terms of practicality.We propose SHARP, a robust and practical accelerator for FHE. We analyze the implications of various hardware design choices on the functionality, performance, and efficiency of FHE. We conduct a multifaceted analysis of the impacts of the machine word length choice on the FHE acceleration, which, despite its importance with regard to hardware efficiency, has yet to be explored due to its complex correlation with various FHE parameters. A relatively short word length of 36 bits is discovered to be a robust and efficient solution for FHE accelerators. We devise an efficient hierarchical SHARP microarchitecture with a novel data organization and specialized functional units and substantially reduce the on-chip memory capacity requirement through architectural and software enhancements. This study demonstrates that SHARP delivers superior performance over prior FHE accelerators with a distinctly smaller chip area and lower power budget.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {18},
numpages = {15},
keywords = {hierarchical architecture, word length, accelerator, fully homomorphic encryption},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589054,
author = {Gerogiannis, Gerasimos and Yesil, Serif and Lenadora, Damitha and Cao, Dingyuan and Mendis, Charith and Torrellas, Josep},
title = {SPADE: A Flexible and Scalable Accelerator for SpMM and SDDMM},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589054},
doi = {10.1145/3579371.3589054},
abstract = {The widespread use of Sparse Matrix Dense Matrix Multiplication (SpMM) and Sampled Dense Matrix Dense Matrix Multiplication (SDDMM) kernels makes them candidates for hardware acceleration. However, accelerator design for these kernels faces two main challenges: (1) the overhead of moving data between CPU and accelerator (often including an address space conversion from the CPU's virtual addresses) and (2) marginal flexibility to leverage the fact that different sparse input matrices benefit from different variations of the SpMM and SDDMM algorithms.To address these challenges, this paper proposes SPADE, a new SpMM and SDDMM hardware accelerator. SPADE avoids data transfers by tightly-coupling accelerator processing elements (PEs) with the cores of a multicore, as if the accelerator PEs were advanced functional units---allowing the accelerator to reuse the CPU memory system and its virtual addresses. SPADE attains flexibility and programmability by supporting a tile-based ISA---high level enough to eliminate the overhead of fetching and decoding fine-grained instructions. To prove the SPADE concept, we have taped-out a simplified SPADE chip. Further, simulations of a SPADE system with 224--1792 PEs show its high performance and scalability. A 224-PE SPADE system is on average 2.3x, 1.3x and 2.5x faster than a 56-core CPU, a server-class GPU, and an SpMM accelerator, respectively, without accounting for the host-accelerator data transfer overhead. If such overhead is taken into account, the 224-PE SPADE system is on average 43.4x and 52.4x faster than the GPU and the accelerator, respectively. Further, SPADE has a small area and power footprint.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {19},
numpages = {15},
keywords = {SDDMM, SpMM, sparse computations, hardware accelerator},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589055,
author = {E. Becker, Pedro H. and Arnau, Jos\'{e}-Mar\'{\i}a and Gonz\'{a}lez, Antonio},
title = {K-D Bonsai: ISA-Extensions to Compress K-D Trees for Autonomous Driving Tasks},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589055},
doi = {10.1145/3579371.3589055},
abstract = {Autonomous Driving (AD) systems extensively manipulate 3D point clouds for object detection and vehicle localization. Thereby, efficient processing of 3D point clouds is crucial in these systems. In this work we propose K-D Bonsai, a technique to cut down memory usage during radius search, a critical building block of point cloud processing. K-D Bonsai exploits value similarity in the data structure that holds the point cloud (a k-d tree) to compress the data in memory. K-D Bonsai further compresses the data using a reduced floating-point representation, exploiting the physically limited range of point cloud values. For easy integration into nowadays systems, we implement K-D Bonsai through Bonsai-extensions, a small set of new CPU instructions to compress, decompress, and operate on points. To maintain baseline safety levels, we carefully craft the Bonsai-extensions to detect precision loss due to compression, allowing re-computation in full precision to take place if necessary. Therefore, K-D Bonsai reduces data movement, improving performance and energy efficiency, while guaranteeing baseline accuracy and programmability. We evaluate K-D Bonsai over the euclidean cluster task of Autoware.ai, a state-of-the-art software stack for AD. We achieve an average of 9.26\% improvement in end-to-end latency, 12.19\% in tail latency, and a reduction of 10.84\% in energy consumption. Differently from expensive accelerators proposed in related work, K-D Bonsai improves radius search with minimal area increase (0.36\%).},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {20},
numpages = {13},
keywords = {ISA-extension, compression, radius search, K-D tree, point cloud, autonomous driving hardware},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589056,
author = {Lee, Junseo and Choi, Kwanseok and Lee, Jungi and Lee, Seokwon and Whangbo, Joonho and Sim, Jaewoong},
title = {NeuRex: A Case for Neural Rendering Acceleration},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589056},
doi = {10.1145/3579371.3589056},
abstract = {This paper presents NeuRex, an accelerator architecture that efficiently performs the modern neural rendering pipeline with an algorithmic enhancement and supporting hardware. NeuRex leverages the insights from an in-depth analysis of the state-of-the-art neural scene representation to make the multi-resolution hash encoding, which is the key operational primitive in modern neural renderings, more hardware-friendly and features a specialized hash encoding engine that enables us to effectively perform the primitive and the overall rendering pipeline. We implement and synthesize NeuRex using a commercial 28nm process technology and evaluate two versions of NeuRex (NeuRex-Edge, NeuRex-Server) on a range of scenes with different image resolutions for mobile and high-end computing platforms. Our evaluation shows that NeuRex achieves up to 9.88\texttimes{} and 3.11\texttimes{} speedups against the mobile and high-end consumer GPUs with a substantially small area overhead and lower energy consumption.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {21},
numpages = {13},
keywords = {accelerators, machine learning, neural networks, NeRF, neural rendering},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589057,
author = {Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
title = {FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589057},
doi = {10.1145/3579371.3589057},
abstract = {Transformer model is becoming prevalent in various AI applications with its outstanding performance. However, the high cost of computation and memory footprint make its inference inefficient. We discover that among the three main computation modules in a Transformer model (QKV generation, attention computation, FFN), it is the QKV generation and FFN that contribute to the most power cost. While the attention computation, focused by most previous works, only has decent power share when dealing with extremely long inputs. Therefore, in this paper, we propose FACT, an efficient algorithm-hardware co-design optimizing all three modules of Transformer. We first propose an eager prediction algorithm which predicts the attention matrix before QKV generation. It further detects the unnecessary computation in QKV generation and assigns mixed-precision FFN with the predicted attention, which helps improve the throughput. Further, we propose FACT accelerator to efficiently support eager prediction with three designs. It avoids the large overhead of prediction by using log-based add-only operations for prediction. It eliminates the latency of prediction through an out-of-order scheduler that makes the eager prediction and computation work in full pipeline. It additionally avoids memory access conflict in the mixed-precision FFN with a novel diagonal storage pattern. Experiments on 22 benchmarks show that our FACT improves the throughput of the whole Transformer by 3.59\texttimes{} on the geomean average. It achieves an enviable 47.64\texttimes{} and 278.1\texttimes{} energy saving when computing attention, compared to previous attention-optimization-only SOTA works ELSA and Sanger. Further, FACT achieves an energy efficiency of 4388 GOPS/W performing the whole Transformer layer on average, which is 94.98\texttimes{} higher than Nvidia V100 GPU.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {22},
numpages = {14},
keywords = {neural network, algorithm-hardware co-design, efficient computing, hardware accelerator, transformer},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589058,
author = {Gottschall, Bj\"{o}rn and Eeckhout, Lieven and Jahre, Magnus},
title = {TEA: Time-Proportional Event Analysis},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589058},
doi = {10.1145/3579371.3589058},
abstract = {As computer architectures become increasingly complex and heterogeneous, it becomes progressively more difficult to write applications that make good use of hardware resources. Performance analysis tools are hence critically important as they are the only way through which developers can gain insight into the reasons why their application performs as it does. State-of-the-art performance analysis tools capture a plethora of performance events and are practically non-intrusive, but performance optimization is still extremely challenging. We believe that the fundamental reason is that current state-of-the-art tools in general cannot explain why executing the application's performance-critical instructions take time.We hence propose Time-Proportional Event Analysis (TEA) which explains why the architecture spends time executing the application's performance-critical instructions by creating time-proportional Per-Instruction Cycle Stacks (PICS). PICS unify performance profiling and performance event analysis, and thereby (i) report the contribution of each static instruction to overall execution time, and (ii) break down per-instruction execution time across the (combinations of) performance events that a static instruction was subjected to across its dynamic executions. Creating time-proportional PICS requires tracking performance events across all in-flight instructions, but TEA only increases per-core power consumption by ~3.2 mW (~0.1\%) because we carefully select events to balance insight and overhead. TEA leverages statistical sampling to keep performance overhead at 1.1\% on average while incurring an average error of 2.1\% compared to a non-sampling golden reference; a significant improvement upon the 55.6\%, 55.5\%, and 56.0\% average error for AMD IBS, Arm SPE, and IBM RIS. We demonstrate that TEA's accuracy matters by using TEA to identify performance issues in the SPEC CPU2017 benchmarks lbm and nab that, once addressed, yield speedups of 1.28\texttimes{} and 2.45\texttimes{}, respectively.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {23},
numpages = {13},
keywords = {performance events, time proportionality, performance analysis},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589059,
author = {Xue, Yuqi and Liu, Yiqi and Nai, Lifeng and Huang, Jian},
title = {V10: Hardware-Assisted NPU Multi-tenancy for Improved Resource Utilization and Fairness},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589059},
doi = {10.1145/3579371.3589059},
abstract = {Modern cloud platforms have deployed neural processing units (NPUs) like Google Cloud TPUs to accelerate online machine learning (ML) inference services. To improve the resource utilization of NPUs, they allow multiple ML applications to share the same NPU, and developed both time-multiplexed and preemptive-based sharing mechanisms. However, our study with real-world NPUs discloses that these approaches suffer from surprisingly low utilization, due to the lack of support for fine-grained hardware resource sharing in the NPU. Specifically, its separate systolic array and vector unit cannot be fully utilized at the same time, which requires fundamental hardware assistance for supporting multi-tenancy.In this paper, we present V10, a hardware-assisted NPU multi-tenancy framework for improving resource utilization, while ensuring fairness for different ML services. We rethink the NPU architecture for supporting multi-tenancy. V10 employs an operator scheduler for enabling concurrent operator executions on the systolic array and the vector unit and offers flexibility for enforcing different priority-based resource-sharing mechanisms. V10 also enables fine-grained operator preemption and lightweight context switch in the NPU. To further improve NPU utilization, V10 also develops a clustering-based workload collocation mechanism for identifying the best-matching ML services on a shared NPU. We implement V10 with an NPU simulator. Our experiments with various ML workloads from MLPerf AI Benchmarks demonstrate that V10 can improve the overall NPU utilization by 1.64\texttimes{}, increase the aggregated throughput by 1.57\texttimes{}, reduce the average latency of ML services by 1.56\texttimes{}, and tail latency by 1.74\texttimes{} on average, in comparison with state-of-the-art NPU multi-tenancy approaches.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {24},
numpages = {15},
keywords = {ML accelerator, multi-tenancy, neural processing unit},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589060,
author = {Gu, Yufeng and Subramaniyan, Arun and Dunn, Tim and Khadem, Alireza and Chen, Kuan-Yu and Paul, Somnath and Vasimuddin, Md and Misra, Sanchit and Blaauw, David and Narayanasamy, Satish and Das, Reetuparna},
title = {GenDP: A Framework of Dynamic Programming Acceleration for Genome Sequencing Analysis},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589060},
doi = {10.1145/3579371.3589060},
abstract = {Genomics is playing an important role in transforming healthcare. Genetic data, however, is being produced at a rate that far outpaces Moore's Law. Many efforts have been made to accelerate genomics kernels on modern commodity hardware such as CPUs and GPUs, as well as custom accelerators (ASICs) for specific genomics kernels. While ASICs provide higher performance and energy efficiency than general-purpose hardware, they incur a high hardware design cost. Moreover, in order to extract the best performance, ASICs tend to have significantly different architectures for different kernels. The divergence of ASIC designs makes it difficult to run commonly used modern sequencing analysis pipelines due to software integration and programming challenges.With the observation that many genomics kernels are dominated by dynamic programming (DP) algorithms, this paper presents GenDP, a framework of dynamic programming acceleration including DPAx, a DP accelerator, and DPMap, a graph partitioning algorithm that maps DP objective functions to the accelerator. DPAx supports DP kernels with various dependency patterns, such as 1D and 2D DP tables and long-range dependencies in the graph structure. DPAx also supports different DP objective functions and precisions required for genomics applications. GenDP is evaluated on genomics kernels in both short-read and long-read analysis pipelines, achieving 157.8\texttimes{} throughput/mm2 over GPU baselines and 132.0\texttimes{} throughput/mm2 over CPU baselines.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {25},
numpages = {15},
keywords = {bioinfomatics, genomics, reconfigurable architectures, hardware accelerators, computer architecture},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589061,
author = {Bleier, Nathaniel and Wezelis, Abigail and Varshney, Lav and Kumar, Rakesh},
title = {Programmable Olfactory Computing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589061},
doi = {10.1145/3579371.3589061},
abstract = {While smell is arguably the most visceral of senses, olfactory computing has been barely explored in the mainstream. We argue that this is a good time to explore olfactory computing since a) a large number of driver applications are emerging, b) odor sensors are now dramatically better, and c) non-traditional form factors such as sensor, wearable, and xR devices that would be required to support olfactory computing are already getting widespread acceptance. Through a comprehensive review of literature, we identify the key algorithms needed to support a wide variety of olfactory computing tasks. We profiled these algorithms on existing hardware and identified several characteristics, including the preponderance of fixed-point computation, and linear operations, and real arithmetic; a variety of data memory requirements; and opportunities for data-level parallelism. We propose Ahromaa, a heterogeneous architecture for olfactory computing targeting extremely power and energy constrained olfactory computing workloads and evaluate it against baseline architectures of an MCU, a state-of-art CGRA, and an MCU with packed SIMD. Across our algorithms, Ahromaa's operating modes outperform the baseline architectures by 1.36, 1.22, and 1.1\texttimes{} in energy efficiency when operating at MEOP. We also show how careful design of data memory organization can lead to significant energy savings in olfactory computing, due to the limited amount of data memory many olfactory computing kernels require. These improvements to the data memory organization lead to additional 4.21, 4.37, and 2.85\texttimes{} improvements in energy efficiency on average.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {26},
numpages = {14},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589062,
author = {Andrulis, Tanner and Emer, Joel S. and Sze, Vivienne},
title = {RAELLA: Reforming the Arithmetic for Efficient, Low-Resolution, and Low-Loss Analog PIM: No Retraining Required!},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589062},
doi = {10.1145/3579371.3589062},
abstract = {Processing-In-Memory (PIM) accelerators have the potential to efficiently run Deep Neural Network (DNN) inference by reducing costly data movement and by using resistive RAM (ReRAM) for efficient analog compute. Unfortunately, overall PIM accelerator efficiency is limited by energy-intensive analog-to-digital converters (ADCs). Furthermore, existing accelerators that reduce ADC cost do so by changing DNN weights or by using low-resolution ADCs that reduce output fidelity. These strategies harm DNN accuracy and/or require costly DNN retraining to compensate.To address these issues, we propose the RAELLA architecture. RAELLA adapts the architecture to each DNN; it lowers the resolution of computed analog values by encoding weights to produce near-zero analog values, adaptively slicing weights for each DNN layer, and dynamically slicing inputs through speculation and recovery. Low-resolution analog values allow RAELLA to both use efficient low-resolution ADCs and maintain accuracy without retraining, all while computing with fewer ADC converts.Compared to other low-accuracy-loss PIM accelerators, RAELLA increases energy efficiency by up to 4.9\texttimes{} and throughput by up to 3.3\texttimes{}. Compared to PIM accelerators that cause accuracy loss and retrain DNNs to recover, RAELLA achieves similar efficiency and throughput without expensive DNN retraining.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {27},
numpages = {16},
keywords = {ReRAM, ADC, slicing, architecture, accelerator, neural networks, analog, compute in memory, processing in memory},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589063,
author = {Luo, Haocong and Olgun, Ataberk and Ya\u{g}l\i{}k\c{c}\i{}, Abdullah Giray and Tu\u{g}rul, Yahya Can and Rhyner, Steve and Cavlak, Meryem Banu and Lindegger, Jo\"{e}l and Sadrosadati, Mohammad and Mutlu, Onur},
title = {RowPress: Amplifying Read Disturbance in Modern DRAM Chips},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589063},
doi = {10.1145/3579371.3589063},
abstract = {Memory isolation is critical for system reliability, security, and safety. Unfortunately, read disturbance can break memory isolation in modern DRAM chips. For example, RowHammer is awell-studied read-disturb phenomenon where repeatedly opening and closing (i.e., hammering) a DRAM row many times causes bitflips in physically nearby rows.This paper experimentally demonstrates and analyzes another widespread read-disturb phenomenon, RowPress, in real DDR4 DRAM chips. RowPress breaks memory isolation by keeping a DRAM row open for a long period of time, which disturbs physically nearby rows enough to cause bitflips. We show that RowPress amplifies DRAM's vulnerability to read-disturb attacks by significantly reducing the number of row activations needed to induce a bitflip by one to two orders of magnitude under realistic conditions. In extreme cases, RowPress induces bitflips in a DRAM row when an adjacent row is activated only once. Our detailed characterization of 164 real DDR4 DRAM chips shows that RowPress 1) affects chips from all three major DRAM manufacturers, 2) gets worse as DRAM technology scales down to smaller node sizes, and 3) affects a different set of DRAM cells from RowHammer and behaves differently from RowHammer as temperature and access pattern changes. We also show that cells vulnerable to RowPress are very different from cells vulnerable to retention failures.We demonstrate in a real DDR4-based system with RowHammer protection that 1) a user-level program induces bitflips by leveraging RowPress while conventional RowHammer cannot do so, and 2) a memory controller that adaptively keeps the DRAM row open for a longer period of time based on access pattern can facilitate RowPress-based attacks. To prevent bitflips due to RowPress, we describe and analyze four potential mitigation techniques, including a new methodology that adapts existing RowHammer mitigation techniques to also mitigate RowPress with low additional performance overhead. We evaluate this methodology and demonstrate that it is effective on a variety of workloads. We open source all our code and data to facilitate future research on RowPress.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {28},
numpages = {18},
keywords = {testing, safety, security, reliability, RowHammer, rowpress, read disturbance, DRAM},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589064,
author = {Ma, Tianrui and Feng, Yu and Zhang, Xuan and Zhu, Yuhao},
title = {CAMJ: Enabling System-Level Energy Modeling and Architectural Exploration for In-Sensor Visual Computing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589064},
doi = {10.1145/3579371.3589064},
abstract = {CMOS Image Sensors (CIS) are fundamental to emerging visual computing applications. While conventional CIS are purely imaging devices for capturing images, increasingly CIS integrate processing capabilities such as Deep Neural Network (DNN). Computational CIS expand the architecture design space, but to date no comprehensive energy model exists. This paper proposes CamJ, a detailed energy modeling framework that provides a component-level energy breakdown for computational CIS and is validated against nine recent CIS chips. We use CamJ to demonstrate three use-cases that explore architectural trade-offs including computing in vs. off CIS, 2D vs. 3D-stacked CIS design, and analog vs. digital processing inside CIS. The code of CamJ is available at: https://github.com/horizon-research/CamJ.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {29},
numpages = {14},
keywords = {analog modeling, energy modeling, in-sensor computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589065,
author = {Soria-Pardos, V\'{\i}ctor and Armejach, Adri\`{a} and M\"{u}ck, Tiago and Su\'{a}rez-Gracia, Dario and Joao, Jos\'{e} and Rico, Alejandro and Moret\'{o}, Miquel},
title = {DynAMO: Improving Parallelism Through Dynamic Placement of Atomic Memory Operations},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589065},
doi = {10.1145/3579371.3589065},
abstract = {With increasing core counts in modern multi-core designs, the overhead of synchronization jeopardizes the scalability and efficiency of parallel applications. To mitigate these overheads, modern cache-coherent protocols offer support for Atomic Memory Operations (AMOs) that can be executed near-core (near) or remotely in the on-chip memory hierarchy (far).This paper evaluates current available static AMO execution policies implemented in multi-core Systems-on-Chip (SoC) designs, which select AMOs' execution placement (near or far) based on the cache block coherence state. We propose three static policies and show that the performance of static policies is application dependent. Moreover, we show that one of our proposed static policies outperforms currently available implementations.Furthermore, we propose DynAMO, a predictor that selects the best location to execute the AMOs. DynAMO identifies the different locality patterns to make informed decisions, improving AMO latency and increasing overall throughput. DynAMO outperforms the best-performing static policy and provides geometric mean speed-ups of 1.09\texttimes{} across all workloads and 1.31\texttimes{} on AMO-intensive applications with respect to executing all AMOs near.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {30},
numpages = {13},
keywords = {data placement, atomic memory operations, microarchitecture, multi-core architectures},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589066,
author = {Hou, Xiaofeng and Liu, Jiacheng and Tang, Xuehan and Li, Chao and Chen, Jia and Liang, Luhong and Cheng, Kwang-Ting and Guo, Minyi},
title = {Architecting Efficient Multi-modal AIoT Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589066},
doi = {10.1145/3579371.3589066},
abstract = {Multi-modal computing (M2C) has recently exhibited impressive accuracy improvements in numerous autonomous artificial intelligence of things (AIoT) systems. However, this accuracy gain is often tethered to an incredible increase in energy consumption. Particularly, various highly-developed modality sensors devour most of the energy budget, which would make the deployment of M2C for real-world AIoT applications a difficult challenge.To address the above issue, we propose AMG, an innovative HW/SW co-design solution tailored to multi-modal AIoT systems. The key behind AMG is modality gating (throttling) that allows for adaptively sensing and computing modalities for different tasks. This is non-trivial since we must balance situational awareness, energy conservation, and execution latency. AMG achieves our goal with two first-of-its-kind designs. 1) It introduces a novel decoupled modality sensor architecture to support partial throttling of modality sensors. Doing so allows one to greatly save AIoT power but maintains sensor data flow. 2) AMG also features a smart power management strategy based on the new architecture, allowing the device to initialize and tune itself with the optimal configuration. It can predict whether a reasonable degree of accuracy will be satisfied during runtime, and react proactively to remediate the gating process. Extensive evaluation based on our prototype system confirms that AMG improves the AIoT lifespan by 74.5\% to 133.7\% with the same energy budget while meeting the performance requirements.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {31},
numpages = {13},
keywords = {energy efficiency, modality gating, multi-modal computing, autonomous embedded systems, edge artificial intelligence},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589067,
author = {Pan, Rui and Liu, Chubo and Xiao, Guoqing and Duan, Mingxing and Li, Keqin and Li, Kenli},
title = {An Algorithm and Architecture Co-design for Accelerating Smart Contracts in Blockchain},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589067},
doi = {10.1145/3579371.3589067},
abstract = {Modern blockchains supporting smart contracts implement a new form of state machine replication with a trusted and decentralized paradigm. However, inefficient smart contract transaction execution severely limits system throughput and hinders the further application of blockchain.In this work, we achieve efficient multi-layer parallelism of transactions through an algorithm and architecture co-design. Based on the analysis of smart contract invocations, we find that transactions are widely redundant in spatio-temporal distribution and reveal a restriction of scheduling benefits resulting from redundant operations. We propose a spatio-temporal scheduling algorithm to solve this restriction and design a multi-transaction processing unit (MTPU) to accomplish the optimization of transaction parallelism and redundancy. Transactions are asynchronously parallel in the spatial dimension through decoupling of execution and scheduling. Fine-grained data and instruction reuse enables transaction de-redundancy in the time dimension. In addition, we collect execution information for frequently invoked hotspot contracts and perform deep optimization in the idle time slice. Finally, our evaluation shows that the algorithm and architecture co-design is able to achieve a further acceleration of 3.53\texttimes{} -- 16.19\texttimes{} compared to existing schemes.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {32},
numpages = {13},
keywords = {parallelism, smart contract, blockchain},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589068,
author = {Stojkovic, Jovan and Liu, Chunao and Shahbaz, Muhammad and Torrellas, Josep},
title = {Î¼Manycore: A Cloud-Native CPU for Tail at Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589068},
doi = {10.1145/3579371.3589068},
abstract = {Microservices are emerging as a popular cloud-computing paradigm. Microservice environments execute typically-short service requests that interact with one another via remote procedure calls (often across machines), and are subject to stringent tail-latency constraints. In contrast, current processors are designed for traditional monolithic applications. They support global hardware cache coherence, provide large caches, incorporate microarchitecture for long-running, predictable applications (such as advanced prefetching), and are optimized to minimize average latency rather than tail latency.To address this imbalance, this paper proposes Î¼Manycore, an architecture optimized for cloud-native microservice environments. Based on a characterization of microservice applications, Î¼Manycore is designed to minimize unnecessary microarchitecture and mitigate overheads to reduce tail latency. Indeed, rather than supporting manycore-wide hardware cache coherence, Î¼Manycore has multiple small hardware cache-coherent domains, called Villages. Clusters of villages are interconnected with an on-package leaf-spine network, which has many redundant, low-hop-count paths between clusters. To minimize latency overheads, Î¼Manycore schedules and queues service requests in hardware, and includes hardware support to save and restore process state when doing a context-switch. Our simulation-based results show that Î¼Manycore delivers high performance. A cluster of 10 servers with a 1024-core Î¼Manycore in each server delivers 3.7\texttimes{} lower average latency, 15.5\texttimes{} higher throughput, and, importantly, 10.4\texttimes{} lower tail latency than a cluster with iso-power conventional server-class multicores. Similar good results are attained compared to a cluster with power-hungry iso-area conventional server-class multicores.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {33},
numpages = {15},
keywords = {manycore architecture, cloud computing, microservices},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589069,
author = {Stojkovic, Jovan and Xu, Tianyin and Franke, Hubertus and Torrellas, Josep},
title = {MXFaaS: Resource Sharing in Serverless Environments for Parallelism and Efficiency},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589069},
doi = {10.1145/3579371.3589069},
abstract = {Although serverless computing is a popular paradigm, current serverless environments have high overheads. Recently, it has been shown that serverless workloads frequently exhibit bursts of invocations of the same function. Such pattern is not handled well in current platforms. Supporting it efficiently can speed-up serverless execution substantially.In this paper, we target this dominant pattern with a new server-less platform design named MXFaaS. MXFaaS improves function performance by efficiently multiplexing (i.e., sharing) processor cycles, I/O bandwidth, and memory/processor state between concurrently executing invocations of the same function. MXFaaS introduces a new container abstraction called MXContainer. To enable efficient use of processor cycles, an MXContainer carefully helps schedule same-function invocations for minimal response time. To enable efficient use of I/O bandwidth, an MXContainer coalesces remote storage accesses and remote function calls from same-function invocations. Finally, to enable efficient use of memory/processor state, an MXContainer first initializes the state of its container and only later, on demand, spawns a process per function invocation, so that all invocations can share unmodified memory state and hence minimize memory footprint.We implement MXFaaS in two serverless platforms and run diverse serverless benchmarks. With MXFaaS, serverless environments are much more efficient. Compared to a state-of-the-art serverless environment, MXFaaS on average speeds-up execution by 5.2\texttimes{}, reduces P99 tail latency by 7.4\texttimes{}, and improves throughput by 4.8\texttimes{}. In addition, it reduces the average memory usage by 3.4\texttimes{}.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {34},
numpages = {15},
keywords = {resource management, cloud computing, serverless computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589070,
author = {Ghaniyoun, Moein and Barber, Kristin and Xiao, Yuan and Zhang, Yinqian and Teodorescu, Radu},
title = {TEESec: Pre-Silicon Vulnerability Discovery for Trusted Execution Environments},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589070},
doi = {10.1145/3579371.3589070},
abstract = {Trusted execution environments (TEE) are CPU hardware extensions that provide security guarantees for applications running on untrusted operating systems. The security of TEEs is threatened by a variety of microarchitectural vulnerabilities, which have led to a large number of demonstrated attacks. While various solutions for verifying the correctness and security of TEE designs have been proposed, they generally do not extend to jointly verifying the security of the underlying microarchitecture. This paper presents TEESec, the first pre-silicon framework for discovering microarchitectural vulnerabilities in the context of trusted execution environments. TEESec is designed to jointly and systematically test the TEE and underlying microarchitecture against data and metadata leakage across isolation boundaries. We implement TEESec in the Chipyard framework and evaluate it on two open-source RISC-V out-of-order processors running the Keystone TEE. Using TEESec we uncover 10 distinct vulnerabilities in these processors that violate TEE security principles and could lead to leakage of enclave secrets.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {35},
numpages = {15},
keywords = {verification, trusted execution environments, security},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589071,
author = {Nadig, Rakesh and Sadrosadati, Mohammad and Mao, Haiyu and Ghiasi, Nika Mansouri and Tavakkol, Arash and Park, Jisung and Sarbazi-Azad, Hamid and Luna, Juan G\'{o}mez and Mutlu, Onur},
title = {Venice: Improving Solid-State Drive Parallelism at Low Cost via Conflict-Free Accesses},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589071},
doi = {10.1145/3579371.3589071},
abstract = {The performance and capacity of solid-state drives (SSDs) are continuously improving to meet the increasing demands of modern data-intensive applications. Unfortunately, communication between the SSD controller and memory chips (e.g., 2D/3D NAND flash chips) is a critical performance bottleneck for many applications. SSDs use a multi-channel shared bus architecture where multiple memory chips connected to the same channel communicate to the SSD controller with only one path. As a result, path conflicts often occur during the servicing of multiple I/O requests, which significantly limits SSD parallelism. It is critical to handle path conflicts well to improve SSD parallelism and performance.Our goal is to fundamentally tackle the path conflict problem by increasing the number of paths between the SSD controller and memory chips at low cost. To this end, we build on the idea of using an interconnection network to increase the path diversity between the SSD controller and memory chips. We propose Venice, a new mechanism that introduces a low-cost interconnection network between the SSD controller and memory chips and utilizes the path diversity to intelligently resolve path conflicts. Venice employs three key techniques: 1) a simple router chip added next to each memory chip without modifying the memory chip design, 2) a path reservation technique that reserves a path from the SSD controller to the target memory chip before initiating a transfer, and 3) a fully-adaptive routing algorithm that effectively utilizes the path diversity to resolve path conflicts. Our experimental results show that Venice 1) improves performance by an average of 2.65\texttimes{}/1.67\texttimes{} over a baseline performance-optimized/cost-optimized SSD design across a wide range of workloads, 2) reduces energy consumption by an average of 61\% compared to a baseline performance-optimized SSD design. Venice's benefits come at a relatively low area overhead.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {36},
numpages = {16},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589072,
author = {Liang, Mingyu and Fu, Wenyin and Feng, Louis and Lin, Zhongyi and Panakanti, Pavani and Zheng, Shengbao and Sridharan, Srinivas and Delimitrou, Christina},
title = {Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589072},
doi = {10.1145/3579371.3589072},
abstract = {Building large AI fleets to support the rapidly growing DL workloads is an active research topic for modern cloud providers. Generating accurate benchmarks plays an essential role in designing the fast-paced software and hardware solutions in this space. Two fundamental challenges to make this scalable are (i) workload representativeness and (ii) the ability to quickly incorporate changes to the fleet into the benchmarks.To overcome these issues, we propose Mystique, an accurate and scalable framework for production AI benchmark generation. It leverages the PyTorch execution trace (ET), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable and representative. Mystique is scalable, due to its lightweight data collection, in terms of runtime overhead and instrumentation effort. It is also adaptive because ET composability allows flexible control on benchmark creation.We evaluate our methodology on several production AI models, and show that benchmarks generated with Mystique closely resemble original AI models, both in execution time and system-level metrics. We also showcase the portability of the generated benchmarks across platforms, and demonstrate several use cases enabled by the fine-grained composability of the execution trace.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {37},
numpages = {13},
keywords = {code generation, performance cloning, benchmarking, cloud computing, artificial intelligence},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589073,
author = {Deutsch, Peter W. and Na, Weon Taek and Bourgeat, Thomas and Emer, Joel S. and Yan, Mengjia},
title = {Metior: A Comprehensive Model to Evaluate Obfuscating Side-Channel Defense Schemes},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589073},
doi = {10.1145/3579371.3589073},
abstract = {Microarchitectural side-channels enable an attacker to exfiltrate information via the observable side-effects of a victim's execution. Obfuscating mitigation schemes have recently gained in popularity for their appealing performance characteristics. These schemes, including randomized caches and DRAM traffic shapers, limit, but do not completely eliminate, side-channel leakage. An important (yet under-explored) research challenge is the quantitative study of the security effectiveness of these schemes, identifying whether these obfuscating schemes help increase the security level of a system, and if so, by how much.In this paper, we address this research challenge by presenting Metior, a comprehensive model to quantitatively evaluate the effectiveness of obfuscating side-channel mitigations. Metior offers a way to reason about the flow of information through obfuscating schemes. Metior builds upon existing information theoretic approaches, allowing for the comprehensive side-channel leakage evaluation of active attackers, real victim applications, and state-of-the-art microarchitectural obfuscation schemes. We demonstrate the use of Metior in the concrete leakage evaluation of three microarchitectural obfuscation schemes (fully-associative random replacement caches, CEASER-S, and Camouflage), identifying unintuitive leakage behaviours across all three schemes.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {38},
numpages = {16},
keywords = {leakage quantification, side-channels, hardware security},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589074,
author = {Karandikar, Sagar and Udipi, Aniruddha N. and Choi, Junsun and Whangbo, Joonho and Zhao, Jerry and Kanev, Svilen and Lim, Edwin and Alakuijala, Jyrki and Madduri, Vrishab and Shao, Yakun Sophia and Nikolic, Borivoje and Asanovic, Krste and Ranganathan, Parthasarathy},
title = {CDPU: Co-designing Compression and Decompression Processing Units for Hyperscale Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589074},
doi = {10.1145/3579371.3589074},
abstract = {General-purpose lossless data compression and decompression ("(de)compression") are used widely in hyperscale systems and are key "datacenter taxes". However, designing optimal hardware compression and decompression processing units ("CDPUs") is challenging due to the variety of algorithms deployed, input data characteristics, and evolving costs of CPU cycles, network bandwidth, and memory/storage capacities.To navigate this vast design space, we present the first large-scale data-driven analysis of (de)compression usage at a major cloud provider by profiling Google's datacenter fleet. We find that (de)compression consumes 2.9\% of fleet CPU cycles and 10--50\% of cycles in key services. Demand is also artificially limited; 95\% of bytes compressed in the fleet use less capable algorithms to reduce compute, motivating a CDPU that changes cost vs. size tradeoffs.Prior work has improved the microarchitectural state-of-the-art for CDPUs supporting various algorithms in fixed contexts. However, we find that higher-level design parameters like CDPU placement, hash table sizing, history window sizes, and more have as significant of an impact on the viability of CDPU integration, but are not well-studied. Thus, we present the first end-to-end design/evaluation framework for CDPUs, including: 1. An open-source RTL-based CDPU generator that supports many run-time and compile-time parameters. 2. Integration into an open-source RISC-V SoC for rapid performance and silicon area evaluation across CDPU placements and parameters. 3. An open-source (de)compression benchmark, HyperCompressBench, that is representative of (de)compression usage in Google's fleet.Using our framework, we perform an extensive design space exploration running HyperCompressBench. Our exploration spans a 46\texttimes{} range in CDPU speedup, 3\texttimes{} range in silicon area (for a single pipeline), and evaluates a variety of CDPU integration techniques to optimize CDPU designs for hyperscale contexts. Our final hyperscale-optimized CDPU instances are up to 10\texttimes{} to 16\texttimes{} faster than a single Xeon core, while consuming a small fraction (as little as 2.4\% to 4.7\%) of the area.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {39},
numpages = {17},
keywords = {profiling, hyperscale systems, warehouse-scale computing, hardware-acceleration, decompression, compression},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589075,
author = {McKinney, Evan and Zhou, Chao and Xia, Mingkang and Hatridge, Michael and Jones, Alex K.},
title = {Parallel Driving for Fast Quantum Computing Under Speed Limits},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589075},
doi = {10.1145/3579371.3589075},
abstract = {Increasing quantum circuit fidelity requires an efficient instruction set to minimize errors from decoherence. The choice of a two-qubit (2Q) hardware basis gate depends on a quantum modulator's native Hamiltonian interactions and applied control drives. In this paper, we propose a collaborative design approach to select the best ratio of drive parameters that determine the best basis gate for a particular modulator. This requires considering the theoretical computing power of the gate along with the practical speed limit of that gate, given the modulator drive parameters. The practical speed limit arises from the couplers' tolerance for strong driving when one or more pumps is applied, for which some combinations can result in higher overall speed limits than others. Moreover, as this 2Q basis gate is typically applied multiple times in succession, interleaved by 1Q gates applied directly to the qubits, the speed of the 1Q gates can become a limiting factor for the quantum circuit, particularly as the pulse length of the 2Q basis gate is optimized. We propose parallel-drive to drive the modulator and qubits simultaneously, allowing a richer capability of the 2Q basis gate and in some cases for this 1Q drive time to be absorbed entirely into the 2Q operation. This allows increasingly short duration 2Q gates to be more practical while mitigating a significant source of overhead in some quantum systems. On average, this approach can decrease circuit duration by 17.8\% and decrease infidelity for random 2Q gates by 10.5\% compared to the currently best reported basic 2Q gate, âiSWAP.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {40},
numpages = {13},
keywords = {weyl chamber, basis gate, transpilation},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589076,
author = {Ujjainkar, Nisarg and Leng, Jingwen and Zhu, Yuhao},
title = {ImaGen: A General Framework for Generating Memory- and Power-Efficient Image Processing Accelerators},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589076},
doi = {10.1145/3579371.3589076},
abstract = {Image processing algorithms are prime targets for hardware acceleration as they are commonly used in resource- and power-limited applications. Today's image processing accelerator designs make rigid assumptions about the algorithm structures and/or on-chip memory resources. As a result, they either have narrow applicability or result in inefficient designs.This paper presents a compiler framework that automatically generates memory- and power-efficient image processing accelerators. We allow programmers to describe generic image processing algorithms (in a domain specific language) and specify on-chip memory structures available. Our framework then formulates a constrained optimization problem that minimizes on-chip memory usage while maintaining theoretical maximum throughput. The key challenge we address is to analytically express the throughput bottleneck, on-chip memory contention, to enable a lightweight compilation. FPGA prototyping and ASIC synthesis show that, compared to existing approaches, accelerators generated by our framework reduce the on-chip memory usage and/or power consumption by double digits. ImaGen code is available at: https://github.com/horizon-research/imagen.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {41},
numpages = {13},
keywords = {compiler, synthesis, constrained optimization, image processing, line buffer, accelerator},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589077,
author = {Zhang, Jie and Huang, Hongjing and Zhu, Lingjun and Ma, Shu and Rong, Dazhong and Hou, Yijun and Sun, Mo and Gu, Chaojie and Cheng, Peng and Shi, Chao and Wang, Zeke},
title = {SmartDS: Middle-Tier-centric SmartNIC Enabling Application-aware Message Split for Disaggregated Block Storage},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589077},
doi = {10.1145/3579371.3589077},
abstract = {The widespread deployment of storage disaggregation in the cloud has facilitated flexible scaling and storage overprovisioning, allowing for high utilization of storage capacity and IOPS. Instead of utilizing remote storage protocols to access remote disks, a middle-tier is introduced between compute servers and storage servers in order to serve I/O requests from compute servers and provide computations such as compression and decompression. However, due to the need for a cloud to concurrently serve millions of VMs that require access to disaggregated storage, the middle-tier requires a massive number of servers to process network traffic between computing and storage nodes. For example, a major cloud company may deploy hundreds of thousands of high-end servers to provide such a service for its cloud storage, because the existing CPU-based middle-tier suffers from a severe issue of compute-intensive compression/decompression on high-throughput storage traffic. To address this issue, we introduce SmartDS, a middle-tier-centric SmartNIC that serves storage I/O requests with low latency and high throughput, while maintaining high flexibility and programmability. The key idea behind SmartDS is the application-aware message split (AAMS) mechanism, which allows for the processing of the message's header on the host CPU to achieve high flexibility, and the message's payload on the SmartDS. Experimental results demonstrate that SmartDS provides up to 4.3\texttimes{} more throughput than a CPU-based middle-tier and enables the linear scale-up of multiple network ports and multiple SmartNICs, thus significantly reducing cloud infrastructure costs for disaggregated block storage.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {42},
numpages = {13},
keywords = {payload/-header split, disaggregated block storage, middle tier, SmartNIC},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589078,
author = {Zhang, Shiqing and Naderan-Tahan, Mahmood and Jahre, Magnus and Eeckhout, Lieven},
title = {SAC: Sharing-Aware Caching in Multi-Chip GPUs},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589078},
doi = {10.1145/3579371.3589078},
abstract = {Bandwidth non-uniformity in multi-chip GPUs poses a major design challenge for its last-level cache (LLC) architecture. Whereas a memory-side LLC caches data from the local memory partition while being accessible by all chips, an SM-side LLC is private to a chip while caching data from all memory partitions. We find that some workloads prefer a memory-side LLC while others prefer an SM-side LLC, and this preference solely depends on which organization maximizes the effective LLC bandwidth. In contrast to prior work which optimizes bandwidth beyond the LLC, we make the observation that the effective bandwidth ahead of the LLC is critical to end-to-end application performance. We propose Sharing-Aware Caching (SAC) to adopt either a memory-side or SM-side LLC organization by dynamically reconfiguring the routing policies in the intra-chip interconnection network and LLC controllers. SAC is driven by a simple and lightweight analytical model that predicts the impact of data sharing across chips on the effective LLC bandwidth. SAC improves average performance by 76\% and 12\% (and up to 157\% and 49\%) compared to a memory-side and SM-side LLC, respectively. We demonstrate significant performance improvements across the design space and across workloads.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {43},
numpages = {13},
keywords = {multi-socket gpu, multi-chip-module (MCM) GPU, network-on-chip (NoC), data sharing, cache organization},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589079,
author = {Zhao, Kaiyang and Xue, Kaiwen and Wang, Ziqi and Schatzberg, Dan and Yang, Leon and Manousis, Antonis and Weiner, Johannes and Van Riel, Rik and Sharma, Bikash and Tang, Chunqiang and Skarlatos, Dimitrios},
title = {Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589079},
doi = {10.1145/3579371.3589079},
abstract = {The unabating growth of the memory needs of emerging datacenter applications has exacerbated the scalability bottleneck of virtual memory. However, reducing the excessive overhead of address translation will remain onerous until the physical memory contiguity predicament gets resolved. To address this problem, this paper presents Contiguitas, a novel redesign of memory management in the operating system and hardware that provides ample physical memory contiguity. We identify that the primary cause of memory fragmentation in Meta's datacenters is unmovable allocations scattered across the address space that impede large contiguity from being formed. To provide ample physical memory contiguity by design, Contiguitas first separates regular movable allocations from unmovable ones by placing them into two different continuous regions in physical memory and dynamically adjusts the boundary of the two regions based on memory demand. Drastically reducing unmovable allocations is challenging because the majority of unmovable pages cannot be moved with software alone given that access to the page cannot be blocked for a migration to take place. Furthermore, page migration is expensive as it requires a long downtime to (a) perform TLB shootdowns that scale poorly with the number of victim TLBs, and (b) copy the page. To this end, Contiguitas eliminates the primary source of unmovable allocations by introducing hardware extensions in the last-level cache to enable the transparent and efficient migration of unmovable pages even while the pages remain in use.We build the operating system component of Contiguitas into the Linux kernel and run our experiments in a production environment at Meta's datacenters. Our results show that Contiguitas's OS component successfully confines unmovable allocations, drastically reducing unmovable 2MB blocks from an average of 31\% scattered across the address space down to 7\% confined in the unmovable region, leading to significant performance gains. Specifically, we show that for three major production services, Contiguitas achieves end-to-end performance improvements of 2--9\% for partially fragmented servers, and 7--18\% for highly fragmented servers, which account for nearly a quarter of Meta's fleet. We further use full-system simulations to demonstrate the effectiveness of the hardware extensions of Contiguitas. Our evaluation shows that Contiguitas-HW enables the efficient migration of unmovable allocations, scales well with the number of victim TLBs, and does not affect application performance. We are currently in the process of upstreaming Contiguitas into Linux.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {44},
numpages = {15},
keywords = {virtual memory, memory management, operating systems, datacenters},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589080,
author = {Dutta, Sankha Baran and Naghibijouybari, Hoda and Gupta, Arjun and Abu-Ghazaleh, Nael and Marquez, Andres and Barker, Kevin},
title = {Spy in the GPU-box: Covert and Side Channel Attacks on Multi-GPU Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589080},
doi = {10.1145/3579371.3589080},
abstract = {The deep learning revolution has been enabled in large part by GPUs, and more recently accelerators, which make it possible to carry out computationally demanding training and inference in acceptable times. As the size of machine learning networks and workloads continues to increase, multi-GPU machines have emerged as an important platform offered on High Performance Computing and cloud data centers. Since these machines are shared among multiple users, it becomes increasingly important to protect applications against potential attacks. In this paper, we explore the vulnerability of Nvidia's DGX multi-GPU machines to covert and side channel attacks. These machines consist of a number of discrete GPUs that are interconnected through a combination of custom interconnect (NVLink) and PCIe connections. We reverse engineer the interconnected cache hierarchy and show that it is possible for an attacker on one GPU to cause contention on the L2 cache of another GPU. We use this observation to first develop a covert channel attack across two GPUs, achieving the best bandwidth of around 4 MB/s. We also develop a prime and probe attack on a remote GPU allowing an attacker to recover the cache access pattern of another workload. This access pattern can be used in any number of side channel attacks: we demonstrate a proof of concept attack that fingerprints the application running on the remote GPU, with high accuracy. We also develop a proof of concept attack to extract hyperparameters of a machine learning workload. Our work establishes for the first time the vulnerability of these machines to microarchitectural attacks and can guide future research to improve their security.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {45},
numpages = {13},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589081,
author = {Kong, Xiangyu and Huang, Yi and Zhu, Jianfeng and Man, Xingchen and Liu, Yang and Feng, Chunyang and Gou, Pengfei and Tang, Minggui and Wei, Shaojun and Liu, Leibo},
title = {MapZero: Mapping for Coarse-grained Reconfigurable Architectures with Reinforcement Learning and Monte-Carlo Tree Search},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589081},
doi = {10.1145/3579371.3589081},
abstract = {Coarse-grained reconfigurable architecture (CGRA) has become a promising candidate for data-intensive computing due to its flexibility and high energy efficiency. CGRA compilers map data flow graphs (DFGs) extracted from applications onto CGRAs, playing a fundamental role in fully exploiting hardware resources for acceleration. Yet the existing compilers are time-demanding and cannot guarantee optimal results due to the traversal search of enormous search spaces brought about by the spatio-temporal flexibility of CGRA structures and the complexity of DFGs. Inspired by the amazing progress in reinforcement learning (RL) and Monte-Carlo tree search (MCTS) for real-world problems, we consider constructing a compiler that can learn from past experiences and comprehensively understand the target DFG and CGRA.In this paper, we propose an architecture-aware compiler for CGRAs based on RL and MCTS, called MapZero - a framework to automatically extract the characteristics of DFG and CGRA hardware and map operations onto varied CGRA fabrics. We apply Graph Attention Network to generate an adaptive embedding for DFGs and also model the functionality and interconnection status of the CGRA, aiming at training an RL agent to perform placement and routing intelligently.Experimental results show that MapZero can generate superior-quality mappings and reduce compilation time hundreds of times compared to state-of-the-art methods. MapZero can find high-quality mappings very quickly when the feasible solution space is rather small and all other compilers fail. We also demonstrate the scalability and broad applicability of our framework.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {46},
numpages = {14},
keywords = {reinforcement learning, graph neural network, compiler, coarse-grained reconfigurable architecture},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589082,
author = {Gonzalez, Abraham and Kolli, Aasheesh and Khan, Samira and Liu, Sihang and Dadu, Vidushi and Karandikar, Sagar and Chang, Jichuan and Asanovic, Krste and Ranganathan, Parthasarathy},
title = {Profiling Hyperscale Big Data Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589082},
doi = {10.1145/3579371.3589082},
abstract = {Computing demand continues to grow exponentially, largely driven by "big data" processing on hyperscale data stores. At the same time, the slowdown in Moore's law is leading the industry to embrace custom computing in large-scale systems. Taken together, these trends motivate the need to characterize live production traffic on these large data processing platforms and understand the opportunity of acceleration at scale.This paper addresses this key need. We characterize three important production distributed database and data analytics platforms at Google to identify key hardware acceleration opportunities and perform a comprehensive limits study to understand the trade-offs among various hardware acceleration strategies.We observe that hyperscale data processing platforms spend significant time on distributed storage and other remote work across distributed workers. Therefore, optimizing storage and remote work in addition to compute acceleration is critical for these platforms. We present a detailed breakdown of the compute-intensive functions in these platforms and identify dominant key data operations related to datacenter and systems taxes. We observe that no single accelerator can provide a significant benefit but collectively, a sea of accelerators, can accelerate many of these smaller platform-specific functions. We demonstrate the potential gains of the sea of accelerators proposal in a limits study and analytical model. We perform a comprehensive study to understand the trade-offs between accelerator location (on-chip/off-chip) and invocation model (synchronous/asynchronous). We propose and evaluate a chained accelerator execution model where identified compute-intensive functions are accelerated and pipelined to avoid invocation from the core, achieving a 3x improvement over the baseline system while nearly matching identical performance to an ideal fully asynchronous execution model.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {47},
numpages = {16},
keywords = {accelerator-chaining, accelerators, profiling, warehouse-scale computing, cloud computing, hyperscale computing, databases, data analytics},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589083,
author = {Li, Jiajun and Zhang, Yuxuan and Zheng, Hao and Wang, Ke},
title = {FDMAX: An Elastic Accelerator Architecture for Solving Partial Differential Equations},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589083},
doi = {10.1145/3579371.3589083},
abstract = {Partial Differential Equations (PDEs) are widely employed to describe natural phenomena in many science and engineering fields. Many PDEs do not have analytical solutions, hence, numerical methods have become prevalent for approximating PDE solutions. The most widely used numerical method is the Finite Difference Method (FDM), which requires fine grids and high-precision numerical iterations that are both compute- and memory-intensive. PDE-solving accelerators have been proposed in the literature, however, they usually focus on specific types of PDEs with rigid grid sizes which limits their broader applicability. Besides, they rarely provided insight into the optimizations of parallel computing and data accesses for solving PDEs, which hinders further improvements in performance and energy efficiency.This paper presents FDMAX, an elastic accelerator to efficiently support FDM for different types of PDEs with any grid size. FDMAX employs a customized Processing Element (PE) array architecture that maximizes data reuse with minimized interconnection overhead. The PE array can be reconfigured to break into a set of subarrays to adapt to different grid sizes for optimal efficiency. Moreover, the PE array exploits computation and data reuse for increased performance and energy efficiency, and is reconfigurable to support a wide range of PDEs such as elliptic, parabolic, and hyperbolic equations. Evaluated on four well-known PDEs, our simulation results show that FDMAX achieves on average 1189\texttimes{} speedup with 1123\texttimes{} energy reduction over Intel Xeon CPU, and 4.9\texttimes{} speedup with 6.3\texttimes{} energy reduction over NVIDIA RTX3090 GPU, and 2.9\texttimes{} speedup over Alrescha, the state-of-the-art PDE-solving accelerator.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {48},
numpages = {12},
keywords = {dataflow architecture, accelerator, partial differential equations},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589084,
author = {Wang, Dong Kai and Lou, Jiaqi and Jin, Naiyin and Mascarenhas, Edwin and Mahapatra, Rohan and Kinzer, Sean and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kim, Nam Sung},
title = {MESA: Microarchitecture Extensions for Spatial Architecture Generation},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589084},
doi = {10.1145/3579371.3589084},
abstract = {Modern heterogeneous CPUs incorporate hardware accelerators to enable domain-specialized execution and achieve improved efficiency. A well-known class among them, spatial accelerators, are designed with reconfigurability to accelerate a wide range of compute-heavy and data-parallel applications. Unlike CPU cores, however, they tend to require specialized compilers and software stacks, libraries, or languages to operate and cannot be utilized with ease by all applications. As a result, the accelerator's large pool of compute and memory resources sit wastefully idle when it is not explicitly programmed. Our goal is to dismantle this CPU-accelerator barrier by monitoring CPU threads for acceleration opportunities during execution and, if viable, dynamically reconfigure the accelerator to allow transparent offloading. We develop MESA (Microarchitecture Extensions for Spatial Architecture Generation), a hardware block on the CPU that translates machine code to build an accelerator configuration specialized for the running program. While such a dynamic translation/reconfiguration approach is challenging, it has a key advantage over ahead-of-time compilers: access to runtime information, revealing not only dynamic dependencies but also performance characteristics. MESA maintains a real-time performance model of the program mapped on the accelerator in the form of a spatial dataflow graph with nodes weighted by operation latency and edges weighted by data transfer latency. Features of this dataflow graph are continuously updated with runtime information captured by performance counters, allowing a feedback loop of optimization, reconfiguration, and acceleration. This performance model allows MESA to identify the accelerator's critical paths and pinpoint its bottlenecks, upon which we implement in hardware a data-driven instruction mapping algorithm that locally minimizes latency. Backed by a synthesized RTL implementation, we evaluate the feasibility of our microarchitectural solution with different accelerator configurations. Across the Rodinia benchmarks, results demonstrate an average 1.3\texttimes{} speedup in performance and 1.8\texttimes{} gain in energy efficiency against a multicore CPU baseline.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {49},
numpages = {14},
keywords = {microarchitecture, binary translation, spatial accelerator},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589085,
author = {Mubarik, Muhammad Husnain and Kanungo, Ramakrishna and Zirr, Tobias and Kumar, Rakesh},
title = {Hardware Acceleration of Neural Graphics},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589085},
doi = {10.1145/3579371.3589085},
abstract = {Rendering and inverse rendering techniques have recently attained powerful new capabilities and building blocks in the form of neural representations (NR), with derived rendering techniques quickly becoming indispensable tools next to classic computer graphics algorithms, covering a wide range of functions throughout the full pipeline from sensing to pixels. NRs have recently been used to directly learn the geometric and appearance properties of scenes that were previously hard to capture, and to re-synthesize photo realistic imagery based on this information, thereby promising simplifications and replacements for several complex traditional computer graphics problems and algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (graphics based on NRs) need hardware support? We studied four representative neural graphics applications (NeRF, NSDF, NVR, and GIA) showing that, if we want to render 4k resolution frames at 60 frames per second (FPS) there is a gap of ~ 1.51\texttimes{} to 55.50\texttimes{} in the desired performance on current GPUs. For AR and VR applications, there is an even larger gap of ~ 2--4 orders of magnitude (OOM) between the desired performance and the required system power. We identify that the input encoding and the multi-layer perceptron kernels are the performance bottlenecks, consuming 72.37\%, 60.0\% and 59.96\% of application time for multi resolution hashgrid encoding, multi resolution densegrid encoding and low resolution densegrid encoding, respectively. We propose a neural graphics processing cluster (NGPC) - a scalable and flexible hardware architecture that directly accelerates the input encoding and multi-layer perceptron kernels through dedicated engines and supports a wide range of neural graphics applications. To achieve good overall application level performance improvements, we also accelerate the rest of the kernels by fusion into a single kernel, leading to a ~ 9.94\texttimes{} speedup compared to previous optimized implementations [17] which is sufficient to remove this performance bottleneck. Our results show that, NGPC gives up to 58.36\texttimes{} end-to-end application-level performance improvement, for multi resolution hashgrid encoding on average across the four neural graphics applications, the performance benefits are 12.94\texttimes{}, 20.85\texttimes{}, 33.73\texttimes{} and 39.04\texttimes{} for the hardware scaling factor of 8, 16, 32 and 64, respectively. Our results show that with multi resolution hashgrid encoding, NGPC enables the rendering of 4k Ultra HD resolution frames at 30 FPS for NeRF and 8k Ultra HD resolution frames at 120 FPS for all our other neural graphics applications.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {50},
numpages = {12},
keywords = {neural graphics, hardware accelerators, instant-ngp, NeRF},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589086,
author = {Wu, Yibo and Zhu, Jianfeng and Wei, Wenrui and Chen, Longlong and Wang, Liang and Wei, Shaojun and Liu, Leibo},
title = {Shogun: A Task Scheduling Framework for Graph Mining Accelerators},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589086},
doi = {10.1145/3579371.3589086},
abstract = {Graph mining is an emerging application of great importance to big data analytic. Graph mining algorithms are bottle-necked by both computation complexity and memory access, hence necessitating specialized hardware accelerators to improve the processing efficiency. Current accelerators have extensively exploited task-level and fine-grained parallelism in these algorithms. However, their task scheduling still has room for optimization. They use either breadth-first search, depth-first search or a combination of both, leading to either poor intermediate data locality, low parallelism or inter-depth barriers.In this paper, two key insights on graph mining scheduling are gained, inspired by which a novel task scheduling framework named Shogun is proposed. First, task execution can be out-of-order to eliminate unnecessary barriers and improve PE utilization rate. Second, sacrificing intermediate data locality causes little harm to performance, when a locality monitoring mechanism is adopted to avoid severe locality loss. Hence, Shogun enables adaptive locality-aware out-of-order task scheduling by deploying a task tree to decouple the task generation and execution pipeline stages. Moreover, based on the flexible scheduling design, Shogun further develops accelerator optimizations including task tree splitting for load balance, and search tree merging to explore multiple search trees in parallel on one PE. Experimental results show that Shogun improves performance of a state-of-the-art accelerator by 63\% on average with an area overhead of approximately 4\%.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {51},
numpages = {15},
keywords = {locality, parallelism, scheduling, graph mining},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589087,
author = {Gupta, Siddharth and Li, Yuanlong and Kang, Qingxuan and Bhattacharjee, Abhishek and Falsafi, Babak and Oh, Yunho and Payer, Mathias},
title = {Imprecise Store Exceptions},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589087},
doi = {10.1145/3579371.3589087},
abstract = {Precise exceptions are a cornerstone of modern computing as they provide the abstraction of sequential instruction execution to programmers while accommodating microarchitectural optimizations. However, increasing compute capabilities in deep memory hierarchies (e.g., software event handlers, programmable accelerators) expose long exception detection latencies that forgo precise exception semantics for retired stores awaiting completion. Unfortunately, well-known post-retirement speculation mechanisms to tolerate these latencies require excessively large microarchitectural structures per core. This paper rethinks the role of architecture and OS in supporting precise exceptions. We show that instead of forcing the architecture to support precise exceptions transparently in all cases, it is preferable to employ hardware-software co-design to handle imprecise store exceptions efficiently. We develop formalism to prove that this approach complies with underlying memory consistency models and design a RISC-V prototype that passes all litmus tests, demonstrating its efficacy.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {52},
numpages = {15},
keywords = {memory consistency, exception handling, memory hierarchies},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589088,
author = {Kvalsvik, Amund Bergland and Aimoniotis, Pavlos and Kaxiras, Stefanos and Sj\"{a}lander, Magnus},
title = {Doppelganger Loads: A Safe, Complexity-Effective Optimization for Secure Speculation Schemes},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589088},
doi = {10.1145/3579371.3589088},
abstract = {Speculative side-channel attacks have forced computer architects to rethink speculative execution. Effectively preventing microarchitectural state from leaking sensitive information will be a key requirement in future processor design.An important limitation of many secure speculation schemes is a reduction in the available memory parallelism, as unsafe loads (depending on the particular scheme) are blocked, as they might potentially leak information. Our contribution is to show that it is possible to recover some of this lost memory parallelism, by safely predicting the addresses of these loads in a threat-model transparent way, i.e., without worsening the security guarantees of the underlying secure scheme. To demonstrate the generality of the approach, we apply it to three different secure speculation schemes: Non-speculative Data Access (NDA), Speculative Taint Tracking (STT), and Delay-on-Miss (DoM).An address predictor is trained on non-speculative data, and can afterwards predict the addresses of unsafe slow-to-issue loads, preloading the target registers with speculative values, that can be released faster on correct predictions than starting the entire load process. This new perspective on speculative execution encompasses all loads, and gives speedups, separately from prefetching.We call the address-predicted counterparts of loads Doppelganger Loads. They give notable performance improvements for the three secure speculation schemes we evaluate, NDA, STT, and DoM. The Doppelganger Loads reduce the geometric mean slowdown by 42\%, 48\%, and 30\% respectively, as compared to an unsafe baseline, for a wide variety of SPEC2006 and SPEC2017 benchmarks. Furthermore, Doppelganger Loads can be efficiently implemented with only minor core modifications, reusing existing resources such as a stride prefetcher, and most importantly, requiring no changes to the memory hierarchy outside the core.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {53},
numpages = {13},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589089,
author = {Ma, Tianrui and Boloor, Adith Jagadish and Yang, Xiangxing and Cao, Weidong and Williams, Patrick and Sun, Nan and Chakrabarti, Ayan and Zhang, Xuan},
title = {LeCA: In-Sensor Learned Compressive Acquisition for Efficient Machine Vision on the Edge},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589089},
doi = {10.1145/3579371.3589089},
abstract = {With the rapid advances of deep learning-based computer vision (CV) technology, digital images are increasingly consumed, not by humans, but by downstream CV algorithms. However, capturing high-fidelity and high-resolution images is energy-intensive. It not only dominates the energy consumption of the sensor itself (i.e. in low-power edge devices), but also contributes to significant memory burdens and performance bottlenecks in the later storage, processing, and communication stages. In this paper, we systematically explore a new paradigm of in-sensor processing, termed "learned compressive acquisition" (LeCA). Targeting machine vision applications on the edge, the LeCA framework exploits the joint learning of a sensor autoencoder structure with the downstream CV algorithms to effectively compress the original image into low-dimensional features with adaptive bit depth. We employ column-parallel analog-domain processing directly inside the image sensor to perform the compressive encoding of the raw image, resulting in meaningful hardware savings, and energy efficiency improvements. Evaluated within a modern machine vision processing pipeline, LeCA achieves 4\texttimes{}, 6\texttimes{}, and 8\texttimes{} compression ratios prior to any digital compression, with minimal accuracy loss of 0.97\%, 0.98\%, and 2.01\% on ImageNet, outperforming existing methods. Compared with the conventional full-resolution image sensor and the state-of-the-art compressive sensing sensor, our LeCA sensor is 6.3\texttimes{} and 2.2\texttimes{} more energy-efficient while reaching a 2\texttimes{} higher compression ratio.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {54},
numpages = {14},
keywords = {autoencoder, image compression, CMOS image sensor},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589090,
author = {Boo, Junehyuk and Chung, Yujin and Baek, Eunjin and Na, Seongmin and Kim, Changsu and Kim, Jangwoo},
title = {F4T: A Fast and Flexible FPGA-based Full-stack TCP Acceleration Framework},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589090},
doi = {10.1145/3579371.3589090},
abstract = {As complex workloads that run on many servers are pursuing higher networking throughput, more CPU cycles are consumed to support the TCP stack. To mitigate the high CPU burden from executing the compute-intensive TCP, prior works have proposed to offload TCP processing to the embedded processors, ASICs, or FPGAs in network devices. However, none of the approaches satisfy all of the critical requirements of TCP simultaneously, which are high performance, many connections, and high flexibility. Embedded processors do not provide enough performance to fully offload the TCP stack, while ASICs fail to provide enough flexibility. Meanwhile, existing FPGA-based TCP accelerators either fail to provide high performance or give up some of the critical features and requirements to achieve high performance due to their inefficient processing architecture.This paper proposes F4T, a fast and flexible FPGA-based full-stack TCP acceleration framework to effectively save CPU cycles by achieving high performance, supporting many connections, and providing high flexibility. By analyzing the existing FPGA-based TCP accelerators, we identify that the key performance bottleneck arises from two factors: (1) inefficient processing of stateful operations that incurs stalls and (2) lack of TCP state management necessary for utilizing multiple memory modules. To resolve the identified bottlenecks, we design F4T to process stateful operations back-to-back without stalls and efficiently manage the TCP states among multiple memory modules. F4T also provides a full software-hardware stack that allows applications to easily utilize F4T without any modifications. Our evaluation results show that F4T saturates the 100Gbps link bandwidth with only two CPU cores while supporting many connections and providing high flexibility. F4T saves 64\% CPU cycles and provides 2.8\texttimes{} CPU cycles to applications compared to Linux's TCP stack when running the Nginx web server.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {55},
numpages = {13},
keywords = {high performance networking, full-stack TCP acceleration, FPGA accelerator, FPGA prototyping, TCP offload engine (TOE), transmission control protocol (TCP)},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589091,
author = {Chen, Dan and He, Haiheng and Jin, Hai and Zheng, Long and Huang, Yu and Shen, Xinyang and Liao, Xiaofei},
title = {MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589091},
doi = {10.1145/3579371.3589091},
abstract = {Heterogeneous graph neural networks (HGNNs) based on metapath exhibit powerful capturing of rich structural and semantic information in the heterogeneous graph. HGNNs are highly memory-bound and thus can be accelerated by near-memory processing. However, they also suffer from significant memory footprint (due to storing metapath instances as intermediate data) and severe redundant computation (when vertex features are aggregated among metapath instances). To address these issues, this paper proposes MetaNMP, the first DIMM-based near-memory processing HGNNs accelerator with reduced memory footprint and high performance. Specifically, we first propose a cartesian-like product paradigm to generate all metapath instances on the fly for heterogeneous graphs. In this way, metapath instances no longer need to be stored as intermediate data, avoiding significant memory consumption. We then design a data flow for aggregating vertex features on metapath instances, which aggregates vertex features along the direction of the metapath instances dispersed from the starting vertex to exploit shareable aggregation computations, eliminating most of the redundant computations. Finally, we integrate specialized hardware units in DIMM to accelerate HGNNs with near-memory processing, and introduce a broadcast mechanism for edge data and vertex features to mitigate the inter-DIMM communication. Our evaluation shows that MetaNMP achieves the memory space reduction of 51.9\% on average and the performance improvement by 415.18\texttimes{} compared to NVIDIA Tesla V100 GPU.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {56},
numpages = {13},
keywords = {near-memory processing, cartesian product, heterogeneous graph neural networks},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589092,
author = {Shah, Deval and Yang, Ningfeng and Aamodt, Tor M.},
title = {Energy-Efficient Realtime Motion Planning},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589092},
doi = {10.1145/3579371.3589092},
abstract = {Motion planning is a fundamental problem in autonomous robotics with real-time and low-energy requirements for safe navigation through a dynamic environment. More than 90\% of computation time in motion planning is spent on collision detection between the robot and the environment. Several motion planning approaches, such as deep learning-based motion planning, have shown significant improvements in motion planning quality and runtime with ample parallelism available in collision detection. However, naive parallelization of collision detection queries significantly increases computation compared to sequential execution. In this work, we investigate the sources of redundant computations in coarsegrained (inter-collision detection) and fine-grained (intracollision detection) parallelism. We find that the physical spatial locality of obstacles results in redundant computation in coarse-grained parallelism. We further show that the primary sources of redundant computation in fine-grained parallelism are easy cases where objects are far apart or significantly overlapping. Based on these insights, we propose MPAccel to improve the energy efficiency of parallelization in motion planning. MPAccel consists of SAS, a Spatially Aware Scheduler for coarse-grained parallelism, and CECDUs, Cascaded Early-exit Collision Detection Units for fine-grained parallelism. SAS results in 7\texttimes{} speedup using 8\texttimes{} parallelization with 6\% increase in the computation compared to 3.7\texttimes{} speedup with 83\% increase in computation for naive parallelization. CECDU can perform collision detection in 46 -- 154 cycles for a robot with 6 degrees of freedom. We evaluate MPAccel to execute a state-of-the-art learning-based motion planning algorithm. Our simulations suggest MPAccel can achieve real-time motion planning for a robot with 7 degrees of freedom in 0.014ms-0.49ms with an average latency of 0.099ms compared to 1.42ms on a CPU-GPU system.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {57},
numpages = {17},
keywords = {collision detection, motion planning, hardware acceleration, robotics},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589093,
author = {Li, Siqi and Tu, Fengbin and Liu, Liu and Lin, Jilan and Wang, Zheng and Kang, Yangwook and Ding, Yufei and Xie, Yuan},
title = {ECSSD: Hardware/Data Layout Co-Designed In-Storage-Computing Architecture for Extreme Classification},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589093},
doi = {10.1145/3579371.3589093},
abstract = {With the rapid growth of classification scale in deep learning systems, the final classification layer becomes extreme classification with a memory footprint exceeding the main memory capacity of the CPU or GPU. The emerging in-storage-computing technique offers an opportunity on account of the fact that SSD has enough storage capacity for the parameters of extreme classification. However, the limited performance of naive in-storage-computing schemes is insufficient to support the heavy workload of extreme classification.To this end, we propose ECSSD, the first hardware/data layout co-designed in-storage-computing architecture for extreme classification, based on the approximate screening algorithm. We propose an alignment-free floating-point MAC circuit technique to improve the computational ability under the limited area budget of in-storage-computing schemes so that the computational ability can match SSD's high internal bandwidth. We present a heterogeneous data layout design for the 4/32-bit weight data in the approximate screening algorithm to avoid data transfer interference and further utilize the internal DRAM bandwidth of SSD. Moreover, we propose a learning-based adaptive interleaving framework to balance the access workload in each flash channel and improve channel-level bandwidth utilization. Putting them together, our ECSSD achieves 3.24--49.87x performance improvements compared with state-of-the-art baselines.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {58},
numpages = {14},
keywords = {extreme classification, in-storage-computing architecture},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589094,
author = {Yang, Yuheng and Bourgeat, Thomas and Lau, Stella and Yan, Mengjia},
title = {Pensieve: Microarchitectural Modeling for Security Evaluation},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589094},
doi = {10.1145/3579371.3589094},
abstract = {Traditional modeling approaches in computer architecture aim to obtain an accurate estimation of performance, area, and energy of a processor design. With the advent of speculative execution attacks and their security concerns, these traditional modeling techniques fall short when used for security evaluation of defenses against these attacks.This paper presents Pensieve, a security evaluation framework targeting early-stage microarchitectural defenses against speculative execution attacks. At the core, it introduces a modeling discipline for systematically studying early-stage defenses. This discipline allows us to cover a space of designs that are functionally equivalent while precisely capturing timing variations due to resource contention and microarchitectural optimizations. We implement a model checking framework to automatically find vulnerabilities in designs. We use Pensieve to evaluate a series of state-of-the-art invisible speculation defense schemes, including Delay-on-Miss, InvisiSpec, and GhostMinion, against a formally defined security property, speculative non-interference. Pensieve finds Spectre-like attacks in all those defenses, including a new speculative interference attack variant that breaks GhostMinion, one of the latest defenses.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {59},
numpages = {15},
keywords = {uninterpreted function, model checking, microarchitectural model, speculative execution attacks, hardware security},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589095,
author = {Mandava, Meghna and Reckamp, Paul and Chen, Deming},
title = {Nimblock: Scheduling for Fine-grained FPGA Sharing through Virtualization},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589095},
doi = {10.1145/3579371.3589095},
abstract = {As FPGAs become ubiquitous compute platforms, existing research has focused on enabling virtualization features to facilitate finegrained FPGA sharing. We employ an overlay architecture which enables arbitrary, independent user logic to share portions of a single FPGA by dividing the FPGA into independently reconfigurable slots. We then explore scheduling possibilities to effectively time- and space-multiplex the virtualized FPGA by introducing Nimblock. The Nimblock scheduling algorithm balances application priorities and performance degradation to improve response time and reduce deadline violations. Unlike other algorithms, Nimblock explores preemption as a scheduling parameter to dynamically change resource allocations, and automatically allocates resources to enable suitable parallelism for an application without additional user input. In our exploration, we evaluate five scheduling algorithms: a baseline, three existing algorithms, and our novel Nimblock algorithm. We demonstrate system feasibility by realizing the complete system on a Xilinx ZCU106 FPGA and evaluating on a set of real-world benchmarks. In our results, we achieve up to 5.7\texttimes{} lower average response times when compared to a no-sharing and no-virtualization scheduling algorithm and up to 2.1\texttimes{} average response time improvement over competitive scheduling algorithms that support sharing within our virtualization environment. We additionally demonstrate up to 49\% fewer deadline violations and up to 2.6\texttimes{} lower tail response times when compared to other high-performance algorithms.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {60},
numpages = {13},
keywords = {overlay architectures, real-time scheduling, reconfigurable computing, virtualization},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589096,
author = {Kim, Jiho and Jung, Myoungsoo and Kim, John},
title = {Decoupled SSD: Rethinking SSD Architecture through Network-based Flash Controllers},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589096},
doi = {10.1145/3579371.3589096},
abstract = {Modern NAND Flash memory-based Solid State Drives (SSDs) are designed to provide high-bandwidth for I/O requests through high-speed NVMe interface and increased internal flash memory bandwidth. In addition to providing high performance for incoming I/O requests, the flash translation layer (FTL) also handles other flash memory management processes including garbage collection that can negatively impact I/O performance. In this work, we address how the sharing of system resources (e.g., system-bus and DRAM) for I/O requests and garbage collection can cause interference and performance degradation. In particular, we propose to rethink SSD architecture through a Decoupled SSD (dSSD) system that decouples the front-end (i.e. cores, system-bus, DRAM) with the back-end (i.e. flash memory). A flash-controller network-on-chip (fNoC) that interconnects the flash controllers together is introduced to enable decoupling of the I/O path and garbage collection path to improve performance and reliability. dSSD enables advanced commands such as copyback command to be exploited for efficient garbage collection and we propose to extend copyback command with global copyback through the fNoC. To improve reliability, we propose to recycle superblocks through superblock recycle table within the flash controller. Without any modification to the FTL, a hardware-based offloading mechanism within the flash controller of the dSSD is proposed to dynamically re-organize a superblock. Our evaluations show that decoupled SSD results in up to 42.7\% I/O bandwidth improvement and 63.8\% GC performance improvement, while achieving approximately 31.4\texttimes{} improvement in tail-latency on average. Dynamic superblock management through the dSSD results in approximately 23\% improvement in lifetime with minimal impact on performance and cost.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {61},
numpages = {13},
keywords = {superblocks, on-chip network, garbage collection, flash controller, solid-state drives},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589097,
author = {Nagendra, Nayana Prasad and Godala, Bhargav Reddy and Chaturvedi, Ishita and Patel, Atmn and Kanev, Svilen and Moseley, Tipp and Stark, Jared and Pokam, Gilles A. and Campanoni, Simone and August, David I.},
title = {EMISSARY: Enhanced Miss Awareness Replacement Policy for L2 Instruction Caching},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589097},
doi = {10.1145/3579371.3589097},
abstract = {For decades, architects have designed cache replacement policies to reduce cache misses. Since not all cache misses affect processor performance equally, researchers have also proposed cache replacement policies focused on reducing the total miss cost rather than the total miss count. However, all prior cost-aware replacement policies have been proposed specifically for data caching and are either inappropriate or unnecessarily complex for instruction caching. This paper presents EMISSARY, the first cost-aware cache replacement family of policies specifically designed for instruction caching. Observing that modern architectures entirely tolerate many instruction cache misses, EMISSARY resists evicting those cache lines whose misses cause costly decode starvations. In the context of a modern processor with fetch-directed instruction prefetching and other aggressive front-end features, EMISSARY applied to L2 cache instructions delivers an impressive 3.24\% geomean speedup (up to 23.7\%) and a geomean energy savings of 2.1\% (up to 17.7\%) when evaluated on widely used server applications with large code footprints. This speedup is 21.6\% of the total speedup obtained by an unrealizable L2 cache with a zero-cycle miss latency for all capacity and conflict instruction misses.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {62},
numpages = {13},
keywords = {instruction caching, cost-aware replacement policy, cache replacement policy, cache microarchitecture},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589098,
author = {Choi, Jongouk and Zeng, Jianping and Lee, Dongyoon and Min, Changwoo and Jung, Changhee},
title = {Write-Light Cache for Energy Harvesting Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589098},
doi = {10.1145/3579371.3589098},
abstract = {Energy harvesting system has huge potential to enable battery-less Internet of Things (IoT) services. However, it has been designed without a cache due to the difficulty of crash consistency guarantee, limiting its performance. This paper introduces Write-Light Cache (WL-Cache), a specialized cache architecture with a new write policy for energy harvesting systems. WL-Cache combines benefits of a write-back cache and a write-through cache while avoiding their downsides. Unlike a write-through cache, WL-Cache does not access a non-volatile main memory (NVM) at every store but it holds dirty cache lines in a cache to exploit locality, saving energy and improving performance. Unlike a write-back cache, WL-Cache limits the number of dirty lines in a cache. When power is about to be cut off, WL-Cache flushes the bounded set of dirty lines to NVM in a failure-atomic manner by leveraging a just-in-time (JIT) checkpointing mechanism to achieve crash consistency across power failure. For optimization, WL-Cache interacts with a run-time system that estimates the quality of energy source during each power-on period, and adaptively reconfigures the possible number of dirty cache lines at boot time. Our experiments demonstrate that WL-Cache reduces hardware complexity and provides a significant speedup over the state-of-the-art volatile cache design with non-volatile backup. For two representative power outage traces, WL-Cache achieves 1.35x and 1.44x average speedups, respectively, across 23 benchmarks used in prior work.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {63},
numpages = {13},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589099,
author = {Nikiforov, Dima and Dong, Shengjun Chris and Zhang, Chengyi Lux and Kim, Seah and Nikolic, Borivoje and Shao, Yakun Sophia},
title = {RoS\'{E}: A Hardware-Software Co-Simulation Infrastructure Enabling Pre-Silicon Full-Stack Robotics SoC Evaluation},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589099},
doi = {10.1145/3579371.3589099},
abstract = {Robotic systems, such as autonomous unmanned aerial vehicles (UAVs) and self-driving cars, have been widely deployed in many scenarios and have the potential to revolutionize the future generation of computing. To improve the performance and energy efficiency of robotic platforms, significant research efforts are being devoted to developing hardware accelerators for workloads that form bottlenecks in the robotics software pipeline. Although domain-specific accelerators can offer improved efficiency over general-purpose processors on isolated robotics benchmarks, system-level constraints such as data movement and contention over shared resources can significantly impact the achievable end-to-end acceleration. In addition, the closed-loop nature of robotic systems, where there is a tight interaction across different deployed environments, software stacks, and hardware architecture, further exacerbates the difficulties of evaluating robotics SoCs.To address this limitation, we develop RoS\'{E}, an open-source, hardware-software co-simulation infrastructure for full-stack, pre-silicon hardware-in-the-loop evaluation of robotics SoCs, together with the full software stack and realistic environments created to support robotics workloads. RoS\'{E} captures the complex interactions across hardware, algorithm, and environment, enabling new architectural research directions in hardware-software co-design for robotic systems.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {64},
numpages = {15},
keywords = {hardware-software co-design, simulation, robotics},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589100,
author = {Yu, Jiyong and Jaeger, Trent and Fletcher, Christopher Wardlaw},
title = {All Your PC Are Belong to Us: Exploiting Non-control-Transfer Instruction BTB Updates for Dynamic PC Extraction},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589100},
doi = {10.1145/3579371.3589100},
abstract = {Leaking a program's instruction address (PC) pattern, completely and precisely, has long been a sought-after capability for microarchitectural side-channel attackers. Case in point, such a primitive would be sufficient to construct powerful control-flow leakage attacks (inferring program secrets impacting control flow) that defeat existing control-flow leakage mitigations, or even reverse-engineer private binaries through PC-trace granular fingerprinting. However, current side-channel attack techniques only capture PCs at a coarse granularity or for only specific instruction types.In this paper, we propose the first micro-architectural side-channel attack that is capable of directly observing the exact PCs of arbitrary victim dynamic instructions---i.e., even the PCs of non-control-transfer instructions and even if the program code is private. Our attack exploits several previously overlooked characteristics in modern Intel Branch Target Buffers (BTBs). The core observation is perhaps counter-intuitive: despite being a structure related to control-flow prediction, the BTB incurs observable state changes after the execution of potentially any instruction, not just control-transfer instructions.Through reverse-engineering and analyzing said BTB vulnerabilities, we design and implement an attack framework named NightVision. We demonstrate how NightVision is capable of efficiently and accurately identifying a subset, or the entirety, of a victim program's dynamic PC trace (depending on the attacker's capabilities). We show how NightVision enables a new control-flow attack that bypasses prior defenses. Additionally, we show that when combined with code fingerprinting techniques, NightVision enables reverse-engineering of private programs.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {65},
numpages = {14},
keywords = {intel SGX, function fingerprinting, code privacy, branch target buffer, hardware security, side-channel attack},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589101,
author = {Liu, Haifeng and Zheng, Long and Huang, Yu and Liu, Chaoqiang and Ye, Xiangyu and Yuan, Jingrui and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {Accelerating Personalized Recommendation with Cross-level Near-Memory Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589101},
doi = {10.1145/3579371.3589101},
abstract = {The memory-intensive embedding layers of the personalized recommendation systems are the performance bottleneck as they demand large memory bandwidth and exhibit irregular and sparse memory access patterns. Recent studies propose near memory processing (NMP) to accelerate memory-bound embedding operations. However, due to the load imbalance caused by the skewed access frequency of the embedding data, existing NMP solutions that exploit fine-grained memory parallelism fail to translate the increasingly massive internal bandwidth to performance improvements, leading to resource underutilization and hardware overhead.We propose an efficient yet practical fine-grained NMP accelerator for embedding operations. We architect ReCross, a cross-level NMP architecture that exploits rank, bank-group, and subarray-level memory parallelism in a unified DIMM-based memory system by supporting rank, bank-group, and bank-level NMP to accommodate various bandwidth requirements of embedding data. In addition, we present a novel embedding partitioning technique to quantify the bandwidth requirements of embedding tables and allocate them to appropriate NMP levels. ReCross innovatively collaborates the data and architecture characteristics for the NMP embedding layer acceleration, achieving high resource utilization and performance. Our evaluation shows that ReCross outperforms a state-of-the-art bank-group level NMP solution, TRiM-G, by 2.5\texttimes{} with nearly the same area overhead and bank-level NMP solution, TRiM-B, by 1.8\texttimes{} with an area overhead reduction of 4\texttimes{}.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {66},
numpages = {13},
keywords = {DRAM, DIMM, memory system, near-memory-processing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589102,
author = {Sullivan, Michael B. and Ziad, Mohamed Tarek Ibn and Jaleel, Aamer and Keckler, Stephen W.},
title = {Implicit Memory Tagging: No-Overhead Memory Safety Using Alias-Free Tagged ECC},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589102},
doi = {10.1145/3579371.3589102},
abstract = {Memory safety is a major security concern for unsafe programming languages, including C/C++ and CUDA/OpenACC. Hardware-accelerated memory tagging is an effective mechanism for detecting memory safety violations; however, its adoption is challenged by significant meta-data storage and memory traffic overheads. This paper proposes Implicit Memory Tagging (IMT), a novel approach that provides no-overhead hardware-accelerated memory tagging by leveraging the system error correcting code (ECC) to check for the equivalence of a memory tag in addition to its regular duties of detecting and correcting data errors. Implicit Memory Tagging relies on a new class of ECC codes called Alias-Free Tagged ECC (AFT-ECC) that can unambiguously identify tag mismatches in the absence of data errors, while maintaining the efficacy of ECC when data errors are present. When applied to GPUs, IMT addresses the increasing importance of GPU memory safety and the costs of adding meta-data to GPU memory. Ultimately, IMT detects memory safety violations without meta-data storage or memory access overheads. In practice, IMT can provide larger tag sizes than existing industry memory tagging implementations, enhancing security.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {67},
numpages = {13},
keywords = {error correcting codes, memory security, memory tagging},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589103,
author = {Gong, Yu and Yin, Miao and Huang, Lingyi and Xiao, Jinqi and Sui, Yang and Deng, Chunhua and Yuan, Bo},
title = {ETTE: Efficient Tensor-Train-based Computing Engine for Deep Neural Networks},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589103},
doi = {10.1145/3579371.3589103},
abstract = {Tensor-train (TT) decomposition enables ultra-high compression ratio, making the deep neural network (DNN) accelerators based on this method very attractive. TIE, the state-of-the-art TT based DNN accelerator, achieved high performance by leveraging a compact inference scheme to remove unnecessary computations and memory access. However, TIE increases memory costs for stage-wise intermediate results and additional intra-layer data transfer, leading to limited speedups even the models are highly compressed.To unleash the full potential of TT decomposition, this paper proposes ETTE, an algorithm and hardware co-optimization framework for Efficient Tensor-Train Engine. At the algorithm level, ETTE proposes new tensor core construction and computation ordering mechanism to reduce stage-wise computation and storage cost at the same time. At the hardware level, ETTE proposes a lookahead-style across-stage processing scheme to eliminate the unnecessary stage-wise data movement. By fully leveraging the decoupled input and output dimension factors, ETTE develops an efficient low-cost memory partition-free access scheme to efficiently support the desired matrix transformation.We demonstrate the effectiveness of ETTE via implementing a 16-PE hardware prototype with CMOS 28nm technology. Compared with GPU on various workloads, ETTE achieves 6.5\texttimes{} â 253.1\texttimes{} higher throughput and 189.2\texttimes{} â 9750.5\texttimes{} higher energy efficiency. Compared with the state-of-the-art DNN accelerators, ETTE brings 1.1\texttimes{} â 58.3\texttimes{}, 2.6\texttimes{} â 1170.4\texttimes{} and 1.8\texttimes{} â 2098.2\texttimes{} improvement on throughput, energy efficiency and area efficiency, respectively.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {68},
numpages = {13},
keywords = {accelerator, low rank, neural networks, tensor decomposition},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589104,
author = {Neuman, Sabrina M. and Ghosal, Radhika and Bourgeat, Thomas and Plancher, Brian and Reddi, Vijay Janapa},
title = {RoboShape: Using Topology Patterns to Scalably and Flexibly Deploy Accelerators Across Robots},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589104},
doi = {10.1145/3579371.3589104},
abstract = {A key challenge for hardware acceleration of robotics applications is the enormous diversity of possible deployment scenarios. To create efficient accelerators while minimizing non-recurring engineering costs, it is essential to identify high-level computational patterns that are prescribed by the physical characteristics of the deployed robot system and directly embed these domain-specific insights into the accelerator design process. To address this challenge, we present RoboShape, an accelerator framework that leverages two topology-based computational patterns that scale with robot size: (1) topology traversals, and (2) large topology-based matrices. Using these patterns and building on prior work, we expose opportunities to directly use robot topology to inform architectural mechanisms including task scheduling and allocation, data placement, block matrix operations, and sparse I/O data. Designing architectures according to topology-based patterns enables flexible, scalable, optimized accelerator deployment across the nonlinear design space of robot shape and computing resources. With this insight, we establish a systematic framework to generate accelerators, and use it to implement three accelerators for three different robots, achieving speedups over state-of-the-art CPU and GPU solutions. For the topologically-diverse iiwa manipulator, HyQ quadruped, and Baxter torso robots, RoboShape accelerators on an FPGA provide a 4.0\texttimes{} to 4.4\texttimes{} speedup in compute latency over CPU and a 8.0\texttimes{} to 15.1\texttimes{} speedup over GPU for the dynamics gradients, a key bottleneck preventing online execution of nonlinear optimal motion control for legged robots. Taking a broader view, for topology-based applications, RoboShape enables analysis of performance and resource utilization tradeoffs that will be critical to managing resources across accelerators in future full robotics domain-specific SoCs.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {69},
numpages = {13},
keywords = {motion planning, dynamics, hardware accelerators, robotics},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589105,
author = {He, Yi and Hutton, Mike and Chan, Steven and De Gruijl, Robert and Govindaraju, Rama and Patil, Nishant and Li, Yanjing},
title = {Understanding and Mitigating Hardware Failures in Deep Learning Training Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589105},
doi = {10.1145/3579371.3589105},
abstract = {Deep neural network (DNN) training workloads are increasingly susceptible to hardware failures in datacenters. For example, Google experienced "mysterious, difficult to identify problems" in their TPU training systems due to hardware failures [7]. Although these particular problems were subsequently corrected through significant efforts, they have raised the urgency of addressing the growing challenges emerging from hardware failures impacting many DNN training workloads.In this paper, we present the first in-depth resilience study targeting DNN training workloads and hardware failures that occur in the logic portion of deep learning (DL) accelerator systems. We developed a fault injection framework to accurately simulate the effects of various hardware failures based on the design of an industrial DL accelerator, and conducted &gt; 2.9M experiments (&gt; 490K node-hours) using representative workloads. Based on our experiments, we present (1) a comprehensive characterization of hardware failure effects, (2) the fundamental understanding on how hardware failures propagate in training devices and interact with training workloads, and (3) the necessary conditions that must be satisfied for these failures to eventually cause unexpected training outcomes.The insights obtained from our study enabled us to develop ultralight-weight software techniques to mitigate hardware failures. Our techniques require 24--32 lines of code change, and introduce 0.003\% -- 0.025\% performance overhead for various representative workloads. Our observations and techniques are generally applicable to mitigate various hardware failures in DL training accelerator systems.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {70},
numpages = {16},
keywords = {silent data curroption, hardware failures, reliability, resilience, neural network training, deep learning accelerator systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589106,
author = {Litteken, Andrew and Seifert, Lennart Maximilian and Chadwick, Jason D. and Nottingham, Natalia and Roy, Tanay and Li, Ziqian and Schuster, David and Chong, Frederic T. and Baker, Jonathan M.},
title = {Dancing the Quantum Waltz: Compiling Three-Qubit Gates on Four Level Architectures},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589106},
doi = {10.1145/3579371.3589106},
abstract = {Superconducting quantum devices are a leading technology for quantum computation, but they face several challenges. Gate errors, coherence errors and a lack of connectivity all contribute to low fidelity results. In particular, connectivity restrictions enforce a gate set that requires three-qubit gates to be decomposed into one- or two-qubit gates. This substantially increases the number of two-qubit gates that need to be executed. However, many quantum devices have access to higher energy levels. We can expand the qubit abstraction of |0ã and |1ã to a ququart which has access to the |2ã and |3ã state, but with shorter coherence times. This allows for two qubits to be encoded in one ququart, enabling increased virtual connectivity between physical units from two adjacent qubits to four fully connected qubits. This connectivity scheme allows us to more efficiently execute three-qubit gates natively between two physical devices.We present direct-to-pulse implementations of several three-qubit gates, synthesized via optimal control, for compilation of three-qubit gates onto a superconducting-based architecture with access to four-level devices with the first experimental demonstration of four-level ququart gates designed through optimal control. We demonstrate strategies that temporarily use higher level states to perform Toffoli gates and always use higher level states to improve fidelities for quantum circuits. We find that these methods improve expected fidelities with increases of 2x across circuit sizes using intermediate encoding, and increases of 3x for fully-encoded ququart compilation.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {71},
numpages = {14},
keywords = {compilation, qudit, quantum computing},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589107,
author = {Sriram, Karthik and Pothukuchi, Raghavendra Pradyumna and Gerasimiuk, Micha\l{} and Ugur, Muhammed and Ye, Oliver and Manohar, Rajit and Khandelwal, Anurag and Bhattacharjee, Abhishek},
title = {SCALO: An Accelerator-Rich Distributed System for Scalable Brain-Computer Interfacing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589107},
doi = {10.1145/3579371.3589107},
abstract = {SCALO is the first distributed brain-computer interface (BCI) consisting of multiple wireless-networked implants placed on different brain regions. SCALO unlocks new treatment options for debilitating neurological disorders and new research into brain-wide network behavior. Achieving the fast and low-power communication necessary for real-time processing has historically restricted BCIs to single brain sites. SCALO also adheres to tight power constraints, but enables fast distributed processing. Central to SCALO's efficiency is its realization as a full stack distributed system of brain implants with accelerator-rich compute. SCALO balances modular system layering with aggressive cross-layer hardware-software co-design to integrate compute, networking, and storage. The result is a lesson in designing energy-efficient networked distributed systems with hardware accelerators from the ground up.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {72},
numpages = {20},
keywords = {low power, hardware accelerators, BCI, brain-computer interfaces},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589108,
author = {Wang, Maolin and McInerney, Ian and Stellato, Bartolomeo and Boyd, Stephen and So, Hayden Kwok-Hay},
title = {RSQP: Problem-specific Architectural Customization for Accelerated Convex Quadratic Optimization},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589108},
doi = {10.1145/3579371.3589108},
abstract = {Convex optimization is at the heart of many performance-critical applications across a wide range of domains. Although many high-performance hardware accelerators have been developed for specific optimization problems in the past, designing such accelerator is a challenging task and the resulting computing architecture is often so specific to the targeted application that they can hardly be reused even in a related application within the same domain. To accelerate general-purpose optimization solvers that must operate on diverse user input during run time, an ideal hardware solver should be able to adapt to the provided optimization problem dynamically while achieving high performance and power-efficiency. In this work, a hardware-accelerated general-purpose quadratic program solver, called RSQP, with reconfigurable functional units and data path that facilitate problem-specific customization is presented. RSQP uses a string-based encoding to describe the problem structure with fine granularity. Based on this encoding, functional units and datapath customized to the sparsity pattern of the problem are created by solving a dictionary-based lossless string compression problem and a mixed integer linear program respectively. RSQP has been integrated to accelerate the general-purpose quadratic programming solver OSQP and has been tested using an extensive benchmark with 120 optimization problems from 6 application domains. Through architectural customization, RSQP achieves up to 7\texttimes{} performance improvement over its baseline generic design. Furthermore, when compared with a CPU and a GPU-accelerated implementation, RSQP achieves up to 31.2\texttimes{} and 6.9\texttimes{} end-to-end speedup on these benchmark programs, respectively. Finally, the FPGA accelerator operates at up to 6.6\texttimes{} lower dynamic power consumption and up to 22.7\texttimes{} higher power efficiency over the GPU implementation, making it an attractive solution for power-conscious datacenter applications.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {73},
numpages = {12},
keywords = {reconfigurable computing, domain-specific architectures, quadratic programming, FPGA, convex optimization},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589109,
author = {Fu, Yonggan and Ye, Zhifan and Yuan, Jiayi and Zhang, Shunyao and Li, Sixu and You, Haoran and Lin, Yingyan},
title = {Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via Algorithm-Hardware Co-Design},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589109},
doi = {10.1145/3579371.3589109},
abstract = {Novel view synthesis is an essential functionality for enabling immersive experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for which Neural Radiance Field (NeRF) has emerged as the state-of-the-art (SOTA) technique. In particular, generalizable NeRFs have gained increasing popularity thanks to their cross-scene generalization capability, which enables NeRFs to be instantly serviceable for new scenes without per-scene training. Despite their promise, generalizable NeRFs aggravate the prohibitive complexity of NeRFs due to their required extra memory accesses needed to acquire scene features, causing NeRFs' ray marching process to be memory-bounded. To tackle this dilemma, existing sparsity-exploitation techniques for NeRFs fall short, because they require knowledge of the sparsity distribution of the target 3D scene which is unknown when generalizing NeRFs to a new scene.To this end, we propose Gen-NeRF, an algorithm-hardware co-design framework dedicated to generalizable NeRF acceleration, which aims to win both rendering efficiency and generalization capability in NeRFs. To the best of our knowledge, Gen-NeRF is the first to enable real-time generalizable NeRFs, demonstrating a promising NeRF solution for next-generation AR/VR devices. On the algorithm side, Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact that different regions of a 3D scene contribute differently to the rendered pixels depending on where the objects are located in the scene, to enable sparse yet effective s ampling. In addition, Gen-NeRF replaces the ray transformer, which is generally included in SOTA generalizable NeRFs to enhance density estimation, with a novel Ray-Mixer module to reduce workload heterogeneity. On the hardware side, Gen-NeRF highlights an accelerator micro-architecture dedicated to accelerating the resulting model workloads from our Gen-NeRF algorithm to maximize the data reuse opportunities among different rays by making use of their epipolar geometric relationship. Furthermore, our Gen-NeRF accelerator features a customized dataflow to enhance data locality during point-to-hardware mapping and an optimized scene feature storage strategy to minimize memory bank conflicts across camera rays of NeRFs. Extensive experiments validate the effectiveness of our proposed Gen-NeRF framework in enabling real-time and generalizable novel view synthesis.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {74},
numpages = {12},
keywords = {hardware accelerator, neural radiance field},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589110,
author = {Shiflett, Kyle and Karanth, Avinash and Bunescu, Razvan and Louri, Ahmed},
title = {Flumen: Dynamic Processing in the Photonic Interconnect},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589110},
doi = {10.1145/3579371.3589110},
abstract = {In chiplet-based heterogeneous architectures, electrical network-on-package (NoP) designs are typically over-provisioned with routers and channels to provide sufficient bandwidth during periods of high network load. Observing that there are significant periods of low/idle network utilization, prior work has proposed modified network-on-chip (NoC) architectures to enable in-network compute, especially for compute-intensive operations (e.g. linear algebra). However, electrical package-level interconnects impose fundamental energy and bandwidth scaling issues for future chiplet architectures.This paper proposes Flumen, a dual-purpose photonic interconnect that provides communication at the package-level while also doubling as an accelerator, performing parallel linear computation when network load is low. The proposed architecture utilizes the inherent parallelism of light to create energy-efficient interconnects that support en route computation with minimal changes to the network. By dynamically adjusting the topology, Flumen can change the communication and compute sections of the architecture to adapt to workload fluctuations. Performance evaluation on linear algebra applications shows that Flumen achieves a 2.5\texttimes{} reduction in energy, a 3.6\texttimes{} speedup improvement, and a 9.3\texttimes{} reduction in energy-delay product on average when compared to an electrical mesh network that is used exclusively for communication.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {75},
numpages = {13},
keywords = {hardware acceleration, silicon photonics, networks-on-chip},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589111,
author = {Rajat, Rachit and Wang, Yongqin and Annavaram, Murali},
title = {LAORAM: A Look Ahead ORAM Architecture for Training Large Embedding Tables},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589111},
doi = {10.1145/3579371.3589111},
abstract = {Memory access patterns have been demonstrated to leak critical information such as security keys and a program's spatial and temporal information. This information leak poses a significant privacy challenge in machine learning models with embedding tables. Embedding tables are used to learn categorical features from training data. The address of an embedding table entry carries privacy sensitive information since the address of an entry discloses features associated with a user. Oblivious RAM (ORAM), and its enhanced variants, such as PathORAM, have emerged as viable solutions to hide leakage from memory access streams. PathORAM fetches an entire path of memory blocks for every memory fetch request, thereby leading to substantial bandwidth and performance overheads.In this work, we present Look Ahead ORAM (LAORAM), an ORAM framework designed to protect user privacy during embedding table training. LAORAM exploits the unique property of ML training, namely the training samples that are going to be used in the future are known beforehand. LAORAM preprocesses the training samples to identify the memory blocks which are accessed together in the near future. LAORAM combines multiple blocks accessed together as superblocks and tries to assign all blocks in a superblock to few paths. Thus, future accesses to a collection of blocks can be satisfied from a few paths, effectively reducing the number of reads and writes to the ORAM. To further increase performance, LAORAM uses a fat-tree structure for PathORAM, i.e. a tree with variable bucket size, effectively reducing the number of background evictions required, which improves the stash usage. We have evaluated LAORAM using both a recommendation model (DLRM) and an NLP model (XLM-R) embedding table configurations. LAORAM performs 5 times faster than PathORAM on a recommendation dataset (Kaggle) and 5.4 times faster on an NLP dataset (XNLI) while guaranteeing the same security guarantees as the original PathORAM.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {76},
numpages = {15},
keywords = {embedding tables, recommendation systems, security, ORAM, memory},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589112,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {CPU, hyperthreading, prefetching, reuse distance, irregular memory accesses, embeddings, recommendation systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589113,
author = {Ying, Ziyu and Bhuyan, Sandeepa and Kang, Yan and Zhang, Yingtian and Kandemir, Mahmut T. and Das, Chita R.},
title = {EdgePC: Efficient Deep Learning Analytics for Point Clouds on Edge Devices},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589113},
doi = {10.1145/3579371.3589113},
abstract = {Recently, point cloud (PC) has gained popularity in modeling various 3D objects (including both synthetic and real-life) and has been extensively utilized in a wide range of applications such as AR/VR, 3D reconstruction, and autonomous driving. For such applications, it is critical to analyze/understand the surrounding scenes properly. To achieve this, deep learning based methods (e.g., convolutional neural networks (CNNs)) have been widely employed for higher accuracy. Unlike the deep learning on conventional 2D images/videos, where the feature computation (matrix multiplication) is the major bottleneck, in point cloud-based CNNs, the sample and neighbor search stages are the primary bottlenecks, and collectively contribute to 54\% (up to 80\%) of the overall execution latency on a typical edge device. While prior efforts have attempted to solve this issue by designing custom ASICs or pipelining the neighbor search with other stages, to our knowledge, none of them has tried to "structurize" the unstructured PC data for improving computational efficiency.In this paper, we first explore the opportunities of structurizing PC data using Morton code (which is originally designed to map data from a high dimensional space to one dimension, while preserving spatial locality) and observe that there is a huge scope to "skip" the sample and neighbor search computation by operating on the "structurized" PC data. Based on this, we propose two approximation techniques for the sampling and neighbor search stages. We implemented our proposals on an NVIDIA Jetson AGX Xavier edge GPU board. The evaluation results collected on six different workloads show that our design can accelerate the sample and neighbor search stages by 3.68\texttimes{} (up to 5.21\texttimes{}) with minimal impact on inference accuracy. This acceleration in turn results in 1.55\texttimes{} speedup in the end-to-end execution latency and saves 33\% of energy expenditure.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {78},
numpages = {14},
keywords = {edge device, energy-efficiency, approximation, deep neural network, point cloud},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589114,
author = {Resch, Salonik and Cilasun, Husrev and Chowdhury, Zamshed and Zabihi, Masoud and Zhao, Zhengyang and Wang, Jian-Ping and Sapatnekar, Sachin and Karpuzcu, Ulya R.},
title = {On Endurance of Processing in (Nonvolatile) Memory},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589114},
doi = {10.1145/3579371.3589114},
abstract = {Processing-in-Memory (PIM) architectures have gained popularity due to their ability to alleviate the memory wall by performing large numbers of operations within the memory itself. On top of this, nonvolatile memory (NVM) technologies offer highly energy-efficient operations, rendering processing in NVM especially promising. Unfortunately, a major drawback is that NVM has limited endurance. Even when used for standard memory, nonvolatile technologies face limited lifetimes, which is exacerbated by imbalanced usage of memory cells. PIM significantly increases the number of operations the memory is required to perform, making the problem much worse. In this work, we quantitatively analyze the impact of PIM applications on endurance considering representative memory technologies. Our findings indicate that limited endurance can easily block the performance and energy efficiency potential of PIM architectures. Even the best known technologies of today can fall short of meeting practical lifetime expectations. This highlights the importance of research efforts to improve endurance especially at the device technology level. Our study represents the first step in characterizing the very demanding endurance needs of PIM applications to derive a detailed technology level design specification.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {79},
numpages = {13},
keywords = {nonvolatile memory, endurance, processing in memory},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589348,
author = {Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and Hutchin, Adam and Diril, Utku and Nair, Krishnakumar and Aredestani, Ehsan K. and Schatz, Martin and Hao, Yuchen and Komuravelli, Rakesh and Ho, Kunming and Abu Asal, Sameer and Shajrawi, Joe and Quinn, Kevin and Sreedhara, Nagesh and Kansal, Pankaj and Wei, Willie and Jayaraman, Dheepak and Cheng, Linda and Chopda, Pritam and Wang, Eric and Bikumandla, Ajay and Karthik Sengottuvel, Arun and Thottempudi, Krishna and Narasimha, Ashwin and Dodds, Brian and Gao, Cao and Zhang, Jiyuan and Al-Sanabani, Mohammed and Zehtabioskuie, Ana and Fix, Jordan and Yu, Hangchen and Li, Richard and Gondkar, Kaustubh and Montgomery, Jack and Tsai, Mike and Dwarakapuram, Saritha and Desai, Sanjay and Avidan, Nili and Ramani, Poorvaja and Narayanan, Karthik and Mathews, Ajit and Gopal, Sethu and Naumov, Maxim and Rao, Vijay and Noru, Krishna and Reddy, Harikrishna and Venkatapuram, Prahlad and Bjorlin, Alexis},
title = {MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589348},
doi = {10.1145/3579371.3589348},
abstract = {Meta has traditionally relied on using CPU-based servers for running inference workloads, specifically Deep Learning Recommendation Models (DLRM), but the increasing compute and memory requirements of these models have pushed the company towards using specialized solutions such as GPUs or other hardware accelerators. This paper describes the company's effort in constructing its first silicon specifically designed for recommendation systems; it describes the accelerator architecture and platform design, the software stack for enabling and optimizing PyTorch-based models and provides an initial performance evaluation. With our emerging software stack, we have made significant progress towards reaching the same or higher efficiency as the GPU: We averaged 0.9x perf/W across various DLRMs, and benchmarks show operators such as GEMMs reaching 2x perf/W. Finally, the paper describes the lessons we learned during this journey which can improve the performance and programmability of future generations of architecture.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {80},
numpages = {13},
keywords = {programmability, performance, recommendation systems, inference, machine learning, accelerators},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589349,
author = {Loh, Gabriel H. and Schulte, Michael J. and Ignatowski, Mike and Adhinarayanan, Vignesh and Aga, Shaizeen and Aguren, Derrick and Agrawal, Varun and Aji, Ashwin M. and Alsop, Johnathan and Bauman, Paul and Beckmann, Bradford M. and Beigi, Majed Valad and Blagodurov, Sergey and Boraten, Travis and Boyer, Michael and Brantley, William C. and Chalmers, Noel and Chen, Shaoming and Cheng, Kevin and Chu, Michael L. and Cownie, David and Curtis, Nicholas and Del Pino, Joris and Duong, Nam and Duundefinedu, Alexandru and Eckert, Yasuko and Erb, Christopher and Freitag, Chip and Greathouse, Joseph L. and Gurumurthi, Sudhanva and Gutierrez, Anthony and Hamidouche, Khaled and Hossamani, Sachin and Huang, Wei and Islam, Mahzabeen and Jayasena, Nuwan and Kalamatianos, John and Kayiran, Onur and Kotra, Jagadish and Lee, Alan and Lowell, Daniel and Madan, Niti and Majumdar, Abhinandan and Malaya, Nicholas and Manne, Srilatha and Mashimo, Susumu and McDougall, Damon and Mednick, Elliot and Mishkin, Michael and Nutter, Mark and Paul, Indrani and Poremba, Matthew and Potter, Brandon and Punniyamurthy, Kishore and Puthoor, Sooraj and Raasch, Steven E. and Rao, Karthik and Rodgers, Gregory and Scrbak, Marko and Seyedzadeh, Mohammad and Slice, John and Sridharan, Vilas and van Oostrum, Ren\'{e} and van Tassell, Eric and Vishnu, Abhinav and Wasmundt, Samuel and Wilkening, Mark and Wolfe, Noah and Wyse, Mark and Yalavarti, Adithya and Yudanov, Dmitri},
title = {A Research Retrospective on AMD's Exascale Computing Journey},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589349},
doi = {10.1145/3579371.3589349},
abstract = {The pace of advancement of the top-end supercomputers historically followed an exponential curve similar to (and driven in part by) Moore's Law. Shortly after hitting the petaflop mark, the community started looking ahead to the next milestone: Exascale. However, many obstacles were already looming on the horizon, such as the slowing of Moore's Law, and others like the end of Dennard Scaling had already arrived. Anticipating significant challenges for the overall high-performance computing (HPC) community to achieve the next 1000x improvement, the U.S. Department of Energy (DOE) launched the Exascale Computing Program to enable and accelerate fundamental research across the many technologies needed to achieve exascale computing.AMD had the opportunity to contribute to the so-called "*Forward" programs from the DOE, which were a series of public-private partnerships focused on research and co-design activities covering compute architectures, interconnects, memory systems, chiplets and packaging, software stacks, applications, and more. Some of the research from these programs can now be found in the world's first exascale supercomputer, some were a little ahead of their time and may have an impact in the coming years, and others simply did not pan out. In this paper, we provide a retrospective of AMD's nearly decade-long research journey covering how we tried to predict the architecture of a supercomputer a decade into the future, what we got right, what we got wrong, and some of the insights and learnings that we discovered along the way.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {81},
numpages = {14},
keywords = {research, accelerated processing unit, heterogeneous compute, chiplets, memory, frontier, supercomputing, high-performance computing, HPC, exascale},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589350,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5\% of system cost and &lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {CO2 equivalent emissions, energy, carbon emissions, warehouse scale computer, power usage effectiveness, large language model, embeddings, reconfigurable, optical interconnect, supercomputer, IPU, GPU, TPU, domain specific architecture, machine learning},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589351,
author = {Darvish Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and Shao, Lai and Kolhe, Gaurav and Melts, Dimitry and Klar, Jasmine and L'Heureux, Renee and Perry, Matt and Burger, Doug and Chung, Eric and Deng, Zhaoxia (Summer) and Naghshineh, Sam and Park, Jongsoo and Naumov, Maxim},
title = {With Shared Microexponents, A Little Shifting Goes a Long Way},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589351},
doi = {10.1145/3579371.3589351},
abstract = {This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {83},
numpages = {13},
keywords = {AI data types, compute efficiency, artificial intelligence},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3579371.3589352,
author = {Smith, Kaitlin N. and Perlin, Michael A. and Gokhale, Pranav and Frederick, Paige and Owusu-Antwi, David and Rines, Richard and Omole, Victory and Chong, Frederic},
title = {Clifford-based Circuit Cutting for Quantum Simulation},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589352},
doi = {10.1145/3579371.3589352},
abstract = {Quantum computing has potential to provide exponential speedups over classical computing for many important applications. However, today's quantum computers are in their early stages, and hardware quality issues hinder the scale of program execution. Benchmarking and simulation of quantum circuits on classical computers is therefore essential to advance the understanding of how quantum computers and programs operate, enabling both algorithm discovery that leads to high-impact quantum computation and engineering improvements that deliver to more powerful quantum systems. Unfortunately, the nature of quantum information causes simulation complexity to scale exponentially with problem size.In this paper, we debut Super.tech's SuperSim framework, a new approach for high fidelity and scalable quantum circuit simulation. SuperSim employs two key techniques for accelerated quantum circuit simulation: Clifford-based simulation and circuit cutting. Through the isolation of Clifford subcircuit fragments within a larger non-Clifford circuit, resource-efficient Clifford simulation can be invoked, leading to significant reductions in runtime. After fragments are independently executed, circuit cutting and recombination procedures allow the final output of the original circuit to be reconstructed from fragment execution results. Through the combination of these two state-of-art techniques, SuperSim is a product for quantum practitioners that allows quantum circuit evaluation to scale beyond the frontiers of current simulators. Our results show that Clifford-based circuit cutting accelerates the simulation of near-Clifford circuits, allowing 100s of qubits to be evaluated with modest runtimes.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {84},
numpages = {13},
keywords = {Clifford+T simulation, circuit cutting, quantum circuit simulation, quantum computation},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

