@inproceedings{10.1145/3307650.3322207,
author = {Bhatia, Eshan and Chacon, Gino and Pugsley, Seth and Teran, Elvira and Gratz, Paul V. and Jim\'{e}nez, Daniel A.},
title = {Perceptron-based prefetch filtering},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322207},
doi = {10.1145/3307650.3322207},
abstract = {Hardware prefetching is an effective technique for hiding cache miss latencies in modern processor designs. Prefetcher performance can be characterized by two main metrics that are generally at odds with one another: coverage, the fraction of baseline cache misses which the prefetcher brings into the cache; and accuracy, the fraction of prefetches which are ultimately used. An overly aggressive prefetcher may improve coverage at the cost of reduced accuracy. Thus, performance may be harmed by this over-aggressiveness because many resources are wasted, including cache capacity and bandwidth. An ideal prefetcher would have both high coverage and accuracy.In this paper, we introduce Perceptron-based Prefetch Filtering (PPF) as a way to increase the coverage of the prefetches generated by an underlying prefetcher without negatively impacting accuracy. PPF enables more aggressive tuning of the underlying prefetcher, leading to increased coverage by filtering out the growing numbers of inaccurate prefetches such an aggressive tuning implies. We also explore a range of features to use to train PPF's perceptron layer to identify inaccurate prefetches. PPF improves performance on a memory-intensive subset of the SPEC CPU 2017 benchmarks by 3.78\% for a single-core configuration, and by 11.4\% for a 4-core configuration, compared to the underlying prefetcher alone.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {1–13},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322267,
author = {Tarsa, Stephen J. and Chowdhury, Rangeen Basu Roy and Sebot, Julien and Chinya, Gautham and Gaur, Jayesh and Sankaranarayanan, Karthik and Lin, Chit-Kwan and Chappell, Robert and Singhal, Ronak and Wang, Hong},
title = {Post-silicon CPU adaptation made practical using machine learning},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322267},
doi = {10.1145/3307650.3322267},
abstract = {Processors that adapt architecture to workloads at runtime promise compelling performance per watt (PPW) gains, offering one way to mitigate diminishing returns from pipeline scaling. State-of-the-art adaptive CPUs deploy machine learning (ML) models on-chip to optimize hardware by recognizing workload patterns in event counter data. However, despite breakthrough PPW gains, such designs are not yet widely adopted due to the potential for systematic adaptation errors in the field.This paper presents an adaptive CPU based on Intel SkyLake that (1) closes the loop to deployment, and (2) provides a novel mechanism for post-silicon customization. Our CPU performs predictive cluster gating, dynamically setting the issue width of a clustered architecture while clock-gating unused resources. Gating decisions are driven by ML adaptation models that execute on an existing microcontroller, minimizing design complexity and allowing performance characteristics to be adjusted with the ease of a firmware update. Crucially, we show that although adaptation models can suffer from statistical blindspots that risk degrading performance on new workloads, these can be reduced to minimal impact with careful design and training.Our adaptive CPU improves PPW by 31.4\% over a comparable non-adaptive CPU on SPEC2017, and exhibits two orders of magnitude fewer Service Level Agreement (SLA) violations than the state-of-the-art. We show how to optimize PPW using models trained to different SLAs or to specific applications, e.g. to improve datacenter hardware in situ. The resulting CPU meets real world deployment criteria for the first time and provides a new means to tailor hardware to individual customers, even as their needs change.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {adaptive hardware, clustered architectures, machine learning, runtime optimization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322217,
author = {Garza, Elba and Mirbagher-Ajorpaz, Samira and Khan, Tahsin Ahmad and Jim\'{e}nez, Daniel A.},
title = {Bit-level perceptron prediction for indirect branches},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322217},
doi = {10.1145/3307650.3322217},
abstract = {Modern software uses indirect branches for various purposes including, but not limited to, virtual method dispatch and implementation of switch statements. Because an indirect branch's target address cannot be determined prior to execution, high-performance processors depend on highly-accurate indirect branch prediction techniques to mitigate control hazards.This paper proposes a new indirect branch prediction scheme that predicts target addresses at the bit level. Using a series of perceptron-based predictors, our predictor predicts individual branch target address bits based on correlations within branch history. Our evaluations show this new branch target predictor is competitive with state-of-the-art branch target predictors at an equivalent hardware budget. For instance, over a set of workloads including SPEC and mobile applications, our predictor achieves a misprediction rate of 0.183 mispredictions per 1000 instructions, compared with 0.193 for the state-of-the-art ITTAGE predictor and 0.29 for a VPC-based indirect predictor.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {27–38},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3326633,
author = {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
title = {Generative and multi-phase learning for computer systems optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326633},
doi = {10.1145/3307650.3326633},
abstract = {Machine learning and artificial intelligence are invaluable for computer systems optimization: as computer systems expose more resources for management, ML/AI is necessary for modeling these resources' complex interactions. The standard way to incorporate ML/AI into a computer system is to first train a learner to accurately predict the system's behavior as a function of resource usage---e.g., to predict energy efficiency as a function of core usage---and then deploy the learned model as part of a system---e.g., a scheduler. In this paper, we show that (1) continued improvement of learning accuracy may not improve the systems result, but (2) incorporating knowledge of the systems problem into the learning process improves the systems results even though it may not improve overall accuracy. Specifically, we learn application performance and power as a function of resource usage with the systems goal of meeting latency constraints with minimal energy. We propose a novel generative model which improves learning accuracy given scarce data, and we propose a multi-phase sampling technique, which incorporates knowledge of the systems problem. Our results are both positive and negative. The generative model improves accuracy, even for state-of-the-art learning systems, but negatively impacts energy. Multi-phase sampling reduces energy consumption compared to the state-of-the-art, but does not improve accuracy. These results imply that learning for systems optimization may have reached a point of diminishing returns where accuracy improvements have little effect on the systems outcome. Thus we advocate that future work on learning for systems should de-emphasize accuracy and instead incorporate the system problem's structure into the learner.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {39–52},
numpages = {14},
keywords = {energy, heterogeneous architectures, machine learning, real-time systems, resource allocation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322247,
author = {Xie, Chenhao and Xin, Fu and Chen, Mingsong and Song, Shuaiwen Leon},
title = {OO-VR: NUMA friendly &lt;u&gt;o&lt;/u&gt;bject-&lt;u&gt;o&lt;/u&gt;riented &lt;u&gt;VR&lt;/u&gt; rendering framework for future NUMA-based multi-GPU systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322247},
doi = {10.1145/3307650.3322247},
abstract = {With the strong computation capability, NUMA-based multi-GPU system is a promising candidate to provide sustainable and scalable performance for Virtual Reality (VR) applications and deliver the excellent user experience. However, the entire multi-GPU system is viewed as a single GPU under the single programming model which greatly ignores the data locality among VR rendering tasks during the workload distribution, leading to tremendous remote memory accesses among GPU models (GPMs). The limited inter-GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major obstacle when executing VR applications in the multi-GPU system. By conducting comprehensive characterizations on different kinds of parallel rendering frameworks, we observe that distributing the rendering object along with its required data per GPM can reduce the inter-GPM memory accesses. However, this object-level rendering still faces two major challenges in NUMA-based multi-GPU system: (1) the large data locality between the left and right views of the same object and the data sharing among different objects and (2) the unbalanced workloads induced by the software-level distribution and composition mechanisms.To tackle these challenges, we propose object-oriented VR rendering framework (OO-VR) that conducts the software and hardware co-optimization to provide a NUMA friendly solution for VR multi-view rendering in NUMA-based multi-GPU systems. We first propose an object-oriented VR programming model to exploit the data sharing between two views of the same object and group objects into batches based on their texture sharing levels. Then, we design an object aware runtime batch distribution engine and distributed hardware composition unit to achieve the balanced workloads among GPMs and further improve the performance of VR rendering. Finally, evaluations on our VR featured simulator show that OO-VR provides 1.58x overall performance improvement and 76\% inter-GPM memory traffic reduction over the state-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly performance scalability for the future larger multi-GPU scenarios with ever increasing asymmetric bandwidth between local and remote memory.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {53–65},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322248,
author = {Feng, Yu and Zhu, Yuhao},
title = {PES: proactive event scheduling for responsive and energy-efficient mobile web computing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322248},
doi = {10.1145/3307650.3322248},
abstract = {Web applications are gradually shifting toward resource-constrained mobile devices. As a result, the Web runtime system must simultaneously address two challenges: responsiveness and energy-efficiency. Conventional Web runtime systems fall short due to their reactive nature: they react to a user event only after it is triggered. The reactive strategy leads to local optimizations that schedule event executions one at a time, missing global optimization opportunities.This paper proposes Proactive Event Scheduling (PES). The key idea of PES is to proactively anticipate future events and thereby globally coordinate scheduling decisions across events. Specifically, PES predicts events that are likely to happen in the near future using a combination of statistical inference and application code analysis. PES then speculatively executes future events ahead of time in a way that satisfies the QoS constraints of all the events while minimizing the global energy consumption. Fundamentally, PES unlocks more optimization opportunities by enlarging the scheduling window, which enables coordination across both outstanding events and predicted events. Hardware measurements show that PES reduces the QoS violation and energy consumption by 61.2\% and 26.5\%, respectively, over the Android's default Interactive CPU governor. It also reduces the QoS violation and energy consumption by 63.1\% and 17.9\%, respectively, compared to EBS, a state-of-the-art reactive scheduler.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {66–78},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322260,
author = {Chen, Huixiang and Song, Mingcong and Zhao, Jiechen and Dai, Yuting and Li, Tao},
title = {Retracted on January 26, 2021: 3D-based video recognition acceleration by leveraging temporal locality},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322260},
doi = {10.1145/3307650.3322260},
abstract = {NOTICE OF RETRACTION: This Work has been Retracted by ACM because one or more of the authors of this Work were proven to have known or believed that the Work contained incorrect and/or falsified results prior to publication of the Work and one or more of the authors of this Work violated the anonymity and independence of the review process for their paper "3D-based video recognition acceleration by leveraging temporal locality" to the Proceedings of the 46th International Symposium on Computer Architecture (ISCA '19). Association for Computing Machinery, New York, NY, USA, 79-90.https://dl.acm.org/doi/10.1145/3307650.3322260},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {79–90},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322264,
author = {Leng, Yue and Chen, Chi-Chun and Sun, Qiuyue and Huang, Jian and Zhu, Yuhao},
title = {Energy-efficient video processing for virtual reality},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322264},
doi = {10.1145/3307650.3322264},
abstract = {Virtual reality (VR) has huge potential to enable radically new applications, behind which spherical panoramic video processing is one of the backbone techniques. However, current VR systems reuse the techniques designed for processing conventional planar videos, resulting in significant energy inefficiencies. Our characterizations show that operations that are unique to processing 360° VR content constitute 40\% of the total processing energy consumption.We present EVR, an end-to-end system for energy-efficient VR video processing. EVR recognizes that the major contributor to the VR tax is the projective transformation (PT) operations. EVR mitigates the overhead of PT through two key techniques: semantic-aware streaming (SAS) on the server and hardware-accelerated rendering (HAR) on the client device. EVR uses SAS to reduce the chances of executing projective transformation on VR devices by pre-rendering 360° frames in the cloud. Different from conventional pre-rendering techniques, SAS exploits the key semantic information inherent in VR content that is previously ignored. Complementary to SAS, HAR mitigates the energy overhead of on-device rendering through a new hardware accelerator that is specialized for projective transformation. We implement an EVR prototype on an Amazon AWS server instance and a NVIDA Jetson TX2 board combined with a Xilinx Zynq-7000 FPGA. Real system measurements show that EVR reduces the energy of VR rendering by up to 58\%, which translates to up to 42\% energy saving for VR devices.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {91–103},
numpages = {13},
keywords = {energy efficiency, hardware accelerator, pre-rendering, projective transformation, video processing, virtual reality},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322250,
author = {Awad, Amro and Ye, Mao and Solihin, Yan and Njilla, Laurent and Zubair, Kazi Abu},
title = {Triad-NVM: persistency for integrity-protected and encrypted non-volatile memories},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322250},
doi = {10.1145/3307650.3322250},
abstract = {Non-Volatile Memory is here and provides an attractive fabric for main memory. Unlike DRAM, non-volatile main memory (NVMM) retains data after power loss. This allows memory to host data persistently across crashes and reboots, but opens up opportunities for attackers to snoop and/or tamper with data between boot episodes. While memory encryption and integrity verification have been well studied for DRAM systems, new challenges surface for NVMM if we want to simultaneously preserve security guarantees, data recovery across crashes/reboots, good persistence performance, and fast recovery.In this paper, we explore persistency of data with all security metadata (counters, MACs, and Merkle Tree) to achieve secure persistency. We show that to ensure security guarantees, message authentication code (MAC) and Bonsai Merkle Tree (BMT) need to be maintained, in addition to counters, and they provide the majority of persistency overheads. We analyze the requirements for achieving secure persistency for both persistent and non-persistent memory regions. We found that the non-volatility nature of memory may trigger integrity verification failure at reboot, hence we propose a separate mechanism to support non-persistent memory region. Fourth, we propose designs to make recovery fast. Our evaluation shows that the proposed design, Triad-NVM, can improve the throughput by an average of 2\texttimes{} relative to strict persistence. Moreover, Triad-NVM can achieve orders of magnitude faster recovery time compared to systems without security metadata persistence.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {104–115},
numpages = {12},
keywords = {non-volatile memories, persistence, persistent security, security},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322275,
author = {Matam, Kiran Kumar and Koo, Gunjae and Zha, Haipeng and Tseng, Hung-Wei and Annavaram, Murali},
title = {GraphSSD: graph semantics aware SSD},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322275},
doi = {10.1145/3307650.3322275},
abstract = {Graph analytics play a key role in a number of applications such as social networks, drug discovery, and recommendation systems. Given the large size of graphs that may exceed the capacity of the main memory, application performance is bounded by storage access time. Out-of-core graph processing frameworks try to tackle this storage access bottleneck through techniques such as graph sharding, and sub-graph partitioning. Even with these techniques, the need to access data across different graph shards or sub-graphs causes storage systems to become a significant performance hurdle. In this paper, we propose a graph semantic aware solid state drive (SSD) framework, called GraphSSD, which is a full system solution for storing, accessing, and performing graph analytics on SSDs. Rather than treating storage as a collection of blocks, GraphSSD considers graph structure while deciding on graph layout, access, and update mechanisms. GraphSSD replaces the conventional logical to physical page mapping mechanism in an SSD with a novel vertex-to-page mapping scheme and exploits the detailed knowledge of the flash properties to minimize page accesses. GraphSSD also supports efficient graph updates (vertex and edge modifications) by minimizing unnecessary page movement overheads. GraphSSD provides a simple programming interface that enables application developers to access graphs as native data in their applications, thereby simplifying the code development. It also augments the NVMe (non-volatile memory express) interface with a minimal set of changes to map the graph access APIs to appropriate storage access mechanisms.Our evaluation results show that the GraphSSD framework improves the performance by up to 1.85 \texttimes{} for the basic graph data fetch functions and on average 1.40\texttimes{}, 1.42\texttimes{}, 1.60\texttimes{}, 1.56\texttimes{}, and 1.29\texttimes{} for the widely used breadth-first search, connected components, random-walk, maximal independent set, and page rank applications, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {116–128},
numpages = {13},
keywords = {SSD, flash storage, graphs},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322231,
author = {Hassan, Hasan and Patel, Minesh and Kim, Jeremie S. and Yaglikci, A. Giray and Vijaykumar, Nandita and Ghiasi, Nika Mansouri and Ghose, Saugata and Mutlu, Onur},
title = {CROW: a low-cost substrate for improving DRAM performance, energy efficiency, and reliability},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322231},
doi = {10.1145/3307650.3322231},
abstract = {DRAM has been the dominant technology for architecting main memory for decades. Recent trends in multi-core system design and large-dataset applications have amplified the role of DRAM as a critical system bottleneck. We propose Copy-Row DRAM (CROW), a flexible substrate that enables new mechanisms for improving DRAM performance, energy efficiency, and reliability. We use the CROW substrate to implement 1) a low-cost in-DRAM caching mechanism that lowers DRAM activation latency to frequently-accessed rows by 38\% and 2) a mechanism that avoids the use of short-retention-time rows to mitigate the performance and energy overhead of DRAM refresh operations. CROW's flexibility allows the implementation of both mechanisms at the same time. Our evaluations show that the two mechanisms synergistically improve system performance by 20.0\% and reduce DRAM energy by 22.3\% for memory-intensive four-core workloads, while incurring 0.48\% extra area overhead in the DRAM chip and 11.3 KiB storage overhead in the memory controller, and consuming 1.6\% of DRAM storage capacity, for one particular implementation.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {129–142},
numpages = {14},
keywords = {DRAM, energy, memory systems, performance, power, reliability},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322206,
author = {Liu, Sihang and Seemakhupt, Korakit and Pekhimenko, Gennady and Kolli, Aasheesh and Khan, Samira},
title = {Janus: optimizing memory and storage support for non-volatile memory systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322206},
doi = {10.1145/3307650.3322206},
abstract = {Non-volatile memory (NVM) technologies can manipulate persistent data directly in memory. Ensuring crash consistency of persistent data enforces that data updates reach all the way to NVM, which puts these write requests on the critical path. Recent literature sought to reduce this performance impact. However, prior works have not fully accounted for all the backend memory operations (BMOs) performed at the memory controller that are necessary to maintain persistent data in NVM. These BMOs include support for encryption, integrity protection, compression, deduplication, etc., necessary to provide security, endurance, and lifetime guarantees. These BMOs significantly increase the NVM write latency and exacerbate the performance degradation caused by the critical write requests. The goal of this work is to minimize the BMO overhead of write requests in an NVM system.The central challenge is to figure out how to optimize these seemingly dependent and monolithic BMOs. Our key insight is to decompose each BMO into a series of sub-operations and then reduce their overall latency through two mechanisms: (i) parallelize sub-operations across BMOs and (ii) pre-execute sub-operations off the critical path as soon as their inputs are ready. We expose a generic software interface that can be used to issue pre-execution requests compatible with common crash-consistency programming models and various BMOs. Based on these ideas, we propose Janus1 - a hardware-software co-design that parallelizes and pre-executes BMOs in an NVM system. We evaluate Janus in an NVM system that integrates encryption, integrity verification, and deduplication and issues pre-execution requests through the proposed software interface, either manually or using an automated compiler pass. Compared to a system that performs these operations serially, Janus achieves 2.35\texttimes{} and 2.00\texttimes{} speedup using manual and automated instrumentation, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {143–156},
numpages = {14},
keywords = {crash consistency, non-volatile memory, parallelization, pre-execution},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322252,
author = {Zubair, Kazi Abu and Awad, Amro},
title = {Anubis: ultra-low overhead and recovery time for secure non-volatile memories},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322252},
doi = {10.1145/3307650.3322252},
abstract = {Implementing secure Non-Volatile Memories (NVMs) is challenging, mainly due to the necessity to persist security metadata along with data. Unlike conventional secure memories, NVM-equipped systems are expected to recover data after crashes and hence security metadata must be recoverable as well. While prior work explored recovery of encryption counters, fewer efforts have been focused on recovering integrity-protected systems. In particular, how to recover Merkle Tree. We observe two major challenges for this. First, recovering parallelizable integrity trees, e.g., Intel's SGX trees, requires very special handling due to inter-level dependency. Second, the recovery time of practical NVM sizes (terabytes are expected) would take hours. Most data centers, cloud systems, intermittent-power devices and even personal computers, are anticipated to recover almost instantly after power restoration. In fact, this is one of the major promises of NVMs.In this paper, we propose Anubis, a novel hardware-only solution that speeds up recovery time by almost 107 times (from 8 hours to only 0.03 seconds). Moreover, we propose a novel and elegant way to recover inter-level dependent trees, as in Intel's SGX. Most importantly, while ensuring recoverability of one of the most challenging integrity-protection schemes among others, Anubis incurs performance overhead that is only 2\% higher than the state-of-the-art scheme, Osiris, which takes hours to recover systems with general Merkle Tree and fails to recover SGX-style trees.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {157–168},
numpages = {12},
keywords = {non-volatile memories, persistence, persistent security, security},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322221,
author = {Gubran, Ayub A. and Aamodt, Tor M.},
title = {Emerald: graphics modeling for SoC systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322221},
doi = {10.1145/3307650.3322221},
abstract = {Mobile systems-on-chips (SoCs) have become ubiquitous computing platforms, and, in recent years, they have become increasingly heterogeneous and complex. A typical SoC includes CPUs, graphics processor units (GPUs), image processors, video encoders/decoders, AI engines, digital signal processors (DSPs) and 2D engines among others [33, 70, 71]. One of the most significant SoC units in terms of both off-chip memory bandwidth and SoC die area is the GPU. In this paper, we present Emerald, a simulator that builds on existing tools to provide a unified model for graphics and GPGPU applications. Emerald enables OpenGL (v4.5) and OpenGL ES (v3.2) shaders to run on GPGPU-Sim's timing model and is integrated with gem5 and Android to simulate full SoCs. Emerald thus provides a platform for studying system-level SoC interactions while including the impact of graphics.We present two case studies using Emerald. First, we use Emerald's full-system mode to highlight the importance of system-wide interactions by studying and analyzing memory organization and scheduling schemes for SoC systems. Second, we use Emerald's standalone mode to evaluate a novel mechanism for balancing the graphics shading work assigned to each GPU core.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {169–182},
numpages = {14},
keywords = {GPU, SoC, graphics, simulation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322222,
author = {Oh, Yunho and Koo, Gunjae and Annavaram, Murali and Ro, Won Woo},
title = {Linebacker: preserving victim cache lines in idle register files of GPUs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322222},
doi = {10.1145/3307650.3322222},
abstract = {Modern GPUs suffer from cache contention due to the limited cache size that is shared across tens of concurrently running warps. To increase the per-warp cache size prior techniques proposed warp throttling which limits the number of active warps. Warp throttling leaves several registers to be dynamically unused whenever a warp is throttled. Given the stringent cache size limitation in GPUs this work proposes a new cache management technique named Linebacker (LB) that improves GPU performance by utilizing idle register file space as victim cache space. Whenever a CTA becomes inactive, linebacker backs up the registers of the throttled CTA to the off-chip memory. Then, linebacker utilizes the corresponding register file space as victim cache space. If any load instruction finds data in the victim cache line, the data is directly copied to the destination register through a simple register-register move operation. To further improve the efficiency of victim cache linebacker allocates victim cache space only to a select few load instructions that exhibit high data locality. Through a careful design of victim cache indexing and management scheme linebacker provides 29.0\% of speedup compared to the previously proposed warp throttling techniques.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {183–196},
numpages = {14},
keywords = {CTA scheduling, GPU, cache, register file},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322230,
author = {Sun, Yifan and Baruah, Trinayan and Mojumder, Saiful A. and Dong, Shi and Gong, Xiang and Treadway, Shane and Bao, Yuhui and Hance, Spencer and McCardwell, Carter and Zhao, Vincent and Barclay, Harrison and Ziabari, Amir Kavyan and Chen, Zhongliang and Ubal, Rafael and Abell\'{a}n, Jos\'{e} L. and Kim, John and Joshi, Ajay and Kaeli, David},
title = {MGPUSim: enabling multi-GPU performance modeling and optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322230},
doi = {10.1145/3307650.3322230},
abstract = {The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of Graphics Processing Units (GPUs). As single-GPU platforms struggle to satisfy these performance demands, multi-GPU platforms have started to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabric, runtime libraries, and associated programming models. The research community currently lacks a publicly available and comprehensive multi-GPU simulation framework to evaluate next-generation multi-GPU system designs.In this work, we present MGPUSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. MGPUSim comes with in-built support for multi-threaded execution to enable fast, parallelized, and accurate simulation. In terms of performance accuracy, MGPUSim differs by only 5.5\% on average from the actual GPU hardware. We also achieve a 3.5\texttimes{} and a 2.5\texttimes{} average speedup running functional emulation and detailed timing simulation, respectively, on a 4-core CPU, while delivering the same accuracy as serial simulation.We illustrate the flexibility and capability of the simulator through two concrete design studies. In the first, we propose the Locality API, an API extension that allows the GPU programmer to both avoid the complexity of multi-GPU programming, while precisely controlling data placement in the multi-GPU memory. In the second design study, we propose &lt;u&gt;P&lt;/u&gt;rogressive P&lt;u&gt;a&lt;/u&gt;ge &lt;u&gt;S&lt;/u&gt;plitting M&lt;u&gt;i&lt;/u&gt;gration (PASI), a customized multi-GPU memory management system enabling the hardware to progressively improve data placement. For a discrete 4-GPU system, we observe that the Locality API can speed up the system by 1.6\texttimes{} (geometric mean), and PASI can improve the system performance by 2.6\texttimes{} (geometric mean) across all benchmarks, compared to a unified 4-GPU platform.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {197–209},
numpages = {13},
keywords = {memory management, multi-GPU systems, simulation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322212,
author = {Pattnaik, Ashutosh and Tang, Xulong and Kayiran, Onur and Jog, Adwait and Mishra, Asit and Kandemir, Mahmut T. and Sivasubramaniam, Anand and Das, Chita R.},
title = {Opportunistic computing in GPU architectures},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322212},
doi = {10.1145/3307650.3322212},
abstract = {Data transfer overhead between computing cores and memory hierarchy has been a persistent issue for von Neumann architectures and the problem has only become more challenging with the emergence of manycore systems. A conceptually powerful approach to mitigate this overhead is to bring the computation closer to data, known as Near Data Computing (NDC). Recently, NDC has been investigated in different flavors for CPU-based multicores, while the GPU domain has received little attention. In this paper, we present a novel NDC solution for GPU architectures with the objective of minimizing on-chip data transfer between the computing cores and Last-Level Cache (LLC). To achieve this, we first identify frequently occurring Load-Compute-Store instruction chains in GPU applications. These chains, when offloaded to a compute unit closer to where the data resides, can significantly reduce data movement. We develop two offloading techniques, called LLC-Compute and Omni-Compute. The first technique, LLC-Compute, augments the LLCs with computational hardware for handling the computation offloaded to them. The second technique (Omni-Compute) employs simple bookkeeping hardware to enable GPU cores to compute instructions offloaded by other GPU cores. Our experimental evaluations on nine GPGPU workloads indicate that the LLC-Compute technique provides, on an average, 19\% performance improvement (IPC), 11\% performance/watt improvement, and 29\% reduction in on-chip data movement compared to the baseline GPU design. The Omni-Compute design boosts these benefits to 31\%, 16\% and 44\%, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {210–223},
numpages = {14},
keywords = {GPU, computation offloading, near data computing},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322224,
author = {Ganguly, Debashis and Zhang, Ziyu and Yang, Jun and Melhem, Rami},
title = {Interplay between hardware prefetcher and page eviction policy in CPU-GPU unified virtual memory},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322224},
doi = {10.1145/3307650.3322224},
abstract = {Memory capacity in GPGPUs is a major challenge for data-intensive applications with their ever increasing memory requirement. To fit a workload into the limited GPU memory space, a programmer needs to manually divide the workload by tiling the working set and perform user-level data migration. To relieve the programmer from this burden, Unified Virtual Memory (UVM) was developed to support on-demand paging and migration, transparent to the user. It further takes care of the memory over-subscription issue by automatically performing page replacement in an oversubscribed GPU memory situation. However, we found that na\"{\i}ve handling of page faults can cause orders of magnitude slowdown in performance. Moreover, we observed that although prefetching of data from CPU to GPU can hide the page fault latency, the difference among various prefetching mechanisms can lead to drastically different performance results. To this end, we performed extensive experiments on GeForceGTX 1080ti GPUs with PCI-e 3.0 16x to discover that there exists an effective prefetch mechanism to enhance locality in GPU memory. However, as the GPU memory is filled to its capacity, such prefetching mechanism quickly proves to be counterproductive due to locality unaware eviction policy. This necessitates the design of new eviction policies that are aware of the hardware prefetcher semantics. We propose two new programmer-agnostic, locality-aware pre-eviction policies which leverage the mechanics of existing hardware prefetcher and thus incur no additional implementation and performance overhead. We demonstrate that combining the proposed tree-based pre-eviction policy with the hardware prefetcher provides an average of 93\% and 18.5\% performance speed-up compared to LRU based 4KB and 2MB page replacement strategies, respectively. We further examine the memory access pattern of GPU workloads under consideration to analyze the achieved performance speed-up.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {224–235},
numpages = {12},
keywords = {gpu, hardware prefetcher, page eviction policy, unified virtual memory},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322271,
author = {Yang, Tzu-Hsien and Cheng, Hsiang-Yun and Yang, Chia-Lin and Tseng, I-Ching and Hu, Han-Wen and Chang, Hung-Sheng and Li, Hsiang-Pang},
title = {Sparse ReRAM engine: joint exploration of activation and weight sparsity in compressed neural networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322271},
doi = {10.1145/3307650.3322271},
abstract = {Exploiting model sparsity to reduce ineffectual computation is a commonly used approach to achieve energy efficiency for DNN inference accelerators. However, due to the tightly coupled crossbar structure, exploiting sparsity for ReRAM-based NN accelerator is a less explored area. Existing architectural studies on ReRAM-based NN accelerators assume that an entire crossbar array can be activated in a single cycle. However, due to inference accuracy considerations, matrix-vector computation must be conducted in a smaller granularity in practice, called Operation Unit (OU). An OU-based architecture creates a new opportunity to exploit DNN sparsity. In this paper, we propose the first practical Sparse ReRAM Engine that exploits both weight and activation sparsity. Our evaluation shows that the proposed method is effective in eliminating ineffectual computation, and delivers significant performance improvement and energy savings.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {236–249},
numpages = {14},
keywords = {ReRAM, accelerator architecture, neural network, sparsity},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322214,
author = {Jang, Hanhwi and Kim, Joonsung and Jo, Jae-Eon and Lee, Jaewon and Kim, Jangwoo},
title = {MnnFast: a fast and scalable system architecture for memory-augmented neural networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322214},
doi = {10.1145/3307650.3322214},
abstract = {Memory-augmented neural networks are getting more attention from many researchers as they can make an inference with the previous history stored in memory. Especially, among these memory-augmented neural networks, memory networks are known for their huge reasoning power and capability to learn from a large number of inputs rather than other networks. As the size of input datasets rapidly grows, the necessity of large-scale memory networks continuously arises. Such large-scale memory networks provide excellent reasoning power; however, the current computer infrastructure cannot achieve scalable performance due to its limited system architecture.In this paper, we propose MnnFast, a novel system architecture for large-scale memory networks to achieve fast and scalable reasoning performance. We identify the performance problems of the current architecture by conducting extensive performance bottleneck analysis. Our in-depth analysis indicates that the current architecture suffers from three major performance problems: high memory bandwidth consumption, heavy computation, and cache contention. To overcome these performance problems, we propose three novel optimizations. First, to reduce the memory bandwidth consumption, we propose a new column-based algorithm with streaming which minimizes the size of data spills and hides most of the off-chip memory accessing overhead. Second, to decrease the high computational overhead, we propose a zero-skipping optimization to bypass a large amount of output computation. Lastly, to eliminate the cache contention, we propose an embedding cache dedicated to efficiently cache the embedding matrix.Our evaluations show that MnnFast is significantly effective in various types of hardware: CPU, GPU, and FPGA. MnnFast improves the overall throughput by up to 5.38\texttimes{}, 4.34\texttimes{}, and 2.01\texttimes{} on CPU, GPU, and FPGA respectively. Also, compared to CPU-based MnnFast, our FPGA-based MnnFast achieves 6.54\texttimes{} higher energy efficiency.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {250–263},
numpages = {14},
keywords = {accelerator, algorithm-hardware co-design, architecture, attention-based neural networks, computation/dataflow optimization, machine learning, memory networks, parallel algorithm},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322258,
author = {Deng, Chunhua and Sun, Fangxuan and Qian, Xuehai and Lin, Jun and Wang, Zhongfeng and Yuan, Bo},
title = {TIE: energy-efficient tensor train-based inference engine for deep neural network},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322258},
doi = {10.1145/3307650.3322258},
abstract = {In the era of artificial intelligence (AI), deep neural networks (DNNs) have emerged as the most important and powerful AI technique. However, large DNN models are both storage and computation intensive, posing significant challenges for adopting DNNs in resource-constrained scenarios. Thus, model compression becomes a crucial technique to ensure wide deployment of DNNs.This paper advances the state-of-the-art by considering tensor train (TT) decomposition, an very promising but yet explored compression technique in architecture domain. The method features with the extremely high compression ratio. However, the challenge is that the inference on the TT-format DNN models inherently incurs massive amount of redundant computations, causing significant energy consumption. Thus, the straightforward application of TT decomposition is not feasible.To address this fundamental challenge, this paper develops a computation-efficient inference scheme for TT-format DNN, which enjoys two key merits: 1) it achieves theoretical limit of number of multiplications, thus eliminating all redundant computations; and 2) the multi-stage processing scheme reduces the intensive memory access to all tensor cores, bringing significant energy saving.Based on the novel inference scheme, we develop TIE, a TT-format compressed DNN-targeted inference engine. TIE is highly flexible, supporting different types of networks for different needs. A 16-processing elements (PE) prototype is implemented using CMOS 28nm technology. Operating on 1000MHz, the TIE accelerator consumes 1.74mm2 and 154.8mW. Compared with EIE, TIE achieves 7.22\texttimes{} ~ 10.66\texttimes{} better area efficiency and 3.03\texttimes{} ~ 4.48\texttimes{} better energy efficiency on different workloads, respectively. Compared with CirCNN, TIE achieves 5.96\texttimes{} and 4.56\texttimes{} higher throughput and energy efficiency, respectively. The results show that TIE exhibits significant advantages over state-of-the-art solutions.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {264–278},
numpages = {15},
keywords = {acceleration, compression, deep learning, tensor-train decomposition},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322259,
author = {Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian},
title = {Accelerating distributed reinforcement learning with in-switch computing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322259},
doi = {10.1145/3307650.3322259},
abstract = {Reinforcement learning (RL) has attracted much attention recently, as new and emerging AI-based applications are demanding the capabilities to intelligently react to environment changes. Unlike distributed deep neural network (DNN) training, the distributed RL training has its unique workload characteristics - it generates orders of magnitude more iterations with much smaller sized but more frequent gradient aggregations. More specifically, our study with typical RL algorithms shows that their distributed training is latency critical and that the network communication for gradient aggregation occupies up to 83.2\% of the execution time of each training iteration.In this paper, we present iSwitch, an in-switch acceleration solution that moves the gradient aggregation from server nodes into the network switches, thus we can reduce the number of network hops for gradient aggregation. This not only reduces the end-to-end network latency for synchronous training, but also improves the convergence with faster weight updates for asynchronous training. Upon the in-switch accelerator, we further reduce the synchronization overhead by conducting on-the-fly gradient aggregation at the granularity of network packets rather than gradient vectors. Moreover, we rethink the distributed RL training algorithms and also propose a hierarchical aggregation mechanism to further increase the parallelism and scalability of the distributed RL training at rack scale.We implement iSwitch using a real-world programmable switch NetFPGA board. We extend the control and data plane of the programmable switch to support iSwitch without affecting its regular network functions. Compared with state-of-the-art distributed training approaches, iSwitch offers a system-level speedup of up to 3.66\texttimes{} for synchronous distributed training and 3.71\texttimes{} for asynchronous distributed training, while achieving better scalability.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {279–291},
numpages = {13},
keywords = {distributed machine learning, in-network computing, in-switch accelerator, reinforcement learning},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322263,
author = {Zhang, Jiaqi and Chen, Xiangru and Song, Mingcong and Li, Tao},
title = {Eager pruning: algorithm and architecture support for fast training of deep neural networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322263},
doi = {10.1145/3307650.3322263},
abstract = {Today's big and fast data and the changing circumstance require fast training of Deep Neural Networks (DNN) in various applications. However, training a DNN with tons of parameters involves intensive computation. Enlightened by the fact that redundancy exists in DNNs and the observation that the ranking of the significance of the weights changes slightly during training, we propose Eager Pruning, which speeds up DNN training by moving pruning to an early stage.Eager Pruning is supported by an algorithm and architecture co-design. The proposed algorithm dictates the architecture to identify and prune insignificant weights during training without accuracy loss. A novel architecture is designed to transform the reduced training computation into performance improvement. Our proposed Eager Pruning system gains an average of 1.91x speedup over state-of-the-art hardware accelerator and 6.31x energy-efficiency over Nvidia GPUs.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {292–303},
numpages = {12},
keywords = {neural network pruning, neural network training, software-hardware co-design},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322255,
author = {Sharify, Sayeh and Lascorz, Alberto Delmas and Mahmoud, Mostafa and Nikolic, Milos and Siu, Kevin and Stuart, Dylan Malone and Poulos, Zissis and Moshovos, Andreas},
title = {Laconic deep learning inference acceleration},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322255},
doi = {10.1145/3307650.3322255},
abstract = {We present a method for transparently identifying ineffectual computations during inference with Deep Learning models. Specifically, by decomposing multiplications down to the bit level, the amount of work needed by multiplications during inference can be potentially reduced by at least 40\texttimes{} across a wide selection of neural networks (8b and 16b). This method produces numerically identical results and does not affect overall accuracy. We present Laconic, a hardware accelerator that implements this approach to boost energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators: an optimized DaDianNao-like design [13], Eyeriss [15], SCNN [71], Pragmatic [3], and BitFusion [83]. We study 16b, 8b, and 1b/2b fixed-point quantized models.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {304–317},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322228,
author = {Skarlatos, Dimitrios and Yan, Mengjia and Gopireddy, Bhargava and Sprabery, Read and Torrellas, Josep and Fletcher, Christopher W.},
title = {MicroScope: enabling microarchitectural replay attacks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322228},
doi = {10.1145/3307650.3322228},
abstract = {The popularity of hardware-based Trusted Execution Environments (TEEs) has recently skyrocketed with the introduction of Intel's Software Guard Extensions (SGX). In SGX, the user process is protected from supervisor software, such as the operating system, through an isolated execution environment called an enclave. Despite the isolation guarantees provided by TEEs, numerous microarchitectural side channel attacks have been demonstrated that bypass their defense mechanisms. But, not all hope is lost for defenders: many modern fine-grain, high-resolution side channels---e.g., execution unit port contention---introduce large amounts of noise, complicating the adversary's task to reliably extract secrets.In this work, we introduce Microarchitectural Replay Attacks, whereby an SGX adversary can denoise nearly arbitrary microarchitectural side channels in a single run of the victim, by causing the victim to repeatedly replay on a page faulting instruction. We design, implement, and demonstrate our ideas in a framework, called MicroScope, and use it to denoise notoriously noisy side channels. Our main result shows how MicroScope can denoise the execution unit port contention channel. Specifically, we show how Micro-Scope can reliably detect the presence or absence of as few as two divide instructions in a single logical run of the victim program. Such an attack could be used to detect subnormal input to individual floating-point instructions, or infer branch directions in an enclave despite today's countermeasures that flush the branch predictor at the enclave boundary. We also use MicroScope to single-step and denoise a cache-based attack on the OpenSSL implementation of AES. Finally, we discuss the broader implications of microarchitectural replay attacks---as well as discuss other mechanisms that can cause replays.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {318–331},
numpages = {14},
keywords = {operating system, security, side-channel, virtual memory},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3326635,
author = {Yan, Mengjia and Wen, Jen-Yang and Fletcher, Christopher W. and Torrellas, Josep},
title = {SecDir: a secure directory to defeat directory side-channel attacks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326635},
doi = {10.1145/3307650.3326635},
abstract = {Directories for cache coherence have been recently shown to be vulnerable to conflict-based side-channel attacks. By forcing directory conflicts, an attacker can evict victim directory entries, which in turn trigger the eviction of victim cache lines from private caches. This evidence strongly suggests that directories need to be redesigned for security. The key to a secure directory is to block interference between processes. Sadly, in an environment with many cores, this is hard or expensive to do.This paper presents the first design of a scalable secure directory. We call it SecDir. SecDir takes part of the storage used by a conventional directory and re-assigns it to per-core private directory areas used in a victim-cache manner called Victim Directories (VDs). The partitioned nature of VDs prevents directory interference across cores, defeating directory side-channel attacks. The VD of a core is distributed, and holds as many entries as lines in the private L2 cache of the core. To minimize victim self-conflicts in a VD during an attack, a VD is organized as a cuckoo directory. Such a design also obscures the victim's conflict patterns from the attacker. For our evaluation, we model with simulations the directory of an Intel Skylake-X server with and without SecDir. Our results show that SecDir has a negligible performance overhead. Furthermore, SecDir is area-efficient.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {332–345},
numpages = {14},
keywords = {cache-coherence directories, cuckoo hashing, side-channel attacks},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322238,
author = {Deng, Shuwen and Xiong, Wenjie and Szefer, Jakub},
title = {Secure TLBs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322238},
doi = {10.1145/3307650.3322238},
abstract = {This paper focuses on a new attack vector in modern processors: the timing-based side and covert channel attacks due to the Translation Look-aside Buffers (TLBs). This paper first presents a novel three-step modeling approach that is used to exhaustively enumerate all possible TLB timing-based vulnerabilities. Building on the three-step model, this paper then shows how to automatically generate micro security benchmarks that test for the TLB vulnerabilities. After showing the insecurity of standard TLBs, two new secure TLB designs are presented: a Static-Partition (SP) TLB and a Random-Fill (RF) TLB. The new secure TLBs are evaluated using the Rocket Core implementation of the RISC-V processor architecture enhanced with the two new designs. The three-step model and the security benchmarks are used to analyze the security of the new designs in simulation. Based on the analysis, the proposed secure TLBs can defend not only against the previously publicized attacks but also against other new timing-based attacks in TLBs found using the new three-step model. The performance overhead is evaluated on an FPGA-based setup, and, for example, shows that the RF TLB has less than 10\% overhead while defending all the attacks.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {346–359},
numpages = {14},
keywords = {TLBs, timing attack defenses, timing side and covert channel attacks},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322246,
author = {Qureshi, Moinuddin K.},
title = {New attacks and defense for encrypted-address cache},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322246},
doi = {10.1145/3307650.3322246},
abstract = {Conflict-based cache attacks can allow an adversary to infer the access pattern of a co-running application by orchestrating evictions via cache conflicts. Such attacks can be mitigated by randomizing the location of the lines in the cache. Our recent proposal, CEASER, makes cache randomization practical by accessing the cache using an encrypted address and periodically changing the encryption key. CEASER was analyzed with the state-of-the-art algorithm on forming eviction sets, and the analysis showed that CEASER with a Remap-Rate of 1\% is sufficient to tolerate years of attack.In this paper, we present two new attacks that significantly push the state-of-the-art in forming eviction sets. Our first attack reduces the time required to form the eviction set from O(L2) to O(L), where L is the number of lines in the attack. This attack is 35x faster than the best-known attack and requires that the Remap-Rate of CEASER be increased to 35\%. Our second attack exploits the replacement policy (we analyze LRU, RRIP, and Random) to form eviction set quickly and requires that the Remap-Rate of CEASER be increased to more than 100\%, incurring impractical overheads.To improve the robustness of CEASER against these attacks in a practical manner, we propose Skewed-CEASER (CEASER-S), which divides the cache ways into multiple partitions and maps the cache line to be resident in a different set in each partition. This design significantly improves the robustness of CEASER, as the attacker must form an eviction set that can dislodge the line from multiple possible locations. We show that CEASER-S can tolerate years of attacks while retaining a Remap-Rate of 1\%. CEASER-S incurs negligible slowdown (within 1\%) and a storage overhead of less than 100 bytes for the newly added structures.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {360–371},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322265,
author = {Aga, Shaizeen and Narayanasamy, Satish},
title = {InvisiPage: oblivious demand paging for secure enclaves},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322265},
doi = {10.1145/3307650.3322265},
abstract = {State-of-art secure processors like Intel SGX remain susceptible to leaking page-level address trace of an application via the page fault channel in which a malicious OS induces spurious page faults and deduces application's secrets from it. Prior works which fix this vulnerability do not provision for OS demand paging to be oblivious. In this work, we present InvisiPage which obfuscates page fault channel while simultaneously making OS demand paging oblivious. To do so, InvisiPage first carefully distributes page management actions between the application and the OS. Second, InvisiPage secures application's page management interactions with the OS using a novel construct which is derived from Oblivious RAM (ORAM) but is customized for page management. Finally, we lower overheads of our approach by reducing page management interactions with the OS via a novel memory partition. For a suite of cloud applications which process sensitive data we show that page fault channel can be tackled while enabling oblivious demand paging at low overheads.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {372–384},
numpages = {13},
keywords = {Intel SGX, ORAM, page fault channel},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322232,
author = {Lee, Eojin and Kang, Ingab and Lee, Sukhan and Suh, G. Edward and Ahn, Jung Ho},
title = {TWiCe: preventing row-hammering by exploiting time window counters},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322232},
doi = {10.1145/3307650.3322232},
abstract = {Computer systems using DRAM are exposed to row-hammer (RH) attacks, which can flip data in a DRAM row without directly accessing a row but by frequently activating its adjacent ones. There have been a number of proposals to prevent RH, but they either incur large area overhead, suffer from noticeable performance drop on adversarial memory access patterns, or provide probabilistic protection with no capability to detect attacks.In this paper, we propose a new counter-based RH prevention solution named Time Window Counter (TWiCe) based row refresh, which accurately detects potential RH attacks only using a small number of counters with a minimal performance impact. We first make a key observation that the number of rows that can cause RH is limited by the maximum values of row activation frequency and DRAM cell retention time. We calculate the maximum number of required counter entries per DRAM bank, with which TWiCe prevents RH with a strong deterministic guarantee. We leverage pseudo-associative cache design and separate the TWiCe table to further reduce area and energy overheads. TWiCe incurs no performance overhead on normal DRAM operations and less than 0.7\% area and energy overheads over contemporary DRAM devices. Our evaluation shows that TWiCe makes no more than 0.006\% of additional DRAM row activations for adversarial memory access patterns including RH attack scenarios.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {385–396},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322257,
author = {Fujiki, Daichi and Mahlke, Scott and Das, Reetuparna},
title = {Duality cache for data parallel acceleration},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322257},
doi = {10.1145/3307650.3322257},
abstract = {Duality Cache is an in-cache computation architecture that enables general purpose data parallel applications to run on caches. This paper presents a holistic approach of building Duality Cache system stack with techniques of performing in-cache floating point arithmetic and transcendental functions, enabling a data-parallel execution model, designing a compiler that accepts existing CUDA programs, and providing flexibility in adopting for various workload characteristics.Exposure to massive parallelism that exists in the Duality Cache architecture improves performance of GPU benchmarks by 3.6\texttimes{} and OpenACC benchmarks by 4.0\texttimes{} over a server class GPU. Re-purposing existing caches provides 72.6\texttimes{} better performance for CPUs with only 3.5\% of area cost. Duality Cache reduces energy by 5.8\texttimes{} over GPUs and 21\texttimes{} over CPUs.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {397–410},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322235,
author = {Zhao, Xia and Adileh, Almutaz and Yu, Zhibin and Wang, Zhiying and Jaleel, Aamer and Eeckhout, Lieven},
title = {Adaptive memory-side last-level GPU caching},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322235},
doi = {10.1145/3307650.3322235},
abstract = {Emerging GPU applications exhibit increasingly high computation demands which has led GPU manufacturers to build GPUs with an increasingly large number of streaming multiprocessors (SMs). Providing data to the SMs at high bandwidth puts significant pressure on the memory hierarchy and the Network-on-Chip (NoC). Current GPUs typically partition the memory-side last-level cache (LLC) in equally-sized slices that are shared by all SMs. Although a shared LLC typically results in a lower miss rate, we find that for workloads with high degrees of data sharing across SMs, a private LLC leads to a significant performance advantage because of increased bandwidth to replicated cache lines across different LLC slices.In this paper, we propose adaptive memory-side last-level GPU caching to boost performance for sharing-intensive workloads that need high bandwidth to read-only shared data. Adaptive caching leverages a lightweight performance model that balances increased LLC bandwidth against increased miss rate under private caching. In addition to improving performance for sharing-intensive workloads, adaptive caching also saves energy in a (co-designed) hierarchical two-stage crossbar NoC by power-gating and bypassing the second stage if the LLC is configured as a private cache. Our experimental results using 17 GPU workloads show that adaptive caching improves performance by 28.1\% on average (up to 38.1\%) compared to a shared LLC for sharing-intensive workloads. In addition, adaptive caching reduces NoC energy by 26.6\% on average (up to 29.7\%) and total system energy by 6.1\% on average (up to 27.2\%) when configured as a private cache. Finally, we demonstrate through a GPU NoC design space exploration that a hierarchical two-stage crossbar is both more power- and area-efficient than full and concentrated crossbars with the same bisection bandwidth, thus providing a low-cost cooperative solution to exploit workload sharing behavior in memory-side last-level caches.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {411–423},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322254,
author = {Segura, Albert and Arnau, Jose-Maria and Gonz\'{a}lez, Antonio},
title = {SCU: a GPU stream compaction unit for graph processing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322254},
doi = {10.1145/3307650.3322254},
abstract = {Graph processing algorithms are key in many emerging applications in areas such as machine learning and data analytics. Although the processing of large scale graphs exhibits a high degree of parallelism, the memory access pattern tend to be highly irregular, leading to poor GPGPU efficiency due to memory divergence. To ameliorate this issue, GPGPU applications perform a stream compaction operation each iteration of the algorithm to extract the subset of active nodes/edges, so subsequent steps work on compacted dataset.We show that GPGPU architectures are inefficient for stream compaction, and propose to offload this task to a programmable Stream Compaction Unit (SCU) tailored to the requirements of this kernel. The SCU is a small unit tightly integrated in the GPU that efficiently gathers the active nodes/edges into a compacted array in memory. Applications can make use of it through a simple API. The remaining steps of the graph-based algorithm are executed on the GPU cores taking benefit of the large amount of parallelism in the GPU, but they operate on the SCU-prepared data and achieve larger memory coalescing and, hence, much higher efficiency. Besides, the SCU performs filtering of repeated and already visited nodes during the compaction process, significantly reducing GPGPU workload, and writes the compacted nodes/edges in an order that improves memory coalescing by reducing memory divergence.We evaluate the performance of a state-of-the-art GPGPU architecture extended with our SCU for a wide variety of applications. Results show that for high-performance and for low-power GPU systems the SCU achieves speedups of 1.37x and 2.32x, 84.7\% and 69\% energy savings, and an area increase of 3.3\% and 4.1\% respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {424–435},
numpages = {12},
keywords = {GPGPU, graph processing, stream compaction},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322269,
author = {Alves, Ricardo and Ros, Alberto and Black-Schaffer, David and Kaxiras, Stefanos},
title = {Filter caching for free: the untapped potential of the store-buffer},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322269},
doi = {10.1145/3307650.3322269},
abstract = {Modern processors contain store-buffers to allow stores to retire under a miss, thus hiding store-miss latency. The store-buffer needs to be large (for performance) and searched on every load (for correctness), thereby making it a costly structure in both area and energy. Yet on every load, the store-buffer is probed in parallel with the L1 and TLB, with no concern for the store-buffer's intrinsic hit rate or whether a store-buffer hit can be predicted to save energy by disabling the L1 and TLB probes.In this work we cache data that have been written back to memory in a unified store-queue/buffer/cache, and predict hits to avoid L1/TLB probes and save energy. By dynamically adjusting the allocation of entries between the store-queue/buffer/cache, we can achieve nearly optimal reuse, without causing stalls. We are able to do this efficiently and cheaply by recognizing key properties of stores: free caching (since they must be written into the store-buffer for correctness we need no additional data movement), cheap coherence (since we only need to track state changes of the local, dirty data in the store-buffer), and free and accurate hit prediction (since the memory dependence predictor already does this for scheduling).As a result, we are able to increase the store-buffer hit rate and reduce store-buffer/TLB/L1 dynamic energy by 11.8\% (up to 26.4\%) on SPEC2006 without hurting performance (average IPC improvements of 1.5\%, up to 4.7\%). The cost for these improvements is a 0.2\% increase in L1 cache capacity (1 bit per line) and one additional tail pointer in the store-buffer.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {436–448},
numpages = {13},
keywords = {energy efficient architecture, filter-cache, memory architecture, single thread performance, store-buffer},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322225,
author = {Wu, Hao and Nathella, Krishnendra and Sunwoo, Dam and Jain, Akanksha and Lin, Calvin},
title = {Efficient metadata management for irregular data prefetching},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322225},
doi = {10.1145/3307650.3322225},
abstract = {Temporal prefetchers have the potential to prefetch arbitrary memory access patterns, but they require large amounts of metadata that must typically be stored in DRAM. In 2013, the Irregular Stream Buffer (ISB), showed how this metadata could be cached on chip and managed implicitly by synchronizing its contents with that of the TLB. This paper reveals the inefficiency of that approach and presents a new metadata management scheme that uses a simple metadata prefetcher to feed the metadata cache. The result is the Managed ISB (MISB), a temporal prefetcher that significantly advances the state-of-the-art in terms of both traffic overhead and IPC.Using a highly accurate proprietary simulator for single-core workloads, and using the ChampSim simulator for multi-core workloads, we evaluate MISB on programs from the SPEC CPU 2006 and CloudSuite benchmarks suites. Our results show that for single-core workloads, MISB improves performance by 22.7\%, compared to 10.6\% for an idealized STMS and 4.5\% for a realistic ISB. MISB also significantly reduces off-chip traffic; for SPEC, MISB's traffic overhead of 70\% is roughly one fifth of STMS's (342\%) and one sixth of ISB's (411\%). On 4-core multi-programmed workloads, MISB improves performance by 27.5\%, compared to 13.6\% for idealized STMS. For CloudSuite, MISB improves performance by 12.8\% (vs. 6.0\% for idealized STMS), while achieving a traffic reduction of 7 \texttimes{} (83.5\% for MISB vs. 572.3\% for STMS).},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {449–461},
numpages = {13},
keywords = {CPUs, data prefetching, irregular temporal prefetching},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322234,
author = {Ayers, Grant and Nagendra, Nayana Prasad and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
title = {AsmDB: understanding and mitigating front-end stalls in warehouse-scale computers},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322234},
doi = {10.1145/3307650.3322234},
abstract = {The large instruction working sets of private and public cloud workloads lead to frequent instruction cache misses and costs in the millions of dollars. While prior work has identified the growing importance of this problem, to date, there has been little analysis of where the misses come from, and what the opportunities are to improve them. To address this challenge, this paper makes three contributions. First, we present the design and deployment of a new, always-on, fleet-wide monitoring system, AsmDB, that tracks front-end bottlenecks. AsmDB uses hardware support to collect bursty execution traces, fleet-wide temporal and spatial sampling, and sophisticated offline post-processing to construct full-program dynamic control-flow graphs. Second, based on a longitudinal analysis of AsmDB data from real-world online services, we present two detailed insights on the sources of front-end stalls: (1) cold code that is brought in along with hot code leads to significant cache fragmentation and a corresponding large number of instruction cache misses; (2) distant branches and calls that are not amenable to traditional cache locality or next-line prefetching strategies account for a large fraction of cache misses. Third, we prototype two optimizations that target these insights. For misses caused by fragmentation, we focus on memcmp, one of the hottest functions contributing to cache misses, and show how fine-grained layout optimizations lead to significant benefits. For misses at the targets of distant jumps, we propose new hardware support for software code prefetching and prototype a new feedback-directed compiler optimization that combines static program flow analysis with dynamic miss profiles to demonstrate significant benefits for several large warehouse-scale workloads. Improving upon prior work, our proposal avoids invasive hardware modifications by prefetching via software in an efficient and scalable way. Simulation results show that such an approach can eliminate up to 96\% of instruction cache misses with negligible overheads.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {462–473},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322236,
author = {Jiang, Weixiang and Jia, Ziyang and Feng, Sirui and Liu, Fangming and Jin, Hai},
title = {Fine-grained warm water cooling for improving datacenter economy},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322236},
doi = {10.1145/3307650.3322236},
abstract = {Driven by the increasing power consumption of datacenters, the industry is focusing more on water cooling for improving the energy efficiency. Using warm water to cool servers has been considered as an efficient method to reduce the cooling energy. However, warm water cooling may lead to the risk of cooling failure and its energy efficiency suffers from the thermal imbalance among servers, due to the lack of fine-grained cooling control. In this paper, we propose a hybrid cooling architecture design that incorporates thermoelectric cooler into the water cooling system, to deal with cooling mismatching in a fine-grained manner. We exploit the warm water cooling strategy and design an adaptive cooling control framework according to workload variations, to make water cooling system more economical for datacenters. We evaluate the hybrid water cooling design based on a real hardware prototype and cluster traces from Google and Alibaba. Compared with conventional water cooling system, our hybrid water cooling system can reduce the energy consumption by 58.72\%~78.43\% to handle the cooling mismatching.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {474–486},
numpages = {13},
keywords = {datacenter energy, temperature, thermoelectric cooler, water cooling},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322251,
author = {Chen, Huili and Fu, Cheng and Rouhani, Bita Darvish and Zhao, Jishen and Koushanfar, Farinaz},
title = {DeepAttest: an end-to-end attestation framework for deep neural networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322251},
doi = {10.1145/3307650.3322251},
abstract = {Emerging hardware architectures for Deep Neural Networks (DNNs) are being commercialized and considered as the hardware-level Intellectual Property (IP) of the device providers. However, these intelligent devices might be abused and such vulnerability has not been identified. The unregulated usage of intelligent platforms and the lack of hardware-bounded IP protection impair the commercial advantage of the device provider and prohibit reliable technology transfer. Our goal is to design a systematic methodology that provides hardware-level IP protection and usage control for DNN applications on various platforms. To address the IP concern, we present DeepAttest, the first on-device DNN attestation method that certifies the legitimacy of the DNN program mapped to the device. DeepAttest works by designing a device-specific fingerprint which is encoded in the weights of the DNN deployed on the target platform. The embedded fingerprint (FP) is later extracted with the support of the Trusted Execution Environment (TEE). The existence of the pre-defined FP is used as the attestation criterion to determine whether the queried DNN is authenticated. Our attestation framework ensures that only authorized DNN programs yield the matching FP and are allowed for inference on the target device. DeepAttest provisions the device provider with a practical solution to limit the application usage of her manufactured hardware and prevents unauthorized or tampered DNNs from execution.We take an Algorithm/Software/Hardware co-design approach to optimize DeepAttest's overhead in terms of latency and energy consumption. To facilitate the deployment, we provide a high-level API of DeepAttest that can be seamlessly integrated into existing deep learning frameworks and TEEs for hardware-level IP protection and usage control. Extensive experiments corroborate the fidelity, reliability, security, and efficiency of DeepAttest on various DNN benchmarks and TEE-supported platforms.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {487–498},
numpages = {12},
keywords = {IP protection, attestation, deep neural networks, software/hardware co-design},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3326634,
author = {Wang, Yuzhao and Li, Lele and Wu, You and Yu, Junqing and Yu, Zhibin and Qian, Xuehai},
title = {Retracted on May 10, 2023: TPShare: a time-space sharing scheduling abstraction for shared cloud via vertical labels},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326634},
doi = {10.1145/3307650.3326634},
abstract = {NOTICE OF RETRACTION: This Work has been retracted by ACM because one of the authors of this Work violated the anonymity and independence of the review process for their paper "TPShare: a time-space sharing scheduling abstraction for shared cloud via vertical labels" in the Proceedings of the 46th International Symposium on Computer Architecture (ISCA '19). Association for Computing Machinery, New York, NY, USA, 499-512.https://dl.acm.org/doi/10.1145/3307650.3326634},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {499–512},
numpages = {14},
keywords = {cloud computing, resource management, task scheduling},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322227,
author = {Sriraman, Akshitha and Dhanotia, Abhishek and Wenisch, Thomas F.},
title = {SoftSKU: optimizing server architectures for microservice diversity @scale},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322227},
doi = {10.1145/3307650.3322227},
abstract = {The variety and complexity of microservices in warehouse-scale data centers has grown precipitously over the last few years to support a growing user base and an evolving product portfolio. Despite accelerating microservice diversity, there is a strong requirement to limit diversity in underlying server hardware to maintain hardware resource fungibility, preserve procurement economies of scale, and curb qualification/test overheads. As such, there is an urgent need for strategies that enable limited server CPU architectures (a.k.a "SKUs") to provide performance and energy efficiency over diverse microservices. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook.Our characterization reveals profound diversity in OS and I/O interaction, cache misses, memory bandwidth utilization, instruction mix, and CPU stall behavior. Whereas customizing a CPU SKU for each microservice might be beneficial, it is prohibitive. Instead, we argue for "soft SKUs", wherein we exploit coarse-grain (e.g., boot time) configuration knobs to tune the platform for a particular microservice. We develop a tool, μSKU, that automates search over a soft-SKU design space using A/B testing in production and demonstrate how it can obtain statistically significant gains (up to 7.2\% and 4.5\% performance improvement over stock and production servers, respectively) with no additional hardware requirements.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {513–526},
numpages = {14},
keywords = {microservice, resource fungibility, soft SKU},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322273,
author = {Murali, Prakash and Linke, Norbert Matthias and Martonosi, Margaret and Abhari, Ali Javadi and Nguyen, Nhung Hong and Alderete, Cinthia Huerta},
title = {Full-stack, real-system quantum computer studies: architectural comparisons and design insights},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322273},
doi = {10.1145/3307650.3322273},
abstract = {In recent years, Quantum Computing (QC) has progressed to the point where small working prototypes are available for use. Termed Noisy Intermediate-Scale Quantum (NISQ) computers, these prototypes are too small for large benchmarks or even for Quantum Error Correction (QEC), but they do have sufficient resources to run small benchmarks, particularly if compiled with optimizations to make use of scarce qubits and limited operation counts and coherence times. QC has not yet, however, settled on a particular preferred device implementation technology, and indeed different NISQ prototypes implement qubits with very different physical approaches and therefore widely-varying device and machine characteristics.Our work performs a full-stack, benchmark-driven hardware-software analysis of QC systems. We evaluate QC architectural possibilities, software-visible gates, and software optimizations to tackle fundamental design questions about gate set choices, communication topology, the factors affecting benchmark performance and compiler optimizations. In order to answer key cross-technology and cross-platform design questions, our work has built the first top-to-bottom toolflow to target different qubit device technologies, including superconducting and trapped ion qubits which are the current QC front-runners. We use our toolflow, TriQ, to conduct real-system measurements on seven running QC prototypes from three different groups, IBM, Rigetti, and University of Maryland. Overall, we demonstrate that leveraging microarchitecture details in the compiler improves program success rate up to 28x on IBM (geomean 3x), 2.3x on Rigetti (geomean 1.45x), and 1.47x on UMDTI (geomean 1.17x), compared to vendor toolflows. In addition, from these real-system experiences at QC's hardware-software interface, we make observations and recommendations about native and software-visible gates for different QC technologies, as well as communication topologies, and the value of noise-aware compilation even on lower-noise platforms. This is the largest cross-platform real-system QC study performed thus far; its results have the potential to inform both QC device and compiler design going forward.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {527–540},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322213,
author = {Huang, Yipeng and Martonosi, Margaret},
title = {Statistical assertions for validating patterns and finding bugs in quantum programs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322213},
doi = {10.1145/3307650.3322213},
abstract = {In support of the growing interest in quantum computing experimentation, programmers need new tools to write quantum algorithms as program code. Compared to debugging classical programs, debugging quantum programs is difficult because programmers have limited ability to probe the internal states of quantum programs; those states are difficult to interpret even when observations exist; and programmers do not yet have guidelines for what to check for when building quantum programs. In this work, we present quantum program assertions based on statistical tests on classical observations. These allow programmers to decide if a quantum program state matches its expected value in one of classical, superposition, or entangled types of states. We extend an existing quantum programming language with the ability to specify quantum assertions, which our tool then checks in a quantum program simulator. We use these assertions to debug three benchmark quantum programs in factoring, search, and chemistry. We share what types of bugs are possible, and lay out a strategy for using quantum programming patterns to place assertions and prevent bugs.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {541–553},
numpages = {13},
keywords = {assertions, chi-square test, correctness, debugging, program patterns, quantum computing, validation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322253,
author = {Gokhale, Pranav and Baker, Jonathan M. and Duckering, Casey and Brown, Natalie C. and Brown, Kenneth R. and Chong, Frederic T.},
title = {Asymptotic improvements to quantum circuits via qutrits},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322253},
doi = {10.1145/3307650.3322253},
abstract = {Quantum computation is traditionally expressed in terms of quantum bits, or qubits. In this work, we instead consider three-level qutrits. Past work with qutrits has demonstrated only constant factor improvements, owing to the log2(3) binary-to-ternary compression factor. We present a novel technique using qutrits to achieve a logarithmic depth (runtime) decomposition of the Generalized Toffoli gate using no ancilla-a significant improvement over linear depth for the best qubit-only equivalent. Our circuit construction also features a 70x improvement in two-qudit gate count over the qubit-only equivalent decomposition. This results in circuit cost reductions for important algorithms like quantum neurons and Grover search. We develop an open-source circuit simulator for qutrits, along with realistic near-term noise models which account for the cost of operating qutrits. Simulation results for these noise models indicate over 90\% mean reliability (fidelity) for our circuit construction, versus under 30\% for the qubit-only baseline. These results suggest that qutrits offer a promising path towards scaling quantum computation.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {554–566},
numpages = {13},
keywords = {quantum computing, quantum information, qutrits},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322270,
author = {Cai, Ruizhe and Ren, Ao and Chen, Olivia and Liu, Ning and Ding, Caiwen and Qian, Xuehai and Han, Jie and Luo, Wenhui and Yoshikawa, Nobuyuki and Wang, Yanzhi},
title = {A stochastic-computing based deep learning framework using adiabatic quantum-flux-parametron superconducting technology},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322270},
doi = {10.1145/3307650.3322270},
abstract = {The Adiabatic Quantum-Flux-Parametron (AQFP) superconducting technology has been recently developed, which achieves the highest energy efficiency among superconducting logic families, potentially 104--105 gain compared with state-of-the-art CMOS. In 2016, the successful fabrication and testing of AQFP-based circuits with the scale of 83,000 JJs have demonstrated the scalability and potential of implementing large-scale systems using AQFP. As a result, it will be promising for AQFP in high-performance computing and deep space applications, with Deep Neural Network (DNN) inference acceleration as an important example.Besides ultra-high energy efficiency, AQFP exhibits two unique characteristics: the deep pipelining nature since each AQFP logic gate is connected with an AC clock signal, which increases the difficulty to avoid RAW hazards; the second is the unique opportunity of true random number generation (RNG) using a single AQFP buffer, far more efficient than RNG in CMOS. We point out that these two characteristics make AQFP especially compatible with the stochastic computing (SC) technique, which uses a time-independent bit sequence for value representation, and is compatible with the deep pipelining nature. Further, the application of SC has been investigated in DNNs in prior work, and the suitability has been illustrated as SC is more compatible with approximate computations.This work is the first to develop an SC-based DNN acceleration framework using AQFP technology. The deep-pipelining nature of AQFP circuits translates into the difficulty in designing accumulators/counters in AQFP, which makes the prior design in SC-based DNN not suitable. We overcome this limitation taking into account different properties in CONV and FC layers: (i) the inner product calculation in FC layers has more number of inputs than that in CONV layers; (ii) accurate activation function is critical in CONV rather than FC layers. Based on these observations, we propose (i) accurate integration of summation and activation function in CONV layers using bitonic sorting network and feedback loop, and (ii) low-complexity categorization block for FC layers based on chain of majority gates. For complete design we also develop (i) ultra-efficient stochastic number generator in AQFP, (ii) a high-accuracy sub-sampling (pooling) block in AQFP, and (iii) majority synthesis for further performance improvement and automatic buffer/splitter insertion for requirement of AQFP circuits. Experimental results suggest that the proposed SC-based DNN using AQFP can achieve up to 6.8 \texttimes{} 104 times higher energy efficiency compared to CMOS-based implementation while maintaining 96\% accuracy on the MNIST dataset.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {567–578},
numpages = {12},
keywords = {adiabatic quantum-flux-parametron, deep learning, stochastic computing, superconducting},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322262,
author = {Smith, Kaitlin N. and Thornton, Mitchell A.},
title = {A quantum computational compiler and design tool for technology-specific targets},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322262},
doi = {10.1145/3307650.3322262},
abstract = {Quantum computing, once just a theoretical field, is quickly advancing as physical quantum technology increases in size, capability, and reliability. In order to fully harness the power of a general quantum computer or an application-specific device, compilers and tools must be developed that optimize specifications and map them to a realization on a specific architecture. In this work, a technique and prototype tool for synthesizing algorithms into a quantum computer is described and evaluated. Most recently reported methods produce technologically-independent reversible cascades comprised of a functionally complete set of operators with no regard to actual technologically-dependent cell libraries or constraints due to a device's pre-configured interconnectivity. In contrast, our prototype tool synthesizes algorithms into technologically-dependent specifications that consist of a set of primitives and connectivity constraints present in the computer architecture. The tool performs optimizations based on actual architectural constraints, and a high-quality technology-dependent synthesized result is achieved through the use of optimizing cost functions derived from real hardware and architecture parameters. Additionally, another important aspect of our tool is the incorporation of internal formal equivalence checking that ensures the initially specified algorithm is functionally equivalent to the optimized, technologically-mapped output. Experimental results are provided that target the IBM Q family of quantum computers.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {579–588},
numpages = {10},
keywords = {QIS, compilation, logic synthesis, quantum computing, quantum information science},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322274,
author = {Wang, Ke and Louri, Ahmed and Karanth, Avinash and Bunescu, Razvan},
title = {IntelliNoC: a holistic design framework for energy-efficient and reliable on-chip communication for manycores},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322274},
doi = {10.1145/3307650.3322274},
abstract = {As technology scales, Network-on-Chips (NoCs), currently being used for on-chip communication in manycore architectures, face several problems including high network latency, excessive power consumption, and low reliability. Simultaneously addressing these problems is proving to be difficult due to the explosion of the design space and the complexity of handling many trade-offs. In this paper, we propose IntelliNoC, an intelligent NoC design framework which introduces architectural innovations and uses reinforcement learning to manage the design complexity and simultaneously optimize performance, energy-efficiency, and reliability in a holistic manner. IntelliNoC integrates three NoC architectural techniques: (1) multifunction adaptive channels (MFACs) to improve energy-efficiency; (2) adaptive error detection/correction and re-transmission control to enhance reliability; and (3) a stress-relaxing bypass feature which dynamically powers off NoC components to prevent overheating and fatigue. To handle the complex dynamic interactions induced by these techniques, we train a dynamic control policy using Q-learning, with the goal of providing improved fault-tolerance and performance while reducing power consumption and area overhead. Simulation using PARSEC benchmarks shows that our proposed IntelliNoC design improves energy-efficiency by 67\% and mean-time-to-failure (MTTF) by 77\%, and decreases end-to-end packet latency by 32\% and area requirements by 25\% over baseline NoC architecture.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {589–600},
numpages = {12},
keywords = {NoC performance, energy-efficiency, network-on-chip (NoC), reinforcement learning, reliability},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322272,
author = {Yuan, Yifan and Wang, Yipeng and Wang, Ren and Huang, Jian},
title = {HALO: accelerating flow classification for scalable packet processing in NFV},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322272},
doi = {10.1145/3307650.3322272},
abstract = {Network Function Virtualization (NFV) has become the new standard in the cloud platform, as it provides the flexibility and agility for deploying various network services on general-purpose servers. However, it still suffers from sub-optimal performance in software packet processing. Our characterization study of virtual switches shows that the flow classification is the major bottleneck that limits the throughput of the packet processing in NFV, even though a large portion of the classification rules can be cached in the last level cache (LLC) in modern servers.To overcome this bottleneck, we propose Halo, an effective near-cache computing solution for accelerating the flow classification. Halo exploits the hardware parallelism of the cache architecture consists of Non-Uniform Cache Access (NUCA) and Caching and Home Agent (CHA) available in almost all Intel® multi-core CPUs. It associates the accelerator with each CHA component to speed up and scale the flow classification within LLC. To make Halo more generic, we extend the x86--64 instruction set with three simple data lookup instructions for utilizing the proposed near-cache accelerators. We develop Halo with the full-system simulator gem5. The experiments with a variety of real-world workloads of network services demonstrate that Halo improves the throughput of basic flow-rule lookup operations by 3.3\texttimes{}, and scales the representative flow classification algorithm - tuple space search by up to 23.4\texttimes{} with negligible negative impact on the performance of collocated network services, compared with state-of-the-art software-based solutions. Halo also performs up to 48.2\texttimes{} more energy-efficient than the fastest but expensive ternary content-addressable memory (TCAM), with trivial power and area overhead.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {601–614},
numpages = {14},
keywords = {flow classification, hash-table lookup, near-cache computing, network function virtualization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322249,
author = {Zhang, Yaqi and Rucker, Alexander and Vilim, Matthew and Prabhakar, Raghu and Hwang, William and Olukotun, Kunle},
title = {Scalable interconnects for reconfigurable spatial architectures},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322249},
doi = {10.1145/3307650.3322249},
abstract = {Recent years have seen the increased adoption of Coarse-Grained Reconfigurable Architectures (CGRAs) as flexible, energy-efficient compute accelerators. Obtaining performance using spatial architectures while supporting diverse applications requires a flexible, high-bandwidth interconnect. Because modern CGRAs support vector units with wide datapaths, designing an interconnect that balances dynamism, communication granularity, and programmability is a challenging task.In this work, we explore the space of spatial architecture interconnect dynamism, granularity, and programmability. We start by characterizing several benchmarks' communication patterns and showing links' imbalanced bandwidth requirements, fanout, and data width. We then describe a compiler stack that maps applications to both static and dynamic networks and performs virtual channel allocation to guarantee deadlock freedom. Finally, using a cycle-accurate simulator and 28 nm ASIC synthesis, we perform a detailed performance, area, and power evaluation across the identified design space for a variety of benchmarks. We show that the best network design depends on both applications and the underlying accelerator architecture. Network performance correlates strongly with bandwidth for streaming accelerators, and scaling raw bandwidth is more area- and energy-efficient with a static network. We show that the application mapping can be optimized to move less data by using a dynamic network as a fallback from a high-bandwidth static network. This static-dynamic hybrid network provides a 1.8x energy-efficiency and 2.8x performance advantage over the purely static and purely dynamic networks, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {615–628},
numpages = {14},
keywords = {CGRAs, hardware accelerators, interconnection network, reconfigurable architectures},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322266,
author = {Boroumand, Amirali and Ghose, Saugata and Patel, Minesh and Hassan, Hasan and Lucia, Brandon and Ausavarungnirun, Rachata and Hsieh, Kevin and Hajinazar, Nastaran and Malladi, Krishna T. and Zheng, Hongzhong and Mutlu, Onur},
title = {CoNDA: efficient cache coherence support for near-data accelerators},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322266},
doi = {10.1145/3307650.3322266},
abstract = {Specialized on-chip accelerators are widely used to improve the energy efficiency of computing systems. Recent advances in memory technology have enabled near-data accelerators (NDAs), which reside off-chip close to main memory and can yield further benefits than on-chip accelerators. However, enforcing coherence with the rest of the system, which is already a major challenge for accelerators, becomes more difficult for NDAs. This is because (1) the cost of communication between NDAs and CPUs is high, and (2) NDA applications generate a lot of off-chip data movement. As a result, as we show in this work, existing coherence mechanisms eliminate most of the benefits of NDAs. We extensively analyze these mechanisms, and observe that (1) the majority of off-chip coherence traffic is unnecessary, and (2) much of the off-chip traffic can be eliminated if a coherence mechanism has insight into the memory accesses performed by the NDA.Based on our observations, we propose CoNDA, a coherence mechanism that lets an NDA optimistically execute an NDA kernel, under the assumption that the NDA has all necessary coherence permissions. This optimistic execution allows CoNDA to gather information on the memory accesses performed by the NDA and by the rest of the system. CoNDA exploits this information to avoid performing unnecessary coherence requests, and thus, significantly reduces data movement for coherence.We evaluate CoNDA using state-of-the-art graph processing and hybrid in-memory database workloads. Averaged across all of our workloads operating on modest data set sizes, CoNDA improves performance by 19.6\% over the highest-performance prior coherence mechanism (66.0\%/51.7\% over a CPU-only/NDA-only system) and reduces memory system energy consumption by 18.0\% over the most energy-efficient prior coherence mechanism (43.7\% over CPU-only). CoNDA comes within 10.4\% and 4.4\% of the performance and energy of an ideal mechanism with no cost for coherence. The benefits of CoNDA increase with large data sets, as CoNDA improves performance over the highest-performance prior coherence mechanism by 38.3\% (8.4x/7.7x over CPU-only/NDA-only), and comes within 10.2\% of an ideal no-cost coherence mechanism.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {629–642},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322233,
author = {Gopireddy, Bhargava and Torrellas, Josep},
title = {Designing vertical processors in monolithic 3D},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322233},
doi = {10.1145/3307650.3322233},
abstract = {A processor laid out vertically in stacked layers can benefit from reduced wire delays, low energy consumption, and a small footprint. Such a design can be enabled by Monolithic 3D (M3D), a technology that provides short wire lengths, good thermal properties, and high integration. In current M3D technology, due to manufacturing constraints, the layers in the stack are asymmetric: the bottom-most one has a relatively higher performance.In this paper, we examine how to partition a processor for M3D. We partition logic and storage structures into two layers, taking into account that the top layer has lower-performance transistors. In logic structures, we place the critical paths in the bottom layer. In storage structures, we partition the hardware unequally, assigning to the top layer fewer ports with larger access transistors, or a shorter bitcell subarray with larger bitcells. We find that, with conservative assumptions on M3D technology, an M3D core executes applications on average 25\% faster than a 2D core, while consuming 39\% less energy. With aggressive technology assumptions, the M3D core performs even better: it is on average 38\% faster than a 2D core and consumes 41\% less energy. Further, under a similar power budget, an M3D multicore can use twice as many cores as a 2D multicore, executing applications on average 92\% faster with 39\% less energy. Finally, an M3D core is thermally efficient.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {643–656},
numpages = {14},
keywords = {3D integration, monolithic 3D, processor architecture},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322268,
author = {Fan, Yuanbo and Campanoni, Simone and Joseph, Russ},
title = {Time squeezing for tiny devices},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322268},
doi = {10.1145/3307650.3322268},
abstract = {Dynamic timing slack has emerged as a compelling opportunity for eliminating inefficiency in ultra-low power embedded systems. This slack arises when all the signals have propagated through logic paths well in advance of the clock signal. When it is properly identified, the system can exploit this unused cycle time for energy savings. In this paper, we describe compiler and architecture co-design that opens new opportunities for timing slack that are otherwise impossible. Through cross-layer optimization, we introduce novel mechanisms in the hardware and in the compiler that work together to improve the benefit of circuit-level timing speculation by effectively squeezing time during execution. This approach is particularly well-suited to tiny embedded devices. Our evaluation on a gate-level model of a complete processor shows that our co-design saves (on average) 40.5\% of the original energy consumption (additional 16.5\% compared to the existing clock scheduling technique) across 13 workloads while retaining transparency to developers.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {657–670},
numpages = {14},
keywords = {code generation, timing slack, timing speculation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322218,
author = {Du, Dong and Hua, Zhichao and Xia, Yubin and Zang, Binyu and Chen, Haibo},
title = {XPC: architectural support for secure and efficient cross process call},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322218},
doi = {10.1145/3307650.3322218},
abstract = {Microkernel has many intriguing features like security, fault-tolerance, modularity and customizability, which recently stimulate a resurgent interest in both academia and industry (including seL4, QNX and Google's Fuchsia OS). However, IPC (inter-process communication), which is known as the Achilles' Heel of microkernels, is still the major factor for the overall (poor) OS performance. Besides, IPC also plays a vital role in monolithic kernels like Android Linux, as mobile applications frequently communicate with plenty of user-level services through IPC. Previous software optimizations of IPC usually cannot bypass the kernel which is responsible for domain switching and message copying/remapping; hardware solutions like tagged memory or capability replace page tables for isolation, but usually require non-trivial modification to existing software stack to adapt the new hardware primitives. In this paper, we propose a hardware-assisted OS primitive, XPC (Cross Process Call), for fast and secure synchronous IPC. XPC enables direct switch between IPC caller and callee without trapping into the kernel, and supports message passing across multiple processes through the invocation chain without copying. The primitive is compatible with the traditional address space based isolation mechanism and can be easily integrated into existing microkernels and monolithic kernels. We have implemented a prototype of XPC based on a Rocket RISC-V core with FPGA boards and ported two microkernel implementations, seL4 and Zircon, and one monolithic kernel implementation, Android Binder, for evaluation. We also implement XPC on GEM5 simulator to validate the generality. The result shows that XPC can reduce IPC call latency from 664 to 21 cycles, up to 54.2x improvement on Android Binder, and improve the performance of real-world applications on microkernels by 1.6x on Sqlite3 and 10x on an HTTP server with minimal hardware resource cost.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {671–684},
numpages = {14},
keywords = {accelerators, inter-process communication, microkernel, operating system},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322215,
author = {Liu, Zhenhong and Yazdanbakhsh, Amir and Wang, Dong Kai and Esmaeilzadeh, Hadi and Kim, Nam Sung},
title = {AxMemo: hardware-compiler co-design for approximate code memoization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322215},
doi = {10.1145/3307650.3322215},
abstract = {Historically, continuous improvements in general-purpose processors have fueled the economic success and growth of the IT industry. However, the diminishing benefits from transistor scaling and conventional optimization techniques necessitates moving beyond common practices. Approximate computing is one such unconventional technique that has shown promise in pushing the boundaries of general-purpose processing. This paper sets out to employ approximation for processors that are commonly used in cyber-physical domains and may become building blocks of Internet of Things. To this end, we propose AxMemo to exploit the computation redundancy that stems from data similarity in the inputs of code blocks. Such input behavior is prevalent in cyber-physical systems as they deal with real-world data that naturally harbors redundancy. Therefore, in contrast to existing memoization techniques that replace costly floating-point arithmetic operations with limited number of inputs, AxMemo focuses on memoizing blocks of code with potentially many inputs. As such, AxMemo aims to replace long sequences of instructions with a few hash and lookup operations. By reducing the number of dynamic instructions, AxMemo alleviates the von Neumann and execution overheads of passing instructions through the processor pipeline altogether. The challenge AxMemo facing is to provide low-cost hashing mechanisms that can generate rather unique signature for each multi-input combination. To address this challenge, we develop a novel use of Cyclic Redundancy Checking (CRC) to hash the inputs. To increase lookup table hit rate, AxMemo employs a two-level memoization lookup, which utilizes small dedicated SRAM and spare storage in the last level cache. These solutions enable AxMemo to efficiently memoize relatively large code regions with variable input sizes and types using the same underlying hardware. Our experiment shows that AxMemo offers 2.64\texttimes{} speedup and 2.58 \texttimes{} energy reduction with mere 0.2\% of quality loss averaged across ten benchmarks. These benefits come with an area overhead of just 2.1\%.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {685–697},
numpages = {13},
keywords = {approximate computing, hardware-software co-design, memoization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322223,
author = {Yan, Zi and Lustig, Daniel and Nellans, David and Bhattacharjee, Abhishek},
title = {Translation ranger: operating system support for contiguity-aware TLBs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322223},
doi = {10.1145/3307650.3322223},
abstract = {Virtual memory (VM) eases programming effort but can suffer from high address translation overheads. Architects have traditionally coped by increasing Translation Lookaside Buffer (TLB) capacity; this approach, however, requires considerable hardware resources. One promising alternative is to rely on software-generated translation contiguity to compress page translation encodings within the TLB. To enable this, operating systems (OSes) have to assign spatially-adjacent groups of physical frames to contiguous groups of virtual pages, as doing so allows compression or coalescing of these contiguous translations in hardware. Unfortunately, modern OSes do not currently guarantee translation contiguity in many real-world scenarios; as systems remain online for long periods of time, their memory can and does become fragmented.We propose Translation Ranger, an OS service that recovers lost translation contiguity even where previous contiguity-generation proposals struggle with memory fragmentation. Translation Ranger increases contiguity by actively coalescing scattered physical frames into contiguous regions and can be leveraged by any contiguity-aware TLB without requiring changes to applications. We implement and evaluate Translation Ranger in Linux on real hardware and find that it generates contiguous memory regions 40\texttimes{} larger than the Linux default configuration, permitting TLB coverage of 120GB memory with typically no more than 128 contiguous translation regions. This is achieved with less than 2\% run time overhead, a number that is outweighed by the TLB coverage improvements that Translation Ranger provides.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {698–710},
numpages = {13},
keywords = {heterogeneous memory management, memory defragmentation, operating system, translation lookaside buffers},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322256,
author = {McMahan, Joseph and Christensen, Michael and Dewey, Kyle and Hardekopf, Ben and Sherwood, Timothy},
title = {Bouncer: static program analysis in hardware},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322256},
doi = {10.1145/3307650.3322256},
abstract = {When discussing safety and security for embedded systems, we typically divide the world into software checks (which are either static or dynamic) or hardware checks (which are dynamic). As others have pointed out, hardware checks offer more than just efficiency. They are intrinsic to the device's functionality and thus are live from power-up; they require little to no dependency on other software functioning correctly, and due to the fact they are wired directly into the operation of the system, are difficult or impossible to bypass. We explore an experimental new embedded system that uses special-purpose hardware for static analysis that prevents all program binaries with memory errors, invalid control flow, and several other undesirable properties from ever being loaded onto the device. Static analysis often requires whole-binary-level, rather than instruction-level, examination. We show that a carefully constructed hardware state machine, using available scratch-pad memory, is capable of efficiently checking functional binaries in a streaming and verifiably non-bypassable way directly in hardware as they are loaded into the embedded program store. The resulting system is surprisingly small (taking no more than .0079 mm2), efficient (capable of checking binaries at an average throughput of around 60 cycles per instruction), and yet guarantees execution free from many of the fragile behaviors that result in security and safety concerns. We believe this is the first time any static analysis has been implemented at the hardware level and opens the door to more complex hardware-checked properties.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {711–722},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322216,
author = {Sakalis, Christos and Kaxiras, Stefanos and Ros, Alberto and Jimborean, Alexandra and Sj\"{a}lander, Magnus},
title = {Efficient invisible speculative execution through selective delay and value prediction},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322216},
doi = {10.1145/3307650.3322216},
abstract = {Speculative execution, the base on which modern high-performance general-purpose CPUs are built on, has recently been shown to enable a slew of security attacks. All these attacks are centered around a common set of behaviors: During speculative execution, the architectural state of the system is kept unmodified, until the speculation can be verified. In the event that a misspeculation occurs, then anything that can affect the architectural state is reverted (squashed) and re-executed correctly. However, the same is not true for the microarchitectural state. Normally invisible to the user, changes to the microarchitectural state can be observed through various side-channels, with timing differences caused by the memory hierarchy being one of the most common and easy to exploit. The speculative side-channels can then be exploited to perform attacks that can bypass software and hardware checks in order to leak information. These attacks, out of which the most infamous are perhaps Spectre and Meltdown, have led to a frantic search for solutions.In this work, we present our own solution for reducing the microarchitectural state-changes caused by speculative execution in the memory hierarchy. It is based on the observation that if we only allow accesses that hit in the L1 data cache to proceed, then we can easily hide any microarchitectural changes until after the speculation has been verified. At the same time, we propose to prevent stalls by value predicting the loads that miss in the L1. Value prediction, though speculative, constitutes an invisible form of speculation, not seen outside the core. We evaluate our solution and show that we can prevent observable microarchitectural changes in the memory hierarchy while keeping the performance and energy costs at 11\% and 7\%, respectively. In comparison, the current state of the art solution, InvisiSpec, incurs a 46\% performance loss and a 51\% energy increase.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {723–735},
numpages = {13},
keywords = {caches, side-channel attacks, speculative execution},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322229,
author = {Wang, Zhengrong and Nowatzki, Tony},
title = {Stream-based memory access specialization for general purpose processors},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322229},
doi = {10.1145/3307650.3322229},
abstract = {Because of severe limitations in technology scaling, architects have innovated in specializing general purpose processors for computation primitives (e.g. vector instructions, loop accelerators). The general principle is exposing rich semantics to the ISA. An opportunity to explore is whether richer semantics of memory access patterns could also be used to improve the efficiency of memory and communication. Two important open questions are how to convey higher level memory information and how to take advantage of this information in hardware.We find that a majority of memory accesses follow a small number of simple patterns; we term these streams (e.g. affine, indirect). Streams can often be decoupled from core execution, and patterns persist long enough to express useful behavior. Our approach is therefore to express streams as ISA primitives, which we argue can enable: prefetch stream accesses to hide memory latency, semi-binding decoupled access to remove address computation and optimize the memory interface, and finally inform cache policies.In this work, we propose ISA-extensions for decoupled-streams, which interact with the core using a FIFO-based interface. We implement optimizations for each of the aforementioned opportunities on an aggressive wide-issue OOO core and evaluate with SPEC CPU 2017 and CortexSuite[1, 2]. Across all workloads, we observe about 1.37\texttimes{} speedup and energy efficiency improvement over hardware stride prefetching.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {736–749},
numpages = {14},
keywords = {DAE, ISA, cache bypassing, decoupled access execute, microarchitecture, prefetching, specialization, stream},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322261,
author = {Vilanova, Llu\'{\i}s and Amit, Nadav and Etsion, Yoav},
title = {Using SMT to accelerate nested virtualization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322261},
doi = {10.1145/3307650.3322261},
abstract = {IaaS datacenters offer virtual machines (VMs) to their clients, who in turn sometimes deploy their own virtualized environments, thereby running a VM inside a VM. This is known as nested virtualization.VMs are intrinsically slower than bare-metal execution, as they often trap into their hypervisor to perform tasks like operating virtual I/O devices. Each VM trap requires loading and storing dozens of registers to switch between the VM and hypervisor contexts, thereby incurring costly runtime overheads. Nested virtualization further magnifies these overheads, as every VM trap in a traditional virtualized environment triggers at least twice as many traps.We propose to leverage the replicated thread execution resources in simultaneous multithreaded (SMT) cores to alleviate the overheads of VM traps in nested virtualization. Our proposed architecture introduces a simple mechanism to colocate different VMs and hypervisors on separate hardware threads of a core, and replaces the costly context switches of VM traps with simple thread stall and resume events. More concretely, as each thread in an SMT core has its own register set, trapping between VMs and hypervisors does not involve costly context switches, but simply requires the core to fetch instructions from a different hardware thread. Furthermore, our inter-thread communication mechanism allows a hypervisor to directly access and manipulate the registers of its subordinate VMs, given that they both share the same in-core physical register file.A model of our architecture shows up to 2.3\texttimes{} and 2.6\texttimes{} better I/O latency and bandwidth, respectively. We also show a software-only prototype of the system using existing SMT architectures, with up to 1.3\texttimes{} and 1.5\texttimes{} better I/O latency and bandwidth, respectively, and 1.2--2.2\texttimes{} speedups on various real-world applications.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {750–761},
numpages = {12},
keywords = {computer architecture, nested virtualization, virtualization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322220,
author = {Lottarini, Andrea and Cerqueira, Jo\~{a}o P. and Repetti, Thomas J. and Edwards, Stephen A. and Ross, Kenneth A. and Seok, Mingoo and Kim, Martha A.},
title = {Master of none acceleration: a comparison of accelerator architectures for analytical query processing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322220},
doi = {10.1145/3307650.3322220},
abstract = {Hardware accelerators are one promising solution to contend with the end of Dennard scaling and the slowdown of Moore's law. For mature workloads that are regular and have high compute per byte, hardening an application into one or more hardware modules is a standard approach. However, for some applications, we find that a programmable homogeneous architecture is preferable.This paper compares a previously proposed heterogeneous hardware accelerator for analytical query processing to a homogeneous systolic array alternative. We find that the heterogeneous and homogeneous accelerators are equivalent for large designs, while for small designs the homogeneous is better. Our analysis explains this counter-intuitive result, finding that the homogeneous architecture has higher average resource utilization and lower relative costs for the communication infrastructure.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {762–773},
numpages = {12},
keywords = {accelerators, database implementation, design space exploration, spatial architectures},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322219,
author = {Lee, Gyu-hyeon and Min, Dongmoon and Byun, Ilkwon and Kim, Jangwoo},
title = {Cryogenic computer architecture modeling with memory-side case studies},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322219},
doi = {10.1145/3307650.3322219},
abstract = {Modern computer architectures suffer from lack of architectural innovations, mainly due to the power wall and the memory wall. That is, architectural innovations become infeasible because they can prohibitively increase power consumption and their performance impacts are eventually bounded by slow memory accesses. To address the challenges, making computer systems run at ultra-low temperatures (or cryogenic computer systems) has emerged as a highly promising solution as both power consumption and wire resistivity are expected to significantly reduce at ultra-low temperatures. However, cryogenic computers have not been yet realized as computer architects do not fully understand the behaviors of existing computer systems and their cost effectiveness at such ultra-low temperatures.In this paper, we first develop CryoRAM, a validated computer architecture simulation tool to incorporate cryogenic memory devices. For this work, we focus on 77K temperature (easily achieved by applying low-cost liquid nitrogen), at which modern CMOS devices still reliably operate. We also focus on reducing the temperature of memory devices only as a pilot study prior to building a full cryogenic computer. Next, driven by the modeling tool, we propose our temperature-aware memory device and architecture designs to improve the DRAM access speed by 3.8 times or reduce the power consumption to 9.2\%. Finally, we provide three promising case studies using cryogenic memories to significantly improve (1) server performance (up to 2.5 times), (2) server power (down to 6\% on average), and (3) datacenter's power cost (by 8.4\%).We will release our modeling and simulation tools deliberately implemented on top of only open-source simulators combined, even though some experiments were conducted under industry-confidential environments.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {774–787},
numpages = {14},
keywords = {DRAM, cryogenic computing, memory, modeling, simulation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322226,
author = {Zhao, Yongwei and Du, Zidong and Guo, Qi and Liu, Shaoli and Li, Ling and Xu, Zhiwei and Chen, Tianshi and Chen, Yunji},
title = {Cambricon-F: machine learning computers with fractal von neumann architecture},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322226},
doi = {10.1145/3307650.3322226},
abstract = {Machine learning techniques are pervasive tools for emerging commercial applications and many dedicated machine learning computers on different scales have been deployed in embedded devices, servers, and data centers. Currently, most machine learning computer architectures still focus on optimizing performance and energy efficiency instead of programming productivity. However, with the fast development in silicon technology, programming productivity, including programming itself and software stack development, becomes the vital reason instead of performance and power efficiency that hinders the application of machine learning computers.In this paper, we propose Cambricon-F, which is a series of homogeneous, sequential, multi-layer, layer-similar, machine learning computers with the same ISA. A Cambricon-F machine has a fractal von Neumann architecture to iteratively manage its components: it is with von Neumann architecture and its processing components (sub-nodes) are still Cambricon-F machines with von Neumann architecture and the same ISA. Since different Cambricon-F instances with different scales can share the same software stack on their common ISA, Cambricon-Fs can significantly improve the programming productivity. Moreover, we address four major challenges in Cambricon-F architecture design, which allow Cambricon-F to achieve a high efficiency. We implement two Cambricon-F instances at different scales, i.e., Cambricon-F100 and Cambricon-F1. Compared to GPU based machines (DGX-1 and 1080Ti), Cambricon-F instances achieve 2.82x, 5.14x better performance, 8.37x, 11.39x better efficiency on average, with 74.5\%, 93.8\% smaller area costs, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {788–801},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322237,
author = {Imani, Mohsen and Gupta, Saransh and Kim, Yeseong and Rosing, Tajana},
title = {FloatPIM: in-memory acceleration of deep neural network training with high precision},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322237},
doi = {10.1145/3307650.3322237},
abstract = {Processing In-Memory (PIM) has shown a great potential to accelerate inference tasks of Convolutional Neural Network (CNN). However, existing PIM architectures do not support high precision computation, e.g., in floating point precision, which is essential for training accurate CNN models. In addition, most of the existing PIM approaches require analog/mixed-signal circuits, which do not scale, exploiting insufficiently reliable multi-bit Non-Volatile Memory (NVM). In this paper, we propose FloatPIM, a fully-digital scalable PIM architecture that accelerates CNN in both training and testing phases. FloatPIM natively supports floating-point representation, thus enabling accurate CNN training. FloatPIM also enables fast communication between neighboring memory blocks to reduce internal data movement of the PIM architecture. We evaluate the efficiency of FloatPIM on ImageNet dataset using popular large-scale neural networks. Our evaluation shows that FloatPIM supporting floating point precision can achieve up to 5.1\% higher classification accuracy as compared to existing PIM architectures with limited fixed-point precision. FloatPIM training is on average 303.2\texttimes{} and 48.6\texttimes{} (4.3\texttimes{} and 15.8\texttimes{}) faster and more energy efficient as compared to GTX 1080 GPU (PipeLayer [1] PIM accelerator). For testing, FloatPIM also provides 324.8\texttimes{} and 297.9\texttimes{} (6.3\texttimes{} and 21.6\texttimes{}) speedup and energy efficiency as compared to GPU (ISAAC [2] PIM accelerator) respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {802–815},
numpages = {14},
keywords = {deep neural network, machine learning acceleration, non-volatile memory, processing in-memory},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

