@inproceedings{10.1109/ISCA.2018.00012,
author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
title = {A configurable cloud-scale DNN processor for real-time AI},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00012},
doi = {10.1109/ISCA.2018.00012},
abstract = {Interactive AI-powered services require low-latency evaluation of deep neural network (DNN) models---aka "realtime AI". The growing demand for computationally expensive, state-of-the-art DNNs, coupled with diminishing performance gains of general-purpose architectures, has fueled an explosion of specialized Neural Processing Units (NPUs). NPUs for interactive services should satisfy two requirements: (1) execution of DNN models with low latency, high throughput, and high efficiency, and (2) flexibility to accommodate evolving state-of-the-art models (e.g., RNNs, CNNs, MLPs) without costly silicon updates.This paper describes the NPU architecture for Project Brainwave, a production-scale system for real-time AI. The Brainwave NPU achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large RNNs at a batch size of 1. The NPU attains this performance using a single-threaded SIMD ISA paired with a distributed microarchitecture capable of dispatching over 7M operations from a single instruction. The spatially distributed microarchitecture, scaled up to 96,000 multiply-accumulate units, is supported by hierarchical instruction decoders and schedulers coupled with thousands of independently addressable high-bandwidth on-chip memories, and can transparently exploit many levels of fine-grain SIMD parallelism. When targeting an FPGA, microarchitectural parameters such as native datapaths and numerical precision can be "synthesis specialized" to models at compile time, enabling high FPGA performance competitive with hardened NPUs. When running on an Intel Stratix 10 280 FPGA, the Brainwave NPU achieves performance ranging from ten to over thirty-five teraflops, with no batching, on large, memory-intensive RNNs.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {1–14},
numpages = {14},
keywords = {neural network hardware, field programmable gate arrays, accelerator architectures},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00013,
author = {Skach, Matt and Arora, Manish and Tullsen, Dean and Tang, Lingjia and Mars, Jason},
title = {Virtual melting temperature: managing server load to minimize cooling overhead with phase change materials},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00013},
doi = {10.1109/ISCA.2018.00013},
abstract = {As the power density and power consumption of large scale datacenters continue to grow, the challenges of removing heat from these datacenters and keeping them cool is an increasingly urgent and costly. With the largest datacenters now exceeding over 200 MW of power, the cooling systems that prevent overheating cost on the order of tens of millions of dollars. Prior work proposed to deploy phase change materials (PCM) and use Thermal Time Shifting (TTS) to reshape the thermal load of a datacenter by storing heat during peak hours of high utilization and releasing it during off hours when utilization is low, enabling a smaller cooling system to handle the same peak load. The peak cooling load reduction enabled by TTS is greatly beneficial, however TTS is a passive system that cannot handle many workload mixtures or adapt to changing load or environmental characteristics.In this work we propose VMT, a thermal aware job placement technique that adds an active, tunable component to enable greater control over datacenter thermal output. We propose two different job placement algorithms for VMT and perform a scale out study of VMT in a simulated server cluster. We provide analysis of the use cases and trade-offs of each algorithm, and show that VMT reduces peak cooling load by up to 12.8\% to provide over two million dollars in cost savings when a smaller cooling system is installed, or allows for over 7,000 additional servers to be added in scenarios where TTS is ineffective.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {15–28},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00014,
author = {Karandikar, Sagar and Mao, Howard and Kim, Donggyu and Biancolin, David and Amid, Alon and Lee, Dayeol and Pemberton, Nathan and Amaro, Emmanuel and Schmidt, Colin and Chopra, Aditya and Huang, Qijing and Kovacs, Kyle and Nikolic, Borivoje and Katz, Randy and Bachrach, Jonathan and Asanovi\'{c}, Krste},
title = {Firesim: FPGA-accelerated cycle-exact scale-out system simulation in the public cloud},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00014},
doi = {10.1109/ISCA.2018.00014},
abstract = {We present FireSim, an open-source simulation platform that enables cycle-exact microarchitectural simulation of large scale-out clusters by combining FPGA-accelerated simulation of silicon-proven RTL designs with a scalable, distributed network simulation. Unlike prior FPGA-accelerated simulation tools, FireSim runs on Amazon EC2 F1, a public cloud FPGA platform, which greatly improves usability, provides elasticity, and lowers the cost of large-scale FPGA-based experiments. We describe the design and implementation of FireSim and show how it can provide sufficient performance to run modern applications at scale, to enable true hardware-software co-design. As an example, we demonstrate automatically generating and deploying a target cluster of 1,024 3.2 GHz quad-core server nodes, each with 16 GB of DRAM, interconnected by a 200 Gbit/s network with 2 microsecond latency, which simulates at a 3.4 MHz processor clock rate (less than 1,000x slowdown over real-time). In aggregate, this FireSim instantiation simulates 4,096 cores and 16 TB of memory, runs ∼14 billion instructions per second, and harnesses 12.8 million dollars worth of FPGAs---at a total cost of only ∼$100 per simulation hour to the user. We present several examples to show how FireSim can be used to explore various research directions in warehouse-scale machine design, including modeling networks with high-bandwidth and low-latency, integrating arbitrary RTL designs for a variety of commodity and specialized datacenter nodes, and modeling a variety of datacenter organizations, as well as reusing the scale-out FireSim infrastructure to enable fast, massively parallel cycle-exact single-node microarchitectural experimentation.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {29–42},
numpages = {14},
keywords = {scalability, performance analysis, field programmable gate arrays, distributed computing, data centers, computer simulation, computer networks, computer architecture},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00015,
author = {Srivastava, Prakalp and Kang, Mingu and Gonugondla, Sujan K. and Lim, Sungmin and Choi, Jungwook and Adve, Vikram and Kim, Nam Sung and Shanbhag, Naresh},
title = {PROMISE: an end-to-end design of a programmable mixed-signal accelerator for machine-learning algorithms},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00015},
doi = {10.1109/ISCA.2018.00015},
abstract = {Analog/mixed-signal machine learning (ML) accelerators exploit the unique computing capability of analog/mixed-signal circuits and inherent error tolerance of ML algorithms to obtain higher energy efficiencies than digital ML accelerators. Unfortunately, these analog/mixed-signal ML accelerators lack programmability, and even instruction set interfaces, to support diverse ML algorithms or to enable essential software control over the energy-vs-accuracy tradeoffs. We propose PROMISE, the first end-to-end design of a PROgrammable MIxed-Signal accElerator from Instruction Set Architecture (ISA) to high-level language compiler for acceleration of diverse ML algorithms. We first identify prevalent operations in widely-used ML algorithms and key constraints in supporting these operations for a programmable mixed-signal accelerator. Second, based on that analysis, we propose an ISA with a PROMISE architecture built with silicon-validated components for mixed-signal operations. Third, we develop a compiler that can take a ML algorithm described in a high-level programming language (Julia) and generate PROMISE code, with an IR design that is both language-neutral and abstracts away unnecessary hardware details. Fourth, we show how the compiler can map an application-level error tolerance specification for neural network applications down to low-level hardware parameters (swing voltages for each application Task) to minimize energy consumption. Our experiments show that PROMISE can accelerate diverse ML algorithms with energy efficiency competitive even with fixed-function digital ASICs for specific ML algorithms, and the compiler optimization achieves significant additional energy savings even for only 1\% extra errors.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {43–56},
numpages = {14},
keywords = {programmable machine learning accelerator, deep in-memory computing, analog ISA},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00016,
author = {Riera, Marc and Arnau, Jose-Maria and Gonz\'{a}lez, Antonio},
title = {Computation reuse in DNNs by exploiting input similarity},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00016},
doi = {10.1109/ISCA.2018.00016},
abstract = {In recent years, Deep Neural Networks (DNNs) have achieved tremendous success for diverse problems such as classification and decision making. Efficient support for DNNs on CPUs, GPUs and accelerators has become a prolific area of research, resulting in a plethora of techniques for energy-efficient DNN inference. However, previous proposals focus on a single execution of a DNN.Popular applications, such as speech recognition or video classification, require multiple back-to-back executions of a DNN to process a sequence of inputs (e.g., audio frames, images). In this paper, we show that consecutive inputs exhibit a high degree of similarity, causing the inputs/outputs of the different layers to be extremely similar for successive frames of speech or images of a video.Based on this observation, we propose a technique to reuse some results of the previous execution, instead of computing the entire DNN. Computations related to inputs with negligible changes can be avoided with minor impact on accuracy, saving a large percentage of computations and memory accesses.We propose an implementation of our reuse-based inference scheme on top of a state-of-the-art DNN accelerator. Results show that, on average, more than 60\% of the inputs of any neural network layer tested exhibit negligible changes with respect to the previous execution. Avoiding the memory accesses and computations for these inputs results in 63\% energy savings on average.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {57–68},
numpages = {12},
keywords = {input similarity, hardware accelerator, computation reuse, DNN},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00017,
author = {Fujiki, Daichi and Subramaniyan, Aran and Zhang, Tianjun and Zeng, Yu and Das, Reetuparna and Blaauw, David and Narayanasamy, Satish},
title = {Genax: a genome sequencing accelerator},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00017},
doi = {10.1109/ISCA.2018.00017},
abstract = {Genomics can transform health-care through precision medicine. Plummeting sequencing costs would soon make genome testing affordable to the masses. Compute efficiency, however, has to improve by orders of magnitude to sequence and analyze the raw genome data. Sequencing software used today can take several hundreds to thousands of CPU hours to align reads to a reference sequence.This paper presents GenAx, an accelerator for read alignment, a time-consuming step in genome sequencing. It consists of a seeding and seed-extension accelerator. The latter is based on an innovative automata design that was designed from the ground-up to enable hardware acceleration. Unlike conventional Levenshtein automata, it is string independent and scales quadratically with edit distance, instead of string length. It supports critical features commonly used in sequencing such as affine gap scoring and traceback.GenAx provides a throughput of 4,058K reads/s for Illumina 101 bp reads. GenAx achieves 31.7x speedup over the standard BWA-MEM sequence aligner running on a 56--thread dualsocket 14-core Xeon E5 server processor, while reducing power consumption by 12 x and area by 5.6 x.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {69–82},
numpages = {14},
keywords = {sequence alignment, automaton, accelerator},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00018,
author = {Kondguli, Sushant and Huang, Michael},
title = {Division of labor: a more effective approach to prefetching},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00018},
doi = {10.1109/ISCA.2018.00018},
abstract = {Prefetching is a central component in most microarchitectures. Many different algorithms have been proposed with varying degrees of complexity and effectiveness. There are inherent tradeoffs among various metrics especially when we try to exploit both simpler access patterns and more complex ones simultaneously. Hypothetically, therefore, it is better to have collaboration of sub-components each specialized in exploiting a different access pattern than to have a monolithic design trying to have a similar prefetching scope. In this paper, we present some empirical evidence. We use a few components dedicated for some simple patterns such as canonical strided accesses. We show that a composite prefetcher with these components can significantly out-perform state-of-the-art prefetchers. But more importantly, the composite prefetcher achieves better performance through a more limited prefetching scope while attaining a much higher accuracy. This suggests that the design can be more readily expanded with additional components targeting other patterns.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {83–95},
numpages = {13},
keywords = {prefetching},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00019,
author = {Nori, Anant Vithal and Gaur, Jayesh and Rai, Siddharth and Subramoney, Sreenivas and Wang, Hong},
title = {Criticality aware tiered cache hierarchy: a fundamental relook at multi-level cache hierarchies},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00019},
doi = {10.1109/ISCA.2018.00019},
abstract = {On-die caches are a popular method to help hide the main memory latency. However, it is difficult to build large caches without substantially increasing their access latency, which in turn hurts performance. To overcome this difficulty, on-die caches are typically built as a multi-level cache hierarchy. One such popular hierarchy that has been adopted by modern microprocessors is the three level cache hierarchy. Building a three level cache hierarchy enables a low average hit latency since most requests are serviced from faster inner level caches. This has motivated recent microprocessors to deploy large level-2 (L2) caches that can help further reduce the average hit latency.In this paper, we do a fundamental analysis of the popular three level cache hierarchy and understand its performance delivery using program criticality. Through our detailed analysis we show that the current trend of increasing L2 cache sizes to reduce average hit latency is, in fact, an inefficient design choice. We instead propose Criticality Aware Tiered Cache Hierarchy (CATCH) that utilizes an accurate detection of program criticality in hardware and using a novel set of inter-cache prefetchers ensures that on-die data accesses that lie on the critical path of execution are served at the latency of the fastest level-1 (L1) cache. The last level cache (LLC) serves the purpose of reducing slow memory accesses, thereby making the large L2 cache redundant for most applications. The area saved by eliminating the L2 cache can then be used to create more efficient processor configurations. Our simulation results show that CATCH outperforms the three level cache hierarchy with a large 1 MB L2 and exclusive LLC by an average of 8.4\%, and a baseline with 256 KB L2 and inclusive LLC by 10.3\%. We also show that CATCH enables a powerful framework to explore broad chip-level area, performance and power tradeoffs in cache hierarchy design. Supported by CATCH, we evaluate radical architecture directions such as eliminating the L2 altogether and show that such architectures can yield 4.5\% performance gain over the baseline at nearly 30\% lesser area or improve the performance by 7.3\% at the same area while reducing energy consumption by 11\%.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {96–109},
numpages = {14},
keywords = {prefetching, criticality, caching},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00020,
author = {Jain, Akanksha and Lin, Calvin},
title = {Rethinking belady's algorithm to accommodate prefetching},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00020},
doi = {10.1109/ISCA.2018.00020},
abstract = {This paper shows that in the presence of data prefetchers, cache replacement policies are faced with a large unexplored design space. In particular, we observe that while Belady's MIN algorithm minimizes the total number of cache misses---including those for prefetched lines---it does not minimize the number of demand misses. To address this shortcoming, we introduce Demand-MIN, a variant of Belady's algorithm that minimizes the number of demand misses at the cost of increased prefetcher traffic. Together, MIN and Demand-MIN define the boundaries of an important design space, with many intermediate points lying between them.To reason about this design space, we introduce a simple conceptual framework, which we use to define a new cache replacement policy called Harmony. Our empirical evaluation shows that for a mix of SPEC 2006 benchmarks running on a 4-core system with a stride prefetcher, Harmony improves IPC by 7.7\% over an LRU baseline, compared to 3.7\% for the previous state-of-the-art. On an 8-core system, Harmony improves IPC by 9.4\% compared to 4.4\% for the previous state-of-the-art.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {110–123},
numpages = {14},
keywords = {prefetching, caches, cache replacement},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00021,
author = {Zhang, Sizhuo and Vijayaraghavan, Muralidaran and Wright, Andrew and Alipour, Mehdi and Arvind},
title = {Constructing a weak memory model},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00021},
doi = {10.1109/ISCA.2018.00021},
abstract = {Weak memory models are a consequence of the desire on part of architects to preserve all the uniprocessor optimizations while building a shared memory multiprocessor. The efforts to formalize weak memory models of ARM and POWER over the last decades are mostly empirical - they try to capture empirically observed behaviors - and end up providing no insight into the inherent nature of weak memory models. This paper takes a constructive approach to find a common base for weak memory models: we explore what a weak memory would look like if we constructed it with the explicit goal of preserving all the uniprocessor optimizations. We will disallow some optimizations which break a programmer's intuition in highly unexpected ways. The constructed model, which we call General Atomic Memory Model (GAM), allows all four load/store reorderings. We give the construction procedure of GAM, and provide insights which are used to define its operational and axiomatic semantics. Though no attempt is made to match GAM to any existing weak memory model, we show by simulation that GAM has comparable performance with other models. No deep knowledge of memory models is needed to read this paper.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {124–137},
numpages = {14},
keywords = {memory model},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00022,
author = {Maas, Martin and Asanovi\'{c}, Krste and Kubiatowicz, John},
title = {A hardware accelerator for tracing garbage collection},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00022},
doi = {10.1109/ISCA.2018.00022},
abstract = {A large number of workloads are written in garbage-collected languages. These applications spend up to 10-35\% of their CPU cycles on GC, and these numbers increase further for pause-free concurrent collectors. As this amounts to a significant fraction of resources in scenarios ranging from data centers to mobile devices, reducing the cost of GC would improve the efficiency of a wide range of workloads.We propose to decrease these overheads by moving GC into a small hardware accelerator that is located close to the memory controller and performs GC more efficiently than a CPU. We first show a general design of such a GC accelerator and describe how it can be integrated into both stop-the-world and pause-free garbage collectors. We then demonstrate an end-to-end RTL prototype of this design, integrated into a RocketChip RISC-V System-on-Chip (SoC) executing full Java benchmarks within JikesRVM running under Linux on FPGAs.Our prototype performs the mark phase of a tracing GC at 4.2x the performance of an in-order CPU, at just 18.5\% the area (an amount equivalent to 64KB of SRAM). By prototyping our design in a real system, we show that our accelerator can be adopted without invasive changes to the SoC, and estimate its performance, area and energy.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {138–151},
numpages = {14},
keywords = {soCs, memory management, language runtime systems, hardware accelerators, garbage collection},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00023,
author = {Cui, Weilong and Ding, Yongshan and Dangwal, Deeksha and Holmes, Adam and McMahan, Joseph and Javadi-Abhari, Ali and Tzimpragos, Georgios and Chong, Frederic T. and Sherwood, Timothy},
title = {Charm: a language for closed-form high-level architecture modeling},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00023},
doi = {10.1109/ISCA.2018.00023},
abstract = {As computer architecture continues to expand beyond software-agnostic microarchitecture to data center organization, reconfigurable logic, heterogeneous systems, application-specific logic, and even radically different technologies such as quantum computing, detailed cycle-level simulation is no longer presupposed. Exploring designs under such complex interacting relationships (e.g., performance, energy, thermal, cost, voltage, frequency, cooling energy, leakage, etc.) calls for a more integrative but higher-level approach. We propose Charm, a domain specific language supporting Closed-form High-level ARchitecture Modeling. Charm enables mathematical representations of mutually dependent architectural relationships to be specified, composed, checked, evaluated and reused. The language is interpreted through a combination of symbolic evaluation (e.g., restructuring) and compiler techniques (e.g., memoization and invariant hoisting), generating executable evaluation functions and optimized analysis procedures. Further supporting reuse, a type system constrains architectural quantities and ensures models operate only in a validated domain. Through two case studies, we demonstrate that Charm allows one to define high-level architecture models concisely, maximize reusability, capture unreasonable assumptions and inputs, and significantly speedup design space exploration.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {152–165},
numpages = {14},
keywords = {modeling, abstraction, DSL},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00024,
author = {Liu, Yuxi and Zhao, Xia and Jahre, Magnus and Wang, Zhenlin and Wang, Xiaolin and Luo, Yingwei and Eeckhout, Lieven},
title = {Get out of the valley: power-efficient address mapping for GPUs},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00024},
doi = {10.1109/ISCA.2018.00024},
abstract = {GPU memory systems adopt a multi-dimensional hardware structure to provide the bandwidth necessary to support 100s to 1000s of concurrent threads. On the software side, GPU-compute workloads also use multi-dimensional structures to organize the threads. We observe that these structures can combine unfavorably and create significant resource imbalance in the memory subsystem --- causing low performance and poor power-efficiency. The key issue is that it is highly application-dependent which memory address bits exhibit high variability.To solve this problem, we first provide an entropy analysis approach tailored for the highly concurrent memory request behavior in GPU-compute workloads. Our window-based entropy metric captures the information content of each address bit of the memory requests that are likely to co-exist in the memory system at runtime. Using this metric, we find that GPU-compute workloads exhibit entropy valleys distributed throughout the lower order address bits. This indicates that efficient GPU-address mapping schemes need to harvest entropy from broad address-bit ranges and concentrate the entropy into the bits used for channel and bank selection in the memory subsystem. This insight leads us to propose the Page Address Entropy (PAE) mapping scheme which concentrates the entropy of the row, channel and bank bits of the input address into the bank and channel bits of the output address. PAE maps straightforwardly to hardware and can be implemented with a tree of XOR-gates. PAE improves performance by 1.31X and power-efficiency by 1.25X compared to state-of-the-art permutation-based address mapping.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {166–179},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00025,
author = {Shin, Seunghee and Cox, Guilherme and Oskin, Mark and Loh, Gabriel H. and Solihin, Yan and Bhattacharjee, Abhishek and Basu, Arkaprava},
title = {Scheduling page table walks for irregular GPU applications},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00025},
doi = {10.1109/ISCA.2018.00025},
abstract = {Recent studies on commercial hardware demonstrated that irregular GPU applications can bottleneck on virtual-to-physical address translations. In this work, we explore ways to reduce address translation overheads for such applications.We discover that the order of servicing a GPU's address translation requests (specifically, page table walks) plays a key role in determining the amount of translation overhead experienced by an application. We find that different SIMD instructions executed by an application require vastly different amounts of work to service their address translation needs, primarily depending upon the number of distinct pages they access. We show that better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their address translation needs.Further, in the GPU's Single-Instruction-Multiple-Thread (SIMT) execution paradigm, all threads that execute in lockstep (wavefront) need to finish operating on their respective data elements (and thus, finish their address translations) before the execution moves ahead. Thus, batching walk requests originating from the same SIMD instruction could reduce unnecessary stalls. We demonstrate that the reordering of translation requests based on the above principles improves the performance of several irregular GPU applications by 30\% on average.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {180–192},
numpages = {13},
keywords = {virtual address, computer architecture, GPU},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00026,
author = {Parasar, Mayank and Bhattacharjee, Abhishek and Krishna, Tushar},
title = {SEESAW: using superpages to improve VIPT caches},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00026},
doi = {10.1109/ISCA.2018.00026},
abstract = {Hardware caches balance fast lookup, high hit rates, energy efficiency, and simplicity of implementation. For L1 caches however, achieving this balance is difficult because of constraints imposed by virtual memory. L1 caches are usually virtually-indexed and physically tagged (VIPT), but this means that they must be highly associative to achieve good capacity. Unfortunately, excessive associativity compromises performance by degrading access times without significantly boosting hit rates, and increases access energy.We propose Seesaw to overcome this problem. Seesaw leverages the increasing ubiquity of superpages1 - since super-pages have more page offset bits, they can accommodate VIPT caches with more sets than what is traditionally possible with only base page sizes. Seesaw dynamically reduces the number of ways that are looked up based on the page size, improving performance and energy. Seesaw requires modest hardware and no OS or application changes.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {193–206},
numpages = {14},
keywords = {virtual memory, superpages, memory systems, L1 caches},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00027,
author = {Vijaykumar, Nandita and Jain, Abhilasha and Majumdar, Diptesh and Hsieh, Kevin and Pekhimenko, Gennady and Ebrahimi, Eiman and Hajinazar, Nastaran and Gibbons, Phillip B. and Mutlu, Onur},
title = {A case for richer cross-layer abstractions: bridging the semantic gap with expressive memory},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00027},
doi = {10.1109/ISCA.2018.00027},
abstract = {This paper makes a case for a new cross-layer interface, Expressive Memory (XMem), to communicate higher-level program semantics from the application to the system software and hardware architecture. XMem provides (i) a flexible and extensible abstraction, called an Atom, enabling the application to express key program semantics in terms of how the program accesses data and the attributes of the data itself, and (ii) new cross-layer interfaces to make the expressed higher-level information available to the underlying OS and architecture. By providing key information that is otherwise unavailable, XMem exposes a new, rich view of the program data to the OS and the different architectural components that optimize memory system performance (e.g., caches, memory controllers).By bridging the semantic gap between the application and the underlying memory resources, XMem provides two key benefits. First, it enables architectural/system-level techniques to leverage key program semantics that are challenging to predict or infer. Second, it improves the efficacy and portability of software optimizations by alleviating the need to tune code for specific hardware resources (e.g., cache space). While XMem is designed to enhance and enable a wide range of memory optimizations, we demonstrate the benefits of XMem using two use cases: (i) improving the performance portability of software-based cache optimization by expressing the semantics of data locality in the optimization and (ii) improving the performance of OS-based page placement in DRAM by leveraging the semantics of data structures and their access properties.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {207–220},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00028,
author = {Ros, Alberto and Kaxiras, Stefanos},
title = {Non-speculative store coalescing in total store order},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00028},
doi = {10.1109/ISCA.2018.00028},
abstract = {We present a non-speculative solution for a coalescing store buffer in total store order (TSO) consistency. Coalescing violates TSO with respect to both conflicting loads and conflicting stores, if partial state is exposed to the memory system. Proposed solutions for coalescing in TSO resort to speculation-and-rollback or centralized arbitration to guarantee atomicity for the set of stores whose order is affected by coalescing. These solutions can suffer from scalability, complexity, resource-conflict deadlock, and livelock problems. A non-speculative solution that writes out coalesced cachelines, one at a time, over a typical directory-based MESI coherence layer, has the potential to transcend these problems if it can guarantee absence of deadlock in a practical way.There are two major problems for a non-speculative coalescing store buffer: i) how to present to the memory system a group of coalesced writes as atomic, and ii) how to not deadlock while attempting to do so. For this, we introduce a new lexicographical order. Relying on this order, conflicting atomic groups of coalesced writes can be individually performed per cache block, without speculation, rollback, or replay, and without deadlock or livelock, yet appear atomic to conflicting parties and preserve TSO. One of our major contributions is to show that lexicographical orders based on a small part of the physical address (sub-address order) are deadlock-free throughout the system when taking into account resource-conflict deadlocks. Our approach exceeds the performance and energy benefits of two baseline TSO store buffers and matches the coalescing (and energy savings) of a release-consistency store buffer, at comparable cost.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {221–234},
numpages = {14},
keywords = {store coalescing, store buffer, memory consistency, lexicographical order, deadlock free},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00029,
author = {Jin, Zhaoxiang and \"{O}nder, Soner},
title = {Dynamic memory dependence predication},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00029},
doi = {10.1109/ISCA.2018.00029},
abstract = {Store-queue-free architectures remove the store queue and use memory cloaking to communicate in-flight stores instead. In these architectures, frequent mispredictions may occur when the store to load dependencies are inconsistent. We present DMDP (Dynamic Memory Dependence Predication) which modifies the microarchitecture behavior for such loads to mitigate memory dependence mispredictions. When a given dependence is hard to predict, i.e., a given load occasionally depends on a particular store, but it is independent at other times, DMDP predicates the load so that the address of the load is compared with the address of the predicted store to compute a predicate. This predicate guides the load to obtain the value from either the cache or the colliding store.The predication provided by DMDP i) enables the loads and their dependent instructions to execute much earlier, ii) reduces the hardware complexity of store-queue-free mechanisms, and iii) reduces the number of mispredictions. DMDP outperforms a state-of-the-art store-queue-free architecture by 7.17\% on Integer benchmarks and 4.48\% on Float benchmarks in our Spec 2006 evaluation. We further show that despite executing extra predication instructions, DMDP is power efficient as it saves about 6.7\% on EDP.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {235–246},
numpages = {12},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00030,
author = {Oswald, Nicolai and Nagarajan, Vijay and Sorin, Daniel J.},
title = {Protogen: automatically generating directory cache coherence protocols from atomic specifications},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00030},
doi = {10.1109/ISCA.2018.00030},
abstract = {Designing directory cache coherence protocols is complicated because coherence transactions are not atomic in modern multicore processors. A coherence transaction comprises multiple messages, and these messages can interleave with other conflicting coherence transactions initiated by other cores. To overcome this architectural challenge, we present ProtoGen, an automated tool for taking the description of a directory protocol with atomic transactions (i.e., no concurrency) and generating the corresponding protocol for a multicore with non-atomic transactions. ProtoGen outputs the finite state machines for the cache and directory controllers, including all of the transient states that are possible with concurrent transactions. We have used ProtoGen to generate complete MSI, MESI, and MOSI protocols given their stable state protocol specifications. We have verified the generated protocols for safety and deadlock freedom using the Murϕ model checker. Our generated protocols are identical to or better than manually generated protocols, at times even discovering opportunities to reduce stalling.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {247–260},
numpages = {14},
keywords = {hardware synthesis, design automation, cache coherence protocols},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00031,
author = {Alsop, Johnathan and Sinclair, Matthew D. and Adve, Sarita V.},
title = {Spandex: a flexible interface for efficient heterogeneous coherence},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00031},
doi = {10.1109/ISCA.2018.00031},
abstract = {Recent heterogeneous architectures have trended toward tighter integration and shared memory largely due to the efficient communication and programmability enabled by this shift. However, such integration is complex, because accelerators have widely disparate methods for accessing and keeping data coherent. Some processors use caches backed by hardware coherence protocols like MESI, while others prefer lightweight software coherence protocols or use specialized memories like scratchpads with differing state and communication granularities. Modern solutions tend to build interfaces that extend existing MESI-style CPU coherence protocols, often by adding hierarchical indirection through intermediate shared caches. Although functionally correct, these strategies lack flexibility and generally suffer from performance limitations that make them sub-optimal for some emerging accelerators and workloads.Instead, we need a flexible interface that can efficiently integrate existing and future devices - without requiring intrusive changes to their memory structure. We introduce Spandex, an improved coherence interface based on the simple and scalable DeNovo coherence protocol. Spandex (which takes its name from the flexible material commonly used in one-size-fits-all textiles) directly interfaces devices with diverse coherence properties and memory demands, enabling each device to communicate in a manner appropriate for its specific access properties. We demonstrate the importance of this flexibility by comparing this strategy against a more conventional MESI-based hierarchical solution for a diverse range of heterogeneous applications. On average for the applications studied, Spandex reduces execution time by 16\% (max 29\%) and network traffic by 27\% (max 58\%) relative to the MESI-based hierarchical solution.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {261–274},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00032,
author = {Lee, Dayeol and Lee, Gwangmu and Kwon, Dongup and Lee, Sunghwa and Kim, Youngsok and Kim, Jangwoo},
title = {Flexon: a flexible digital neuron for efficient spiking neural network simulations},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00032},
doi = {10.1109/ISCA.2018.00032},
abstract = {Spiking Neural Networks (SNNs) play an important role in neuroscience as they help neuroscientists understand how the nervous system works. To model the nervous system, SNNs incorporate the concept of time into neurons and inter-neuron interactions called spikes; a neuron's internal state changes with respect to time and input spikes, and a neuron fires an output spike when its internal state satisfies certain conditions. As the neurons forming the nervous system behave differently, SNN simulation frameworks must be able to simulate the diverse behaviors of the neurons. To support any neuron models, some frameworks rely on general-purpose processors at the cost of inefficiency in simulation speed and energy consumption. The other frameworks employ specialized accelerators to overcome the inefficiency; however, the accelerators support only a limited set of neuron models due to their model-driven designs, making accelerator-based frameworks unable to simulate target SNNs.In this paper, we present Flexon, a flexible digital neuron which exploits the biologically common features shared by diverse neuron models, to enable efficient SNN simulations. To design Flexon, we first collect SNNs from prior work in neuroscience research and analyze the neuron models the SNNs employ. From the analysis, we observe that the neuron models share a set of biologically common features, and that the features can be combined to simulate a significantly larger set of neuron behaviors than the existing model-driven designs. Furthermore, we find that the features share a small set of computational primitives which can be exploited to further reduce the chip area. The resulting digital neurons, Flexon and spatially folded Flexon, are flexible, highly efficient, and can be easily integrated with existing hardware. Our prototyping results using TSMC 45 nm standard cell library show that a 12-neuron Flexon array improves energy efficiency by 6,186x and 422x over CPU and GPU, respectively, in a small footprint of 9.26 mm2. The results also show that a 72-neuron spatially folded Flexon array incurs a smaller footprint of 7.62 mm2 and achieves geomean speedups of 122.45x and 9.83x over CPU and GPU, respectively.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {275–288},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00033,
author = {Smith, James E.},
title = {Space-time algebra: a model for neocortical computation},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00033},
doi = {10.1109/ISCA.2018.00033},
abstract = {A proposed first step in replicating the computational methods used in the brain's neocortex is the development of a feedforward computing paradigm based on temporal relationships among inter-neuron voltage spikes. A "space-time" algebra captures the essential features of such a paradigm. The space-time algebra supports biologically plausible neural networks, as envisioned by theoretical neuroscientists. It also supports a generalization of previously proposed "race logic". A key feature of race logic is that it can be directly implemented with off-the-shelf CMOS digital circuits. This opens the possibility of designing brain-like neural networks in the neuroscience domain and implementing them directly in the CMOS digital circuit domain.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {289–300},
numpages = {12},
keywords = {temporal neural networks, spiking neurons, space-time algebra, race logic},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00034,
author = {Zhang, Xiangyu and Bashizade, Ramin and LaBoda, Craig and Dwyer, Chris and Lebeck, Alvin R.},
title = {Architecting a stochastic computing unit with molecular optical devices},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00034},
doi = {10.1109/ISCA.2018.00034},
abstract = {The increasing difficulty in leveraging CMOS scaling for improved performance requires exploring alternative technologies. A promising technique is to exploit the physical properties of devices to specialize certain computations. A recently proposed approach uses molecular-scale optical devices to construct a Resonance Energy based Sampling Unit (RSU) to accelerate sampling from parameterized probability distributions. Sampling is an important component of many algorithms, including statistical machine learning.This paper explores the relationship between application result quality and RSU design. The previously proposed RSU-G focuses on Gibbs sampling using Markov Chain Monte Carlo (MCMC) solvers for Markov Random Field (MRF) Bayesian Inference. By quantitatively analyzing the result quality across three computer vision applications, we find that the previously proposed RSU-G lacks both sufficient precision and dynamic range in key design parameters, which limits the overall result quality compared to software-only MCMC implementations. Naively scaling the problematic parameters to increase precision and dynamic range consumes too much area and power. Therefore, we introduce a new RSU-G microarchitecture that exploits an alternative approach to increase precision that incurs 1.27X power and equivalent area, while maintaining the significant speedups of the previous design and supporting a wider set of applications.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {301–314},
numpages = {14},
keywords = {resonance energy transfer, machine learning, emerging technology, accelerator, Markov random field, Markov chain Monte Carlo, Bayesian inference},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00035,
author = {Korgaonkar, Kunal and Bhati, Ishwar and Liu, Huichu and Gaur, Jayesh and Manipatruni, Sasikanth and Subramoney, Sreenivas and Karnik, Tanay and Swanson, Steven and Young, Ian and Wang, Hong},
title = {Density tradeoffs of non-volatile memory as a replacement for SRAM based last level cache},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00035},
doi = {10.1109/ISCA.2018.00035},
abstract = {Increasing the capacity of the Last Level Cache (LLC) can help scale the memory wall. Due to prohibitive area and leakage power, however, growing conventional SRAM LLC already incurs diminishing returns. Emerging Non-Volatile Memory (NVM) technologies like Spin Torque Transfer RAM (STTRAM) promise high density and low leakage, thereby offering an attractive alternative for building large capacity LLCs. However these technologies have significantly longer write latency compared to SRAM, which interferes with reads and severely limits their performance potential. Despite the recent work showing the write latency reduction at NVM technology level, practical considerations like high yield and low bit error rates will result a significant loss of NVM density when these techniques are implemented. Therefore, improving the write latency while compromising on the density results in sub-optimal usage of the NVM technology.In this paper we present a novel STTRAM LLC design that mitigates the long write latency, thereby delivering SRAM like performance while preserving the benefits of high density. Based on a light-weight learning mechanism, our solution relieves LLC congestion through two schemes. Firstly, we propose write congestion aware bypass that eliminates a large fraction of writes. Despite dropping LLC hit rates which could severely degrade performance in a conventional LLC, our policy smartly modulates the bypass, overcomes the hit rate loss and delivers significant performance gain. Furthermore, our solution establishes a virtual hybrid cache that absorbs and eliminates the redundant writes, which otherwise might be repeatedly and slowly written to the NVM LLC. Detailed simulation of traditional SPEC CPU 2006 suite as well as important industry workloads running on a 4-core system shows that our proposal delivers on an average 26\% performance improvement over a baseline LLC design using 8MB STTRAM, while reducing the memory system energy by 10\%. Our design outperforms a similar area SRAM LLC by nearly 18\%, thereby making NVM technology an attractive alternative for future high performance computing.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {315–327},
numpages = {13},
keywords = {non-volatile, STTRAM, LLC},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00036,
author = {Young, Vinson and Chou, Chiachen and Jaleel, Aamer and Qureshi, Moinuddin K.},
title = {ACCORD: enabling associativity for gigascale DRAM caches by coordinating way-install and way-prediction},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00036},
doi = {10.1109/ISCA.2018.00036},
abstract = {Stacked-DRAM technology has enabled high bandwidth gigascale DRAM caches. Since DRAM caches require a tag store of several tens of megabytes, commercial DRAM cache designs typically co-locate tag and data within the DRAM array. DRAM caches are organized as a direct-mapped structure so that the tag and data can be streamed out in a single access. While direct-mapped DRAM caches provide low hit-latency, they suffer from low hit-rate due to conflict misses. Ideally, we want the hit-rate of a set-associative DRAM cache, without incurring additional latency and bandwidth costs of increasing associativity. To address this problem, way prediction can be applied to a set-associative DRAM cache to achieve the latency and bandwidth of a direct-mapped DRAM cache. Unfortunately, conventional way prediction policies typically require per-set storage, causing multi-megabyte storage overheads for gigascale DRAM caches. If we can obtain accurate way prediction without incurring significant storage overheads, we can efficiently enable set-associativity for DRAM caches.This paper proposes &lt;u&gt;A&lt;/u&gt;sso&lt;u&gt;c&lt;/u&gt;iativity via &lt;u&gt;C&lt;/u&gt;o&lt;u&gt;ord&lt;/u&gt;inated Way-Install and Way-Prediction (ACCORD), a design that steers an incoming line to a "preferred way" based on the line address and uses the preferred way as the default way prediction. We propose two way-steering policies that are effective for 2-way caches. First, Probabilistic Way-Steering (PWS), which steers lines to a preferred way with high probability, while still allowing lines to be installed in an alternate way in case of conflicts. Second, Ganged Way-Steering (GWS), which steers lines of a spatially contiguous region to the way where an earlier line from that region was installed. On a 2-way cache, ACCORD (PWS+GWS) obtains a way prediction accuracy of 90\% and retains a hit-rate similar to a baseline 2-way cache while incurring 320 bytes of storage overhead. We extend ACCORD to support highly-associative caches using a Skewed Way-Steering (SWS) design that steers a line to at-most two ways in the highly-associative cache. This design retains the low-latency of the 2-way ACCORD while obtaining most of the hit-rate benefits of a highly associative design. Our studies with a 4GB DRAM cache backed by non-volatile memory shows that ACCORD provides an average of 11\% speedup (up to 54\%) across a wide range of workloads.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {328–339},
numpages = {12},
keywords = {stacked DRAM, cache, associativity, HBM},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00037,
author = {Tu, Fengbin and Wu, Weiwei and Yin, Shouyi and Liu, Leibo and Wei, Shaojun},
title = {RANA: towards efficient neural acceleration with refresh-optimized embedded DRAM},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00037},
doi = {10.1109/ISCA.2018.00037},
abstract = {The growing size of convolutional neural networks (CNNs) requires large amounts of on-chip storage. In many CNN accelerators, their limited on-chip memory capacity causes massive off-chip memory access and leads to very high system energy consumption. Embedded DRAM (eDRAM), with higher density than SRAM, can be used to improve on-chip buffer capacity and reduce off-chip access. However, eDRAM requires periodic refresh to maintain data retention, which costs much energy consumption.Refresh is unnecessary if the data's lifetime in eDRAM is shorter than the eDRAM's retention time. Based on this principle, we propose a Retention-Aware Neural Acceleration (RANA) framework for CNN accelerators to save total system energy consumption with refresh-optimized eDRAM. The RANA framework includes three levels of techniques: a retention-aware training method, a hybrid computation pattern and a refresh-optimized eDRAM controller. At the training level, CNN's error resilience is exploited in training to improve eDRAM's tolerable retention time. At the scheduling level, RANA assigns each CNN layer with a computation pattern that consumes the lowest energy. At the architecture level, a refresh-optimized eDRAM controller is proposed to alleviate unnecessary refresh operations. We implement an evaluation platform to verify RANA. Owing to the RANA framework, 99.7\% eDRAM refresh operations can be removed with negligible performance and accuracy loss. Compared with the conventional SRAM-based CNN accelerator, an eDRAM-based CNN accelerator strengthened by RANA can save 41.7\% off-chip memory access and 66.2\% system energy consumption, with the same area cost.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {340–352},
numpages = {13},
keywords = {retention time, refresh optimization, neural network, embedded DRAM (eDRAM)},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00038,
author = {Fuchs, Adi and Wentzlaff, David},
title = {Scaling datacenter accelerators with compute-reuse architectures},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00038},
doi = {10.1109/ISCA.2018.00038},
abstract = {Hardware specialization is commonly used in data-centers to ameliorate the nearing end of CMOS technology scaling. While offering superior performance and energy-efficiency returns compared to general-purpose processors, specialized accelerators are bound to the same device technology constraints, and are thus prone to similar limitations in the future. Once technology scaling plateaus, accelerator and application tuning will reach a point of near-optimum, with no clear direction for further improvements.Emerging non-volatile memory (NVM) technologies follow different scaling trends due to different physical properties and manufacturing techniques. NVMs have inspired recent efforts of innovation in computer systems, as they possess appealing qualities such as high capacity and low energy.We present the COmpute-REuse Accelerators (COREx) architecture that shifts computations from the scalability-hindered transistor-based logic towards the continuing-to-scale storage domain. COREx leverages datacenter redundancy by integrating a storage layer together with the accelerator processing layer. The added layer stores the outcomes of previous accelerated computations. The previously computed results are reused in the case of recurring computations, thus eliminating the need to re-compute them.We designed COREx as a combination of an accelerator and specialized storage layer using emerging memory technologies, and evaluated it on a set of datacenter workloads. Our results show that, when integrated with a well-tuned accelerator, COREx achieves an average speedup of 6.4X and average savings of 50\% in energy and 68\% in energy-delay product. We expect further increase in gains in the future, as memory technologies continue to improve steadily.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {353–366},
numpages = {14},
keywords = {memoization, emerging memories, accelerators},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00039,
author = {Feinberg, Ben and Vengalam, Uday Kumar Reddy and Whitehair, Nathan and Wang, Shibo and Ipek, Engin},
title = {Enabling scientific computing on memristive accelerators},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00039},
doi = {10.1109/ISCA.2018.00039},
abstract = {Linear algebra is ubiquitous across virtually every field of science and engineering, from climate modeling to macroeconomics. This ubiquity makes linear algebra a prime candidate for hardware acceleration, which can improve both the run time and the energy efficiency of a wide range of scientific applications. Recent work on memristive hardware accelerators shows significant potential to speed up matrix-vector multiplication (MVM), a critical linear algebra kernel at the heart of neural network inference tasks. Regrettably, the proposed hardware is constrained to a narrow range of workloads: although the eight- to 16-bit computations afforded by memristive MVM accelerators are acceptable for machine learning, they are insufficient for scientific computing where high-precision floating point is the norm.This paper presents the first proposal to enable scientific computing on memristive crossbars. Three techniques are explored---reducing overheads by exploiting exponent range locality, early termination of fixed-point computation, and static operation scheduling---that together enable a fixed-point memristive accelerator to perform high-precision floating point without the exorbitant cost of naive floating-point emulation on fixed-point hardware. A heterogeneous collection of crossbars with varying sizes is proposed to efficiently handle sparse matrices, and an algorithm for mapping the dense subblocks of a sparse matrix to an appropriate set of crossbars is investigated. The accelerator can be combined with existing GPU-based systems to handle datasets that cannot be efficiently handled by the memristive accelerator alone. The proposed optimizations permit the memristive MVM concept to be applied to a wide range of problem domains, respectively improving the execution time and energy dissipation of sparse linear solvers by 10.3x and 10.9x over a purely GPU-based system.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {367–382},
numpages = {16},
keywords = {resistive RAM, accelerator architectures},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00040,
author = {Eckert, Charles and Wang, Xiaowei and Wang, Jingcheng and Subramaniyan, Arun and Iyer, Ravi and Sylvester, Dennis and Blaauw, David and Das, Reetuparna},
title = {Neural cache: bit-serial in-cache acceleration of deep neural networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00040},
doi = {10.1109/ISCA.2018.00040},
abstract = {This paper presents the Neural Cache architecture, which re-purposes cache structures to transform them into massively parallel compute units capable of running inferences for Deep Neural Networks. Techniques to do in-situ arithmetic in SRAM arrays, create efficient data mapping and reducing data movement are proposed. The Neural Cache architecture is capable of fully executing convolutional, fully connected, and pooling layers in-cache. The proposed architecture also supports quantization in-cache.Our experimental results show that the proposed architecture can improve inference latency by 18.3X over state-of-art multi-core CPU (Xeon E5), 7.7X over server class GPU (Titan Xp), for Inception v3 model. Neural Cache improves inference throughput by 12.4X over CPU (2.2X over GPU), while reducing power consumption by 50\% over CPU (53\% over GPU).},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {383–396},
numpages = {14},
keywords = {in-memory architecture, convolution neural network, cache, bit-serial architecture},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00041,
author = {Tavakkol, Arash and Sadrosadati, Mohammad and Ghose, Saugata and Kim, Jeremie S. and Luo, Yixin and Wang, Yaohua and Ghiasi, Nika Mansouri and Orosa, Lois and G\'{o}mez-Luna, Juan and Mutlu, Onur},
title = {FLIN: enabling fairness and enhancing performance in modern NVMe solid state drives},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00041},
doi = {10.1109/ISCA.2018.00041},
abstract = {Modern solid-state drives (SSDs) use new host-interface protocols, such as NVMe, to provide applications with fast access to storage. These new protocols make use of a concept known as the multi-queue SSD (MQ-SSD), where the SSD has direct access to the application-level I/O request queues. This removes most of the OS software stack that was used in older protocols to control how and when the I/O requests were dispatched to storage devices. Unfortunately, while the elimination of the OS software stack leads to a significant performance improvement, we show in this paper that it introduces a new problem: unfairness. This is because the elimination of the OS software stack eliminates the mechanisms that were used to provide fairness among applications in older SSDs.To study application-level unfairness, we perform experiments using four real state-of-the-art MQ-SSDs. We demonstrate that the lack of fair scheduling mechanisms leads to high unfairness among concurrently-executing applications due to the interference among them. For instance, when one of these applications issues many more I/O requests than others, the other applications are slowed down significantly. We perform a comprehensive analysis of interference in real MQ-SSDs, and find four major interference sources: (1) the intensity of requests sent by each application, (2) differences in request access patterns, (3) the ratio of reads to writes, and (4) garbage collection.To alleviate unfairness in MQ-SSDs, we propose the &lt;u&gt;F&lt;/u&gt;lash-&lt;u&gt;L&lt;/u&gt;evel &lt;u&gt;IN&lt;/u&gt;terference-aware scheduler (FLIN). FLIN is a lightweight I/O request scheduling mechanism that provides fairness among requests from different applications. FLIN uses a three-stage scheduling algorithm that protects against all four major sources of interference, while respecting the application-level priorities assigned by the host. FLIN is implemented fully within the SSD controller firmware, requiring no new hardware, and has negligible (&lt;0.06\%) storage cost. Compared to a state-of-the-art I/O scheduler, FLIN improves the fairness and performance of a wide range of enterprise and datacenter storage workloads, with an average improvement of 70\% and 47\%, respectively.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {397–410},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00042,
author = {Jun, Sang-Woo and Wright, Andy and Zhang, Sizhuo and Xu, Shuotao and Arvind},
title = {GraFboost: using accelerated flash storage for external graph analytics},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00042},
doi = {10.1109/ISCA.2018.00042},
abstract = {We describe GraFBoost, a flash-based architecture with hardware acceleration for external analytics of multi-terabyte graphs. We compare the performance of GraFBoost with 1 GB of DRAM against various state-of-the-art graph analytics software including FlashGraph, running on a 32-thread Xeon server with 128 GB of DRAM. We demonstrate that despite the relatively small amount of DRAM, GraFBoost achieves high performance with very large graphs no other system can handle, and rivals the performance of the fastest software platforms on sizes of graphs that existing platforms can handle. Unlike in-memory and semi-external systems, GraFBoost uses a constant amount of memory for all problems, and its performance decreases very slowly as graph sizes increase, allowing GraFBoost to scale to much larger problems than possible with existing systems while using much less resources on a single-node system.The key component of GraFBoost is the sort-reduce accelerator, which implements a novel method to sequentialize fine-grained random accesses to flash storage. The sort-reduce accelerator logs random update requests and then uses hardware-accelerated external sorting with interleaved reduction functions. GraFBoost also stores newly updated vertex values generated in each superstep of the algorithm lazily with the old vertex values to further reduce I/O traffic.We evaluate the performance of GraFBoost for PageRank, breadth-first search and betweenness centrality on our FPGA-based prototype (Xilinx VC707 with 1 GB DRAM and 1 TB flash) and compare it to other graph processing systems including a pure software implementation of GrapFBoost.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {411–424},
numpages = {14},
keywords = {sort-reduce, hardware acceleration, graph analytics, flash storage, FPGA},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00043,
author = {Bae, Duck-Ho and Jo, Insoon and Choi, Youra Adel and Hwang, Joo-Young and Cho, Sangyeun and Lee, Dong-Gi and Jeong, Jaeheon},
title = {2B-SSD: the case for dual, byte- and block-addressable solid-state drives},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00043},
doi = {10.1109/ISCA.2018.00043},
abstract = {Performance critical transaction and storage systems require fast persistence of write data. Typically, a non-volatile RAM (NVRAM) is employed on the datapath to the permanent storage, to temporarily and quickly store write data before the system acknowledges the write request. NVRAM is commonly implemented with battery-backed DRAM. Unfortunately, battery-backed DRAM is small and costly, and occupies a precious DIMM slot. In this paper, we make a case for dual, byte- and block-addressable solid-state drive (2B-SSD), a novel NAND flash SSD architecture designed to offer a dual view of byte addressability and traditional block addressability at the same time. Unlike a conventional storage device, 2B-SSD allows accessing the same file with two independent byte-and block- I/O paths. It controls the data transfer between its internal DRAM and NAND flash memory through an intuitive software interface, and manages the mapping of the two address spaces. 2B-SSD realizes a wholly different way and speed of accessing files on a storage device; applications can access them directly using memory-mapped I/O, and moreover write with a DRAM-like latency. To quantify the benefits of 2B-SSD, we modified logging subsystems of major database engines to store log records directly on it without buffering them in the host memory. When running popular workloads, we measured throughput gains in the range of 1.2X and 2.8X with no risk of data loss.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {425–438},
numpages = {14},
keywords = {non-volatile memory, WAL, 2B-SSD},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00044,
author = {Alshboul, Mohammad and Tuck, James and Solihin, Yan},
title = {Lazy persistency: a high-performing and write-efficient software persistency technique},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00044},
doi = {10.1109/ISCA.2018.00044},
abstract = {Emerging Non-Volatile Memories (NVMs) are expected to be included in future main memory, providing the opportunity to host important data persistently in main memory. However, achieving persistency requires that programs be written with failure-safety in mind. Many persistency models and techniques have been proposed to help the programmer reason about failure-safety. They require that the programmer eagerly flush data out of caches to make it persistent. Eager persistency comes with a large overhead because it adds many instructions to the program for flushing cache lines and incurs costly stalls at barriers to wait for data to become durable.To reduce these overheads, we propose Lazy Persistency (LP), a software persistency technique that allows caches to slowly send dirty blocks to the NVMM through natural evictions. With LP, there are no additional writes to NVMM, no decrease in write endurance, and no performance degradation from cache line flushes and barriers. Persistency failures are discovered using software error detection (checksum), and the system recovers from them by recomputing inconsistent results. We describe the properties and design of LP and demonstrate how it can be applied to loop-based kernels popularly used in scientific computing. We evaluate LP and compare it to the state-of-the-art Eager Persistency technique from prior work. Compared to it, LP reduces the execution time and write amplification overheads from 9\% and 21\% to only 1\% and 3\%, respectively.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {439–451},
numpages = {13},
keywords = {multi-core and parallel architectures, memory systems, emerging memory technology},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00045,
author = {Joshi, Arpit and Nagarajan, Vijay and Cintra, Marcelo and Viglas, Stratis},
title = {DHTM: durable hardware transactional memory},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00045},
doi = {10.1109/ISCA.2018.00045},
abstract = {The emergence of byte-addressable persistent (non-volatile) memory provides a low latency and high bandwidth path to durability. However, programmers need guarantees on what will remain in persistent memory in the event of a system crash. A widely accepted model for crash consistent programming is ACID transactions, in which updates within a transaction are made visible as well as durable in an atomic manner. However, existing software based proposals suffer from significant performance overheads.In this paper, we support both atomic visibility and durability in hardware. We propose DHTM (durable hardware transactional memory) that leverages a commercial HTM to provide atomic visibility and extends it with hardware support for redo logging to provide atomic durability. Furthermore, we leverage the same logging infrastructure to extend the supported transaction size (from being L1-limited to LLC-limited) with only minor changes to the coherence protocol. Our evaluation shows that DHTM outperforms the state-of-the-art by an average of 21\% to 25\% on TATP, TPC-C and a set of microbenchmarks. We believe DHTM is the first complete and practical hardware based solution for ACID transactions that has the potential to significantly ease the burden of crash consistent programming.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {452–465},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00046,
author = {Wang, Tiancong and Sambasivam, Sakthikumaran and Tuck, James},
title = {Hardware supported permission checks on persistent objects for performance and programmability},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00046},
doi = {10.1109/ISCA.2018.00046},
abstract = {Non-Volatile Memory technologies are advancing rapidly and may augment or replace DRAM in future systems. However, a key question is how programmers will use them to construct and manipulate persistent data. One possible approach gives programmers direct access to persistent memory using relocatable persistent pools that hold persistent objects which can be accessed using persistent pointers, called ObjectIDs. Prior work has shown that hardware-supported address translation for ObjectIDs provides significant performance improvement and simplifies programming, however these works did not consider the large overheads incurred to check permissions before accessing persistent objects.In this paper, we identify permission checking in hardware as a critical mechanism that must be included when translating ObjectIDs to addresses in order to simplify programming and fully benefit from hardware translation. To support it, we add a System Persistent Object Table (SPOT) to support translation and permissions checks on ObjectIDs. The SPOT holds all known pools, their physical address, and their permissions information in memory. When a program attempts to access a persistent object, the SPOT is consulted and permissions are verified without trapping to the operating system. We have implemented our new design in a cycle accurate simulator and compared it with software only approaches and prior work. We find that our design offers a compelling 2.9x speedup on average for microbenchmarks that access pools with the RANDOM pattern and 1.4x and 1.8x speedup on TPC-C and vacation, respectively, for the SEPARATE pattern.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {466–478},
numpages = {13},
keywords = {persistent memory programming, persistent data permission check, non-volatile memory},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00047,
author = {Sacks, Jacob and Mahajan, Divya and Lawson, Richard C. and Khaleghi, Behnam and Esmaeilzadeh, Hadi},
title = {Robox: an end-to-end solution to accelerate autonomous control in robotics},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00047},
doi = {10.1109/ISCA.2018.00047},
abstract = {Novel algorithmic advances have paved the way for robotics to transform the dynamics of many social and enterprise applications. To achieve true autonomy, robots need to continuously process and interact with their environment through computationally-intensive motion planning and control algorithms under a low power budget. Specialized architectures offer a potent choice to provide low-power, high-performance accelerators for these algorithms. Instead of taking a traditional route which profiles and maps hot code regions to accelerators, this paper delves into the algorithmic characteristics of the application domain. We observe that many motion planning and control algorithms are formulated as a constrained optimization problems solved online through Model Predictive Control (MPC). While models and objective functions differ between robotic systems and tasks, the structure of the optimization problem and solver remain fixed. Using this theoretical insight, we create RoboX, an end-to-end solution which exposes a high-level domain-specific language to roboticists. This interface allows roboticists to express the physics of the robot and its task in a form close to its concise mathematical expressions. The RoboX backend then automatically maps this high-level specification to a novel programmable architecture, which harbors a programmable memory access engine and compute-enabled interconnects. Hops in the interconnect are augmented with simple functional units that either operate on in-fight data or are bypassed according a micro-program. Evaluations with six different robotic systems and tasks show that RoboX provides a 29.4 x (7.3 x) speedup and 22.1 x (79.4 x) performance-per-watt improvement over an ARM Cortex A57 (Intel Xeon E3). Compared to GPUs, RoboX attains 7.8x, 65.5x, and 71.8x higher Performance-per-Watt to Tegra X2, GTX 650 Ti, and Tesla K40 with a power envelope of only 3.4 Watts at 45 nm.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {479–490},
numpages = {12},
keywords = {robotics, model predictive control, in-network computing, domain-specific languages, accelerators, MPC, DSL},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00048,
author = {Kwon, Dongup and Ahn, Jaehyung and Chae, Dongju and Ajdari, Mohammadamin and Lee, Jaewon and Bae, Suheon and Kim, Youngsok and Kim, Jangwoo},
title = {DCS-ctrl: a fast and flexible device-control mechanism for device-centric server architecture},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00048},
doi = {10.1109/ISCA.2018.00048},
abstract = {Modern high-performance servers leverage a large number of emerging peripheral devices (e.g., data processing accelerators, non-volatile memory storage, high-bandwidth network cards) to meet ever-increasing performance demands of server applications. However, as such servers experience severe kernel overhead due to frequently invoked device operations (e.g., buffer management and data copy), server architects have proposed various hardware and software approaches to enable direct communications among the devices. Unfortunately, existing direct device-to-device (D2D) communication schemes still suffer from low performance and the lack of flexibility. First, software-based schemes depend on complicated kernel routines and necessitate multiple hardware-software and user-kernel boundary crossings, which significantly limit the performance improvement opportunities from direct D2D communications. On the other hand, hardware-based schemes require tight integration and custom-built devices, preventing architects from flexibly adding off-the-shelf devices.In this paper, we propose DCS-ctrl, a novel Hardware-based Device-Control (HDC) mechanism for Device-Centric Server (DCS) architecture to provide fast and CPU-efficient direct D2D communications among a large number of off-the-shelf peripheral devices. The key idea of DCS-ctrl is to implement a low-cost and flexible device-control mechanism on an independent FPGA device called HDC Engine. As HDC Engine manages all data and control transfers among devices at the hardware level, the server achieves high performance, scalability, and flexibility. First, optimizing both data and control paths at the hardware level minimizes the latency of inter-device communications. Second, implementing FPGA-based reconfigurable device controllers enables direct D2D communications among commodity devices and thus improves per-device flexibility. Third, merging heterogeneous device operations with intermediate data processing supports creates more opportunities for direct inter-device communications in server applications. Our DCS-ctrl prototype reduces the latency of software-based direct D2D communications by 42\% and the CPU utilization by 52\%.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {491–504},
numpages = {14},
keywords = {server architecture, device-to-device communication, I/O optimization, FPGA-based accelerator},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00049,
author = {Pothukuchi, Raghavendra Pradyumna and Pothukuchi, Sweta Yamini and Voulgaris, Petros and Torrellas, Josep},
title = {Yukta: multilayer resource controllers to maximize efficiency},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00049},
doi = {10.1109/ISCA.2018.00049},
abstract = {Since computers increasingly execute in constrained environments, they are being equipped with controllers for resource management. However, the operation of modern computer systems is structured in multiple layers, such as the hardware, OS, and networking layers---each with its own resources. Managing such a system scalably and portably requires that we have a controller in each layer, and that the different controllers coordinate their operation. In addition, such controllers should not rely on heuristics, but be based on formal control theory.This paper presents a new approach to build coordinated multilayer formal controllers for computers. The approach uses Structured Singular Value (SSV) controllers from Robust Control Theory. Such controllers are especially suited for multilayer computer system control. Indeed, SSV controllers can read signals from other controllers to coordinate multilayer operation. In addition, they allow designers to specify the discrete values allowed in each input, and the desired bounds on output value deviations. Finally, they accept uncertainty guardbands, which incorporate the effects of interference between the controllers. We call this approach Yukta. To assess its effectiveness, we prototype it in an 8-core big.LITTLE board. We build a two-layer SSV controller, and show that it is very effective. Yukta reduces the ExD and the execution time of a set of applications by an average of 50\% and 38\%, respectively, over advanced heuristic-based coordinated controllers.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {505–518},
numpages = {14},
keywords = {robust control theory, resource management, multilayer computer control, energy efficiency},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00050,
author = {Ajorpaz, Samira Mirbagher and Garza, Elba and Jindal, Sangam and Jim\'{e}nez, Daniel A.},
title = {Exploring predictive replacement policies for instruction cache and branch target buffer},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00050},
doi = {10.1109/ISCA.2018.00050},
abstract = {Modern processors support instruction fetch with the instruction cache (I-cache) and branch target buffer (BTB). Due to timing and area constraints, the I-cache and BTB must efficiently make use of their limited capacities. Blocks in the I-cache or entries in the BTB that have low potential for reuse should be replaced by more useful blocks/entries. This work explores predictive replacement policies based on reuse prediction that can be applied to both the I-cache and BTB.Using a large suite of recently released industrial traces, we show that predictive replacement policies can reduce misses in the I-cache and BTB. We introduce Global History Reuse Prediction (GHRP), a replacement technique that uses the history of past instruction addresses and their reuse behaviors to predict dead blocks in the I-cache and dead entries in the BTB.This paper describes the effectiveness of GHRP as a dead block replacement and bypass optimization for both the I-cache and BTB. For a 64KB set-associative I-cache with a 64B block size, GHRP lowers the I-cache misses per 1000 instructions (MPKI) by an average of 18\% over the least-recently-used (LRU) policy on a set of 662 industrial workloads, performing significantly better than Static Re-reference Interval Prediction (SRRIP) [1] and Sampling Dead Block Prediction (SDBP)[2]. For a 4K-entry BTB, GHRP lowers MPKI by an average of 30\% over LRU, 23\% over SRRIP, and 29\% over SDBP.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {519–532},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00051,
author = {Buckler, Mark and Bedoukian, Philip and Jayasuriya, Suren and Sampson, Adrian},
title = {EVA2: exploiting temporal redundancy in live computer vision},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00051},
doi = {10.1109/ISCA.2018.00051},
abstract = {Hardware support for deep convolutional neural networks (CNNs) is critical to advanced computer vision in mobile and embedded devices. Current designs, however, accelerate generic CNNs; they do not exploit the unique characteristics of real-time vision. We propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. A new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed activation. The technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. We use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. We implement the technique in hardware as an extension to state-of-the-art CNN accelerator designs. The new unit reduces the average energy per frame by 54\%, 62\%, and 87\% for three CNNs with less than 1\% loss in vision accuracy.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {533–546},
numpages = {14},
keywords = {video compression, hardware acceleration, convolutional neural networks, computer vision, computer architecture, application specific integrated circuits},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00052,
author = {Zhu, Yuhao and Samajdar, Anand and Mattina, Matthew and Whatmough, Paul},
title = {Euphrates: algorithm-SoC co-design for low-power mobile continuous vision},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00052},
doi = {10.1109/ISCA.2018.00052},
abstract = {Continuous computer vision (CV) tasks increasingly rely on convolutional neural networks (CNN). However, CNNs have massive compute demands that far exceed the performance and energy constraints of mobile devices. In this paper, we propose and develop an algorithm-architecture co-designed system, Euphrates, that simultaneously improves the energy-efficiency and performance of continuous vision tasks.Our key observation is that changes in pixel data between consecutive frames represents visual motion. We first propose an algorithm that leverages this motion information to relax the number of expensive CNN inferences required by continuous vision applications. We co-design a mobile System-on-a-Chip (SoC) architecture to maximize the efficiency of the new algorithm. The key to our architectural augmentation is to co-optimize different SoC IP blocks in the vision pipeline collectively. Specifically, we propose to expose the motion data that is naturally generated by the Image Signal Processor (ISP) early in the vision pipeline to the CNN engine. Measurement and synthesis results show that Euphrates achieves up to 66\% SoC-level energy savings (4x for the vision computations), with only 1\% accuracy loss.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {547–560},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00053,
author = {Choi, Woo-Seok and Tomei, Matthew and Vicarte, Jose Rodrigo Sanchez and Hanumolu, Pavan Kumar and Kumar, Rakesh},
title = {Guaranteeing local differential privacy on ultra-low-power systems},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00053},
doi = {10.1109/ISCA.2018.00053},
abstract = {Sensors in mobile devices and IoT systems increasingly generate data that may contain private information of individuals. Generally, users of such systems are willing to share their data for public and personal benefit as long as their private information is not revealed. A fundamental challenge lies in designing systems and data processing techniques for obtaining meaningful information from sensor data, while maintaining the privacy of the data and individuals. In this work, we explore the feasibility of providing local differential privacy on ultra-low-power systems that power many sensor and IoT applications. We show that low resolution and fixed point nature of ultra-low-power implementations prevent privacy guarantees from being provided due to low quality noising. We present techniques, resampling and thresholding, to overcome this limitation. The techniques, along with a privacy budget control algorithm, are implemented in hardware to provide privacy guarantees with high integrity. We show that our hardware implementation, DP-Box, has low overhead and provides high utility, while guaranteeing local differential privacy, for a range of sensor/IoT benchmarks.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {561–574},
numpages = {14},
keywords = {randomized response, microcontrollers, low-power systems, differential privacy, RAPPOR, IoT},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00054,
author = {Tan, Cheng and Karunaratne, Manupa and Mitra, Tulika and Peh, Li-Shiuan},
title = {Stitch: fusible heterogeneous accelerators enmeshed with many-core architecture for wearables},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00054},
doi = {10.1109/ISCA.2018.00054},
abstract = {Wearable devices are now leveraging multi-core processors to cater to the increasing computational demands of the applications via multi-threading. However, the power, performance constraints of many wearable applications can only be satisfied when the thread-level parallelism is coupled with hardware acceleration of common computational kernels. The ASIC accelerators with high performance/watt suffer from high non-recurring engineering costs. Configurable accelerators that can be reused across applications present a promising alternative. Autonomous configurable accelerators loosely-coupled to the processor occupy additional silicon area for local data and control and incur data communication overhead. In contrast, configurable instruction set extension (ISE) accelerators tightly integrated into the processor pipeline eliminate such overheads by sharing the existing processor resources. Yet, naively adding full-blown ISE accelerators to each core in a many-core architecture will lead to huge area and power overheads, which is clearly infeasible in resource-constrained wearables. In this paper, we propose Stitch, a many-core architecture where tiny, heterogeneous, configurable and fusible ISE accelerators, called polymorphic patches are effectively enmeshed with the cores. The novelty of our architecture lies in the ability to stitch together multiple polymorphic patches, where each can handle very simple ISEs, across the chip to create large, virtual accelerators that can execute complex ISEs. The virtual connections are realized efficiently with a very lightweight compiler-scheduled network-on-chip (NoC) with no buffers or control logic. Our evaluations across representative wearable applications show an average 2.3X improvement in runtime for Stitch compared to a baseline many-core processor without ISEs, at a modest area and power overhead.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {575–587},
numpages = {13},
keywords = {wearables, network-on-chip, manycore architectures, low-power, customization, accelerators},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00055,
author = {Nguyen, Kate and Lyu, Kehan and Meng, Xianze and Sridharan, Vilas and Jian, Xun},
title = {Nonblocking memory refresh},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00055},
doi = {10.1109/ISCA.2018.00055},
abstract = {Since its inception half a century ago, DRAM has required dynamic/active refresh operations that block read requests and decrease performance. We propose refreshing DRAM in the background without stalling read accesses to refreshing memory blocks, similar to the static/background refresh in SRAM. Our proposed Nonblocking Refresh works by refreshing a portion of the data in a memory block at a time and uses redundant data, such as Reed-Solomon codes, in the block to compute the block's refreshing/unreadable data to satisfy read requests. For proof of concept, we apply Nonblocking Refresh to server memory systems, where every memory block already contains redundant data to provide hardware failure protection. In this context, Nonblocking Refresh can utilize server memory system's existing per-block redundant data in the common-case when there are no hardware faults to correct, without requiring any dedicated redundant data of its own. Our evaluations show that on average across five server memory systems with different redundancy and failure protection strengths, Nonblocking Refresh improves performance by 16.2\% and 30.3\% for 16gb and 32gb DRAM chips, respectively.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {588–599},
numpages = {12},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00056,
author = {Sinha, Kanad and Sethumadhavan, Simha},
title = {Practical memory safety with REST},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00056},
doi = {10.1109/ISCA.2018.00056},
abstract = {In this paper, we propose Random Embedded Secret Tokens (REST), a simple hardware primitive to provide content-based checks, and show how it can be used to mitigate common types of spatial and temporal memory errors at very low cost. REST is simply a very large random value that is embedded into programs. To provide memory safety, REST is used to bookend data structures during allocation. If the hardware accesses a REST value during execution, due to programming errors or adversarial actions, it reports a privileged memory safety exception.Implementing REST requires 1 bit of metadata per L1 data cache line and a comparator to check for REST tokens during a cache fill. The software infrastructure to provide memory safety with REST reuses a production-quality memory error detection tool, AddressSanitizer, by changing less than 1.5K lines of code.REST based memory safety offers several advantages compared to extant methods: (1) it does not require significant redesign of hardware or software, (2) the overhead of heap and stack safety is 2\% compared to 40\% for AddressSanitizer, (3) the security of the memory safety implementation is improved compared AddressSanitizer, and (4) REST based memory safety can mitigate heap safety errors in legacy binaries without recompilation or source code. These advantages provide a significant step towards continuous runtime memory safety monitoring and mitigation for legacy and new binaries.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {600–611},
numpages = {12},
keywords = {random embedded secret tokens, privileged memory safety exception, microarchitecture, memory safety, load store queue, hardware support, cache microarchitecture, addresssanitizer, REST},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00057,
author = {Seyedzadeh, Seyed Mohammad and Jones, Alex K. and Melhem, Rami},
title = {Mitigating wordline crosstalk using adaptive trees of counters},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00057},
doi = {10.1109/ISCA.2018.00057},
abstract = {DRAM technology scaling has the undesirable side effect of degrading cell reliability. One such concern of deeply scaled DRAMs is the increased coupling between adjacent cells, commonly referred to as crosstalk. High access frequency of certain rows in the DRAM may cause data loss in cells of physically adjacent rows due to crosstalk. The malicious exploit of this crosstalk by repeatedly accessing a row to induce this effect is known as row hammering. Additionally, inadvertent row hammering may also occur due to the natural weighted nature of applications' access patterns.In this paper, we analyze the efficiency of existing approaches for mitigating wordline crosstalk and demonstrate that they have been conservatively designed. Given the unbalanced nature of DRAM accesses, a small group of dynamically allocated counters in banks can deterministically detect "hot" rows and mitigate crosstalk. Based on our findings, we propose a Counter-based Adaptive Tree (CAT) approach to mitigate wordline crosstalk using adaptive trees of counters to guide appropriate refreshing of vulnerable rows. The key idea is to tune the distribution of the counters to the rows in a bank based on the memory reference patterns. In contrast to deterministic solutions, CAT utilizes fewer counters, making it practically feasible to be implemented on-chip. Compared to existing probabilistic approaches, CAT more precisely refreshes rows vulnerable to crosstalk based on their access frequency.Experimental results on workloads from four benchmark suites show that CAT reduces the Crosstalk Mitigation Refresh Power Overhead in quad-core systems to 7\%, which is an improvement over the 21\% and 18\% incurred in the leading deterministic and probabilistic approaches, respectively. Moreover, CAT incurs very low performance overhead (∼ 0.5\%). Hardware synthesis evaluation shows that CAT can be implemented on-chip with only a nominal area overhead.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {612–623},
numpages = {12},
keywords = {technology scaling, row hammering, DRAM},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00058,
author = {Taram, Mohammadkazem and Venkat, Ashish and Tullsen, Dean M.},
title = {Mobilizing the micro-ops: exploiting context sensitive decoding for security and energy efficiency},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00058},
doi = {10.1109/ISCA.2018.00058},
abstract = {Modern instruction set decoders feature translation of native instructions into internal micro-ops to simplify CPU design and improve instruction-level parallelism. However, this translation is static in most known instances. This work proposes context-sensitive decoding, a technique that enables customization of the micro-op translation at the microsecond or faster granularity, based on the current execution context and/or preset hardware events. While there are many potential applications, this work demonstrates its effectiveness with two use cases: 1) as a novel security defense to thwart instruction/data cache-based side-channel attacks, as demonstrated on commercial implementations of RSA and AES and 2) as a power management technique that performs selective devectorization to enable efficient unit-level power gating.This architecture, first by allowing execution to transition between different translation modes rapidly, defends against a variety of attacks, completely obfuscating code-dependent cache access, only sacrificing 5\% in steady-state performance - orders of magnitude less than prior art. By selectively disabling the vector units without disabling vector arithmetic, context-sensitive decoding reduces energy by 12.9\% with minimal loss in performance. Both optimizations work with no significant changes to the pipeline or the external ISA.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {624–637},
numpages = {14},
keywords = {side channel, security, power gating, microcode},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00059,
author = {Althoff, Alric and McMahan, Joseph and Vega, Luis and Davidson, Scott and Sherwood, Timothy and Taylor, Michael B. and Kastner, Ryan},
title = {Hiding intermittent information leakage with architectural support for blinking},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00059},
doi = {10.1109/ISCA.2018.00059},
abstract = {As demonstrated by numerous practical attacks, the physical act of computation emits unintended and damaging information through infinitesimal variations in timing, power, and resource contention. While there are many techniques for preventing the leakage of information through power channels for specific cryptographic units, they are typically either built directly into the hardware logic or exploit intricate mathematical properties of the algorithm itself. However, such leaks are not uniform in time but, as we show, rather occur in specific bursts. Exploiting this observation we propose a set of software-controlled techniques allowing for the seamless disconnection and reconnection of general purpose programmable components in a system-on-chip. Such a system is capable of providing brief moments of electrical isolation during which the most critical computations can be performed free from both timing and power measurement. Of course, disconnection comes at a cost. To balance the resulting trade-off between overhead and security effectively, we describe a new analysis technique to uncover the "leakiest" intervals of time, we provide an algorithm to co-optimize the covering of these intervals and the performance/energy costs under a set of architecture imposed constraints, and explore the architectural and software ramifications of such intermittent disconnection. In the end we find that by hiding only between 15\% and 30\% of the trace, at a performance cost of between 15\% and 50\%, we are able to reduce the mutual information between the leakage model and key bits by 75\% on average, and to nearly zero in specific cases.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {638–649},
numpages = {12},
keywords = {side-channel attacks, security metrics, hardware security, electronic countermeasures},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00060,
author = {Yazdanbakhsh, Amir and Falahati, Hajar and Wolfe, Philip J. and Samadi, Kambiz and Kim, Nam Sung and Esmaeilzadeh, Hadi},
title = {GANAX: a unified MIMD-SIMD acceleration for generative adversarial networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00060},
doi = {10.1109/ISCA.2018.00060},
abstract = {Generative Adversarial Networks (GANs) are one of the most recent deep learning models that generate synthetic data from limited genuine datasets. GANs are on the frontier as further extension of deep learning into many domains (e.g., medicine, robotics, content synthesis) requires massive sets of labeled data that is generally either unavailable or prohibitively costly to collect. Although GANs are gaining prominence in various fields, there are no accelerators for these new models. In fact, GANs leverage a new operator, called transposed convolution, that exposes unique challenges for hardware acceleration. This operator first inserts zeros within the multidimensional input, then convolves a kernel over this expanded array to add information to the embedded zeros. Even though there is a convolution stage in this operator, the inserted zeros lead to underutilization of the compute resources when a conventional convolution accelerator is employed. We propose the GANAX architecture to alleviate the sources of inefficiency associated with the acceleration of GANs using conventional convolution accelerators, making the first GAN accelerator design possible. We propose a reorganization of the output computations to allocate compute rows with similar patterns of zeros to adjacent processing engines, which also avoids inconsequential multiply-adds on the zeros. This compulsory adjacency reclaims data reuse across these neighboring processing engines, which had otherwise diminished due to the inserted zeros. The reordering breaks the full SIMD execution model, which is prominent in convolution accelerators. Therefore, we propose a unified MIMD-SIMD design for GANAX that leverages repeated patterns in the computation to create distinct microprograms that execute concurrently in SIMD mode. The interleaving of MIMD and SIMD modes is performed at the granularity of single microprogrammed operation. To amortize the cost of MIMD execution, we propose a decoupling of data access from data processing in GANAX. This decoupling leads to a new design that breaks each processing engine to an access micro-engine and an execute micro-engine. The proposed architecture extends the concept of access-execute architectures to the finest granularity of computation for each individual operand. Evaluations with six GAN models shows, on average, 3.6x speedup and 3.1x energy savings over Eyeriss without compromising the efficiency of conventional convolution accelerators. These benefits come with a mere ≈7.8\% area increase. These results suggest that GANAX is an effective initial step that paves the way for accelerating the next generation of deep neural models.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {650–661},
numpages = {12},
keywords = {transpoed convolution, generative adversarial networks, deep neural networks, dataflow, convolution neural networks, access-execute architecture, accelerators, SIMD-MIMD, GAN, DNN, CNN},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00061,
author = {Akhlaghi, Vahideh and Yazdanbakhsh, Amir and Samadi, Kambiz and Gupta, Rajesh K. and Esmaeilzadeh, Hadi},
title = {SnaPEA: predictive early activation for reducing computation in deep convolutional neural networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00061},
doi = {10.1109/ISCA.2018.00061},
abstract = {Deep Convolutional Neural Networks (CNNs) perform billions of operations for classifying a single input. To reduce these computations, this paper offers a solution that leverages a combination of runtime information and the algorithmic structure of CNNs. Specifically, in numerous modern CNNs, the outputs of compute-heavy convolution operations are fed to activation units that output zero if their input is negative. By exploiting this unique algorithmic property, we propose a predictive early activation technique, dubbed SnaPEA. This technique cuts the computation of convolution operations short if it determines that the output will be negative. SnaPEA can operate in two distinct modes, exact and predictive. In the exact mode, with no loss in classification accuracy, SnaPEA statically re-orders the weights based on their signs and periodically performs a single-bit sign check on the partial sum. Once the partial sum drops below zero, the rest of computations can simply be ignored, since the output value will be zero in any case. In the predictive mode, which trades the classification accuracy for larger savings, SnaPEA speculatively cuts the computation short even earlier than the exact mode. To control the accuracy, we develop a multi-variable optimization algorithm that thresholds the degree of speculation. As such, the proposed algorithm exposes a knob to gracefully navigate the trade-offs between the classification accuracy and computation reduction. Compared to a state-of-the-art CNN accelerator, SnaPEA in the exact mode, yields, on average, 28\% speedup and 16\% energy reduction in various modern CNNs without affecting their classification accuracy. With 3\% loss in classification accuracy, on average, 67.8\% of the convolutional layers can operate in the predictive mode. The average speedup and energy saving of these layers are 2.02X and 1.89X, respectively. The benefits grow to a maximum of 3.59X speedup and 3.14X energy reduction. Compared to static pruning approaches, which are complimentary to the dynamic approach of SnaPEA, our proposed technique offers up to 63\% speedup and 49\% energy reduction across the convolution layers with no loss in classification accuracy.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {662–673},
numpages = {12},
keywords = {predictive early activation, deep neural networks, convolutional neural networks, computation reduction, approximate computing, accelerators, DNN, CNN},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00062,
author = {Hegde, Kartik and Yu, Jiyong and Agrawal, Rohit and Yan, Mengjia and Pellauer, Michael and Fletcher, Christopher W.},
title = {UCNN: exploiting computational reuse in deep neural networks via weight repetition},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00062},
doi = {10.1109/ISCA.2018.00062},
abstract = {Convolutional Neural Networks (CNNs) have begun to permeate all corners of electronic society (from voice recognition to scene generation) due to their high accuracy and machine efficiency per operation. At their core, CNN computations are made up of multi-dimensional dot products between weight and input vectors. This paper studies how weight repetition---when the same weight occurs multiple times in or across weight vectors---can be exploited to save energy and improve performance during CNN inference. This generalizes a popular line of work to improve efficiency from CNN weight sparsity, as reducing computation due to repeated zero weights is a special case of reducing computation due to repeated weights.To exploit weight repetition, this paper proposes a new CNN accelerator called the Unique Weight CNN Accelerator (UCNN). UCNN uses weight repetition to reuse CNN sub-computations (e.g., dot products) and to reduce CNN model size when stored in off-chip DRAM---both of which save energy. UCNN further improves performance by exploiting sparsity in weights. We evaluate UCNN with an accelerator-level cycle and energy model and with an RTL implementation of the UCNN processing element. On three contemporary CNNs, UCNN improves throughput-normalized energy consumption by 1.2X ~ 4X, relative to a similarly provisioned baseline accelerator that uses Eyeriss-style sparsity optimizations. At the same time, the UCNN processing element adds only 17-24\% area overhead relative to the same baseline.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {674–687},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00063,
author = {Park, Eunhyeok and Kim, Dongyoung and Yoo, Sungjoo},
title = {Energy-efficient neural network accelerator based on outlier-aware low-precision computation},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00063},
doi = {10.1109/ISCA.2018.00063},
abstract = {Owing to the presence of large values, which we call outliers, conventional methods of quantization fail to achieve significantly low precision, e.g., four bits, for very deep neural networks, such as ResNet-101. In this study, we propose a hardware accelerator, called the outlier-aware accelerator (OLAccel). It performs dense and low-precision computations for a majority of data (weights and activations) while efficiently handling a small number of sparse and high-precision outliers (e.g., amounting to 3\% of total data). The OLAccel is based on 4-bit multiply-accumulate (MAC) units and handles outlier weights and activations in a different manner. For outlier weights, it equips SIMD lanes of MAC units with an additional MAC unit, which helps avoid cycle overhead for the majority of outlier occurrences, i.e., a single occurrence in the SIMD lanes. The OLAccel performs computations using outlier activation on dedicated, high-precision MAC units. In order to avoid coherence problem due to updates from low-and high-precision computation units, both units update partial sums in a pipelined manner. Our experiments show that the OLAccel can reduce by 43.5\% (27.0\%), 56.7\% (36.3\%), and 62.2\% (49.5\%) energy consumption for AlexNet, VGG-16, and ResNet-18, respectively, compared with a 16-bit (8-bit) state-of-the-art zero-aware accelerator. The energy gain mostly comes from the memory components, the DRAM, and on-chip memory due to reduced precision.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {688–698},
numpages = {11},
keywords = {quantization, neural network, accelerator},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00064,
author = {Ramrakhyani, Aniruddh and Gratz, Paul V. and Krishna, Tushar},
title = {Synchronized progress in interconnection networks (SPIN): a new theory for deadlock freedom},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00064},
doi = {10.1109/ISCA.2018.00064},
abstract = {One of the most fundamental design challenges in any interconnection network is that of routing deadlocks. A deadlock is a cyclic dependence between buffers that renders forward progress impossible. Deadlocks are a necessary evil and almost every on-chip/HPC network today avoids it either via routing restrictions across physical channels (Daily's Theory) or with at least one escape virtual channel (Duato's Theory). This ensures that a cyclic dependence between buffers is never created in the first place. Moreover, each solution is tied to a specific topology, requiring an updated policy if the topology were to change. Alternately, solutions have also been proposed to reserve certain resources (buffers) and allocate them only upon detection of a deadlock, thereby breaking the dependence chain and recovering from the deadlock. Unfortunately, all these approaches fundamentally lead to a loss in available bandwidth due to routing restrictions or buffer resource usage restrictions.In this work, we challenge the theoretical notion of viewing deadlocks as a lack of routing resource (buffers) problem that every solution to date is based on. We argue that a deadlock can in fact be considered as a lack of coordination between distributed entities. We prove that orchestrating a forward movement of every flit in the deadlocked ring at exactly the same time, which we call a spin, can guarantee forward progress and eventually lead to deadlock resolution with a bounded number of spins. We name this novel theory as SPIN (Synchronized Progress in Interconnection Networks). SPIN eliminates the need for virtual channels to achieve deadlock freedom thereby enabling fully adaptive routing with only one buffer per message class. We illustrate this capability by designing FAvORS, a novel truly one VC fully-adaptive routing algorithm. We also present a low-cost distributed implementation of SPIN and compare it against state-of-the-art deadlock avoidance/recovery schemes. SPIN provides up to 80\% higher throughput, 52\% lower area and 50\% lower power for an on-chip 64-core mesh, and up to 83\% higher throughput, 53\% lower area and 55\% lower power for an off-chip 1024-node dragon-fly.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {699–711},
numpages = {13},
keywords = {networks on chip, mesh, interconnection networks, dragon-fly, deadlocks},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00065,
author = {Kim, Gwangsun and Choi, Hayoung and Kim, John},
title = {TCEP: traffic consolidation for energy-proportional high-radix networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00065},
doi = {10.1109/ISCA.2018.00065},
abstract = {High-radix topologies in large-scale networks provide low network diameter and high path diversity, but the idle power from high-speed links results in energy inefficiency, especially at low traffic load. In this work, we exploit the high path diversity and non-minimal adaptive routing in high-radix topologies to consolidate traffic to a smaller number of links to enable more network channels to be power-gated. In particular, we propose TCEP (Traffic Consolidation for Energy-Proportional high-radix networks), a distributed, proactive power management mechanism for large-scale networks that achieves energy-proportionality by proactively power-gating network channels through traffic consolidation. Instead of naively power-gating the least utilized link, TCEP differentiates links with the type of traffic (i.e., minimally vs. non-minimally routed traffic) on them since the performance impact of power-gating on minimal traffic is greater than non-minimal traffic. The performance degradation from the reduced number of channels is minimized by concentrating available links to a small number of routers, instead of distributing them across the network, to maximize path diversity. TCEP introduces a shadow link to quickly reactivate an inactive link and Power-Aware progressive Load-balanced (PAL) routing algorithm that incorporates the link power states in load-balancing the network. Our evaluations show that TCEP achieves significantly higher throughput across various traffic patterns while providing comparable energy savings for real workloads, compared to a prior approach proposed for the flattened butterfly topology.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {712–725},
numpages = {14},
keywords = {link power-gating, high-radix networks, global adaptive routing},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00066,
author = {Yin, Jieming and Lin, Zhifeng and Kayiran, Onur and Poremba, Matthew and Altaf, Muhammad Shoaib Bin and Jerger, Natalie Enright and Loh, Gabriel H.},
title = {Modular routing design for chiplet-based systems},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00066},
doi = {10.1109/ISCA.2018.00066},
abstract = {System-on-Chip (SoC) complexity and the increasing costs of silicon motivate the breaking of an SoC into smaller "chiplets." A chiplet-based SoC design process has the promise to enable fast SoC construction by using advanced packaging technologies to tightly integrate multiple disparate chips (e.g., CPU, GPU, memory, FPGA). However, when assembling chiplets into a single SoC, correctness validation becomes a significant challenge. In particular, the network-on-chip (NoC) used within the individual chiplets and across chiplets to tie them together can easily have deadlocks, especially if each chip is designed in isolation.We introduce a simple, modular, yet elegant methodology for ensuring deadlock-free routing in multi-chiplet systems. As an example, we focus on future systems combining chiplets on an active silicon interposer. To maximize modularity, each individual chiplet is free to implement its own NoC topology and local routing algorithm, and the interposer can implement its own independent topology and routing. Our methodology imposes a few simple turn restrictions applied only to traffic as it flows into or out of the chiplets from the interposer, and we provide a way to determine these restrictions. The end result is an overall approach that enables highly-modular, chiplet-based SoC construction while eliminating deadlocks with high performance.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {726–738},
numpages = {13},
keywords = {routing, deadlock-avoidance, chiplet},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00067,
author = {Kapre, Nachiket and Krishna, Tushar},
title = {FastTrack: leveraging heterogeneous FPGA wires to design low-cost high-performance soft NoCs},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00067},
doi = {10.1109/ISCA.2018.00067},
abstract = {Networks-on-Chip (NoCs) implemented on FPGAs have to be designed differently from ASICs to fully exploit the unique architectural features and properties of the FPGA fabric. The FPGA-friendly bufferless, deflection routed Hoplite NoC is almost an order of magnitude smaller and runs at a faster operating frequency than competing classic buffered FPGA NoCs. It is able achieve this by sacrificing NoC link utilization that suffers due to the cost of packet deflections and associated high latency traversals. In this paper, we address these shortcomings by developing FastTrack, which is an FPGA-optimized, high-radix NoC that exploits the segmented interconnect structure of modern FPGAs. We adapt the NoC organization to use express bypass links in the NoC to skip multiple router stages in a single cycle. Our FastTrack design can be tuned to use different express link lengths for performance, and supports depopulation strategies for controlling the balance between FPGA LUT and wiring cost. For the Xilinx Virtex-7 485T FPGA, an 8X8 FastTrack NoC is 1.7--2.5X larger than a base Hoplite NoC, but operates at almost the same clock frequency. FastTrack delivers throughput and latency improvements across a range of statistical workloads (2.5X), and traces extracted from FPGA accelerator case studies such as Sparse Matrix-Vector Multiplication (2.5X), Graph Analytics (2.8X), Token LU Factorization Dataflow (1.4X) and Multi-processor overlay applications (2X). FastTrack also shows energy efficiency improvements by factors of up to 2.2X over baseline Hoplite due to higher sustained rates and high speed operation of express links made possible by fast FPGA interconnect.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {739–751},
numpages = {13},
keywords = {overlay networks, network-on-chip, FPGA},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00068,
author = {Song, Mingcong and Zhao, Jiechen and Hu, Yang and Zhang, Jiaqi and Li, Tao},
title = {Prediction based execution on deep neural networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00068},
doi = {10.1109/ISCA.2018.00068},
abstract = {Recently, deep neural network based approaches have emerged as indispensable tools in many fields, ranging from image and video recognition to natural language processing. However, the large size of such newly developed networks poses both throughput and energy challenges to the underlying processing hardware. This could be the major stumbling block to many promising applications such as self-driving cars and smart cities.Existing work proposes to weed zeros from input neurons to avoid unnecessary DNN computation (zero-valued operand multiplications). However, we observe that many output neurons are still ineffectual even if the zero-removal technique has been applied. These ineffectual output neurons could not pass their values to the subsequent layer, which means all the computations (including zero-valued and non-zero-valued operand multiplications) related to these output neurons are futile and wasteful. Therefore, there is an opportunity to significantly improve the performance and efficiency of DNN execution by predicting the ineffectual output neurons and thus completely avoid the futile computations by skipping over these ineffectual output neurons.To do so, we propose a two-stage, prediction-based DNN execution model without accuracy loss. We also propose a uniform serial processing element (USPE), for both prediction and execution stages to improve the flexibility and minimize the area overhead. To improve the processing throughput, we further present a scale-out design for USPE. Evaluation results over a set of state-of-the-art DNNs show that our proposed design achieves 2.5X speedup and 1.9X energy-efficiency on average over the traditional accelerator. Moreover, by stacking with our design, we can improve Cnvlutin and Stripes by 1.9X and 2.0X on average, respectively.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {752–763},
numpages = {12},
keywords = {output sparsity, deep learning, approximate computing, accelerator},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00069,
author = {Sharma, Hardik and Park, Jongse and Suda, Naveen and Lai, Liangzhen and Chau, Benson and Kim, Joon Kyung and Chandra, Vikas and Esmaeilzadeh, Hadi},
title = {Bit fusion: bit-level dynamically composable architecture for accelerating deep neural networks},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00069},
doi = {10.1109/ISCA.2018.00069},
abstract = {Hardware acceleration of Deep Neural Networks (DNNs) aims to tame their enormous compute intensity. Fully realizing the potential of acceleration in this domain requires understanding and leveraging algorithmic properties of DNNs. This paper builds upon the algorithmic insight that bitwidth of operations in DNNs can be reduced without compromising their classification accuracy. However, to prevent loss of accuracy, the bitwidth varies significantly across DNNs and it may even be adjusted for each layer individually. Thus, a fixed-bitwidth accelerator would either offer limited benefits to accommodate the worst-case bitwidth requirements, or inevitably lead to a degradation in final accuracy. To alleviate these deficiencies, this work introduces dynamic bit-level fusion/decomposition as a new dimension in the design of DNN accelerators. We explore this dimension by designing Bit Fusion, a bit-flexible accelerator, that constitutes an array of bit-level processing elements that dynamically fuse to match the bitwidth of individual DNN layers. This flexibility in the architecture enables minimizing the computation and the communication at the finest granularity possible with no loss in accuracy. We evaluate the benefits of Bit Fusion using eight real-world feed-forward and recurrent DNNs. The proposed microarchitecture is implemented in Verilog and synthesized in 45 nm technology. Using the synthesis results and cycle accurate simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN accelerators, Eyeriss [1] and Stripes [2]. In the same area, frequency, and process technology, Bit Fusion offers 3.9X speedup and 5.1X energy savings over Eyeriss. Compared to Stripes, Bit Fusion provides 2.6X speedup and 3.9X energy reduction at 45 nm node when Bit Fusion area and frequency are set to those of Stripes. Scaling to GPU technology node of 16 nm, Bit Fusion almost matches the performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while Bit Fusion merely consumes 895 milliwatts of power.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {764–775},
numpages = {12},
keywords = {recurrent neural networks, quantization, long short-term memory, dynamic composability, deep neural networks, convolutional neural networks, bit-level composability, bit fusion, bit brick, accelerators, RNN, LSTM, DNN, CNN},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00070,
author = {Jain, Animesh and Phanishayee, Amar and Mars, Jason and Tang, Lingjia and Pekhimenko, Gennady},
title = {Gist: efficient data encoding for deep neural network training},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00070},
doi = {10.1109/ISCA.2018.00070},
abstract = {Modern deep neural networks (DNNs) training typically relies on GPUs to train complex hundred-layer deep networks. A significant problem facing both researchers and industry practitioners is that, as the networks get deeper, the available GPU main memory becomes a primary bottleneck, limiting the size of networks it can train.In this paper, we investigate widely used DNNs and find that the major contributors to memory footprint are intermediate layer outputs (feature maps). We then introduce a framework for DNN-layer-specific optimizations (e.g., convolution, ReLU, pool) that significantly reduce this source of main memory pressure on GPUs. We find that a feature map typically has two uses that are spread far apart temporally. Our key approach is to store an encoded representation of feature maps for this temporal gap and decode this data for use in the backward pass; the full-fidelity feature maps are used in the forward pass and relinquished immediately.Based on this approach, we present Gist, our system that employs two classes of layer-specific encoding schemes -- lossless and lossy -- to exploit existing value redundancy in DNN training to significantly reduce the memory consumption of targeted feature maps. For example, one insight is by taking advantage of the computational nature of back propagation from pool to ReLU layer, we can store the intermediate feature map using just 1 bit instead of 32 bits per value. We deploy these mechanisms in a state-of-the-art DNN framework (CNTK) and observe that Gist reduces the memory footprint to upto 2X across 5 state-of-the-art image classification DNNs, with an average of 1.8X with only 4\% performance overhead. We also show that further software (e.g., CuDNN) and hardware (e.g., dynamic allocation) optimizations can result in even larger footprint reduction (upto 4.1X).},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {776–789},
numpages = {14},
keywords = {data encodings, compression, DNN training},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00071,
author = {Yazdani, Reza and Riera, Marc and Arnau, Jose-Maria and Gonz\'{a}lez, Antonio},
title = {The dark side of DNN pruning},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00071},
doi = {10.1109/ISCA.2018.00071},
abstract = {DNN pruning has been recently proposed as an effective technique to improve the energy-efficiency of DNN-based solutions. It is claimed that by removing unimportant or redundant connections, the pruned DNN delivers higher performance and energy-efficiency with negligible impact on accuracy. However, DNN pruning has an important side effect: it may reduce the confidence of DNN predictions. We show that, although top-1 accuracy may be maintained with DNN pruning, the likelihood of the class in the top-1 is significantly reduced when using the pruned models. For applications such as Automatic Speech Recognition (ASR), where the DNN scores are consumed by a successive stage, the workload of this stage can be dramatically increased due to the loss of confidence in the DNN.An ASR system consists of a DNN for computing acoustic scores, followed by a Viterbi beam search to find the most likely sequence of words. We show that, when pruning the DNN model used for acoustic scoring, the Word Error Rate (WER) is maintained but the execution time of the ASR system is increased by 33\%. Although pruning improves the efficiency of the DNN, it results in a huge increase of activity in the Viterbi search since the output scores of the pruned model are less reliable.Based on this observation, we propose a novel hardware-based ASR system that effectively integrates a DNN accelerator for pruned models with a Viterbi accelerator. In order to avoid the aforementioned increase in Viterbi search workload, our system loosely selects the N-best hypotheses at every time step, exploring only the N most likely paths. To avoid an expensive sort of the hypotheses based on their likelihoods, our accelerator employs a set-associative hash table to keep track of the best paths mapped to each set. In practice, this solution approaches the selection of N-best, but it requires much simpler hardware. Our approach manages to efficiently combine both DNN pruning and Viterbi search, and achieves 9x energy savings and 4.2x speedup with respect to the state-of-the-art ASR solutions.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {790–801},
numpages = {12},
keywords = {viterbi search, hardware accelerator, deep learning, automatic speech recognition (ASR), DNN pruning},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00072,
author = {Gopireddy, Bhargava and Skarlatos, Dimitrios and Zhu, Wenjuan and Torrellas, Josep},
title = {HetCore: TFET-CMOS hetero-device architecture for CPUs and GPUs},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00072},
doi = {10.1109/ISCA.2018.00072},
abstract = {Tunneling Field-Effect Transistors (TFETs) attain much higher energy efficiency than CMOS at low voltages. However, their performance saturates at high voltages and, therefore, cannot replace CMOS when high performance is needed. Ideally, we desire a core that is as energy-efficient as a TFET core and provides as much performance as a CMOS core.To approach this goal, this paper judiciously integrates both TFET units and CMOS units in a single core, effectively creating a hetero-device core. We call it HetCore, and present CPU and GPU versions. In HetCore, TFETs are used in units that consume high power under CMOS, are amenable to pipelining or are not very latency sensitive, and use a sizable area. HetCore powers CMOS and TFET units at different voltage levels, so they operate optimally. However, all units are clocked at the same frequency. Our results based on simulations running standard applications show the potential of this approach, even with conservative assumptions. A HetCore CPU consumes on average 39\% less energy than a CMOS CPU, while delivering an average performance that is within 10\% of the CMOS CPU. In addition, under a fixed power budget, a multicore with HetCore CPUs can employ twice as many cores as a multicore with CMOS CPUs, resulting in average performance gains of 32\% while, at the same time, improving the energy efficiency (ED2) by an average of 68\%. Similar results are obtained with HetCore GPUs.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {802–815},
numpages = {14},
keywords = {hybrid TFET-CMOS architecture, core architecture, TFET, GPU, CPU},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00073,
author = {Khorasani, Farzad and Esfeden, Hodjat Asghari and Farmahini-Farahani, Amin and Jayasena, Nuwan and Sarkar, Vivek},
title = {RegMutex: inter-warp GPU register time-sharing},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00073},
doi = {10.1109/ISCA.2018.00073},
abstract = {Registers are the fastest and simultaneously the most expensive kind of memory available to GPU threads. Due to existence of a great number of concurrently executing threads, and the high cost of context switching mechanisms, contemporary GPUs are equipped with large register files. However, to avoid over-complicating the hardware, registers are statically assigned and exclusively dedicated to threads for the entire duration of the thread's lifetime. This decomposition takes into account the maximum number of live registers at any given point in the GPU binary although the points at which all the requested registers are used may constitute only a small fraction of the whole program. Therefore, a considerable portion of the register file remains under-utilized.In this paper, we propose a software-hardware co-mechanism named RegMutex (Register Mutual Exclusion) to share a subset of physical registers between warps during the GPU kernel execution. With RegMutex, the compiler divides the architected register set into a base register set and an extended register set. While physical registers corresponding to the base register set are statically and exclusively assigned to the warp, the hardware time-shares the remaining physical registers across warps to provision their extended register set. Therefore, the GPU programs can sustain approximately the same performance with the lower number of registers hence yielding higher performance per dollar. For programs that require a large number of registers for execution, RegMutex will enable a higher number of concurrent warps to be resident in the hardware via sharing their register allocations with each other, leading to a higher device occupancy. Since some aspects of register sharing orchestration are being offloaded to the compiler, RegMutex introduces lower hardware complexity compared to existing approaches. Our experiments show that RegMutex improves the register utilization and reduces the number of execution cycles by up to 23\% for kernels demanding a high number of registers.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {816–828},
numpages = {13},
keywords = {warp, time-sharing, time-multiplexing, register file, register, microarchitecture, compiler, RegMutex, GPU, GPGPU},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00074,
author = {Vijaykumar, Nandita and Ebrahimi, Eiman and Hsieh, Kevin and Gibbons, Phillip B. and Mutlu, Onur},
title = {The locality descriptor: a holistic cross-layer abstraction to express data locality in GPUs},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00074},
doi = {10.1109/ISCA.2018.00074},
abstract = {Exploiting data locality in GPUs is critical to making more efficient use of the existing caches and the NUMA-based memory hierarchy expected in future GPUs. While modern GPU programming models are designed to explicitly express parallelism, there is no clear explicit way to express data locality---i.e., reuse-based locality to make efficient use of the caches, or NUMA locality to efficiently utilize a NUMA system. On the one hand, this lack of expressiveness makes it a very challenging task for the programmer to write code to get the best performance out of the memory hierarchy. On the other hand, hardware-only architectural techniques are often suboptimal as they miss key higher-level program semantics that are essential to effectively exploit data locality.In this work, we propose the Locality Descriptor, a cross-layer abstraction to explicitly express and exploit data locality in GPUs. The Locality Descriptor (i) provides the software a flexible and portable interface to optimize for data locality, requiring no knowledge of the underlying memory techniques and resources, and (ii) enables the architecture to leverage key program semantics and effectively coordinate a range of techniques (e.g., CTA scheduling, cache management, memory placement) to exploit locality in a programmer-transparent manner. We demonstrate that the Locality Descriptor improves performance by 26.6\% on average (up to 46.6\%) when exploiting reuse-based locality in the cache hierarchy, and by 53.7\% (up to 2.8X) when exploiting NUMA locality in a NUMA memory system.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {829–842},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/ISCA.2018.00075,
author = {Vesel\'{y}, J\'{a}n and Basu, Arkaprava and Bhattacharjee, Abhishek and Loh, Gabriel H. and Oskin, Mark and Reinhardt, Steven K.},
title = {Generic system calls for GPUs},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00075},
doi = {10.1109/ISCA.2018.00075},
abstract = {GPUs are becoming first-class compute citizens and increasingly support programmability-enhancing features such as shared virtual memory and hardware cache coherence. This enables them to run a wider variety of programs. However, a key aspect of general-purpose programming where GPUs still have room for improvement is the ability to invoke system calls.We explore how to directly invoke system calls from GPUs. We examine how system calls can be integrated with GPGPU programming models, where thousands of threads are organized in a hierarchy of execution groups. To answer questions on GPU system call usage and efficiency, we implement Genesys, a generic GPU system call interface for Linux. Numerous architectural and OS issues are considered and subtle changes to Linux are necessary, as the existing kernel assumes that only CPUs invoke system calls. We assess the performance of Genesys using micro-benchmarks and applications that exercise system calls for signals, memory management, filesystems, and networking.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {843–856},
numpages = {14},
keywords = {virtualization and OS, multicore and parallel architectures, graphics oriented architecture, accelerators and domain-specific architecture, GPUS},
location = {Los Angeles, California},
series = {ISCA '18}
}

