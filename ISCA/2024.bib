@inproceedings{10.1109/ISCA59077.2024.00011,
author = {Chaturvedi, Ishita and Godala, Bhargav Reddy and Wu, Yucan and Xu, Ziyang and Iliakis, Konstantinos and Eleftherakis, Panagiotis-Eleftherios and Xydis, Sotirios and Soudris, Dimitrios and Sorensen, Tyler and Campanoni, Simone and Aamodt, Tor M. and August, David I.},
title = {GhOST: A GPU Out-of-Order Scheduling Technique for Stall Reduction},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00011},
doi = {10.1109/ISCA59077.2024.00011},
abstract = {Graphics Processing Units (GPUs) use massive multi-threading coupled with static scheduling to hide instruction latencies. Despite this, memory instructions pose a challenge as their latencies vary throughout the application's execution, leading to stalls. Out-of-order (OoO) execution has been shown to effectively mitigate these types of stalls. However, prior OoO proposals involve costly techniques such as reordering loads and stores, register renaming, or two-phase execution, amplifying implementation overhead and consequently creating a substantial barrier to adoption in GPUs. This paper introduces GhOST, a minimal yet effective OoO technique for GPUs. Without expensive components, GhOST can manifest a substantial portion of the instruction reorderings found in an idealized OoO GPU. GhOST leverages the decode stage's existing pool of decoded instructions and the existing issue stage's information about instructions in the pipeline to select instructions for OoO execution with little additional hardware. A comprehensive evaluation of GhOST and the prior state-of-the-art OoO technique across a range of diverse GPU benchmarks yields two surprising insights: (1) Prior works utilized Nvidia's intermediate representation PTX for evaluation; however, the optimized static instruction scheduling of the final binary form negates many purported improvements from OoO execution; and (2) The prior state-of-the-art OoO technique results in an average slowdown across this set of benchmarks. In contrast, GhOST achieves a 36\% maximum and 6.9\% geometric mean speedup on GPU binaries with only a 0.007\% area increase, surpassing previous techniques without slowing down any of the measured benchmarks.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1–16},
numpages = {16},
keywords = {GPU, parallelism, out-of-order execution, GPU microarchitecture, low overhead out-of-order execution},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00012,
author = {Liu, Yunzhe and Li, Xinyu and Zhang, Tingting and Liu, Tianyi and Guo, Qi and Zhang, Fuxin and Wang, Jian},
title = {AVM-BTB: Adaptive and Virtualized Multi-Level Branch Target Buffer},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00012},
doi = {10.1109/ISCA59077.2024.00012},
abstract = {Branch Target Buffer (BTB) plays an important role in modern processors. It is used to identify branches in the instruction stream and predict branch targets. The accuracy of BTB is highly impacted by BTB capacity. However, expanding BTB capacity using traditional methods requires valuable on-chip SRAM. Both timing and area restriction make these approaches unsustainable. Moreover, these methods overlook the different demands of various applications, leading to increased power consumption and resource waste in some cases.To address this problem, we propose AVM-BTB. The key observations behind AVM-BTB come from three aspects: 1) BTB requirements vary over different applications and even over different running stages of the same application. 2) Micro-operation Cache (Uop Cache) and ICache exhibit inefficiency when confronted with instruction footprints that greatly exceed their capacity. 3) In specific scenarios of frontend overload, reducing cache capacity and increasing BTB size can effectively mitigate expensive branch prediction errors. Simultaneously, the implementation of Fetch Directed Instruction Prefetching (FDIP) can offset the limitations in cache capacity to some extent. These observations reveal the feasibility of dynamically borrowing cache capacity as temporary BTB and returning these BTB to cache when they are not needed, further resulting in an adaptive and virtualized multi-level BTB scheme. However, such a BTB structure is non-trivial. In this work, from the perspective of instructions, the cache hierarchy stores instruction data, while the BTB stores metadata used for branch prediction and instruction prefetch. Targeting high performance, AVM-BTB maintains a dynamic balance between data and metadata by monitoring the BTB error rate and effective accesses. Evaluation with 1253 traces shows that AVM-BTB is suitable for both frontend-bound and frontend-friendly scenarios, without consuming additional SRAM and with reasonable implementation efforts. Compared to baseline, AVM-BTB delivers an average performance boost of 18.22\% and a power consumption reduction of 2.77\%. It also outperforms the five state-of-the-art solutions by 6.26\%-18.26\% on average in terms of IPC.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {17–31},
numpages = {15},
keywords = {branch target buffer, multi-level BTB, frontend stalls, resource allocation},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00013,
author = {Bhatla, Anubhav and Navneet and Panda, Biswabandan},
title = {The Maya Cache: A Storage-Efficient and Secure Fully-Associative Last-Level Cache},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00013},
doi = {10.1109/ISCA59077.2024.00013},
abstract = {The last-level cache is vulnerable to cross-core conflict-based attacks as the cache is shared among multiple cores. A fully associative last-level cache with a random replacement policy can mitigate these attacks. However, it is impractical to design a large last-level cache that is fully associative. One of the recent works, named Mirage, provides an illusion of a fully associative cache with a decoupled tag and data store and a random replacement policy. However, it incurs a storage overhead of 20\%, static power overhead of 18.16\%, and area overhead of 6.86\% compared to a non-secure baseline cache of 16MB. One of the primary contributors to the additional storage requirements is the usage of extra invalid tag entries that are used in a skewed way without changing the number of data store entries. These invalid tag entries provide a strong security guarantee. We observe that more than 80\% of last-level cache's data store entries are dead on arrival, providing negligible utility in terms of performance improvement as they do not get reused in their lifetimes. Also, in general, the data store entries occupy ≈ eight times more storage than tag store entries. Based on these observations, we propose Maya, a storage efficient and yet secure last-level randomized cache that compensates for the additional storage of tag store entries by using fewer data store entries. Maya increases the tag store entries for security and reuse detection and uses fewer data store entries that only store the reused data. Our proposal provides a strong security guarantee, which is one set-associative eviction in 1032 line fills at the last-level cache. This is equivalent to a line installed once in 1016 years to mount an eviction attack. Maya provides this security guarantee with a 12MB data store that occupies 28.11\% less area and 5.46\% less static power when compared to a non-secure baseline of 16MB cache.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {32–44},
numpages = {13},
keywords = {cache},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00014,
author = {Song, Ruibing and Wu, Chunshu and Liu, Chuan and Li, Ang and Huang, Michael and Geng, Tony (Tong)},
title = {DS-GL: Advancing Graph Learning via Harnessing Nature's Power within Scalable Dynamical Systems},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00014},
doi = {10.1109/ISCA59077.2024.00014},
abstract = {With the rapid digitization of the world, an increasing number of real-world applications are turning to non-Euclidean data, modeled as graphs. Due to their intrinsic high complexity and irregularity, learning from graph data demands tremendous computational power. Recently, CMOS-compatible Ising machines, i.e., dynamical systems fabricated with CMOS technologies, have emerged as a new approach that harnesses the inherent power of nature within dynamical systems to efficiently resolve binary optimization problems and have been adopted for traditional graph computation, such as max-cut. However, when performing complex Graph Learning (GL) tasks, Ising machines face significant hurdles: (i) they are binary and thus ill-suited for real-valued problems; (ii) their expensive all-to-all coupling network that guarantees generality for optimization problems poses daunting scalability concerns.To address these challenges, this paper proposes a nature-powered graph learning framework dubbed DS-GL, which is the first effort to transform the process of solving graph learning problems into the natural annealing process within a parameterized dynamical system embodied as a CMOS chip. To tackle the two major hurdles, DS-GL first augments the Ising machine architecture to modify the self-reaction term of its Hamiltonian function from linear to quadratic, effectively serving as an energy regulator. This adjustment maintains the system's original physical interpretation while enabling it to process continuous, real-valued data. Second, to address the scaling issue, DS-GL further upgrades the real-valued dense Ising machine by decomposing it into a mesh-based multi-PE dynamical system that supports efficient distributed spatial-temporal co-annealing across different PEs through sparse interconnects. By exploiting the inherent sparsity and community structures in real-world graphs, DS-GL is able to map complex graph learning tasks onto the scalable dynamical system while maintaining high accuracy. Evaluations with four diverse GL applications across seven real-world datasets, including traffic flow and COVID-19 prediction, show that DS-GL can deliver from 103 \texttimes{} to 105 \texttimes{} speedups over Graph Neural Networks on GPUs while operating at a power 2 orders of magnitude lower than GPUs, with 5\% – 30\% accuracy enhancement.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {45–57},
numpages = {13},
keywords = {dynamical system, graph learning, nature-powered computing},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00015,
author = {Chiang, Hao-Wei and Nien, Chin-Fu and Cheng, Hsiang-Yun and Huang, Kuei-Po},
title = {ReAIM: A ReRAM-Based Adaptive Ising Machine for Solving Combinatorial Optimization Problems},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00015},
doi = {10.1109/ISCA59077.2024.00015},
abstract = {Recently, in light of the success of quantum computers, research teams have actively developed quantum-inspired computers using classical computing technology. One notable success story is the Ising Machine (IM), which excels in efficiently solving NP-hard combinatorial optimization problems (COPs) in various domains such as finance, drug development, and logistics. However, IMs may encounter the von Neumann bottleneck due to significant data transfers between computing and memory units. To tackle this issue, processing-in-memory (PiM) leveraging resistive random-access memory (ReRAM) has emerged as a potential solution. Nonetheless, existing ReRAM-based IM accelerators face challenges stemming from non-ideal ReRAM devices and the limited flexibility in selecting solver algorithms.To overcome these limitations, we propose ReAIM, a ReRAM-based Adaptive Ising Machine, co-designed with an adaptive parameter search algorithm to dynamically select an appropriate solver algorithm and hardware/software parameters for the target COP. Based on our in-depth analysis, ReAIM accounts for the impact of both software parameters, such as the choice of local search algorithm and the number of spin flips, and hardware characteristics, including ReRAM-induced errors. Its reconfigurable architecture and optimized mapping/scheduling strategies enable efficient execution of the adaptive algorithm across various COPs, even for large-scale problems. Compared to the state-of-the-art SRAM-based IM accelerator, ReAIM achieves a 2.2\texttimes{} shorter time-to-solution (TTS), showcasing superior hardware performance without compromising solution quality.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {58–72},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00016,
author = {Demirkiran, Cansu and Yang, Guowei and Bunandar, Darius and Joshi, Ajay},
title = {Mirage: An RNS-Based Photonic Accelerator for DNN Training},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00016},
doi = {10.1109/ISCA59077.2024.00016},
abstract = {Photonic computing is a compelling avenue for performing highly efficient matrix multiplication, a crucial operation in Deep Neural Networks (DNNs). While this method has shown great success in DNN inference, meeting the high precision demands of DNN training proves challenging due to the precision limitations imposed by costly data converters and the analog noise inherent in photonic hardware. This paper proposes Mirage, a photonic DNN training accelerator that overcomes the precision challenges in photonic hardware using the Residue Number System (RNS). RNS is a numeral system based on modular arithmetic—allowing us to perform high-precision operations via multiple low-precision modular operations. In this work, we present a novel micro-architecture and dataflow for an RNS-based photonic tensor core performing modular arithmetic in the analog domain. By combining RNS and photonics, Mirage provides high energy efficiency without compromising precision and can successfully train state-of-the-art DNNs achieving accuracy comparable to FP32 training. Our study shows that on average across several DNNs when compared to systolic arrays, Mirage achieves more than 23.8\texttimes{} faster training and 32.1\texttimes{} lower EDP in an iso-energy scenario and consumes 42.8\texttimes{} lower power with comparable or better EDP in an iso-area scenario.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {73–87},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00017,
author = {Bera, Rahul and Ranganathan, Adithya and Rakshit, Joydeep and Mahto, Sujit and Nori, Anant V. and Gaur, Jayesh and Olgun, Ataberk and Kanellopoulos, Konstantinos and Sadrosadati, Mohammad and Subramoney, Sreenivas and Mutlu, Onur},
title = {Constable: Improving Performance and Power Efficiency by Safely Eliminating Load Instruction Execution},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00017},
doi = {10.1109/ISCA59077.2024.00017},
abstract = {Load instructions often limit instruction-level parallelism (ILP) in modern processors due to data and resource dependences they cause. Prior techniques like Load Value Prediction (LVP) and Memory Renaming (MRN) mitigate load data dependence by predicting the data value of a load instruction. However, they fail to mitigate load resource dependence as the predicted load instruction gets executed nonetheless (even on a correct prediction), which consumes hard-to-scale pipeline resources that otherwise could have been used to execute other load instructions.Our goal in this work is to improve ILP by mitigating both load data dependence and resource dependence. To this end, we propose a purely-microarchitectural technique called Constable, that safely eliminates the execution of load instructions. Constable dynamically identifies load instructions that have repeatedly fetched the same data from the same load address. We call such loads likely-stable. For every likely-stable load, Constable (1) tracks modifications to its source architectural registers and memory location via lightweight hardware structures, and (2) eliminates the execution of subsequent instances of the load instruction until there is a write to its source register or a store or snoop request to its load address.Our extensive evaluation using a wide variety of 90 workloads shows that Constable improves performance by 5.1\% while reducing the core dynamic power consumption by 3.4\% on average over a strong baseline system that implements MRN and other dynamic instruction optimizations (e.g., move and zero elimination, constant and branch folding). In presence of 2-way simultaneous multithreading (SMT), Constable's performance improvement increases to 8.8\% over the baseline system. When combined with a state-of-the-art load value predictor (EVES), Constable provides an additional 3.7\% and 7.8\% average performance benefit over the load value predictor alone, in the baseline system without and with 2-way SMT, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {88–102},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00018,
author = {Li, Peiyi and Liu, Ji and Gonzales, Alvin and Saleem, Zain Hamid and Zhou, Huiyang and Hovland, Paul},
title = {QuTracer: Mitigating Quantum Gate and Measurement Errors by Tracing Subsets of Qubits},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00018},
doi = {10.1109/ISCA59077.2024.00018},
abstract = {Quantum error mitigation plays a crucial role in the current noisy-intermediate-scale-quantum (NISQ) era. As we advance towards achieving a practical quantum advantage in the near term, error mitigation emerges as an indispensable component. One notable prior work, Jigsaw, demonstrates that measurement crosstalk errors can be effectively mitigated by measuring subsets of qubits. Jigsaw operates by running multiple copies of the original circuit, each time measuring only a subset of qubits. The localized distributions yielded from measurement subsetting suffer from less crosstalk and are then used to update the global distribution, thereby achieving improved output fidelity.Inspired by the idea of measurement subsetting, we propose QuTracer, a framework designed to mitigate both gate and measurement errors in subsets of qubits by tracing the states of qubit subsets throughout the computational process. In order to achieve this goal, we introduce a technique, qubit subsetting Pauli checks (QSPC), which utilizes circuit cutting and Pauli Check Sandwiching (PCS) to trace the qubit subsets distribution to mitigate errors. The QuTracer framework can be applied to various algorithms including, but not limited to, VQE, QAOA, quantum arithmetic circuits, QPE, and Hamiltonian simulations. In our experiments, we perform both noisy simulations and real device experiments to demonstrate that QuTracer is scalable and significantly outperforms the state-of-the-art approaches.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {103–117},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00019,
author = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, \'{I}\~{n}igo and Maleki, Saeed and Bianchini, Ricardo},
title = {Splitwise: Efficient Generative LLM Inference Using Phase Splitting},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00019},
doi = {10.1109/ISCA59077.2024.00019},
abstract = {Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memoryintensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost.Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today's GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power. Compared to current designs, Splitwise clusters achieve up to 1.4\texttimes{} higher throughput at 20\% lower cost. Alternatively, they can deliver 2.35\texttimes{} more throughput under the same power and cost budgets.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {118–132},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00020,
author = {Marazzi, Michele and Sachsenweger, Tristan and Solt, Flavien and Zeng, Peng and Takashi, Kubo and Yarema, Maksym and Razavi, Kaveh},
title = {HiFi-DRAM: Enabling High-Fidelity DRAM Research by Uncovering Sense Amplifiers with IC Imaging},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00020},
doi = {10.1109/ISCA59077.2024.00020},
abstract = {DRAM vendors do not disclose the architecture of the sense amplifiers deployed in their chips. Unfortunately, this hinders academic research that focuses on studying or improving DRAM. Without knowing the circuit topology, transistor dimensions, and layout of the sense amplifiers, researchers are forced to rely on best guesses, impairing the fidelity of their studies. We aim to fill this gap between academia and industry for the first time by performing Scanning Electron Microscopy (SEM) with Focused Ion Beam (FIB) on recent commodity DDR4 and DDR5 DRAM chips from the three major vendors. This required us to adequately prepare the samples, identify the sensing area, and align images from the different FIB slices. Using the acquired images, we reverse engineer the circuits, measure transistor dimensions and extract physical layouts of sense amplifiers — all previously unavailable to researchers. Our findings show that the commonly assumed classical sense amplifier topology has been replaced with the more sophisticated offset-cancellation design by two of the three major DRAM vendors. Furthermore, the transistor dimensions of sense amplifiers and their revealed physical layouts are significantly different than what is assumed in existing literature. Given commodity DRAM, our analysis shows that the public DRAM models are up to 9x inaccurate, and existing research has up to 175x error when estimating the impact of the proposed changes. To enable high-fidelity DRAM research in the future, we open source our data, including the reverse engineered circuits and layouts.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {133–149},
numpages = {17},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00021,
author = {Huang, Qijing and Tsai, Po-An and Emer, Joel S. and Parashar, Angshuman},
title = {Mind the Gap: Attainable Data Movement and Operational Intensity Bounds for Tensor Algorithms},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00021},
doi = {10.1109/ISCA59077.2024.00021},
abstract = {The architectural design-space exploration (or DSE) process—whether manual or automated—benefits greatly from knowing the limits of the metrics of interest in advance. Data movement is rapidly emerging as a critical metric for DSE due to its increasing impact on both performance and energy efficiency. Unfortunately, the commonly used algorithmic minimum (or "compulsory misses") limit for data movement is extremely loose, limiting its utility in design-space search.In this paper, we present Orojenesis, an approach to compute data movement limits (or bounds) for tensor algorithms. Unlike algorithmic-minimum bounds, Orojenesis comprehends reuse and the ability of a buffer (such as a cache or scratchpad) to exploit reuse to reduce data movement. Orojenesis provides a bound that no dataflow or mapping can possibly exceed under varying on-chip buffer capacity constraints, including mappings that fuse a sequence of tensor operations to exploit producer-consumer reuse. Orojenesis produces a plot that shows the relationship between a buffer's size and the lower data movement limit to/from the next level in a memory hierarchy. This plot, dubbed a ski-slope diagram, allows designers to gain critical insights into the behavior of a workload as a function of storage capacity. This analysis can inform early high-level design decisions before embarking on thorough design space searches.We use Orojenesis to analyze a set of valuable tensor algorithms including batched and grouped matrix multiplications, convolutions, and sequences of operations in Large Language Models (LLMs). Our analysis reveals a range of architectural insights, including the fact that attainable data movement can be orders-of-magnitude higher than algorithmic minimum, that there exists a sweet spot between SRAM and compute resource provisioning for optimal throughput, and that up to 5.6\texttimes{} data movement reduction can be achieved with fusion with a buffer capacity of 320MB for the GPT-3-6.7b LLM.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {150–166},
numpages = {17},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00022,
author = {Hou, Xiaofeng and Xu, Tongqiao and Li, Chao and Xu, Cheng and Liu, Jiacheng and Hu, Yang and Zhao, Jieru and Leng, Jingwen and Cheng, Kwang-Ting and Guo, Minyi},
title = {A Tale of Two Domains: Exploring Efficient Architecture Design for Truly Autonomous Things},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00022},
doi = {10.1109/ISCA59077.2024.00022},
abstract = {Autonomous Things (AuT) refers to a collection of self-sufficient tiny devices capable of performing intelligent computations. Looking ahead, AuT promises to enable ubiquitous deployment of intelligence on many emerging consumer electronics and mission-critical infrastructures. Nevertheless, there is an important research gap to date: architecting efficient AuT systems requires both energy autonomy (EA) and inference autonomy (IA). In other words, practical AuT application scenarios necessitate tailored architectures with significantly expanded inference performance and more efficient use of energy.We present CHRYSALIS, a novel automated EA/IA co-design methodology for autonomous things. It aims to guide the transition from a traditional EA-only and IA-only design approach to a truly AuT-oriented architecture design. To fully understand the interrelationship between the EA domain and the IA domain, CHRYSALIS first introduces an architectural modeling framework encompassing every key AuT module involving energy harvesting, intermittent execution, and accelerator control. Based on the holistic system model, we design an intelligent architecture generation tool that can help find the ideal design for targeted AuT scenarios adhering to different SWaP (Size, Weight and Power) constraints. To validate our work, we use CHRYSALIS for fast construction and exploration of efficient AuT design and pre-RTL design in representative AuT scenarios. Extensive evaluation shows that CHRYSALIS outperforms state-of-the-art designs and our proposed technique shows 56.4\% better performance on average. We believe that the methodology and tools developed in this paper will foster the development of more performant and practical architectures in the upcoming AuT era.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {167–181},
numpages = {15},
keywords = {edge artificial intelligence, autonomous embedded systems, intermittent computing, accelerator, deep learning},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00023,
author = {Li, Weihang and Goens, Andr\'{e}s and Oswald, Nicolai and Nagarajan, Vijay and Sorin, Daniel J.},
title = {Determining the Minimum Number of Virtual Networks for Different Coherence Protocols},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00023},
doi = {10.1109/ISCA59077.2024.00023},
abstract = {We revisit the question of how many virtual networks (VNs) are required to provably avoid deadlock in a cache coherence protocol. The textbook way of reasoning about VNs says that the number of VNs depends on the longest chain of message dependencies in the protocol. We show that this conventional wisdom is incorrect and results in a number of virtual networks that is neither necessary nor sufficient for the general system model of an arbitrary interconnection network (ICN) topology and multiple directories. We have created a formalism for modeling coherence protocols and their interactions with ICN queueing. Using that formalism, we have developed an algorithm that (a) determines the minimum number of virtual networks required to avoid deadlock and (b) generates the mappings from message types to virtual networks.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {182–197},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00024,
author = {Tong, Jianming and Itagi, Anirudh and Chatarasi, Prasanth and Krishna, Tushar},
title = {FEATHER: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00024},
doi = {10.1109/ISCA59077.2024.00024},
abstract = {The inference of ML models composed of diverse structures, types, and sizes boils down to the execution of different dataflows (i.e. different tiling, ordering, parallelism, and shapes). Using the optimal dataflow for every layer of workload can reduce latency by up to two orders of magnitude over a suboptimal dataflow. Unfortunately, reconfiguring hardware for different dataflows involves on-chip data layout reordering and datapath reconfigurations, leading to non-trivial overhead that hinders ML accelerators from exploiting different dataflows, resulting in suboptimal performance. To address this challenge, we propose FEATHER, an innovative accelerator that leverages a novel spatial array termed NEST and a novel multi-stage reduction network called BIRRD for performing flexible data reduction with layout reordering under the hood, enabling seamless switching between optimal dataflows with negligible latency and resources overhead. For systematically evaluating the performance interaction between dataflows and layouts, we enhance Timeloop, a state-of-the-art dataflow cost modeling and search framework, with layout assessment capabilities, and term it as Layoutloop. We model FEATHER into Layoutloop and also deploy FEATHER end-to-end on the edge ZCU104 FPGA. FEATHER delivers 1.27 ~ 2.89\texttimes{} inference latency speedup and 1.3 ~ 6.43\texttimes{} energy efficiency improvement compared to various SoTAs like NVDLA, SIGMA and Eyeriss under ResNet-50 and MobiletNet-V3 in Layoutloop. On practical FPGA devices, FEATHER achieves 2.65/3.91\texttimes{} higher throughput than Xilinx DPU/Gemmini. Remarkably, such performance and energy efficiency enhancements come at only 6\% area over a fixed-dataflow Eyeriss-like accelerator. Our code is released at https://github.com/maeri-project/FEATHER.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {198–214},
numpages = {17},
keywords = {reconfigurable accelerator, dataflow, layout},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00025,
author = {Chen, Shuangliang and Pal, Saptadeep and Kumar, Rakesh},
title = {Waferscale Network Switches},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00025},
doi = {10.1109/ISCA59077.2024.00025},
abstract = {In spite of being a key determinant of latency, cost, power, space, and capability of modern computer systems, network switch radix has not seen much growth over the years due to poor scaling of off-chip IO pitches and switch die sizes. We consider waferscale integration (WSI) as a way to increase the size of the switch substrate to be much bigger than a single die and ask the question: can we use WSI to enable network switches that have dramatically higher radix than today's switches? We show that while a waferscale network switch can support up to 32x higher radix than state-of-the-art network switches when only area constraints are considered, the actual radix of a waferscale network switch is not area-limited. Rather, it is limited by a combination of internal bandwidth, external bandwidth, and power density. In fact, without optimizations, benefits of a waferscale network switch are minimal. To address the scalability bottlenecks, we propose a heterogeneous network switch design that reduces switch power by 30.8\%-33.5\% which, in turn, allows an increase in radix (by up to 4x) by increasing internal I/O bandwidth at the expense of energy efficiency. We also propose subswitch deradixing that increases the overall radix by 2x by decreasing the radix of the subswitches to alleviate the internal I/O bottleneck. We use Area I/O and Optical I/O schemes to alleviate the external I/O bandwidth bottlenecks of conventional SerDes-based external connectivity. In addition to scalability optimization, we present optimizations such as low latency buffering and proprietary routing that improve the performance of waferscale switches. Finally, we present a system architecture for a waferscale network switch that supports its port count, power delivery, and cooling requirements in a compact form factor. We show that the switch can be used to enable new computing systems such as single-switch datacenters and massive-scale singular GPUs. It can also lead to a dramatic reduction in datacenter network costs. Overall, this is the first work quantifying the benefits of waferscale switches and identifying and addressing the unique challenges and opportunities in building them.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {215–229},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00026,
author = {L\'{o}pez-Parad\'{\i}s, Guillem and Hair, Isaac M. and Kannan, Sid and Rabbat, Roman and Murray, Parker and Lopes, Alex and Zahedi, Rory and Zuo, Winston and Balkind, Jonathan},
title = {The Case for Data Centre Hyperloops},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00026},
doi = {10.1109/ISCA59077.2024.00026},
abstract = {Data movement is a hot-button topic today, with workloads like machine learning (ML) training, graph processing, and data analytics consuming datasets as large as 30PB. Such a dataset would take almost a week to transfer at 400gbps while consuming megajoules of energy just to operate the two endpoints' optical transceivers. All of this time and energy is seen as an unavoidable overhead on top of directly accessing the disks that store the data. In this paper, we re-evaluate the fundamental assumption of networked data copying and instead propose the adoption of embodied data movement. Our insight is that solid state disks (SSDs) have been rapidly growing in an under-exploited way: their data density, both in TB per unit volume and unit mass.With data centres reaching kilometres in length, we propose a new architecture featuring data centre hyperloops2 (DHLs) where large datasets, stored on commodity SSDs, are moved via magnetic levitation in low-pressure tubes. By eliminating much of the potential friction inherent to embodied data movement, DHLs offer more efficient data movement, with SSDs potentially travelling at hundreds of metres per second. Consequently, a contemporary dataset can be moved through a DHL in seconds and then accessed with local latency and bandwidth well into the terabytes per second.DHLs have the potential to massively reduce the network bandwidth and energy consumption associated with moving large datasets, but raise a variety of questions regarding the viability of their realisation and deployment. Through flexibility and creative engineering, we argue that many potential issues can be resolved. Further, we present models of DHLs and their application to workloads with growing data movement demands, such as training machine learning algorithms, large-scale physics experiments, and data centre backups. For a fixed data movement task, we obtain energy reductions of 1.6\texttimes{} to 376.1\texttimes{} and time speedups from 114.8\texttimes{} to 646.4\texttimes{} versus 400gbps optical networking. When modelling DHL in simulation, we obtain time speedups of between 5.7\texttimes{} and 118\texttimes{} (iso-power) and communication power reductions of between 6.4\texttimes{} and 135\texttimes{} (iso-time) to train an iteration of a representative DLRM workload. We provide a cost analysis, showing that DHLs are financially practical. With the scale of the improvements realisable through DHLs, we consider this paper a call to action for our community to grapple with the remaining architectural challenges.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {230–244},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00027,
author = {Noh, Si Ung and Hong, Junguk and Lim, Chaemin and Park, Seongyeon and Kim, Jeehyun and Kim, Hanjun and Kim, Youngsok and Lee, Jinho},
title = {PID-Comm: A Fast and Flexible Collective Communication Framework for Commodity Processing-in-DIMM Devices},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00027},
doi = {10.1109/ISCA59077.2024.00027},
abstract = {Recent dual in-line memory modules (DIMMs) are starting to support processing-in-memory (PIM) by associating their memory banks with processing elements (PEs), allowing applications to overcome the data movement bottleneck by offloading memory-intensive operations to the PEs. Many highly parallel applications have been shown to benefit from these PIM-enabled DIMMs, but further speedup is often limited by the huge overhead of inter-PE collective communication. This mainly comes from the slow CPU-mediated inter-PE communication methods, making it difficult for PIM-enabled DIMMs to accelerate a wider range of applications. Prior studies have tried to alleviate the communication bottleneck, but they lack enough flexibility and performance to be used for a wide range of applications.In this paper, we present PID-Comm, a fast and flexible inter-PE collective communication framework for commodity PIM-enabled DIMMs. The key idea of PID-Comm is to abstract the PEs as a multi-dimensional hypercube and allow multiple instances of inter-PE collective communication between the PEs belonging to certain dimensions of the hypercube. Leveraging this abstraction, PID-Comm first defines eight inter-PE collective communication patterns that allow applications to easily express their complex communication patterns. Then, PID-Comm provides high-performance implementations of the inter-PE collective communication patterns optimized for the DIMMs. Our evaluation using 16 UPMEM DIMMs and representative parallel algorithms shows that PID-Comm greatly improves the performance by up to 5.19\texttimes{} compared to the existing inter-PE communication implementations. The implementation of PID-Comm is available at https://github.com/AIS-SNU/PID-Comm.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {245–260},
numpages = {16},
keywords = {processing-in-memory, accelerator, DRAM, collective communication},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00028,
author = {Zhou, Junyu and Liu, Yuhao and Shi, Yunong and Javadi-Abhari, Ali and Li, Gushu},
title = {Bosehedral: Compiler Optimization for Bosonic Quantum Computing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00028},
doi = {10.1109/ISCA59077.2024.00028},
abstract = {Bosonic quantum computing, based on the infinite-dimensional qumodes, has shown promise for various practical applications that are classically hard. However, the lack of compiler optimizations has hindered its full potential. This paper introduces Bosehedral, an efficient compiler optimization framework for (Gaussian) Boson sampling on Bosonic quantum hardware. Bosehedral overcomes the challenge of handling infinite-dimensional qumode gate matrices by performing all its program analysis and optimizations at a higher algorithmic level, using a compact unitary matrix representation. It optimizes qumode gate decomposition and logical-to-physical qumode mapping, and introduces a tunable probabilistic gate dropout method. Overall, Bosehedral significantly improves the performance by accurately approximating the original program with much fewer gates. Our evaluation shows that Bosehedral can largely reduce the program size but still maintain a high approximation fidelity, which can translate to significant end-to-end application performance improvement.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {261–276},
numpages = {16},
keywords = {bosonic quantum computing, gaussian boson sampling, compiler optimization},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00029,
author = {Jin, Yuwei and Li, Zirui and Hua, Fei and Hao, Tianyi and Zhou, Huiyang and Huang, Yipeng and Zhang, Eddy Z.},
title = {Tetris: A Compilation Framework for VQA Applications in Quantum Computing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00029},
doi = {10.1109/ISCA59077.2024.00029},
abstract = {Quantum computing has shown promise in solving complex problems by leveraging the principles of superposition and entanglement. Variational quantum algorithms (VQA) are a class of algorithms suited for near-term quantum computers due to their modest requirements of qubits and depths of computation. This paper introduces Tetris - a compilation framework for VQA applications on near-term quantum devices. Tetris focuses on reducing two-qubit gates in the compilation process since a two-qubit gate has an order of magnitude more significant error and execution time than a single-qubit gate. Tetris exploits unique opportunities in the circuit synthesis stage often overlooked by the state-of-the-art VQA compilers for reducing the number of two-qubit gates. Tetris comes with a refined IR of Pauli string to express such a two-qubit gate optimization opportunity. Moreover, Tetris is equipped with a fast bridging approach that mitigates the hardware mapping cost. Overall, Tetris demonstrates a reduction of up to 41.3\% in CNOT gate counts, 37.9\% in circuit depth, and 42.6\% in circuit duration for various molecules of different sizes and structures compared with the state-of-the-art approaches. Tetris is open-sourced at this link.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {277–292},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00030,
author = {Wang, Hanrui and Liu, Pengyu and Tan, Daniel Bochen and Liu, Yilian and Gu, Jiaqi and Pan, David Z. and Cong, Jason and Acar, Umut A. and Han, Song},
title = {Atomique: A Quantum Compiler for Reconfigurable Neutral Atom Arrays},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00030},
doi = {10.1109/ISCA59077.2024.00030},
abstract = {The neutral atom array has gained prominence in quantum computing for its scalability and operation fidelity. Previous works focus on fixed atom arrays (FAAs) that require extensive SWAP operations for long-range interactions. This work explores a novel architecture reconfigurable atom arrays (RAAs), also known as field programmable qubit arrays (FPQAs), which allows for coherent atom movements during circuit execution under some constraints. Such atom movements, which are unique to this architecture, could reduce the cost of longrange interactions significantly if the atom movements could be scheduled strategically.In this work, we introduce Atomique, a compilation framework designed for qubit mapping, atom movement, and gate scheduling for RAA. Atomique contains a qubit-array mapper to decide the coarse-grained mapping of the qubits to arrays, leveraging MAX k-Cut on a constructed gate frequency graph to minimize SWAP overhead. Subsequently, a qubit-atom mapper determines the fine-grained mapping of qubits to specific atoms in the array and considers load balance to prevent hardware constraint violations. We further propose a router that identifies parallel gates, schedules them simultaneously, and reduces depth.We evaluate Atomique across 20+ diverse benchmarks, including generic circuits (arbitrary, QASMBench, SupermarQ), quantum simulation, and QAOA circuits. Atomique consistently outperforms IBM Superconducting, FAA with long-range gates, and FAA with rectangular and triangular topologies, achieving significant reductions in depth and the number of two-qubit gates.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {293–309},
numpages = {17},
keywords = {quantum computing, quantum compiler, atom array, neutral atom, qubit mapping, gate scheduling, rydberg atom},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00031,
author = {Seif, Alireza and Liao, Haoran and Tripathi, Vinay and Krsulich, Kevin and Malekakhlagh, Moein and Amico, Mirko and Jurcevic, Petar and Javadi-Abhari, Ali},
title = {Suppressing Correlated Noise in Quantum Computers via Context-Aware Compiling},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00031},
doi = {10.1109/ISCA59077.2024.00031},
abstract = {Coherent errors, and especially those that occur in correlation among a set of qubits, are detrimental for large-scale quantum computing. Correlations in noise can occur as a result of spatial and temporal configurations of instructions executing on the quantum processor. In this paper, we perform a detailed experimental characterization of many of these error sources, and theoretically connect them to the physics of superconducting qubits and gate operations. Equipped with this knowledge, we devise compiler strategies to suppress these errors using dynamical decoupling or error compensation into the rest of the circuit. Importantly, these strategies are successful when the context at each layer of computation is taken into account: how qubits are connected, what crosstalk terms exist on the device, and what gates or idle periods occur in that layer. Our context-aware compiler thus suppresses some dominant sources of error, making further error mitigation or error correction substantially less expensive. For example, our experiments show an increase of 18.5\% in layer fidelity for a candidate 10-qubit circuit layer compared to context-unaware suppression. Owing to the exponential nature of error mitigation, these improvements due to error suppression translate to several orders of magnitude reduction of sampling overhead for a circuit consisting of a moderate number of layers.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {310–324},
numpages = {15},
keywords = {quantum error suppression, compiler, dynamical decoupling, error compensation},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00032,
author = {Tan, Daniel Bochen and Niu, Murphy Yuezhen and Gidney, Craig},
title = {A SAT Scalpel for Lattice Surgery: Representation and Synthesis of Subroutines for Surface-Code Fault-Tolerant Quantum Computing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00032},
doi = {10.1109/ISCA59077.2024.00032},
abstract = {Quantum error correction is necessary for large-scale quantum computing. A promising quantum error correcting code is the surface code. For this code, fault-tolerant quantum computing (FTQC) can be performed via lattice surgery, i.e., splitting and merging patches of code. Given the frequent use of certain lattice-surgery subroutines (LaS), it becomes crucial to optimize their design in order to minimize the overall spacetime volume of FTQC. In this study, we define the variables to represent LaS and the constraints on these variables. Leveraging this formulation, we develop a synthesizer for LaS, LaSsynth, that encodes a LaS construction problem into a SAT instance, subsequently querying SAT solvers for a solution. Starting from a baseline design, we can gradually invoke the solver with shrinking spacetime volume to derive more compact designs. Due to our foundational formulation and the use of SAT solvers, LaSsynth can exhaustively explore the design space, yielding optimal designs in volume. For example, it achieves 8\% and 18\% volume reduction respectively over two states-of-the-art human designs for the 15-to-1 T-factory, a bottleneck in FTQC.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {325–339},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00033,
author = {Lee, Yunjae and Kim, Hyeseong and Rhu, Minsoo},
title = {PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00033},
doi = {10.1109/ISCA59077.2024.00033},
abstract = {Training recommendation systems (RecSys) faces several challenges as it requires the "data preprocessing" stage to preprocess an ample amount of raw data and feed them to the GPU for training in a seamless manner. To sustain high training throughput, state-of-the-art solutions reserve a large fleet of CPU servers for preprocessing which incurs substantial deployment cost and power consumption. Our characterization reveals that prior CPU-centric preprocessing is bottlenecked on feature generation and feature normalization operations as it fails to reap out the abundant inter-/intra-feature parallelism in RecSys preprocessing. PreSto is a storage-centric preprocessing system leveraging In-Storage Processing (ISP), which offloads the bottlenecked preprocessing operations to our ISP units. We show that PreSto outperforms the baseline CPU-centric system with a 9.6\texttimes{} speedup in end-to-end preprocessing time, 4.3\texttimes{} enhancement in cost-efficiency, and 11.3\texttimes{} improvement in energy-efficiency on average for production-scale RecSys preprocessing.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {340–353},
numpages = {14},
keywords = {recommendation system, computational storage device, near data processing, neural network},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00034,
author = {Baek, Daehyeon and Hwang, Soojin and Huh, Jaehyuk},
title = {pSyncPIM: Partially Synchronous Execution of Sparse Matrix Operations for All-Bank PIM Architectures},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00034},
doi = {10.1109/ISCA59077.2024.00034},
abstract = {Recent commercial incarnations of processing-in-memory (PIM) maintain the standard DRAM interface and employ the all-bank mode execution to maximize bank-level memory bandwidth. Such a synchronized all-bank PIM control can effectively manage conventional dense matrix-vector operations on evenly distributed matrices across banks with lock-step execution. Sparse matrix processing is another critical computation that can significantly benefit from the PIM architecture, but the current all-bank PIM control cannot support diverging executions due to the random sparsity. To accelerate such sparse matrix applications, this paper proposes a partially synchronous execution on sparse matrix-vector multiplication (SpMV) and sparse triangular matrix-vector solve (SpTRSV), filling the gap between the practical constraint of PIM and the irregular nature of sparse computation. It allows the execution of the processing unit of each bank to diverge in a limited way to manage the irregular execution path of sparse matrix computation. It proposes compaction and distribution policies for the input matrix and vector. In addition to SpMV, this paper identifies SpTRSV is another key kernel, and proposes SpTRSV acceleration on PIM technology. The experimental evaluation shows that the new sparse PIM architecture outperforms NVIDIA Geforce RTX 3080 GPU by 4.43\texttimes{} speedup for SpMV and 3.53\texttimes{} speedup for SpTRSV with a similar amount of DRAM bandwidth.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {354–367},
numpages = {14},
keywords = {processing-in-memory, sparse matrix, memory bandwidth, predicated execution},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00035,
author = {Wang, Yitu and Li, Shiyu and Zheng, Qilin and Song, Linghao and Li, Zongwang and Chang, Andrew and Li, Hai "Helen" and Chen, Yiran},
title = {NDSearch: Accelerating Graph-Traversal-Based Approximate Nearest Neighbor Search through Near Data Processing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00035},
doi = {10.1109/ISCA59077.2024.00035},
abstract = {Approximate nearest neighbor search (ANNS) is a key retrieval technique for vector database and many data center applications, such as person re-identification and recommendation systems. It is also fundamental to retrieval augmented generation (RAG) for large language models (LLM) now. Among all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall rate. However, as the size of dataset increases, the graph may require hundreds of gigabytes of memory, exceeding the main memory capacity of a single workstation node. Although we can do partitioning and use solid-state drive (SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades the performance of the system. To address this challenge, we present NDSearch, a hardware-software co-designed near-data processing (NDP) solution for ANNS processing. NDSearch consists of a novel in-storage computing architecture, namely, SearSSD, that supports the ANNS kernels and leverages logic unit (LUN)-level parallelism inside the NAND flash chips. NDSearch also includes a processing model that is customized for NDP and cooperates with SEARSSD. The processing model enables us to apply a two-level scheduling to improve the data locality and exploit the internal bandwidth in NDSearch, and a speculative searching mechanism to further accelerate the ANNS workload. Our results show that NDSearch improves the throughput by up to 31.7\texttimes{}, 14.6\texttimes{}, 7.4\texttimes{} 2.9\texttimes{} over CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively. NDSearch also achieves two orders-of-magnitude higher energy efficiency than CPU and GPU.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {368–381},
numpages = {14},
keywords = {near data processing, approximate nearest neighbor search, hardware/software co-design},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00036,
author = {Liu, Haifeng and Zheng, Long and Huang, Yu and Zhou, Jingyi and Liu, Chaoqiang and Wang, Runze and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {Enabling Efficient Large Recommendation Model Training with near CXL Memory Processing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00036},
doi = {10.1109/ISCA59077.2024.00036},
abstract = {Personalized recommendation systems have become one of the most important Internet services nowadays. A critical challenge of training and deploying the recommendation models is their high memory capacity and bandwidth demands, with the embedding layers occupying hundreds of GBs to TBs of storage. The advent of memory disaggregation technology and Compute Express Link (CXL) provides a promising solution for memory capacity scaling. However, relocating memory-intensive embedding layers to CXL memory incurs noticeable performance degradation due to its limited transmission bandwidth, which is significantly lower than the host memory bandwidth. To address this, we introduce ReCXL, a CXL memory disaggregation system that utilizes near-memory processing for scalable, efficient recommendation model training. ReCXL features a unified, hardware-efficient NMP architecture that processes the entire embedding training within CXL memory, minimizing data transfers over the bandwidth-limited CXL and enhancing internal bandwidth. To further improve the performance, ReCXL incorporates softwarehardware co-optimizations, including sophisticated dependency-free prefetching and fine-grained update scheduling, to maximize hardware utilization. Evaluation results show that ReCXL outperforms the CPU-GPU baseline and the na\"{\i}ve CXL memory by 7.1\texttimes{} ~ 10.6\texttimes{} (9.4\texttimes{} on average) and 12.7\texttimes{} ~ 31.3\texttimes{} (22.6\texttimes{} on average), respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {382–395},
numpages = {14},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00037,
author = {Yue, Zhiheng and Wang, Huizheng and Fang, Jiahao and Deng, Jinyi and Lu, Guangyang and Tu, Fengbin and Guo, Ruiqi and Li, Yuxuan and Qin, Yubin and Wang, Yang and Li, Chao and Han, Huiming and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
title = {Exploiting Similarity Opportunities of Emerging Vision AI Models on Hybrid Bonding Architecture},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00037},
doi = {10.1109/ISCA59077.2024.00037},
abstract = {While extensive research has focused on optimizing performance and efficiency in vision-based AI accelerators, an unexplored phenomenon, Clustering Similarity Effect, presents a significant opportunity for further improvement. This effect reveals that clusters of neighboring data points exhibit similar values, enabling the potential to skip redundant computations.To fully capitalize on the potential of the Clustering Similarity Effect (CSE), this work integrates hybrid bonding DRAM technology. We conduct a comprehensive analysis of the associated design considerations and integration overhead. Leveraging these insights, we propose a novel CSE-aware architecture specifically tailored for hybrid bonding memory. This architecture facilitates similarity detection and adapts to the inherent data characteristics associated with CSE.Compared with state-of-the-art 2D/2.5D AI accelerators, the hybrid bonding baseline demonstrates an average energy efficiency improvement of 2.89\texttimes{} ~ 14.28\texttimes{} and an area efficiency improvement of 2.67\texttimes{} ~7.68\texttimes{}. Incorporating the similarity optimizations further enhances energy efficiency and area efficiency improvement to 5.69\texttimes{}~28.13\texttimes{} and 3.82\texttimes{} ~ 10.98\texttimes{}, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {396–409},
numpages = {14},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00038,
author = {Choi, Yujeong and Kim, Jiin and Rhu, Minsoo},
title = {ElasticRec: A Microservice-Based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00038},
doi = {10.1109/ISCA59077.2024.00038},
abstract = {With the increasing popularity of recommendation systems (RecSys), the demand for compute resources in data-centers has surged. However, the model-wise resource allocation employed in current RecSys model serving architectures falls short in effectively utilizing resources, leading to sub-optimal total cost of ownership. We propose ElasticRec, a model serving architecture for RecSys providing resource elasticity and high memory efficiency. ElasticRec is based on a microservice-based software architecture for fine-grained resource allocation, tailored to the heterogeneous resource demands of RecSys. Additionally, ElasticRec achieves high memory efficiency via our utility-based resource allocation. Overall, ElasticRec achieves an average 3.3\texttimes{} reduction in memory allocation size and 8.1\texttimes{} increase in memory utility, resulting in an average 1.6\texttimes{} reduction in deployment cost compared to state-of-the-art RecSys inference serving system.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {410–423},
numpages = {14},
keywords = {machine learning, recommendation model, resource management, resource scaling, microservice, model deployment},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00039,
author = {Chen, Liao and Luo, Shutian and Lin, Chenyu and Mo, Zizhao and Xu, Huanle and Ye, Kejiang and Xu, Chengzhong},
title = {Derm: SLA-Aware Resource Management for Highly Dynamic Microservices},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00039},
doi = {10.1109/ISCA59077.2024.00039},
abstract = {Ensuring efficient resource allocation while providing service level agreement (SLA) guarantees for end-to-end (E2E) latency is crucial for microservice applications. Although existing studies have made significant contributions towards achieving this objective, they primarily concentrate on static graphs. However, microservice graphs are inherently dynamic during runtime in production environments, necessitating more effective and scalable resource management solutions.In this paper, we present Derm, a new resource management system designed for microservice applications with highly dynamic graphs. Our principal finding is that prioritizing different microservice graphs can lead to a substantial reduction in resource allocation. To take advantage of this opportunity, we develop three main components. The first is a performance model that describes uncertainties of microservice latency through a conditional exponential distribution. The second is a probabilistic quantification of the dynamics of microservice graphs. The third is an optimization method for adjusting the resource allocation of microservices to minimize resource usage. We evaluate Derm in our cluster using real microservice benchmarks and production traces. The results highlight that Derm reduces the resource usage by 68.4\% and lowers SLA violation probability by 6.7\texttimes{}, compared to existing approaches.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {424–436},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00040,
author = {Stojkovic, Jovan and Misra, Pulkit A. and Goiri, \'{I}\~{n}igo and Whitlock, Sam and Choukse, Esha and Das, Mayukh and Bansal, Chetan and Lee, Jason and Sun, Zoey and Qiu, Haoran and Zimmermann, Reed and Samal, Savyasachi and Warrier, Brijesh and Raniwala, Ashish and Bianchini, Ricardo},
title = {SmartOClock: Workload- and Risk-Aware Overclocking in the Cloud},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00040},
doi = {10.1109/ISCA59077.2024.00040},
abstract = {Operating server components beyond their voltage and power design limit (i.e., overclocking) enables improving performance and lowering cost for cloud workloads. However, overclocking can significantly degrade component lifetime, increase power draw, and cause power capping events, eventually diminishing the performance benefits.In this paper, we characterize the impact of overclocking on cloud workloads by studying their profiles from production deployments. Based on the characterization insights, we propose SmartOClock, the first distributed overclocking management platform specifically designed for cloud environments. SmartOClock is a workload-aware scheme that relies on power predictions to heterogeneously distribute the power budgets across its servers based on their needs and then enforce budget compliance locally, per-server, in a decentralized manner.SmartOClock reduces the tail latency by 9\%, application cost by 30\% and total energy consumption by 10\% for latency-sensitive microservices on a 36-server deployment. Simulation analysis using production traces show that SmartOClock reduces the number of power capping events by up to 95\% while increasing the overclocking success rate by up to 62\%. We also describe lessons from building a first-of-its-kind overclockable cluster in Microsoft Azure for production experiments.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {437–451},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00041,
author = {Wang, Jaylen and Berger, Daniel S. and Kazhamiaka, Fiodar and Irvene, Celine and Zhang, Chaojie and Choukse, Esha and Frost, Kali and Fonseca, Rodrigo and Warrier, Brijesh and Bansal, Chetan and Stern, Jonathan and Bianchini, Ricardo and Sriraman, Akshitha},
title = {Designing Cloud Servers for Lower Carbon},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00041},
doi = {10.1109/ISCA59077.2024.00041},
abstract = {To mitigate climate change, we must reduce carbon emissions from hyperscale cloud computing. We find that cloud compute servers cause the majority of emissions in a general-purpose cloud. Thus, we motivate designing carbon-efficient compute server SKUs, or GreenSKUs, using recently-available low-carbon server components. To this end, we design and build three GreenSKUs using low-carbon components, such as energy-efficient CPUs, reused old DRAM via CXL, and reused old SSDs.We detail several challenges that limit GreenSKUs' carbon savings at scale and may prevent their adoption by cloud providers. To address these challenges, we develop a novel methodology and associated framework, GSF (GreenSKU Framework), that enables a cloud provider to systematically evaluate a GreenSKU's carbon savings at scale. We implement GSF within Microsoft Azure's production constraints to evaluate our three GreenSKUs' carbon savings. Using GSF, we show that our most carbon-efficient GreenSKU reduces emissions per core by 28\% compared to currently-deployed cloud servers. When designing GreenSKUs to meet applications' performance requirements, we reduce emissions by 15\%. When incorporating overall data center overheads, our GreenSKU reduces Azure's net cloud emissions by 8\%.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {452–470},
numpages = {19},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00042,
author = {Stojkovic, Jovan and Iliakopoulou, Nikoleta and Xu, Tianyin and Franke, Hubertus and Torrellas, Josep},
title = {EcoFaaS: Rethinking the Design of Serverless Environments for Energy Efficiency},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00042},
doi = {10.1109/ISCA59077.2024.00042},
abstract = {While serverless computing is increasingly popular, its energy and power consumption behavior is hardly explored. In this work, we perform a thorough characterization of the serverless environment and observe that it poses a set of challenges not effectively handled by existing energy-management schemes. Short serverless functions execute in opaque virtualized sandboxes, are idle for a large fraction of their invocation time, context switch frequently, and are co-located in a highly dynamic manner with many other functions of diverse properties. These features are a radical shift from more traditional application environments and require a new approach to manage energy and power.Driven by these insights, we design EcoFaaS, the first energy management framework for serverless environments. EcoFaaS takes a user-provided end-to-end application Service Level Objective (SLO). It then splits the SLO into per-function deadlines that minimize the total energy consumption. Based on the computed deadlines, EcoFaaS sets the optimal per-invocation core frequency using a prediction algorithm. The algorithm performs a fine-grained analysis of the execution time of each invocation, while taking into account the specific invocation inputs. To maximize efficiency, EcoFaaS splits the cores in a server into multiple Core Pools, where all the cores in a pool run at the same frequency and are controlled by a single scheduler. EcoFaaS dynamically changes the sizes and frequencies of the pools based on the current system state. We implement EcoFaaS on two open-source serverless platforms (OpenWhisk and KNative) and evaluate it using diverse serverless applications. Compared to state-of-the-art energy-management systems, EcoFaaS reduces the total energy consumption of serverless clusters by 42\% while simultaneously reducing the tail latency by 34.8\%.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {471–486},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00043,
author = {Rogers, Joseph and Soliman, Taha and Jahre, Magnus},
title = {AIO: An Abstraction for Performance Analysis across Diverse Accelerator Architectures},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00043},
doi = {10.1109/ISCA59077.2024.00043},
abstract = {Specialization is the key approach for continued performance growth beyond the end of Dennard scaling. Academics and industry are hence continuously proposing new accelerator architectures, including conventional Domain-Specific Accelerators (DSAs) and emerging Processing in Memory (PIM) accelerators. We are thus fast approaching an era in which early-stage accelerator analysis is critical for maintaining the productivity of software developers, system software designers, and computer architects — to ensure that they focus time-consuming implementation and optimization efforts on the most favorable class of accelerators for the problem at hand. Unfortunately, existing approaches fall short because they either adopt a level of abstraction that is too high — and therefore are unable to account for key performance phenomena — or too low — because they focus on details that do not generalize across diverse accelerators.Our Architecture-Independent Operation (AIO) abstraction addresses this issue by leveraging that accelerators typically focus on data-level parallelism, and an AIO is hence a key piece of algorithm-level data-parallel work that remains the same across diverse accelerators. To demonstrate that the AIO abstraction can be accurate and useful, we create the AccMe performance model which predicts kernel performance by estimating the number of clock cycles spent on compute, memory, and invocation overhead while accounting for overlap between compute and memory cycles as well as finite memory bandwidth. We demonstrate that AccMe can be accurate, i.e., it yields an average performance prediction error of 5.6\% across our diverse kernels and accelerators. This is a significant improvement over the 20.6\% average error of curve-fitted Roofline which provides the best-case accuracy of Roofline's operational intensity abstraction. We further demonstrate that AccMe is useful through three case studies that illustrate (i) how developers can use AccMe for accelerator selection under uncertainty; (ii) how system software can use AccMe for scheduling — and thereby improve throughput by 2.8\texttimes{} on average compared to Roofline-driven scheduling; and (iii) how computer architects can use AccMe for architectural exploration.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {487–500},
numpages = {14},
keywords = {specialization, performance modeling, accelerators, processing in memory},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00044,
author = {Whangbo, Joonho and Lim, Edwin and Zhang, Chengyi Lux and Anderson, Kevin and Gonzalez, Abraham and Gupta, Raghav and Krishnakumar, Nivedha and Karandikar, Sagar and Nikoli\'{c}, Borivoje and Shao, Yakun Sophia and Asanovi\'{c}, Krste},
title = {FireAxe: Partitioned FPGA-Accelerated Simulation of Large-Scale RTL Designs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00044},
doi = {10.1109/ISCA59077.2024.00044},
abstract = {Pre-silicon validation and end-to-end system evaluation are integral parts of hardware development as they provide architects with insights about the complex interactions between various hardware components, system software, and application code. Although this process can be accelerated using FPGAs as a simulation host, existing platforms fall short when the resource requirements of a custom hardware design exceed a single FPGA.We present FireAxe, an open-source FPGA-accelerated RTL simulation platform that supports push-button user-guided partitioning across multiple FPGAs, using a compiler called FireRipper. Given a partition point, FireRipper automatically maps a monolithic RTL design onto multiple FPGAs while providing hardware designers quick feedback about the partition interface and expected simulation performance. Furthermore, FireRipper enables users to choose between an exact-mode which provides cycle-exact results with RTL-level fidelity, or a fast-mode that improves simulation rate while sacrificing fidelity only at the partition boundary. Built on FireSim, FireAxe preserves the ability to elastically scale simulations from on-premises FPGAs to cloud FPGAs. For example, pulling out a core from a system-on-chip (SoC) onto a separate FPGA, we achieve simulation rates of 1.6 MHz using on-premises FPGAs connected by direct-attach cables and 1 MHz on AWS F1 FPGAs using peer-to-peer PCIe.To show FireAxe's ability to enable pre-silicon performance validation at unprecedented scale, we show several case studies. First, we replicate full-stack system-level effects such as latency spikes from garbage collection in a Golang application on an SoC containing 4 out-of-order (OoO) cores. We also boot Linux on, to our knowledge, the largest OoO core ever cycle-exactly simulated in academia. Lastly, we simulate a system-on-chip containing 24 OoO cores mapped onto five datacenter-class FPGAs. We discover an RTL bug when trying to run Linux user-space applications that did not appear with less substantial software stacks. This was discovered in less than 2 hours using FireAxe and would have taken weeks in a commercial software RTL simulator.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {501–515},
numpages = {15},
keywords = {computer simulation, RTL simulation, FPGAs, performance analysis, computer architecture, scalability},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00045,
author = {Karystinos, Nikos and Papadimitriou, George and Chatzopoulos, Odysseas and Gizopoulos, Dimitris and Fragkoulis, George-Marios and Gurumurthi, Sudhanva},
title = {Harpocrates: Breaking the Silence of CPU Faults through Hardware-in-the-Loop Program Generation},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00045},
doi = {10.1109/ISCA59077.2024.00045},
abstract = {Several hyperscalers have recently disclosed the occurrence of Silent Data Corruptions (SDCs) in their systems fleets, sparking concerns about the severity of known and the existence of unidentified root causes of faults in CPUs. These incidents reveal that CPU chips have the potential to generate incorrect results for different tasks due to latent manufacturing defects, variability, marginalities, bugs, and aging. To tackle this problem, we present Harpocrates, an automated methodology for the generation of short, constrained-random functional test programs that maximize fault detection in target CPU structures and can be employed at different stages of system lifetime. Harpocrates stands out by adopting a hardware-model-in-the-loop approach, which iteratively refines the generated test programs using a detailed simulation-based microarchitecture engine. The engine models and grades for multiple hardware fault types that can lead to data corruptions during system operation. Harpocrates is versatile and can adapt to various program generators, ISAs, microarchitectures, and fault types. Our results on six important CPU hardware structures show that Harpocrates attains much shorter test generation times than hardware-agnostic publicly available frameworks and outperforms open-source test suites in terms of fault detection capability.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {516–531},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00046,
author = {Zhang, Nathan and Lacouture, Rubens and Sohn, Gina and Mure, Paul and Zhang, Qizheng and Kjolstad, Fredrik and Olukotun, Kunle},
title = {The Dataflow Abstract Machine Simulator Framework},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00046},
doi = {10.1109/ISCA59077.2024.00046},
abstract = {The growing interest in novel dataflow architectures and streaming execution paradigms has created the need for a simulator optimized for modeling dataflow systems.To fill this need, we present three new techniques that make it feasible to simulate complex systems consisting of thousands of components. First, we introduce an interface based on Communicating Sequential Processes which allows users to simultaneously describe functional and timing characteristics. Second, we introduce a scalable point-to-point synchronization scheme that avoids global synchronization. Finally, we demonstrate a technique to exploit slack in the simulated system, such as FIFOs, to increase simulation parallelism.We implement these techniques in the Dataflow Abstract Machine (DAM), a parallel simulator framework for dataflow systems. We demonstrate the benefits of using DAM by highlighting three case studies using the framework. First, we use DAM directly as an exploration tool for streaming algorithms on dataflow hardware. We simulate two different implementations of the attention algorithm used in large language models, and use DAM to show that the second implementation only requires a constant amount of local memory. Second, we re-implement a simulator for a sparse tensor algebra accelerator, resulting in 57\% less code and a simulation speedup of up to four orders of magnitude. Finally, we demonstrate a general technique for time-multiplexing real hardware to simulate multiple virtual copies of the hardware using DAM.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {532–547},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00047,
author = {Bakhshalipour, Mohammad and Gibbons, Phillip B.},
title = {Tartan: Microarchitecting a Robotic Processor},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00047},
doi = {10.1109/ISCA59077.2024.00047},
abstract = {This paper presents Tartan, a CPU architecture designed for a wide range of robotic applications. Tartan provides architectural support for common robotic kernels, ensuring its broad utility across different robotic tasks. The architecture effectively addresses both computational and memory bottlenecks, marking a significant advancement over previous works. Key features of Tartan include architectural support for oriented vectorization, approximate acceleration with accurate outcome, robot-semantic prefetching, and intra-application cache partitioning.On the six end-to-end robots in the RoWild Suite, Tartan boosts the performance of legacy robotic software by 1.2\texttimes{} (up to 1.4\texttimes{}), non-approximable software optimized for Tartan by 1.61\texttimes{} (up to 3.54\texttimes{}), and approximable software optimized for Tartan by 2.11\texttimes{} (up to 3.87\texttimes{}).},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {548–565},
numpages = {18},
keywords = {robotics, domain-specific processors, approximate computing, specialization},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00048,
author = {Shah, Deval and Aamodt, Tor M.},
title = {Collision Prediction for Robotics Accelerators},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00048},
doi = {10.1109/ISCA59077.2024.00048},
abstract = {Motion planning in dynamic environments is an important task for autonomous robotics. Emerging approaches employ neural networks that can learn by observing (e.g., human) experts. Such motion planners react to the environment by continually proposing candidate paths to reach a goal. Some of these candidate paths may be unsafe-i.e., cause collisions. Hence, proposed paths must be checked for safety using collision detection. We observe that 25\% – 41\% of the resulting collision detection queries can be eliminated if we can anticipate which queries will return an unsafe result. We leverage this observation to propose a mechanism, COORD, to predict whether a given robot position (pose) along a proposed path will result in a collision. By prioritizing the detailed evaluation of predicted collisions, COORD enables quickly eliminating invalid paths proposed by neural network and other sampling based motion planners. COORD does this by exploiting the physical spatial locality of different robot poses and using simple hashing and saturating counters. We demonstrate the potential of collision prediction on different computation platforms, including CPU, GPU, and ASIC. We further propose a hardware collision prediction unit (COPU), and integrate it with an existing collision detection accelerator. This results in an average 17.2\% – 32.1\% decrease in number of collision detection queries across different motion planning algorithms and robots. When applied to a state-of-the-art neural motion planner [41], COORD improves performance/watt by 1.23\texttimes{} on average for motion planning queries of varying difficulty levels. Further, we find that the benefits of collision prediction grow as the compute complexity of motion planning queries increases and provides 1.30\texttimes{} improvement in performance/watt in narrow passages and cluttered environments.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {566–581},
numpages = {16},
keywords = {robotics, hardware acceleration, motion planning, collision detection, collision prediction},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00049,
author = {Han, Seunghee and Moon, Seungjae and Suh, Teokkyu and Heo, JaeHoon and Kim, Joo-Young},
title = {BLESS: Bandwidth and Locality Enhanced SMEM Seeding Acceleration for DNA Sequencing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00049},
doi = {10.1109/ISCA59077.2024.00049},
abstract = {In an era marked by the pervasive spread of harmful viruses like COVID-19, the importance of DNA sequencing has grown significantly, given its crucial role in devising effective countermeasures. The seeding process, which aims to find locations of super-maximal exact matches (SMEM) between the DNA samples and reference genome for comparative analysis, has emerged as a major bottleneck due to its memory-intensive characteristics. The learned index approach has been developed that uses machine learning model to partially predict the location of the exact matches, which has effectively reduced the memory access. However, the lack of locality in the current in dexing structure and randomness at runtime of the seeding workload have constrained the memory bandwidth usage and have limited further performance advantage.In this paper, we propose BLESS, a bandwidth and locality enhanced SMEM seeding accelerator for learned-index-based DNA sequence alignment. BLESS is the first domain-specific seeding accelerator to maximize the potential hardware advantage of the learned index approach. We introduce coarse-fine (CF) block data structure, a novel memory mapping of seeding parameters to exploit spatial locality and increase effective bandwidth usage for any memory type, including high bandwidth memory (HBM). We also develop guaranteed search range update (GSRU) algorithm, a method that exploits caching in the search procedure to enable temporal locality and data reuse. Utilizing the CF block and GSRU algorithm, we develop a multi-core seeding accelerator using HBM with context switching and runtime scheduling for maximum core and memory bandwidth utilization. With these improvements, BLESS achieves 35.65\texttimes{} and 15.49\texttimes{} speedup over the state-of-the-art seeding system BWA-MEME and ERT-ASIC, respectively, in raw system performance.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {582–596},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00050,
author = {Pavon, Julian and Valdivieso, Ivan Vargas and Rojas, Carlos and Hernandez, Cesar and Aslan, Mehmet and Figueras, Roger and Yuan, Yichao and Lindegger, Jo\"{e}l and Alser, Mohammed and Moll, Francesc and Marco-Sola, Santiago and Ergin, Oguz and Talati, Nishil and Mutlu, Onur and Unsal, Osman and Valero, Mateo and Cristal, Adrian},
title = {QUETZAL: Vector Acceleration Framework for Modern Genome Sequence Analysis Algorithms},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00050},
doi = {10.1109/ISCA59077.2024.00050},
abstract = {Genome sequence analysis is fundamental to medical breakthroughs such as developing vaccines, enabling genome editing, and facilitating personalized medicine. The exponentially expanding sequencing datasets and complexity of sequencing algorithms necessitate performance enhancements. While the performance of software solutions is constrained by their underlying hardware platforms, the utility of fixed-function accelerators is restricted to only certain sequencing algorithms.This paper presents QUETZAL, the first general-purpose vector acceleration framework designed for high efficiency and broad applicability across a diverse set of genomics algorithms. While a commercial CPU's vector datapath is a promising candidate to exploit the data-level parallelism in genomics algorithms, our analysis finds that its performance is often limited due to long-latency scatter/gather memory instructions. QUETZAL introduces a hardware-software co-design comprising an accelerator microarchitecture closely integrated with the CPU's vector datapath, alongside novel vector instructions to fully capitalize on the proposed hardware. QUETZAL integrates a set of scratchpad-style buffers meticulously designed to minimize latency associated with scatter/gather instructions during the retrieval of input genome sequences data. QUETZAL supports both short and long reads, and different types of sequencing data formats. A combination of hardware and software techniques enables QUETZAL to reduce the latency of memory instructions, perform complex computation using a single instruction, and transform data representations at runtime, resulting in overall efficiency gain. QUETZAL significantly accelerates a vectorized CPU baseline on modern genome sequence analysis algorithms by 5.7\texttimes{}, while incurring a small area overhead of 1.4\% post place-and-route at the 7nm technology node compared to an HPC ARM CPU.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {597–612},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00051,
author = {Huang, Jinghan and Lou, Jiaqi and Vanavasam, Srikar and Kong, Xinhao and Ji, Houxiang and Jeong, Ipoom and Zhuo, Danyang and Lee, Eun Kyung and Kim, Nam Sung},
title = {HAL: Hardware-Assisted Load Balancing for Energy-Efficient SNIC-Host Cooperative Computing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00051},
doi = {10.1109/ISCA59077.2024.00051},
abstract = {A typical SmartNIC (SNIC) integrates a processor comprising Arm CPU and accelerators with a conventional NIC. The processor is designed to energy-efficiently execute network functions frequently used by datacenter applications. With such a processor, the SNIC has promised to notably improve the system-wide energy efficiency of datacenter servers. Nevertheless, the latest trend of integrating accelerators into server CPUs for these functions sparks a question on the SNIC processor's superiority over a host processor (i.e., server CPU with accelerators) in system-wide energy efficiency, especially under given tail latency constraints. Answering this question, we first take an Intel Xeon processor, integrated with various accelerators (e.g., QuickAssist Technology), as a host processor, and then compare it to an NVIDIA BlueField-2 SNIC processor. This uncovers that (1) the host accelerator, coupled with a more powerful memory subsystem, can outperform the SNIC accelerator, and (2) the SNIC processor can improve system-wide energy efficiency only at low packet rates for most functions under tail latency constraints. To provide high system-wide energy efficiency without compromising tail latency at any packet rates, we propose HAL, consisting of a hardware-based load balancer and an intelligent load balancing policy implemented inside the SNIC. When HAL determines that the SNIC processor cannot efficiently process a given function beyond a specific packet rate, it limits the rate of packets to the SNIC processor and lets the host processor handle the excess. We implement a HAL-enabled SNIC with a commodity FPGA and a BlueField-2 SNIC, plug it into a commodity server, and run 10 popular network functions. Our evaluation shows that HAL can improve the system-wide energy efficiency and throughput of the server running these functions by 31\% and 10\%, respectively, without notably increasing the tail latency.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {613–627},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00052,
author = {Tian, Boyu and Li, Yiwei and Jiang, Li and Cai, Shuangyu and Gao, Mingyu},
title = {NDPBridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Architectures},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00052},
doi = {10.1109/ISCA59077.2024.00052},
abstract = {Various near-data processing (NDP) designs have been proposed to alleviate the memory wall challenge for data-intensive applications. Among them, near-DRAM-bank NDP architectures, by incorporating logic near each DRAM bank, promise the highest efficiency and have already been commercially available now. However, due to physical isolation, fast and direct cross-bank communication is impossible in these architectures, limiting their usage to only simple parallel patterns. Applications may also suffer from severe load imbalance if each bank contains data with diverse computation loads. We thus propose NDPBridge, with novel hardware-software co-design to enable cross-bank communication and dynamic load balancing for near-bank NDP systems. We introduce hardware bridges along the DRAM hierarchy to coordinate message transfers among banks. The hardware changes are constrained and do not disrupt the existing DDR links and protocols. We further enable hierarchical and data-transfer-aware load balancing, built upon the above hardware communication path and a task-based programming model. The data transfer overheads are minimized with several novel optimizations to hide latency, avoid congestion, and reduce traffic. Our evaluation shows that NDPBridge significantly outperforms existing NDP designs by 2.23\texttimes{} to 2.98\texttimes{} on average.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {628–643},
numpages = {16},
keywords = {near-data processing, processing-in-memory, DRAM, communication, load balance},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00053,
author = {Zhao, Yilong and Gao, Mingyu and Liu, Fangxin and Hu, Yiwei and Wang, Zongwu and Lin, Han and Li, Ji and Xian, He and Dong, Hanlin and Yang, Tao and Jing, Naifeng and Liang, Xiaoyao and Jiang, Li},
title = {UM-PIM: DRAM-Based PIM with Uniform \&amp; Shared Memory Space},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00053},
doi = {10.1109/ISCA59077.2024.00053},
abstract = {DRAM-based Processing in Memory (PIM) addresses the "memory wall" problem by incorporating computing units (PIM units) into main memory devices for faster and wider local data access. However, critical challenges prevent PIM units from being compatible with existing CPU hosts. Memory interleaving and virtual memory limit the size of contiguous data visible to PIM units that constrains the granularity of PIM tasks. Fine-grained PIM tasks result in significant CPU-PIM offloading overhead, offsetting the speed-up of PIM. Existing PIM systems adopt drastic measures to ensure PIM task offloading efficiency, including isolating PIM memory space and turning off global memory interleaving. These interventions, however, decrease the CPU's memory bandwidth and introduce extra data transfer, leading to an additional "system memory wall". This new "wall" must be eliminated before fully embracing the PIM technology.In this work, we propose UM-PIM, a PIM system with interleaved CPU pages and non-interleaved PIM pages coexisting in a Uniform and Shared Memory space. UM-PIM enables zero-copy during PIM task offloading and maintains the CPU's memory bandwidth while ensuring PIM offloading efficiency. Firstly, we propose a dual-track memory management mechanism consisting of independent page allocation and address translation for the two kinds of pages, respectively. Second, we design UM-PIM interface hardware on the DIMM (with PIMs) side to provide a dynamic address mapping for accelerating the data re-layout. Finally, we provide APIs to reduce PIM-to-PIM communication overhead by optimizing the CPU's access to PIM pages in different communication modes. We compare UM-PIM with a CPU system and the current PIM systems. Results show negligible performance degradation for CPU workloads (&lt;0.1\%) on UM-PIM, contrasting with the 25.8\% degradation on the current PIM system with memory interleaving switched off. For PIM workloads partitioned to CPU and PIM units, UM-PIM can reduce the CPU time by 4.93\texttimes{}, resulting in an end-to-end 1.96\texttimes{} speedup on average.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {644–659},
numpages = {16},
keywords = {processing in memory (PIM), DRAM, address mapping, data re-layout},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00054,
author = {Ghiasi, Nika Mansouri and Sadrosadati, Mohammad and Mustafa, Harun and Gollwitzer, Arvid and Firtina, Can and Eudine, Julien and Mao, Haiyu and Lindegger, Jo\"{e}l and Cavlak, Meryem Banu and Alser, Mohammed and Park, Jisung and Mutlu, Onur},
title = {MegIS: High-Performance, Energy-Efficient, and Low-Cost Metagenomic Analysis with In-Storage Processing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00054},
doi = {10.1109/ISCA59077.2024.00054},
abstract = {Metagenomics, the study of the genome sequences of diverse organisms in a common environment, has led to significant advances in many fields. Since the species present in a metagenomic sample are not known in advance, metagenomic analysis commonly involves the key tasks of determining the species present in a sample and their relative abundances. These tasks require searching large metagenomic databases containing information on different species' genomes. Metagenomic analysis suffers from significant data movement overhead due to moving large amounts of low-reuse data from the storage system to the rest of the system. In-storage processing can be a fundamental solution for reducing this overhead. However, designing an in-storage processing system for metagenomics is challenging because existing approaches to metagenomic analysis cannot be directly implemented in storage effectively due to the hardware limitations of modern SSDs.We propose MegIS, the first in-storage processing system designed to significantly reduce the data movement overhead of the end-to-end metagenomic analysis pipeline. MegIS is enabled by our lightweight design that effectively leverages and orchestrates processing inside and outside the storage system. Through our detailed analysis of the end-to-end metagenomic analysis pipeline and careful hardware/software co-design, we address in-storage processing challenges for metagenomics via specialized and efficient 1) task partitioning, 2) data/computation flow coordination, 3) storage technology-aware algorithmic optimizations, 4) data mapping, and 5) lightweight in-storage accelerators. MegIS's design is flexible, capable of supporting different types of metagenomic input datasets, and can be integrated into various metagenomic analysis pipelines. Our evaluation shows that MegIS outperforms the state-of-the-art performance- and accuracy-optimized software metagenomic tools by 2.7\texttimes{} – 37.2\texttimes{} and 6.9\texttimes{} –100.2\texttimes{}, respectively, while matching the accuracy of the accuracy-optimized tool. MegIS achieves 1.5\texttimes{} –5.1\texttimes{} speedup compared to the state-of-the-art metagenomic hardware-accelerated (using processing-in-memory) tool, while achieving significantly higher accuracy.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {660–677},
numpages = {18},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00055,
author = {C\i{}lasun, H\"{u}srev and Resch, Salonik and Chowdhury, Zamshed I. and Zabihi, Masoud and Lv, Yang and Zink, Brandon and Wang, Jian-Ping and Sapatnekar, Sachin S. and Karpuzcu, Ulya R.},
title = {On Error Correction for Nonvolatile Processing-In-Memory},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00055},
doi = {10.1109/ISCA59077.2024.00055},
abstract = {Processing in memory (PiM) represents a promising computing paradigm to enhance performance of numerous data-intensive applications. Variants performing computing directly in emerging nonvolatile memories can deliver very high energy efficiency. PiM architectures directly inherit the vulnerabilities of the underlying memory substrates, but they also are subject to errors due to the computation in place. Numerous well-established error correcting codes (ECC) for memory exist, and are also considered in the PiM context, however, they typically ignore errors that occur throughout computation. In this paper we revisit the error correction design space for nonvolatile PiM, considering both storage/memory and computation-induced errors, surveying several self-checking and homomorphic approaches. We propose several solutions and analyze their complex performance-area-coverage trade-off, using three representative nonvolatile PiM technologies. All of these solutions guarantee single error correction for both, bulk bitwise computations and ordinary memory/storage errors.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {678–692},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00056,
author = {Chowdhuryy, Md Hafizul Islam and Zheng, Hao and Yao, Fan},
title = {MetaLeak: Uncovering Side Channels in Secure Processor Architectures Exploiting Metadata},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00056},
doi = {10.1109/ISCA59077.2024.00056},
abstract = {Microarchitectural side channels raise severe security concerns. Recent studies indicate that microarchitecture security should be examined holistically (rather than separately) in systems. Although the effects of performance optimizations on side channels are widely studied, the impacts of integrating security mechanisms intended for other threats on microarchitecture security are not well explored.In this paper, we perform the first side channel exploration in secure processor architectures that offer data confidentiality and integrity protection through hardware. We investigate microarchitecture security in the design space of secure processors and identify unique properties in the underlying metadata management schemes, which can be leveraged for new information leakage attacks. We present MetaLeak, an end-to-end side channel attack framework that exploits timing variations due to metadata maintenance to exfiltrate program secrets in secure processors. Particularly, we present two variants of the attack: MetaLeak-T that exploits the sharing of integrity tree metadata, and MetaLeak-C that manipulates counter metadata states. Our evaluation first shows highly accurate covert communication using the security metadata that can operate across cores and sockets without explicit data sharing. We further perform extensive side channel case studies on state-of-the-art secure architecture designs as well as the SGX processors. Our results show that MetaLeak can successfully exfiltrate program secrets (up to 97\% accuracy) from image-processing application and cryptographic software running in enclave. Our study indicates that the fundamental metadata mechanism is the root cause of the leakage, which necessitates the use of leakage-taming techniques in future secure processors. This work highlights the need to synergistically understand microarchitecture security, as new security mechanisms are integrated.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {693–707},
numpages = {15},
keywords = {microarchitecture security, side channel, secure architectures, trusted execution environment, metadata},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00057,
author = {Feng, Erhu and Feng, Dahu and Du, Dong and Xia, Yubin and Chen, Haibo},
title = {sNPU: Trusted Execution Environments on Integrated NPUs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00057},
doi = {10.1109/ISCA59077.2024.00057},
abstract = {Trusted execution environment (TEE) promises strong security guarantee with hardware extensions for security-sensitive tasks. Due to its numerous benefits, TEE has gained widespread adoption, and extended from CPU-only TEEs to FPGA and GPU TEE systems. However, existing TEE systems exhibit inadequate and inefficient support for an emerging (and significant) processing unit, NPU. For instance, commercial TEE systems resort to coarse-grained and static protection approaches for NPUs, resulting in notable performance degradation (10\%–20\%), limited (or no) multitasking capabilities, and suboptimal resource utilization. In this paper, we present a secure NPU architecture, known as sNPU, which aims to mitigate vulnerabilities inherent to the design of NPU architectures. First, sNPU proposes NPU Guarder to enhance the NPU's access control. Second, sNPU defines new attack surfaces leveraging in-NPU structures like scratchpad and NoC, and designs NPU Isolator to guarantee the isolation of scratchpad and NoC routing. Third, our system introduces a trusted software module called NPU Monitor to minimize the software TCB. Our prototype, evaluated on FPGA, demonstrates that sNPU significantly mitigates the runtime costs associated with security checking (from upto 20\% to 0\%) while incurring less than 1\% resource costs.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {708–723},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00058,
author = {Wang, Xin and Kotra, Jagadish and Jones, Alex and Xiong, Wenjie and Jian, Xun},
title = {Counter-Light Memory Encryption},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00058},
doi = {10.1109/ISCA59077.2024.00058},
abstract = {Unlike the well-known counter mode memory encryption (e.g., SGX1), more recent memory encryption (e.g., SGX2, SEV) has no counters. Without accessing any counters, such counterless memory encryption improves performance over counter mode encryption and gains wide adoption as a result.Counterless encryption, however, still incurs a costly performance overhead. Under counterless encryption, the cipher calculations take data as their direct inputs. As such, the ciphers for decrypting data can only be calculated sequentially after the missing data arrive from memory; this requires every last-level cache miss to stall on the cipher calculations after the needed data arrive from memory. Our real-system measurements find counterless encryption can slow down irregular workloads by 9\%, on average.We observe while counter mode encryption incurs costly memory access overhead, its cipher calculations can often complete before data arrive because they take counters as input, instead of data, and counters can fit on-chip much better than data. As such, we explore how to combine both modes of encryption to achieve the best of both worlds - the efficient memory accesses of counterless encryption and fast cipher calculations of counter mode encryption. For irregular workloads, our proposed memory encryption - Counter-light Encryption - achieves 98\% the average performance of no memory encryption. When memory bandwidth is starved, Counter-light Encryption is slower than counterless encryption by only 1.4\% in the worst case.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {724–738},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00059,
author = {Kim, Tae Hoon and Rudo, David and Zhao, Kaiyang and Zhao, Zirui Neil and Skarlatos, Dimitrios},
title = {Perspective: A Principled Framework for Pliable and Secure Speculation in Operating Systems},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00059},
doi = {10.1109/ISCA59077.2024.00059},
abstract = {Transient execution attacks present an unprecedented threat to computing systems. Protecting the operating system (OS) is exceptionally challenging because a transient execution gadget in the OS can potentially leak the entire memory.In this work, we propose Perspective, a principled framework for building pliable and secure speculative execution defenses for the OS. Perspective offers a pliable interface that allows the OS to communicate its security requirements to hardware defenses, enabling tailored protection against transient execution attacks with little performance overhead. The design of Perspective is driven by a taxonomy of transient execution attacks in the OS kernel: (i) active transient execution attacks, where the attacker process exploits its own kernel thread to speculatively execute a transient execution gadget in the kernel, and (ii) passive transient execution attacks, where the attacker coerces the victim process's kernel thread to execute a transient execution gadget. Based on the taxonomy, Perspective introduces Data Speculation Views (DSVs) and Instruction Speculation Views (ISVs), to mitigate active and passive attacks, respectively. DSVs define the ownership of kernel data by a given execution context and block any speculative access to data outside the DSV. ISVs define the set of kernel functions that can be speculatively executed by a given execution context. Any transmitter instructions—whose execution could leak secrets, such as load instructions—that belong to kernel functions outside the ISVs are blocked from speculative execution. ISVs open up new opportunities of (i) swiftly patching gadgets in the OS, (ii) reducing the surface of passive attacks, and (iii) speeding up the process of auditing transient execution gadgets in the OS.We build Perspective's software components in the Linux kernel and model the hardware components in gem5. We evaluate the security and performance of Perspective on a set of microbenchmarks and datacenter applications. Perspective has an execution overhead over an unprotected kernel of only 3.5\% on microbenchmarks and only 1.2\% on datacenter applications.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {739–755},
numpages = {17},
keywords = {operating systems, speculative execution, virtualization},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00060,
author = {Agrawal, Rashmi and Chandrakasan, Anantha and Joshi, Ajay},
title = {HEAP: A Fully Homomorphic Encryption Accelerator with Parallelized Bootstrapping},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00060},
doi = {10.1109/ISCA59077.2024.00060},
abstract = {Fully homomorphic encryption (FHE) is a cryptographic technology with the potential to revolutionize data privacy by enabling computation on encrypted data. Lately, the CKKS FHE scheme has become quite popular because it can process real numbers. However, CKKS computing is not pervasive yet because it is resource-intensive both in terms of compute and memory, and is multiple orders of magnitude slower than computing on unencrypted data. The recent algorithmic and hardware optimizations to accelerate CKKS computing are promising, but CKKS computing continues to underperform due to an expensive operation known as bootstrapping. While there have been several efforts to accelerate bootstrapping, it continues to remain the main performance bottleneck. One of the reasons for this performance bottleneck is that unlike the non-bootstrapping parts of CKKS computing the bootstrapping algorithm is inherently sequential and exhibits interdependencies among the data.To address this challenge, in this paper, we introduce HEAP an accelerator that uses a hybrid scheme-switching approach. HEAP uses the CKKS scheme for the non-bootstrapping steps, but switches to the TFHE scheme when performing the bootstrapping step of the CKKS scheme. The hybrid approach transitions to the TFHE scheme by extracting coefficients from a single RLWE ciphertext to represent multiple LWE ciphertexts. We incorporate the bootstrapping function into the TFHE BlindRotate operation and simultaneously apply the BlindRotate operation to all LWE ciphertexts. A parallelized execution of bootstrapping is then feasible because there are no data dependencies between distinct LWE ciphertexts. With our approach, we require smaller-sized bootstrapping keys leading to about 18\texttimes{} less amount of data to be read from the main memory for the keys. In addition, we introduce a variety of hardware optimizations in HEAP—from modular arithmetic level to NTT and BlindRotate datapath optimizations. The approach in HEAP is agnostic of the hardware and can be mapped to any system with multiple compute nodes. To evaluate HEAP, we implemented it in RTL and mapped it to a single FPGA system and an eight-FPGA system. Our comprehensive evaluation of HEAP for the bootstrapping operation shows a 15.39\texttimes{} improvement when compared to FAB. Similarly, evaluation of HEAP for the logistic regression model training shows 14.71\texttimes{} and 11.57\texttimes{} improvement when compared to FAB and FAB-2 implementations, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {756–769},
numpages = {14},
keywords = {CKKS, TFHE, scheme switching, bootstrapping, FPGA acceleration},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00061,
author = {Jung, Dai Cheol and Ruttenberg, Max and Gao, Paul and Davidson, Scott and Petrisko, Daniel and Li, Kangli and Kamath, Aditya K and Cheng, Lin and Xie, Shaolin and Pan, Peitian and Zhao, Zhongyuan and Yue, Zichao and Veluri, Bandhav and Muralitharan, Sripathi and Sampson, Adrian and Lumsdaine, Andrew and Zhang, Zhiru and Batten, Christopher and Oskin, Mark and Richmond, Dustin and Taylor, Michael Bedford},
title = {Scalable, Programmable and Dense: The HammerBlade Open-Source RISC-V Manycore},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00061},
doi = {10.1109/ISCA59077.2024.00061},
abstract = {Existing tiled manycore architectures propose to convert abundant silicon resources into general-purpose parallel processors with unmatched computational density and programmability. However, as we approach 100K cores in one chip, conventional manycore architectures struggle to navigate three key axes: scalability, programmability, and density. Many manycores sacrifice programmability for density; or scalability for programmability. In this paper, we explore HammerBlade, which simultaneously achieves scalability, programmability and density. HammerBlade is a fully open-source RISC-V manycore architecture, which has been silicon-validated with a 2048-core ASIC implementation using a 14/16nm process. We evaluate the system using a suite of parallel benchmarks that captures a broad spectrum of computation and communication patterns.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {770–784},
numpages = {15},
keywords = {manycore architecture, parallel programming, open-source hardware, RISC-V},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00062,
author = {Kokolis, Apostolos and Psistakis, Antonis and Reidys, Benjamin and Huang, Jian and Torrellas, Josep},
title = {HADES: Hardware-Assisted Distributed Transactions in the Age of Fast Networks and SmartNICs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00062},
doi = {10.1109/ISCA59077.2024.00062},
abstract = {Transactional-based distributed storage applications such as key-value stores and databases are widely used in the cloud. Recently, the hardware on which these applications run has been rapidly improving, with faster networks and powerful network interface cards (NICs). A result of these hardware advances is that the inefficiencies of distributed software have become increasingly obvious.To address this problem, we analyze the sources of software overhead in these distributed transactional applications and propose new hardware structures to eliminate them. The proposed hardware includes Bloom filters for a variety of tasks and SmartNICs for efficient remote communication. We then develop HADES, a new distributed transactional protocol that leverages this hardware to support low-overhead distributed transactions. We also propose a hybrid hardware-software implementation of HADES. Our evaluation shows that HADES increases the throughput of distributed transactional workloads by 2.7\texttimes{} on average over a state-of-the-art distributed transactional system.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {785–800},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00063,
author = {Cochet, Martin and Swaminathan, Karthik and Loscalzo, Erik and Zuckerman, Joseph and Santos, Maico Cassel dos and Giri, Davide and Buyuktosunoglu, Alper and Jia, Tianyu and Brooks, David and Wei, Gu-Yeon and Shepard, Kenneth and Carloni, Luca P. and Bose, Pradip},
title = {BlitzCoin: Fully Decentralized Hardware Power Management for Accelerator-Rich SoCs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00063},
doi = {10.1109/ISCA59077.2024.00063},
abstract = {On-chip power-management techniques have evolved over several processor generations. However, response time and scalability constraints have made it difficult to translate existing power-management strategies to current or next-generation System-on-Chip (SoC) architectures, which are expected to comprise tens to hundreds of cores and accelerators. In this work we present BlitzCoin, a fully decentralized hardware power-management strategy for large, accelerator-rich SoCs, coupled with optimized unified voltage and frequency regulation. We evaluated BlitzCoin through RTL simulations of multiple SoCs targeted toward different application domains. The results are further validated through silicon measurements of a fabricated 12 nm many-accelerator SoC that includes BlitzCoin. Our evaluations show that BlitzCoin is markedly faster, with 8\texttimes{} to 12\texttimes{} lower response times, which provides 25\%-34\% throughput improvement and allows for scaling to 7\texttimes{} to 13\texttimes{} larger SoCs compared to state-of-the-art centralized power-management strategies, all with an area overhead of &lt;1\%.This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {801–817},
numpages = {17},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00064,
author = {Hsia, Samuel and Golden, Alicia and Acun, Bilge and Ardalani, Newsha and DeVito, Zachary and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
title = {MAD-Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00064},
doi = {10.1109/ISCA59077.2024.00064},
abstract = {Training and deploying large-scale machine learning models is time-consuming, requires significant distributed computing infrastructures, and incurs high operational costs. Our analysis, grounded in real-world large model training on datacenter-scale infrastructures, reveals that 14~32\% of all GPU hours are spent on communication with no overlapping computation. To minimize this outstanding communication latency and other inherent at-scale inefficiencies, we introduce an agile performance modeling framework, MAD-Max. This framework is designed to optimize parallelization strategies and facilitate hardware-software co-design opportunities. Through the application of MAD-Max to a suite of real-world large-scale ML models on state-of-the-art GPU clusters, we showcase potential throughput enhancements of up to 2.24\texttimes{} for pre-training and up to 5.27\texttimes{} for inference scenarios, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {818–833},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00065,
author = {Feng, Yuan and Na, Seonjin and Kim, Hyesoon and Jeon, Hyeran},
title = {Barre Chord: Efficient Virtual Memory Translation for Multi-Chip-Module GPUs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00065},
doi = {10.1109/ISCA59077.2024.00065},
abstract = {With the advancement of processor packaging technology and the looming end of Moore's law, multi-chip-module (MCM) GPUs become a promising architecture to continue the performance scaling. However, due to the increasing concurrency, it is challenging to achieve scalable performance. In this study, we show that the limited parallelism in IOMMU is one of the critical bottlenecks and propose Barre Chord to fundamentally reduce the translation loads. By leveraging the unique GPU execution model and page mapping on MCM-GPUs, Barre translates virtual addresses in a unit of coalescing group. Once one page is translated, all the other pages within the same coalescing group can be translated with simple calculations without page table walks. Full Barre (F-Barre) further reduces translations by enabling intra-MCM translation through coalescing information sharing across GPU chiplets and contiguity-aware coalescing group expansion. With the combination of Barre and F-Barre, the Barre Chord outperforms state-of-the-art solutions by an average of 1.36\texttimes{} (2.09\texttimes{} with coalescing group expansion) with negligible area overhead (4.22\% of a GPU L2 TLB).},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {834–847},
numpages = {14},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00066,
author = {Yuan, Yifan and Wang, Ren and Ranganathan, Narayan and Rao, Nikhil and Kumar, Sanjay and Lantz, Philip and Sanjeepan, Vivekananthan and Cabrera, Jorge and Kwatra, Atul and Sankaran, Rajesh and Jeong, Ipoom and Kim, Nam Sung},
title = {Intel Accelerators Ecosystem: An SoC-Oriented Perspective},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00066},
doi = {10.1109/ISCA59077.2024.00066},
abstract = {A growing demand for hyperscale services has compelled hyperscalers to deploy more compute resources at an unprecedented pace, further accelerated by the demise of Dennard scaling. Meanwhile, a considerable portion of the compute resources are consumed to execute common functions present across the hyperscale services, i.e., datacenter taxes. These challenges motivated many to explore specialized accelerators for these functions. Leading such a technology trend, Intel has integrated diverse on-chip accelerators into its recent flagship datacenter CPU products. Furthermore, to support the easy and efficient use of these accelerators for successful deployment in production hyper-scale services, Intel has developed a hardware/software ecosystem.In this paper, we first focus on Intel's holistic efforts to build the hardware/software ecosystem, presenting key SoC-level features that facilitate efficient CPU-accelerator interaction, effortless programming and use, and scalable accelerator sharing and virtualization. Next, we delve into the functions, microarchitectures, and software stacks of three new on-chip accelerators: Data Streaming Accelerator (DSA), In-memory Analytics Accelerator (IAA), and Dynamic Load Balancer (DLB). Lastly, we demonstrate that Intel's on-chip accelerators can not only significantly reduce the datacenter taxes but also accelerate data-intensive applications essential for hyperscale services, with little effort to use the accelerators.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {848–862},
numpages = {15},
keywords = {accelerators, Xeon, SoC, datacenter tax, shared virtual memory, SIOV, DSA, IAA, DLB, QAT},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00067,
author = {Li, Yuan and Zhu, Jianbin and Fu, Yao and Lei, Yu and Nagata, Toshio and Braidwood, Ryan and Fu, Haohuan and Zheng, Juepeng and Luk, Wayne and Fan, Hongxiang},
title = {Circular Reconfigurable Parallel Processor for Edge Computing},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00067},
doi = {10.1109/ISCA59077.2024.00067},
abstract = {Graphics Processing Units (GPUs) have emerged as the predominant hardware platforms for massively parallel computing. However, their inherent von-Neumann architecture still suffers performance inefficiency stemming from the sequential instruction execution and frequent data transfer overheads within the memory system. These intrinsic architectural flaws lead to heavy overhead on the latency, area, and energy efficiency, rendering GPUs suboptimal for edge computing applications. To tackle these challenges, this paper introduces a novel circular Reconfigurable Parallel Processor (RPP) to enable massively parallel applications in edge computing with high efficiency. RPP features a novel circular array of reconfigurable compute engines, enabling efficient streaming dataflow processing. In contrast to traditional Coarse Grained Reconfigurable Architecture (CGRA), the circular network topology of RPP is formed by linear switch networks with an innovative gasket memory, which reduces complicated network routing overheads while allowing versatile datapath mapping and optimized data reuse. A dedicated hierarchical memory system is proposed to support different memory access patterns and address mapping strategies, enabling flexible data access with high memory efficiency. Several hardware optimizations are further introduced to improve hardware utilization and performance such as concurrent kernel execution, register split&amp;refill and heterogeneous scalar&amp;vector computing. To fully utilize the hardware capability of RPP, we develop an end-to-end software stack consisting of a compiler, runtime environment, and different RPP libraries. This software stack is designed to be compatible with the GPGPU computing paradigm, enhancing its potential for broader adoption. Fabricated in a 14nm process, RPP occupies an area of 119 mm2 and operates at a maximum power of 15W with a 1GHz clock frequency. From the runtime measurement of various workloads, RPP achieves up to 27.5\texttimes{} higher energy efficiency than Nvidia edge GPUs in deep learning inference and up to 14062\texttimes{} lower latency than AMD Ryzen 5 CPU in linear algebra operations.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {863–875},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00068,
author = {Smith, Alan and Loh, Gabriel H. and Schulte, Michael J. and Ignatowski, Mike and Naffziger, Samuel and Mantor, Mike and Fowler, Mark and Kalyanasundharam, Nathan and Alla, Vamsi and Malaya, Nicholas and Greathouse, Joseph L. and Chapman, Eric and Swaminathan, Raja},
title = {Realizing the AMD Exascale Heterogeneous Processor Vision},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00068},
doi = {10.1109/ISCA59077.2024.00068},
abstract = {AMD had previously detailed its exascale research journey from initial targets and requirements to the development and evolution of its vision of a high-performance computing (HPC) accelerated processing unit (APU), dubbed the Exascale Heterogeneous Processor or EHP. At the conclusion of that work, the learnings were integrated into the design of the node architecture that went into the Frontier supercomputer, the world's first exascale machine. However, while the Frontier node architecture embodied many of the attributes of the EHP concept, advanced heterogeneous integration capabilities at the time were not yet sufficiently mature to realize our vision of a fully-integrated APU for HPC and AI. In this paper, we finish the EHP's story by digging deeper into why an APU was not the right solution at the time of our first exascale architecture, what the shortcomings were of previous EHP concepts, and how AMD further evolved the concept into the AMD Instinct™ MI300A APU. MI300A is the culmination of years of AMD developments in advanced packaging technologies, its APU hardware and software, and the next step in our highly effective chiplet strategy to not only deliver a groundbreaking design for exascale computing, but to also meet the demands of new large-language model and generative AI applications.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {876–889},
numpages = {14},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00069,
author = {Kim, Hanjoon and Choi, Younggeun and Park, Junyoung and Bae, Byeongwook and Jeong, Hyunmin and Lee, Sang Min and Yeon, Jeseung and Kim, Minho and Park, Changjae and Gu, Boncheol and Lee, Changman and Bae, Jaeick and Bae, SungGyeong and Cha, Yojung and Choe, Wooyoung and Choi, Jonguk and Ha, Juho and Han, Hyuck and Hwang, Namoh and Hwang, Seokha and Jang, Kiseok and Je, Haechan and Jeon, Hojin and Jeon, Jaewoo and Jeong, Hyunjun and Jung, Yeonsu and Kang, Dongok and Kim, Hyewon and Kim, Minjae and Kim, Muhwan and Kim, Sewon and Kim, Suhyung and Kim, Won and Kim, Yong and Kim, Youngsik and Ku, Younki and Lee, Jeong Ki and Lee, Juyun and Lee, Kyungjae and Lee, Seokho and Noh, Minwoo and Oh, Hyuntaek and Park, Gyunghee and Park, Sanguk and Seo, Jimin and Seong, Jungyoung and Paik, June and Lopes, Nuno P. and Yoo, Sungjoo},
title = {TCP: A Tensor Contraction Processor for AI Workloads},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00069},
doi = {10.1109/ISCA59077.2024.00069},
abstract = {We introduce a novel tensor contraction processor (TCP) architecture that offers a paradigm shift from traditional architectures that rely on fixed-size matrix multiplications. TCP aims at exploiting the rich parallelism and data locality inherent in tensor contractions, thereby enhancing both efficiency and performance of AI workloads.TCP is composed of coarse-grained processing elements (PEs) to simplify software development. In order to efficiently process operations with diverse tensor shapes, the PEs are designed to be flexible enough to be utilized as a large-scale single unit or a set of small independent compute units.We aim at maximizing data reuse on both levels of inter and intra compute units. To do that, we propose a circuit switch-based fetch network to flexibly connect compute units to enable inter-compute unit data reuse. We also exploit input broadcast to multiple contraction engines and input buffer based reuse to further exploit reuse behavior in tensor contraction. Our compiler explores the design space of tensor contractions considering tensor shapes and the order of their associated loop operations as well as the underlying accelerator architecture.A TCP chip was designed and fabricated in 5nm technology as the second-generation product of Furiosa AI, offering 256/512/1024 TOPS (BF16/FP8 or INT8/INT4) with 256 MB SRAM and 1.5 TB/s 48 GB HBM3 under 150 W TDP. Commercialization will start in August 2024.We performed an extensive case study of running the LLaMA-2 7B model and evaluated its performance and power efficiency on various configurations of sequence length and batch size. For this model, TCP is 2.7\texttimes{} and 4.1\texttimes{} better than H100 and L40s, respectively, in terms of performance per watt.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {890–902},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00070,
author = {Kong, Weihao and Hao, Yifan and Guo, Qi and Zhao, Yongwei and Song, Xinkai and Li, Xiaqing and Zou, Mo and Du, Zidong and Zhang, Rui and Liu, Chang and Wen, Yuanbo and Jin, Pengwei and Hu, Xing and Li, Wei and Xu, Zhiwei and Chen, Tianshi},
title = {Cambricon-D: Full-Network Differential Acceleration for Diffusion Models},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00070},
doi = {10.1109/ISCA59077.2024.00070},
abstract = {Diffusion models have made significant progress in current image generation tasks, thus becoming a prominent area of research. Diffusion models necessitate repetitive iterations on minimally altered input data across timesteps, each timestep requiring the recalculation of the entire model, resulting in a remarkable computational redundancy and substantial hardware expenditures.Performing differential computing on input data seems to be a feasible approach for addressing such computational redundancy and improving hardware efficacy. However, non-linear operations (particularly activation functions) necessitate the merging of deltas (i.e., differential values) with raw inputs repeatedly to ensure computational correctness, leading to significant memory access for loading raw inputs, which fragmentedly blocks the forwarding of deltas throughout the network and undermines performance.To solve this problem, we propose Cambricon-D, a full-network differential computing architecture with concise memory access. While maintaining the computational efficiency brought by differential computing, Cambricon-D employs a sign-mask dataflow, which requires only the loading of 1-bit signs (instead of large bitwidth raw inputs), thereby facilitating the seamless forwarding of deltas and effectively mitigating memory access overheads. Experimental results show that, compared to Diffy, Cambricon-D's dataflow reduces 66\% ~ 82\% off-chip memory access. In total, Cambricon-D achieves 1.46\texttimes{} ~ 2.38\texttimes{} speedup over A100 on various diffusion models with different resolutions.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {903–914},
numpages = {12},
keywords = {diffusion models, machine learning, computer architecture},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00071,
author = {Pan, Xiurui and An, Yuda and Liang, Shengwen and Mao, Bo and Zhang, Mingzhe and Li, Qiao and Jung, Myoungsoo and Zhang, Jie},
title = {Flagger: Cooperative Acceleration for Large-Scale Cross-Silo Federated Learning Aggregation},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00071},
doi = {10.1109/ISCA59077.2024.00071},
abstract = {Cross-silo federated learning (FL) leverages homomorphic encryption (HE) to obscure the model updates from the clients. However, HE poses the challenges of complex cryptographic computations and inflated ciphertext sizes. As cross-silo FL scales to accommodate larger models and more clients, the overheads of HE can overwhelm a CPU-centric aggregator architecture, including excessive network traffic, enormous data volume, intricate computations, and redundant data movements. Tackling these issues, we propose Flagger, an efficient and high-performance FL aggregator. Flagger meticulously integrates the data processing unit (DPU) with computational storage drives (CSD), employing these two distinct near-data processing (NDP) accelerators as a holistic architecture to collaboratively enhance FL aggregation. With the delicate delegation of complex FL aggregation tasks, we build Flagger-DPU and Flagger-CSD to exploit both in-network and in-storage HE acceleration to streamline FL aggregation. We also implement Flagger-Runtime, a dedicated software layer, to coordinate NDP accelerators and enable direct peer-to-peer data exchanges, markedly reducing data migration burdens. Our evaluation results reveal that Flagger expedites the aggregation in FL training iterations by 436\% on average, compared with traditional CPU-centric aggregators.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {915–930},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00072,
author = {Yang, Yifan and Emer, Joel S. and Sanchez, Daniel},
title = {Trapezoid: A Versatile Accelerator for Dense and Sparse Matrix Multiplications},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00072},
doi = {10.1109/ISCA59077.2024.00072},
abstract = {Accelerating matrix multiplication is crucial to achieve high performance in many application domains, including neural networks, graph analytics, and scientific computing. These applications process matrices with a wide range of sparsities, from completely dense to highly sparse. Ideally, a single accelerator should handle matrices of all sparsity levels well. However, prior matrix multiplication accelerators each target a limited range of sparsity levels.We present Trapezoid, a versatile accelerator that performs matrix multiplication across all sparsity levels effectively. Trapezoid builds on a 2D spatial array design, which excels at dense matrix multiplication, and extends it with new hardware mechanisms that let it handle sparse inputs. We present a novel inner-product-based dataflow with a multi-fiber intersection unit that handles mildly sparse matrices. Furthermore, novel Gustavson-based dataflows and a multi-level memory hierarchy enable high performance on highly sparse matrices. Trapezoid's hardware extensions are reused across dataflows to minimize area overheads.We evaluate Trapezoid on a broad range of dense and sparse matrix multiplication workloads. Trapezoid has gmean 19.7\texttimes{}, 4.3\texttimes{}, and 2.9\texttimes{} better performance/area than TPU, SIGMA, and Flexagon, prior state-of-the-art accelerators that target dense, mildly sparse, and highly sparse matrices, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {931–945},
numpages = {15},
keywords = {sparsity, matrix multiplication, accelerator, dataflow},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00073,
author = {Shivdikar, Kaustubh and Agostini, Nicolas Bohm and Jayaweera, Malith and Jonatan, Gilbert and Abell\'{a}n, Jos\'{e} L. and Joshi, Ajay and Kim, John and Kaeli, David},
title = {NeuraChip: Accelerating GNN Computations with a Hash-Based Decoupled Spatial Accelerator},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00073},
doi = {10.1109/ISCA59077.2024.00073},
abstract = {Graph Neural Networks (GNNs) are emerging as a formidable tool for processing non-euclidean data across various domains, ranging from social network analysis to bioinformatics. Despite their effectiveness, their adoption has not been pervasive because of scalability challenges associated with large-scale graph datasets, particularly when leveraging message passing. They exhibit irregular sparsity patterns, resulting in unbalanced compute resource utilization. Prior accelerators investigating Gustavson's technique adopted look-ahead buffers for prefetching data, aiming to prevent compute stalls. However, these solutions lead to inefficient use of the on-chip memory, leading to redundant data residing in cache.To tackle these challenges, we introduce NeuraChip, a novel GNN spatial accelerator based on Gustavson's algorithm. NeuraChip decouples the multiplication and addition computations in sparse matrix multiplication. This separation allows for independent exploitation of their unique data dependencies, facilitating efficient resource allocation. We introduce a rolling eviction strategy to mitigate data idling in on-chip memory as well as address the prevalent issue of memory bloat in sparse graph computations. Furthermore, the compute resource load balancing is achieved through a dynamic reseeding hash-based mapping, ensuring uniform utilization of computing resources agnostic of sparsity patterns. Finally, we present NeuraSim, an open-source, cycle-accurate, multi-threaded, modular simulator for comprehensive performance analysis.Overall, NeuraChip presents a significant improvement, yielding an average speedup of 22.1\texttimes{} over Intel's MKL, 17.1\texttimes{} over NVIDIA's cuSPARSE, 16.7\texttimes{} over AMD's hipSPARSE, and 1.5\texttimes{} over prior state-of-the-art SpGEMM accelerator and 1.3\texttimes{} over GNN accelerator. The source code for our open-sourced simulator and performance visualizer is publicly accessible on GitHub1.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {946–960},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00074,
author = {Zeng, Jianping and Zhang, Tong and Jung, Changhee},
title = {Compiler-Directed Whole-System Persistence},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00074},
doi = {10.1109/ISCA59077.2024.00074},
abstract = {Nonvolatile memory (NVM) technologies have gained increasing attention thanks to their density and durability benefits. However, leveraging NVM can cause a crash consistency issue. For example, if a younger store is evicted (persisted) to NVM from volatile caches before an older one and power failure occurs in between, it might be impossible to correctly resume the interrupted program in the wake of the failure. Traditionally, addressing this issue involves expensive persist barriers for enforcing the original store order, which not only incurs a high run-time overhead but also places a significant burden on users due to the difficulty of persistent programming.To this end, this paper presents cWSP, compiler/architecture codesign for lightweight yet performant whole-system persistence (WSP). In particular, cWSP compiler partitions not only user applications but also OS and runtime libraries into a series of recoverable regions (epochs), thus enabling persistence and crash consistency for the entire software stack. To achieve high-performance crash consistency, cWSP leverages advanced compiler optimizations for checkpointing a minimal set of registers and proposes simple hardware support for expediting data persistence on the cheap. Experimental results with 37 applications from SPEC CPU2006/2017, DOE Mini-apps, SPLASH3, WHISPER, and STAMP, show that cWSP incurs an average runtime overhead of 6\%, outperforming the state-of-the-art work with a significant margin.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {961–977},
numpages = {17},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00075,
author = {Shoushtary, Mojtaba Abaie and Arnau, Jose Maria and Murgadas, Jordi Tubella and Gonzalez, Antonio},
title = {Memento: An Adaptive, Compiler-Assisted Register File Cache for GPUs},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00075},
doi = {10.1109/ISCA59077.2024.00075},
abstract = {Modern GPUs require an enormous register file (RF) to store the context of thousands of active threads. It consumes considerable energy and contains multiple large banks to provide enough throughput. Thus, a RF caching mechanism can significantly improve the performance and energy consumption of the GPUs by avoiding reads from the large banks that consume significant energy and may cause port conflicts.This paper introduces an energy-efficient RF caching mechanism called Memento that repurposes an existing component in GPUs' RF to operate as a cache in addition to its original functionality. In this way, Memento minimizes the overhead of adding a RF cache to GPUs. Besides, Memento leverages an issue scheduling policy that utilizes the reuse distance of the values in the RF cache and is controlled by a dynamic algorithm. The goal is to adapt the issue policy to the runtime program characteristics to maximize the GPU's performance and the hit ratio of the RF cache. The reuse distance is approximated by the compiler using profiling and is used at run time by the proposed caching scheme. We show that Memento reduces the number of reads to the RF banks by 46.4\% and the dynamic energy of the RF by 28.3\%. Besides, it improves performance by 6.1\% while adding only 2KB of extra storage per core to the baseline RF of 256KB, which represents a negligible overhead of 0.78\%.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {978–990},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00076,
author = {Wang, Fuyu and Shen, Minghua and Ding, Yufei and Xiao, Nong},
title = {Soter: Analytical Tensor-Architecture Modeling and Automatic Tensor Program Tuning for Spatial Accelerators},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00076},
doi = {10.1109/ISCA59077.2024.00076},
abstract = {Spatial accelerator is a specialized hardware to provide noticeable performance speedup for tensor computations. It also brings a challenge to map tensor computations on spatial accelerators. Auto-tuning compiler is one of the most promising directions for tensor mapping. However, existing auto-tuning compilers suffer from either numerous invalid and inefficient programs or inaccurate evaluation of incomplete programs, leading to sub-optimal performance.In this paper, we propose Soter, a novel auto-tuning tensor compilation framework for spatial accelerators. The key is to perform exploration in a both valid and efficient program design space and perform optimization according to accurate evaluation of complete programs. First, we design an analytical model to generate a high-quality program design space, which excludes invalid and inefficient programs. Second, we design an automatic program tuner to efficiently explore the program space and avoid evaluating incomplete programs. Finally, we coordinate the model and the tuner to further improve the quality of program space. The program space is identified by the model and is updated during the exploration of tuner. On average, Soter achieves 2.1\texttimes{} to 3.5\texttimes{} speedup over the state-of-the-art tensor compilers. Moreover, Soter shows better scalability for larger-scale tensor computations and spatial architectures.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {991–1004},
numpages = {14},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00077,
author = {Zhao, Youpeng and Wu, Di and Wang, Jun},
title = {ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00077},
doi = {10.1109/ISCA59077.2024.00077},
abstract = {The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU.In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3\texttimes{} and 1.9\texttimes{}, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1005–1017},
numpages = {13},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00078,
author = {Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao},
title = {Pre-Gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00078},
doi = {10.1109/ISCA59077.2024.00078},
abstract = {Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1018–1031},
numpages = {14},
keywords = {mixture-of-expert, inference system, machine learning, large language model, memory offloading},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00079,
author = {Qin, Yubin and Wang, Yang and Zhao, Zhiren and Yang, Xiaolong and Zhou, Yang and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
title = {MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00079},
doi = {10.1109/ISCA59077.2024.00079},
abstract = {Large language models (LLMs) have been showing surprising performance in processing language tasks, bringing a new prevalence to deploy LLM from cloud to edge. However, being a scaling auto-regressive Transformer with a huge parameter amount and generating output one by one, LLM introduces overwhelming memory footprints and computation during its inference, especially from its linear layers. For example, generating 32 output tokens with LLaMA-7B LLM requires 14GB of weight data and performs over 400 billion operations (98\% from linear layers), which is far beyond the capability of consumer-level GPU and traditional accelerators. To solve these issues, we propose a memory-compute-efficient LLM accelerator, MECLA, with a parameter-efficient scaling sub-matrix partition method (SSMP). It decomposes large weight matrices into several tiny-scale source sub-matrices (SS) and derived sub-matrices (DS). Each DS can be obtained by scaling the corresponding SS with a scalar. For memory issues, SSMP avoids accessing the full weight matrix but only requires small SS and DS scaling scalars. For computation issues, the proposed MECLA processor fully exploits the intermediate data reuse of matrix multiplication via on-chip matrix regrouping, inner-product multiplication reassociation, and outer-product partial sum reuse. Experiments on 20 benchmarks show that MECLA reduces memory access and computation by 83.6\% and 72.2\%. It achieves an energy efficiency of 7088GOPS/W. Compared to V100 GPU and state-of-the-art Transformer accelerator SpAtten and FACT, MECLA saves 113.14\texttimes{}, 12.99\texttimes{}, and 1.62\texttimes{} higher energy efficiency.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1032–1047},
numpages = {16},
keywords = {large language model, transformer, accelerator, artificial intelligence},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00080,
author = {Lee, Jungi and Lee, Wonbeom and Sim, Jaewoong},
title = {Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00080},
doi = {10.1109/ISCA59077.2024.00080},
abstract = {Large language models (LLMs) demonstrate outstanding performance in various tasks in machine learning and have thus become one of the most important workloads in today's computing landscape. However, deploying LLM inference poses challenges due to the high compute and memory requirements stemming from the enormous model size and the difficulty of running it in the integer pipelines. In this paper, we present Tender, an algorithm-hardware co-design solution that enables efficient deployment of LLM inference at low precision. Based on our analysis of outlier values in LLMs, we propose a decomposed quantization technique in which the scale factors of decomposed matrices are powers of two apart. The proposed scheme allows us to avoid explicit requantization (i.e., dequantization/quantization) when accumulating the partial sums from the decomposed matrices, with a minimal extension to the commodity tensor compute hardware. Our evaluation shows that Tender achieves higher accuracy and inference performance compared to the state-of-the-art methods while also being significantly less intrusive to the existing accelerators.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1048–1062},
numpages = {15},
keywords = {large language model, LLM acceleration},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00081,
author = {Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J.},
title = {Heterogeneous Acceleration Pipeline for Recommendation System Training},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00081},
doi = {10.1109/ISCA59077.2024.00081},
abstract = {Recommendation models rely on deep learning networks and large embedding tables, resulting in computationally and memory-intensive processes. These models are typically trained using hybrid CPU-GPU or GPU-only configurations. The hybrid mode combines the GPU's neural network acceleration with the CPUs' memory storage and supply for embedding tables but may incur significant CPU-to-GPU transfer time. In contrast, the GPU-only mode utilizes High Bandwidth Memory (HBM) across multiple GPUs for storing embedding tables. However, this approach is expensive and presents scaling concerns.This paper introduces Hotline, a heterogeneous acceleration pipeline that addresses these concerns. Hotline develops a data-aware and model-aware scheduling pipeline by leveraging the insight that only a few embedding entries are frequently accessed (popular). This approach utilizes CPU main memory for non-popular embeddings and GPUs' HBM for popular embeddings. To achieve this, Hotline accelerator fragments a mini-batch into popular and non-popular micro-batches (μ-batches). It gathers the necessary working parameters for non-popular μ-batches from the CPU, while GPUs execute popular μ-batches. The hardware accelerator dynamically coordinates the execution of popular embeddings on GPUs and non-popular embeddings from the CPU's main memory. Real-world datasets and models confirm Hotline's effectiveness, reducing average end-to-end training time by 2.2\texttimes{} compared to Intel-optimized CPU-GPU DLRM baseline.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1063–1079},
numpages = {17},
keywords = {recommender systems, multi-node distributed training, accelerators},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00082,
author = {Zhang, Hengrui and Ning, August and Prabhakar, Rohan Baskar and Wentzlaff, David},
title = {LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00082},
doi = {10.1109/ISCA59077.2024.00082},
abstract = {The past year has witnessed the increasing popularity of Large Language Models (LLMs). Their unprecedented scale and associated high hardware cost have impeded their broader adoption, calling for efficient hardware designs. With the large hardware needed to simply run LLM inference, evaluating different hardware designs becomes a new bottleneck.This work introduces LLMCompass1, a hardware evaluation framework for LLM inference workloads. LLMCompass is fast, accurate, versatile, and able to describe and evaluate different hardware designs. LLMCompass includes a mapper to automatically find performance-optimal mapping and scheduling. It also incorporates an area-based cost model to help architects reason about their design choices. Compared to real-world hardware, LLMCompass' estimated latency achieves an average 10.9\% error rate across various operators with various input sizes and an average 4.1\% error rate for LLM inference. With LLMCompass, simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done within 16 minutes on commodity hardware, including 26,400 rounds of the mapper's parameter search.With the aid of LLMCompass, this work draws architectural implications and explores new cost-effective hardware designs. By reducing the compute capability or replacing High Bandwidth Memory (HBM) with traditional DRAM, these new designs can achieve as much as 3.41x improvement in performance/cost compared to an NVIDIA A100, making them promising choices for democratizing LLMs.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1080–1096},
numpages = {17},
keywords = {large language model, performance model, area model, cost model, accelerator},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00083,
author = {Nam, Hwayong and Baek, Seungmin and Wi, Minbok and Kim, Michael Jaemin and Park, Jaehyun and Song, Chihun and Kim, Nam Sung and Ahn, Jung Ho},
title = {DRAMScope: Uncovering DRAM Microarchitecture and Characteristics by Issuing Memory Commands},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00083},
doi = {10.1109/ISCA59077.2024.00083},
abstract = {The demand for precise information on DRAM microarchitectures and error characteristics has surged, driven by the need to explore processing in memory, enhance reliability, and mitigate security vulnerability. Nonetheless, DRAM manufacturers have disclosed only a limited amount of information, making it difficult to find specific information on their DRAM microarchitectures. This paper addresses this gap by presenting more rigorous findings on the microarchitectures of commodity DRAM chips and their impacts on the characteristics of activate-induced bitflips (AIBs), such as RowHammer and RowPress. The previous studies have also attempted to understand the DRAM microarchitectures and associated behaviors, but we have found some of their results to be misled by inaccurate address mapping and internal data swizzling, or lack of a deeper understanding of the modern DRAM cell structure. For accurate and efficient reverse-engineering, we use three tools: AIBs, retention time test, and RowCopy, which can be cross-validated. With these three tools, we first take a macroscopic view of modern DRAM chips to uncover the size, structure, and operation of their subarrays, memory array tiles (MATs), and rows. Then, we analyze AIB characteristics based on the microscopic view of the DRAM microarchitecture, such as 6F2 cell layout, through which we rectify misunderstandings regarding AIBs and discover a new data pattern that accelerates AIBs. Lastly, based on our findings at both macroscopic and microscopic levels, we identify previously unknown AIB vulnerabilities and propose a simple yet effective protection solution.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1097–1111},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00084,
author = {Kamath, Aditya K and Peter, Simon},
title = {(MC)2: Lazy MemCopy at the Memory Controller},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00084},
doi = {10.1109/ISCA59077.2024.00084},
abstract = {(MC)2 is a lazy memory copy mechanism which can be used within memcpy-like functions to significantly reduce the CPU overhead for copies that are sparsely accessed. It can also hide copy latencies by enhancing the CPU's ability to execute them asynchronously. (MC)2's lazy memcpy avoids copying data at the time of invocation. Instead, (MC)2 tracks prospective copies. If copied data is later accessed by a CPU or the cache, (MC)2 uses the tracking information to lazily execute a copy, when necessary. Placing (MC)2 at the memory controller puts it at the perfect vantage point to eliminate the largest source of memcpy overhead—CPU stalls due to cache misses in the critical path—while imposing minimal overhead itself.(MC)2 consists of three main components: memory controller extensions that implement a lazy memcpy operation, a new instruction exposing the lazy memcpy, and a flexible software wrapper with semantics identical to memcpy. We implement and evaluate (MC)2 in the gem5 simulator using a variety of microbenchmarks and workloads, including Google's Protobuf, where (MC)2 provides a 43\% speedup and Linux huge page copy-on-write faults, where (MC)2 provides 250\texttimes{} lower latency.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1112–1128},
numpages = {17},
keywords = {lazy copy, memcpy, data transfer, memory controller, memory, DRAM},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00085,
author = {Panwar, Gagandeep and Laghari, Muhammad and Choukse, Esha and Jian, Xun},
title = {DyLeCT: Achieving Huge-Page-Like Translation Performance for Hardware-Compressed Memory},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00085},
doi = {10.1109/ISCA59077.2024.00085},
abstract = {To expand effective memory capacity, hardware memory compression transparently compresses and packs memory values more densely together in DRAM. This requires introducing a new layer of hardware-managed address translation in the memory controller (MC). However, for large and irregular workloads that already suffer from frequent virtual address translation misses in the TLB, adding an additional layer of address translation can double the translation misses (e.g., by adding a new miss in the MC per TLB miss). While TLB misses can be drastically reduced by using huge pages, no prior work has explored huge-page-like translation reach for hardware memory compression. While compressing and moving an entire huge page worth of data at a time can lead to huge-page-like address translation, moving a huge page worth of data together can consume an exorbitant amount of memory bandwidth.This paper explores how to achieve huge-page-like translation performance in this new address translation layer, while keeping compression at the page (instead of huge page) granularity. We propose dynamically shortening the translation entries of hot pages to only a few bits per entry by migrating hot pages to the limited number of DRAM locations whose addresses can be encoded using a few bits; colder pages still use the bigger full-length translations so that colder pages can be placed anywhere in memory to fully utilize all the space in memory. Each short translation is tiny (e.g., 2 bits); as such, a 128KB translation cache filled mostly with short translations can achieve similar (e.g., 2GB) total translation reach as a TLB filled entirely with huge page entries. Evaluations show our idea - Dynamic Length Compressed-Memory Translations (DyLeCT) - improves average performance by 10.25\% over the prior art.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1129–1143},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00086,
author = {Ryu, Yesin and Kim, Yoojin and Jung, Giyong and Ahn, Jung Ho and Kim, Jungrae},
title = {Native DRAM Cache: Re-Architecting DRAM as a Large-Scale Cache for Data Centers},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00086},
doi = {10.1109/ISCA59077.2024.00086},
abstract = {Contemporary data center CPUs are experiencing an unprecedented surge in core count. This trend necessitates scrutinized Last-Level Cache (LLC) strategies to accommodate increasing capacity demands. While DRAM offers significant capacity, using it as a cache poses challenges related to latency and energy. This paper introduces Native DRAM Cache (NDC), a novel DRAM architecture specifically designed to operate as a cache. NDC features innovative approaches, such as conducting tag matching and way selection within a DRAM subarray and repurposing existing precharge transistors for tag matching. These innovations facilitate Caching-In-Memory (CIM) and enable NDC to serve as a high-capacity LLC with high set-associativity, low-latency, high-throughput, and low-energy. Our evaluation demonstrates that NDC significantly outperforms state-of-the-art DRAM cache solutions, enhancing performance by 2.8\%/52.5\%/44.2\% (up to 8.4\%/140.6\%/85.5\%) in SPEC/NPB/GAP benchmark suites, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1144–1156},
numpages = {13},
keywords = {caching-in-memory (CIM), DRAM cache, high-capacity LLC},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00087,
author = {Jaleel, Aamer and Saileshwar, Gururaj and Keckler, Stephen W. and Qureshi, Moinuddin},
title = {PrIDE: Achieving Secure Rowhammer Mitigation with Low-Cost In-DRAM Trackers},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00087},
doi = {10.1109/ISCA59077.2024.00087},
abstract = {Rowhammer-induced bit-flips are a threat to DRAM security. To mitigate Rowhammer, DDR4 devices employ TRR, an in-DRAM tracker, to identify aggressor rows. In-DRAM trackers tend to be severely resource-constrained (1-30 entries), which means they cannot reliably track all the aggressor rows and are bound to fail for some access patterns. Unfortunately, for existing in-DRAM trackers, it is difficult to a priori determine how often they will fail when subjected to the worst-case pattern. Unsurprisingly, all the current low-cost in-DRAM trackers have been broken with specific access patterns within a few minutes. While provably secure alternatives for in-DRAM tracking exist, they require thousands of tracking entries, making them unappealing for commercial adoption. The goal of our paper is to develop a low-cost in-DRAM tracker that is secure (guarantees a time-to-failure in the range of years) against all access patterns.We contend that the root cause of the vulnerability of current low-cost in-DRAM trackers stems from the use of activation-counters to direct policy decisions (e.g. which rows to insert, which to evict, and which to mitigate). Therefore, an attacker can perform frequent accesses to dummy rows to evade the mitigation of an aggressor row. The key insight of our paper is that to ensure security, the policy decisions of an in-DRAM tracker must not depend on the access pattern. To that end, we propose a secure and low-cost in-DRAM tracker called PrIDE, which consists of a FIFO buffer with probabilistic insertion. As the policy decisions of PrIDE do not depend on the access pattern, we develop a framework to calculate the time-to-failure. Our analysis with DDR5 shows that PrIDE (with 4 entries, 10-byte storage) can tolerate Rowhammer thresholds of 1.9K while guaranteeing per-bank time-to-failure of more than 10,000 years for all access patterns. We also co-design PrIDE with RFM to tolerate thresholds as low as 400 with only 1.6\% slowdown. To the best of our knowledge, PrIDE is the first low-cost in-DRAM tracker to achieve provably secure Rowhammer mitigation.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1157–1172},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00088,
author = {Duong, Quang and Jain, Akanksha and Lin, Calvin},
title = {A New Formulation of Neural Data Prefetching},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00088},
doi = {10.1109/ISCA59077.2024.00088},
abstract = {Temporal data prefetchers have the potential to produce significant performance gains by prefetching irregular data streams. Recent work has introduced a neural model for temporal prefetching that outperforms practical table-based temporal prefetchers, but the large storage and latency costs, along with the inability to generalize to memory addresses outside of the training dataset, prevent such a neural network from seeing any practical use in hardware.In this paper, we reformulate the temporal prefetching prediction problem so that neural solutions to it are more amenable for hardware deployment. Our key insight is that while temporal prefetchers typically assume that each address can be followed by any possible successor, there are empirically only a few successors for each address. Utilizing this insight, we introduce a new abstraction of memory addresses, and we show how this abstraction enables the design of a much more efficient neural prefetcher.Our new prefetcher, Twilight, improves upon the previous state-of-the-art neural prefetcher, Voyager, in multiple dimensions: It reduces latency by 988\texttimes{}, shrinks storage by 10.8\texttimes{}, achieves 4\% more speedup on a mix of irregular SPEC 2006, SPEC 2017, and GAP benchmarks, and is capable of predicting new temporal correlations not present in the training data. Twilight outperforms idealized versions of the non-neural temporal prefetchers STMS by 12.2\% and Domino by 8.5\%. While Twilight is still not practical, T-LITE, a slimmed-down version of Twilight that can prefetch across different program runs, further reduces latency and storage (1421\texttimes{} faster and 142\texttimes{} smaller than Voyager), matches Voyager's performance and outperforms the practical non-neural Triage prefetcher by 5.9\%.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1173–1187},
numpages = {15},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00089,
author = {Oh, Surim and Xu, Mingsheng and Khan, Tanvir Ahmed and Kasikci, Baris and Litz, Heiner},
title = {UDP: Utility-Driven Fetch Directed Instruction Prefetching},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00089},
doi = {10.1109/ISCA59077.2024.00089},
abstract = {Datacenter applications exhibit large instruction footprints causing significant instruction cache misses and, as a result, frontend stalls. To address this issue, instruction prefetching mechanisms have been proposed, including state-of-the-art techniques such as fetch-directed instruction prefetching. However, our study shows that existing implementations still fall far short of an ideal system with a perfect instruction cache. In particular, up to 588.47\% of potential IPC speedup of existing processors hides due to frontend stalls, and these frontend stalls are due to inaccurate and untimely instruction prefetches. We quantify the impact of these individual effects, observing that applications exhibit different characteristics that call for adaptive application-specific optimizations. Based on these insights, we propose two novel mechanisms, UDP and UFTQ, to improve the accuracy of FDIP without negatively affecting timeliness while leveraging prefetches on the wrong path. We evaluate our technique on 10 data center workloads showing a maximal IPC improvement of 16.1\% and an average IPC improvement of 3.6\%. Our techniques only introduce moderate hardware modifications and a storage cost of 8KB.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1188–1201},
numpages = {14},
keywords = {instruction prefetching, frontend stalls, data center},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00090,
author = {Ainsworth, Sam and Mukhanov, Lev},
title = {Triangel: A High-Performance, Accurate, Timely On-Chip Temporal Prefetcher},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00090},
doi = {10.1109/ISCA59077.2024.00090},
abstract = {Temporal prefetching, where correlated pairs of addresses are logged and replayed on repeat accesses, has recently become viable in commercial designs. Arm's latest processors include Correlating Miss Chaining prefetchers, which store such patterns in a partition of the on-chip cache. However, the state-of-the-art on-chip temporal prefetcher in the literature, Triage, features some design inconsistencies and inaccuracies that pose challenges for practical implementation. We first examine and design fixes for these inconsistencies to produce an implementable baseline. We then introduce Triangel, a prefetcher that extends Triage with novel sampling-based methodologies to allow it to be aggressive and timely when the prefetcher is able to handle observed long-term patterns, and to avoid inaccurate prefetches when less able to do so. Triangel gives a 26.4\% speedup compared to a baseline system with a conventional stride prefetcher alone, compared with 9.3\% for Triage at degree 1 and 14.2\% at degree 4. At the same time Triangel only increases memory traffic by 10\% relative to baseline, versus 28.5\% for Triage.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1202–1216},
numpages = {15},
keywords = {memory systems, prefetching, temporal prefetching},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00091,
author = {Deshmukh, Aniket and Cai, Lingzhe(Chester) and Patt, Yale N.},
title = {Alternate Path Fetch},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00091},
doi = {10.1109/ISCA59077.2024.00091},
abstract = {Modern out-of-order cores rely on a large instruction supply from the processor frontend to achieve high performance. This requires building wider pipelines with more accurate branch predictors. However, scaling the pipeline width is becoming more challenging due to limitations on the number of instructions that can be renamed and branches that can be predicted in a single cycle. Moreover, mispredictions reduce the useful fetch bandwidth that can be extracted from a wider frontend.Our work, Alternate Path Fetch (APF), effectively uses a wide frontend by dividing the pipeline into two parallel sections. One processes regular instructions, and the other uses a separate pipeline to Branch Predict, Fetch, Decode, and partially Rename instructions on the alternate path of hard-to-predict (H2P) branches. The pipelines operate simultaneously using a Parallel-Fetch scheme we developed. This allows APF to more efficiently utilize the bandwidth of a wider frontend without the overhead associated with building a monolithic, wider pipeline.APF improves performance by reducing the pipeline re-fill delay on branch mispredictions. Unlike other solutions that fully rename and execute instructions on both sides of a branch, we show that stopping after partial Renaming on the alternate path provides better performance through improved coverage and avoids the complexity associated with further processing. APF provides a 5\% geomean speedup over an aggressive 8-wide out-of-order core.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1217–1229},
numpages = {13},
keywords = {branch prediction, predication},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00092,
author = {Singh, Sawan and Perais, Arthur and Jimborean, Alexandra and Ros, Alberto},
title = {Alternate Path μ-op Cache Prefetching},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00092},
doi = {10.1109/ISCA59077.2024.00092},
abstract = {Datacenter applications are well-known for their large code footprints. This has caused frontend design to evolve by implementing decoupled fetching and large prediction structures - branch predictors, Branch Target Buffers (BTBs) - to mitigate the stagnating size of the instruction cache by prefetching instructions well in advance. In addition, many designs feature a micro operation (μ-op) cache, which primarily provides power savings by bypassing the instruction cache and decoders once warmed up. However, this μ-op cache often has lower reach than the instruction cache, and it is not filled up speculatively using the decoupled fetcher. As a result, the μ-op cache is often over-subscribed by datacenter applications, up to the point of becoming a burden.This paper first shows that because of this pressure, blindly prefetching into the μ-op cache using state-of-the-art standalone prefetchers would not provide significant gains. As a consequence, this paper proposes to prefetch only critical μ-ops into the μ-op cache, by focusing on execution points where the μ-op cache provides the most gains: Pipeline refills. Concretely, we use hard-to-predict conditional branches as indicators that a pipeline refill is likely to happen in the near future, and prefetch into the μ-op cache the μ-ops that belong to the path opposed to the predicted path, which we call alternate path. Identifying hard-to-predict branches requires no additional state if the branch predictor confidence is used to classify branches. Including extra alternate branch predictors with limited budget (8.95KB to 12.95KB), our proposal provides average speedups of 1.9\% to 2\% and as high as 12\% on a subset of CVP-1 traces.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1230–1245},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00093,
author = {Kim, Yoonsung and Oh, Changhun and Hwang, Jinwoo and Kim, Wonung and Oh, Seongryong and Lee, Yubin and Sharma, Hardik and Yazdanbakhsh, Amir and Park, Jongse},
title = {DACAPO: Accelerating Continuous Learning in Autonomous Systems for Video Analytics},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00093},
doi = {10.1109/ISCA59077.2024.00093},
abstract = {Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight "student" model at deployment (inference), leverages a larger "teacher" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for autonomous systems due to privacy, network availability, and latency concerns. We propose a hardware-algorithm co-designed solution for continuous learning, DACAPO, that enables autonomous systems to perform concurrent executions of inference, labeling, and retraining in a performant and energy-efficient manner. DACAPO comprises (1) a spatially-partitionable and precision-flexible accelerator enabling parallel execution of kernels on sub-accelerators at their respective precisions, and (2) a spatiotemporal resource allocation algorithm that strategically navigates the resource-accuracy tradeoff space, facilitating optimal decisions for resource allocation to achieve maximal accuracy. Our evaluation shows that DACAPO achieves 6.5\% and 5.5\% higher accuracy than a state-of-the-art GPU-based continuous learning systems, Ekya and EOMU, respectively, while consuming 254\texttimes{} less power.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1246–1261},
numpages = {16},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00094,
author = {Feng, Yu and Ma, Tianrui and Zhu, Yuhao and Zhang, Xuan},
title = {BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse Sampling},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00094},
doi = {10.1109/ISCA59077.2024.00094},
abstract = {Eye tracking is becoming an increasingly important task domain in emerging computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye tracking system suffers from long end-to-end tracking latency and can easily eat up half of the power budget of a mobile VR device. Most existing optimization efforts exclusively focus on the computation pipeline by optimizing the algorithm and/or designing dedicated accelerators while largely ignoring the front-end of any eye tracking pipeline: the image sensor. This paper makes a case for co-designing the imaging system with the computing system.In particular, we propose the notion of "in-sensor sparse sampling", whereby the pixels are drastically downsampled (by 20\texttimes{}) within the sensor. Such in-sensor sampling enhances the overall tracking efficiency by significantly reducing 1) the power consumption of the sensor readout chain and sensor-host communication interfaces, two major power contributors, and 2) the work done on the host, which receives and operates on far fewer pixels. With careful reuse of existing pixel circuitry, our proposed BLISSCAM requires little hardware augmentation to support the in-sensor operations. Our synthesis results show up to 8.2\texttimes{} energy reduction and 1.4\texttimes{} latency reduction over existing eye tracking pipelines.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1262–1277},
numpages = {16},
keywords = {in-sensor computing, eye tracking, sparse sensing, AR/VR},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00095,
author = {Han, Meng and Wang, Liang and Xiao, Limin and Zhang, Hao and Cai, Tianhao and Xu, Jiale and Wu, Yibo and Zhang, Chenhao and Xu, Xiangrong},
title = {BitNN: A Bit-Serial Accelerator for K-Nearest Neighbor Search in Point Clouds},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00095},
doi = {10.1109/ISCA59077.2024.00095},
abstract = {Point cloud-based machine perception applications have achieved great success in various scenarios. In this work, we focus on point cloud k-Nearest Neighbor (kNN) search, an important kernel for point clouds. Existing kNN acceleration techniques have overlooked the operation-level optimization in the Euclidean distance computation operations, which suffer from low efficiency due to a number of unnecessary computations and various data precision requirements.We reconsider point cloud kNN search from a new bit-serial computation perspective and propose BitNN, a bit-serial architecture for point cloud kNN search. BitNN supports adaptive precision processing and unnecessary computing reduction, significantly improving the performance and power efficiency of kNN search. To achieve that, we first propose a bit-serial computation method for kNN search, which derives a recursive expression to compute the Euclidean distance bit by bit. Then, the dimension-wise point cloud encoding method and point-wise data layout method are proposed to enable adaptive precision processing based on bit-serial computation. Furthermore, we present an early termination mechanism for bit-serial kNN search. By estimating the lower bound of distance based on a few bits, a number of unnecessary computations can be reduced. Finally, we design an efficient bit-serial accelerator for kNN search. The accelerator exploits the massive parallelism to improve computing efficiency.We evaluate BitNN with several widely used point cloud datasets. BitNN achieves up to 6.6\texttimes{} speedup and 3.6\texttimes{} power efficiency compared to a comparable sized architecture. Moreover, BitNN can be easily integrated into existing bit-parallel kNN accelerators. We enhance the state-of-the-art kNN accelerator, ParallelNN, with bit-serial computation techniques, achieving up to 4.4\texttimes{} speedup and 2.9\texttimes{} power efficiency.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1278–1292},
numpages = {15},
keywords = {k-nearest neighbor search, bit-serial, accelerators, point clouds},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00096,
author = {Feng, Yu and Liu, Zihan and Leng, Jingwen and Guo, Minyi and Zhu, Yuhao},
title = {Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural Rendering by Radiance Warping and Memory Optimizations},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00096},
doi = {10.1109/ISCA59077.2024.00096},
abstract = {Neural Radiance Field (NeRF) is widely seen as an alternative to traditional physically-based rendering. However, NeRF has not yet seen its adoption in resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR), because it is simply extremely slow. On a mobile Volta GPU, even the state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that the main performance bottlenecks are both algorithmic and architectural. We introduce, CICERO, to tame both forms of inefficiencies. We first introduce two algorithms, one fundamentally reduces the amount of work any NeRF model has to execute, and the other eliminates irregular DRAM accesses. We then describe an on-chip data layout strategy that eliminates SRAM bank conflicts. A pure software implementation of CICERO offers an 8.0\texttimes{} speed-up and 7.9\texttimes{} energy saving over a mobile Volta GPU. When compared to a baseline with a dedicated DNN accelerator, our speed-up and energy reduction increase to 28.2\texttimes{} and 37.8\texttimes{}, respectively — all with minimal quality loss (less than 1.0 dB peak signal-to-noise ratio reduction).},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1293–1308},
numpages = {16},
keywords = {mobile architecture, NeRF acceleration, VR},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@inproceedings{10.1109/ISCA59077.2024.00097,
author = {Bhuyan, Sandeepa and Ying, Ziyu and Kandemir, Mahmut T. and Gowda, Mahanth and Das, Chita R.},
title = {GameStreamSR: Enabling Neural-Augmented Game Streaming on Commodity Mobile Platforms},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00097},
doi = {10.1109/ISCA59077.2024.00097},
abstract = {Cloud gaming (also referred to as Game Streaming) is a rapidly emerging application that is changing the way people enjoy video games. However, if the user demands a high-resolution (e.g., 2K or 4K) stream, the game frames require high bandwidth and the stream often suffers from a significant number of frame drops due to network congestion degrading the Quality of Experience (QoE). Recently, the DNN-based Super Resolution (SR) technique has gained prominence as a practical alternative for streaming low-resolution frames and upscaling them at the client for enhanced video quality. However, performing such DNN-based tasks on resource-constrained and battery-operated mobile platforms is very expensive and also fails to meet the real-time requirement (60 frames per second (FPS)). Unlike traditional video streaming, where the frames can be downloaded and buffered, and then upscaled by their playback turn, Game Streaming is real-time and interactive, where the frames are generated on the fly and cannot tolerate high latency/lags for frame upscaling. Thus, state-of-the-art (SOTA) DNN-based SR cannot satisfy the mobile Game Streaming requirements.Towards this, we propose GameStreamSR, a framework for enabling real-time Super Resolution for Game Streaming applications on mobile platforms. We take visual perception nature into consideration and propose to only apply DNN-based SR to the regions with high visual importance and upscale the remaining regions using traditional solutions such as bilinear interpolation. Especially, we leverage the depth data from the game rendering pipeline to intelligently localize the important regions, called regions of importance (RoI), in the rendered game frames. Our evaluation of ten popular games on commodity mobile platforms shows that our proposal can enable realtime (60 FPS) neurally-augmented SR. Our design achieves a 13\texttimes{} frame rate speedup (and ≈4\texttimes{} Motion-to-Photon latency improvement) for the reference frames and a 1.6\texttimes{} frame rate speedup for the non-reference frames, which translates to, on average 2\texttimes{} FPS performance improvement and 26–33\% energy savings over the SOTA DNN-based SR execution, while achieving about 2dB PSNR gain and better perceptual quality than the current SOTA.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {1309–1322},
numpages = {14},
keywords = {games, mobile computing, interactive systems, real-time systems, client/server systems, emerging technologies},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

