@inproceedings{10.1145/3470496.3527413,
author = {Bhattacharyya, Abhishek and Somashekhar, Abhijith and Miguel, Joshua San},
title = {NvMR: non-volatile memory renaming for intermittent computing},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527413},
doi = {10.1145/3470496.3527413},
abstract = {Intermittent systems on energy-harvesting devices have to frequently back up data because of an unreliable energy supply to make forward progress. These devices come with non-volatile memories like Flash/FRAM on board that are used to back up the system state. However, quite paradoxically, writing to a non-volatile memory consumes a lot of energy that makes backups expensive. Idem-potency violations inherent to intermittent programs are major contributors to the problem, as they render system state inconsistent and force backups to occur even when plenty of energy is available. In this work, we first characterize the complex persist dependencies that are unique to intermittent computing. Based on these insights, we propose NvMR, an intermittent architecture that eliminates idempotency violations in the program by renaming non-volatile memory addresses. This can reduce the number of backups to their theoretical minimum and decouple the decision of when to perform backups from the memory access constraints imposed by the program. Our evaluations show that compared to a state-of-the-art intermittent architecture, NvMR can save about 20\% energy on average when running common embedded applications.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1–13},
numpages = {13},
keywords = {intermittent computing, idempotency, energy-harvesting},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527385,
author = {Asgharzadeh, Ashkan and Cebrian, Juan M. and Perais, Arthur and Kaxiras, Stefanos and Ros, Alberto},
title = {Free atomics: hardware atomic operations without fences},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527385},
doi = {10.1145/3470496.3527385},
abstract = {Atomic Read-Modify-Write (RMW) instructions are primitive synchronization operations implemented in hardware that provide the building blocks for higher-abstraction synchronization mechanisms to programmers. According to publicly available documentation, current x86 implementations serialize atomic RMW operations, i.e., the store buffer is drained before issuing atomic RMWs and subsequent memory operations are stalled until the atomic RMW commits. This serialization, carried out by memory fences, incurs a performance cost which is expected to increase with deeper pipelines.This work proposes Free atomics, a lightweight, speculative, deadlock-free implementation of atomic operations that removes the need for memory fences, thus improving performance, while preserving atomicity and consistency. Free atomics is, to the best of our knowledge, the first proposal to enable store-to-load forwarding for atomic RMWs. Free atomics only requires simple modifications and incurs a small area overhead (15 bytes). Our evaluation using gem5-20 shows that, for a 32-core configuration, Free atomics improves performance by 12.5\%, on average, for a large range of parallel workloads and 25.2\%, on average, for atomic-intensive parallel workloads over a fenced atomic RMW implementation.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {total-store-order (TSO), store-to-load forwarding, multi-core architectures, microarchitecture, atomic read-modify-write instructions},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527420,
author = {Lee, Jaewon and Kim, Yonghae and Cao, Jiashen and Kim, Euna and Lee, Jaekyu and Kim, Hyesoon},
title = {Securing GPU via region-based bounds checking},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527420},
doi = {10.1145/3470496.3527420},
abstract = {Graphics processing units (GPUs) have become essential general-purpose computing platforms to accelerate a wide range of workloads, such as deep learning, scientific, and high-performance computing (HPC) applications. However, recent memory corruption attacks, such as buffer overflow, exposed security vulnerabilities in GPUs. We demonstrate that out-of-bounds writes are reproducible on an Nvidia GPU, which can enable other security attacks.We propose GPUShield, a hardware-software cooperative region-based bounds-checking mechanism, to improve GPU memory safety for global, local, and heap memory buffers. To achieve effective protection, we update the GPU driver to assign a random but unique ID to each buffer and local variable and store individual bounds information in the bounds table allocated in the global memory. The proposed hardware performs efficient bounds checking by indexing the bounds table with unique IDs. We further reduce the bounds-checking overhead by utilizing compile-time bounds analysis, workgroup/warp-level bounds checking, and GPU-specific address mode. Our performance evaluations show that GPUShield incurs little performance degradation across 88 CUDA benchmarks on the Nvidia GPU architecture and 17 OpenCL benchmarks on the Intel GPU architecture with a marginal hardware overhead.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {27–41},
numpages = {15},
keywords = {memory safety, GPU},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527379,
author = {Schwedock, Brian C. and Yoovidhya, Piratach and Seibert, Jennifer and Beckmann, Nathan},
title = {t\"{a}k\={o}: a polymorphic cache hierarchy for general-purpose optimization of data movement},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527379},
doi = {10.1145/3470496.3527379},
abstract = {Current systems hide data movement from software behind the load-store interface. Software's inability to observe and respond to data movement is the root cause of many inefficiencies, including the growing fraction of execution time and energy devoted to data movement itself. Recent specialized memory-hierarchy designs prove that large data-movement savings are possible. However, these designs require custom hardware, raising a large barrier to their practical adoption.This paper argues that the hardware-software interface is the problem, and custom hardware is often unnecessary with an expanded interface. The t\"{a}k\={o} architecture lets software observe data movement and interpose when desired. Specifically, caches in t\"{a}k\={o} can trigger software callbacks in response to misses, evictions, and writebacks. Callbacks run on reconfigurable dataflow engines placed near caches. Five case studies show that this interface covers a wide range of data-movement features and optimizations. Microarchitecturally, t\"{a}k\={o} is similar to recent near-data computing designs, adding ≈5\% area to a baseline multicore. t\"{a}k\={o} improves performance by 1.4X-4.2X, similar to prior custom hardware designs, and comes within 1.8\% of an idealized implementation.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {42–58},
numpages = {17},
keywords = {data-centric computing, data movement, cache hierarchy},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527434,
author = {Stein, Samuel and Wiebe, Nathan and Ding, Yufei and Bo, Peng and Kowalski, Karol and Baker, Nathan and Ang, James and Li, Ang},
title = {EQC: ensembled quantum computing for variational quantum algorithms},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527434},
doi = {10.1145/3470496.3527434},
abstract = {Variational quantum algorithm (VQA), which is comprised of a classical optimizer and a parameterized quantum circuit, emerges as one of the most promising approaches for harvesting the power of quantum computers in the noisy intermediate scale quantum (NISQ) era. However, the deployment of VQAs on contemporary NISQ devices often faces considerable system and time-dependant noise and prohibitively slow training speeds. On the other hand, the expensive supporting resources and infrastructure make quantum computers extremely keen on high utilization.In this paper, we propose a virtualized way of building up a quantum backend for variational quantum algorithms: rather than relying on a single physical device which tends to introduce ever-changing device-specific noise with less reliable performance as time-since-calibration grows, we propose to constitute a quantum ensemble, which dynamically distributes quantum tasks asynchronously across a set of physical devices, and adjusts the ensemble configuration with respect to machine status. In addition to reduced machine-dependant noise, the ensemble can provide significant speedups for VQA training. With this idea, we build a novel VQA training framework called EQC - a distributed gradient-based processor-performance-aware optimization system - that comprises: (i) a system architecture for asynchronous parallel VQA cooperative training; (ii) an analytical model for assessing the quality of a circuit output concerning its architecture, transpilation, and runtime conditions; (iii) a weighting mechanism to adjust the quantum ensemble's computational contribution according to the systems' current performance. Evaluations comprising 500K times' circuit evaluations across 10 IBMQ NISQ devices using a VQE and a QAOA applications demonstrate that EQC can attain error rates very close to the most performant device of the ensemble, while boosting the training speed by 10.5X on average (up to 86X and at least 5.2x). EQC is available at https://github.com/pnnl/eqc.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {59–71},
numpages = {13},
keywords = {variational quantum algorithms, quantum computing, distributed computing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527412,
author = {Mosier, Nicholas and Lachnitt, Hanna and Nemati, Hamed and Trippel, Caroline},
title = {Axiomatic hardware-software contracts for security},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527412},
doi = {10.1145/3470496.3527412},
abstract = {We propose leakage containment models (LCMs)---novel axiomatic security contracts which support formally reasoning about the security guarantees of programs when they run on particular microarchitectures. Our core contribution is an axiomatic vocabulary for formalizing LCMs, derived from the established axiomatic vocabulary for formalizing processor memory consistency models. Using this vocabulary, we formalize microarchitectural leakage---focusing on leakage through hardware memory systems---so that it can be automatically detected in programs and provide a taxonomy for classifying said leakage by severity. To illustrate the efficacy of LCMs, we first demonstrate that our leakage definition faithfully captures a sampling of (transient and non-transient) microarchitectural attacks from the literature. Second, we develop a static analysis tool based on LCMs which automatically identifies Spectre vulnerabilities in programs and scales to analyze real-world crypto-libraries.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {72–86},
numpages = {15},
keywords = {spectre, side-channel attacks, memory consistency models, hardware-software contracts, hardware security},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527392,
author = {Zhou, Xing and Xu, Zhilei and Wang, Cong and Gao, Mingyu},
title = {PPMLAC: high performance chipset architecture for secure multi-party computation},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527392},
doi = {10.1145/3470496.3527392},
abstract = {Privacy issue is a main concern restricting data sharing and cross-organization collaborations. While Privacy-Preserving Machine Learning techniques such as Multi-Party Computations (MPC), Homomorphic Encryption, and Federated Learning are proposed to solve this problem, no solution exists with both strong security and high performance to run large-scale, complex machine learning models. This paper presents PPMLAC, a novel chipset architecture to accelerate MPC, which combines MPC's strong security and hardware's high performance, eliminates the communication bottleneck from MPC, and achieves several orders of magnitudes speed up over software-based MPC. It is carefully designed to only rely on a minimum set of simple hardware components in the trusted domain, thus is robust against side-channel attacks and malicious adversaries. Our FPGA prototype can run mainstream large-scale ML models like ResNet in near real-time under a practical network environment with non-negligible latency, which is impossible for existing MPC solutions.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {87–101},
numpages = {15},
keywords = {side-channel protection, security, secret sharing, privacy-preserving machine learning, privacy, hardware accelerator, MPC},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527433,
author = {Lin, Jilan and Liang, Ling and Qu, Zheng and Ahmad, Ishtiyaque and Liu, Liu and Tu, Fengbin and Gupta, Trinabh and Ding, Yufei and Xie, Yuan},
title = {INSPIRE: in-storage private information retrieval via protocol and architecture co-design},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527433},
doi = {10.1145/3470496.3527433},
abstract = {Private Information Retrieval (PIR) plays a vital role in secure, database-centric applications. However, existing PIR protocols explore a massive working space containing hundreds of GiBs of query and database data. As a consequence, PIR performance is severely bounded by storage communication, making it far from practical for real-world deployment.In this work, we describe INSPIRE, an accelerator for IN-Storage Private Information REtrieval. INSPIRE follows a protocol and architecture co-design approach. We first design the INSPIRE protocol with a multi-stage filtering mechanism, which achieves a constant PIR query size. For a 1-billion-entry database of size 288GiB, INSPIRE's protocol reduces the query size from 27GiB to 3.6MiB. Further, we propose the INSPIRE hardware, a heterogeneous in-storage architecture, which integrates our protocol across the SSD hierarchy. Together with the INSPIRE protocol, the INSPIRE hardware reduces the query time from 28.4min to 36s, relative to the the state-of-the-art FastPIR scheme.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {102–115},
numpages = {14},
keywords = {private information retrieval (PIR), in-storage computing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527409,
author = {Zhao, Jin and Yang, Yun and Zhang, Yu and Liao, Xiaofei and Gu, Lin and He, Ligang and He, Bingsheng and Jin, Hai and Liu, Haikun and Jiang, Xinyu and Yu, Hui},
title = {TDGraph: a topology-driven accelerator for high-performance streaming graph processing},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527409},
doi = {10.1145/3470496.3527409},
abstract = {Many solutions have been recently proposed to support the processing of streaming graphs. However, for the processing of each graph snapshot of a streaming graph, the new states of the vertices affected by the graph updates are propagated irregularly along the graph topology. Despite the years' research efforts, existing approaches still suffer from the serious problems of redundant computation overhead and irregular memory access, which severely underutilizes a many-core processor. To address these issues, this paper proposes a topology-driven programmable accelerator TDGraph, which is the first accelerator to augment the many-core processors to achieve high performance processing of streaming graphs. Specifically, we propose an efficient topology-driven incremental execution approach into the accelerator design for more regular state propagation and better data locality. TDGraph takes the vertices affected by graph updates as the roots to prefetch other vertices along the graph topology and synchronizes the incremental computations of them on the fly. In this way, most state propagations originated from multiple vertices affected by different graph updates can be conducted together along the graph topology, which help reduce the redundant computations and data access cost. Besides, through the efficient coalescing of the accesses to vertex states, TDGraph further improves the utilization of the cache and memory bandwidth. We have evaluated TDGraph on a simulated 64-core processor. The results show that, the state-of-the-art software system achieves the speedup of 7.1~21.4 times after integrating with TDGraph, while incurring only 0.73\% area cost. Compared with four cutting-edge accelerators, i.e., HATS, Minnow, PHI, and DepGraph, TDGraph gains the speedups of 4.6~12.7, 3.2~8.6, 3.8~9.7, and 2.3~6.1 times, respectively.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {116–129},
numpages = {14},
keywords = {streaming graphs, state propagation, many-core processor, incremental computation, accelerator},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527388,
author = {Dai, Guohao and Zhu, Zhenhua and Fu, Tianyu and Wei, Chiyue and Wang, Bangyan and Li, Xiangyu and Xie, Yuan and Yang, Huazhong and Wang, Yu},
title = {DIMMining: pruning-efficient and parallel graph mining on near-memory-computing},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527388},
doi = {10.1145/3470496.3527388},
abstract = {Graph mining, which finds specific patterns in the graph, is becoming increasingly important in various domains. We point out that accelerating graph mining suffers from the following challenges: (1) Heavy comparison for pruning: Pruning technique is widely used to reduce search space in graph mining. It applies constraints on vertex indices and involves massive index comparisons. (2) Low parallelism of set operations: The typical graph mining algorithms can be expressed as a series of set operations between neighbors of vertices, which suffer from low parallelism if vertices are streaming to the computation units. (3) Heavy data transfer: Graph mining needs to transfer intermediate data with two orders of magnitude larger than the original data volume between CPU and memory.To tackle these challenges, we propose DIMMining with four techniques from algorithm to architecture perspectives. The Index Pre-comparison scheme is proposed for efficient pruning. We introduce the self anchor and neighbor partition to enable pre-comparison for vertex indices. Thus, we can reduce comparisons during runtime. We propose a Flexible BCSR (Bitmap with Compressed Sparse Row) format to enable parallelism for set operations from the data structure perspective, which works on continuous vertices without memory space overheads. The Systolic Merge Array is designed to further explore the parallelism on discontinuous vertices from the architecture perspective. Then, we propose a DIMM-based Near-Memory-Computing architecture, which eliminates the large-volume data transfer between the computation and the memory. Extensive experimental results on real-world graphs show that DIMMining achieves 222.23X and 139.51X speedup compared with FPGAs and CPUs, and 3.61X speedup over the state-of-the-art graph mining architecture.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {130–145},
numpages = {16},
keywords = {systolic merge array, near-memory-computing, graph mining},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527437,
author = {Talati, Nishil and Ye, Haojie and Yang, Yichen and Belayneh, Leul and Chen, Kuan-Yu and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
title = {NDMiner: accelerating graph pattern mining using near data processing},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527437},
doi = {10.1145/3470496.3527437},
abstract = {Graph Pattern Mining (GPM) algorithms mine structural patterns in graphs. The performance of GPM workloads is bottlenecked by control flow and memory stalls. This is because of data-dependent branches used in set intersection and difference operations that dominate the execution time.This paper first conducts a systematic GPM workload analysis and uncovers four new observations to inform the optimization effort. First, GPM workloads mostly fetch inputs of costly set operations from different memory banks. Second, to avoid redundant computation, modern GPM workloads employ symmetry breaking that discards several data reads, resulting in cache pollution and wasted DRAM bandwidth. Third, sparse pattern mining algorithms perform redundant memory reads and computations. Fourth, GPM workloads do not fully utilize the in-DRAM data parallelism.Based on these observations, this paper presents NDMiner, a Near Data Processing (NDP) architecture that improves the performance of GPM workloads. To reduce in-memory data transfer of fetching data from different memory banks, NDMiner integrates compute units to offload set operations in the buffer chip of DRAM. To alleviate the wasted memory bandwidth caused by symmetry breaking, NDMiner integrates a load elision unit in hardware that detects the satisfiability of symmetry breaking constraints and terminates unnecessary loads. To optimize the performance of sparse pattern mining, NDMiner employs compiler optimizations and maps reduced reads and composite computation to NDP hardware that improves algorithmic efficiency of sparse GPM. Finally, NDMiner proposes a new graph remapping scheme in memory and a hardware-based set operation reordering technique to best optimize bank, rank, and channel-level parallelism in DRAM. To orchestrate NDP computation, this paper presents design modifications at the host ISA, compiler, and memory controller. We compare the performance of NDMiner with state-of-the-art software and hardware baselines using a mix of dense and sparse GPM algorithms. Our evaluation shows that NDMiner significantly outperforms software and hardware baselines by 6.4X and 2.5X, on average, while incurring a negligible area overhead on CPU and DRAM.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {146–159},
numpages = {14},
keywords = {near data processing, hardware-software co-design, graph pattern mining},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527378,
author = {Umar, Muhammad and Hua, Weizhe and Zhang, Zhiru and Suh, G. Edward},
title = {SoftVN: efficient memory protection via software-provided version numbers},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527378},
doi = {10.1145/3470496.3527378},
abstract = {Trusted execution environments (TEEs) in processors protect off-chip memory (DRAM), and ensure its confidentiality and integrity using memory encryption and integrity verification. However, such memory protection can incur significant performance overhead as it requires additional memory accesses for protection metadata such as version numbers (VNs) and MACs. This paper proposes SoftVN, an extension to the current memory protection schemes, which significantly reduces the overhead of today's state-of-the-art by allowing software to provide VNs for memory accesses. For memory-intensive applications with simple memory access patterns for large data structures, the VNs only need to be maintained for data structures instead of individual cache blocks and can be tracked in software with low efforts. Off-chip VN accesses for memory reads can be removed if they are tracked and provided by software. We evaluate SoftVN by simulating a diverse set of memory-intensive applications, including deep learning, graph processing, and bioinformatics algorithms. The experimental results show that SoftVN reduces the memory protection overhead by 82\% compared to the baseline similar to Intel SGX, and improves the performance by 33\% on average. The maximum performance improvement can be as high as 65\%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {160–172},
numpages = {13},
keywords = {trusted execution environment (TEE), memory protection},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527393,
author = {Samardzic, Nikola and Feldmann, Axel and Krastev, Aleksandar and Manohar, Nathan and Genise, Nicholas and Devadas, Srinivas and Eldefrawy, Karim and Peikert, Chris and Sanchez, Daniel},
title = {CraterLake: a hardware accelerator for efficient unbounded computation on encrypted data},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527393},
doi = {10.1145/3470496.3527393},
abstract = {Fully Homomorphic Encryption (FHE) enables offloading computation to untrusted servers with cryptographic privacy. Despite its attractive security, FHE is not yet widely adopted due to its prohibitive overheads, about 10,000X over unencrypted computation. Recent FHE accelerators have made strides to bridge this performance gap. Unfortunately, prior accelerators only work well for simple programs, but become inefficient for complex programs, which bring additional costs and challenges.We present CraterLake, the first FHE accelerator that enables FHE programs of unbounded size (i.e., unbounded multiplicative depth). Such computations require very large ciphertexts (tens of MBs each) and different algorithms that prior work does not support well. To tackle this challenge, CraterLake introduces a new hardware architecture that efficiently scales to very large cipher-texts, novel functional units to accelerate key kernels, and new algorithms and compiler techniques to reduce data movement.We evaluate CraterLake on deep FHE programs, including deep neural networks like ResNet and LSTMs, where prior work takes minutes to hours per inference on a CPU. CraterLake outperforms a CPU by gmean 4,600X and the best prior FHE accelerator by 11.2X under similar area and power budgets. These speeds enable realtime performance on unbounded FHE programs for the first time.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {173–187},
numpages = {15},
keywords = {hardware acceleration, fully homomorphic encryption},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527425,
author = {Liu, Gang and Li, Kenli and Xiao, Zheng and Wang, Rujia},
title = {PS-ORAM: efficient crash consistency support for oblivious RAM on NVM},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527425},
doi = {10.1145/3470496.3527425},
abstract = {Oblivious RAM (ORAM) is a provable secure primitive to prevent access pattern leakage on the memory bus. By randomly remapping the data blocks and accessing redundant blocks, ORAM prevents access pattern leakage through ob-fuscation. Byte-addressable non-volatile memory (NVM) is considered as the candidate for main memory due to its better scalability, competitive performance, and persistent data store. While there is much prior work focusing on improving ORAM's performance on the conventional DRAM-based memory system, when the memory technology shifts to use NVM, ensuring an efficient crash-consistent ORAM is needed for security, correctness, and performance. Directly using traditional software-based crash consistency support for ORAM system is not only expensive but also insecure.In this work, we study how to persist ORAM construction with an NVM-based memory system. To support crash consistency without damaging ORAM system security and compromising the performance, we propose PS-ORAM. PS-ORAM consists of a novel ORAM controller design and a set of ORAM access protocols that support crash consistency.We evaluate PS-ORAM with the system without crash consistency support, non-recursive and recursive PS-ORAM only incurs 4.29\% and 3.65\% additional performance overhead. The results show that PS-ORAM not only supports effective crash consistency with minimal performance and hardware overhead but also is friendly to NVM lifetime.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {188–203},
numpages = {16},
keywords = {security, persistence, crash consistency, ORAM, NVM},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527416,
author = {Cook, Jack and Drean, Jules and Behrens, Jonathan and Yan, Mengjia},
title = {There's always a bigger fish: a clarifying analysis of a machine-learning-assisted side-channel attack},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527416},
doi = {10.1145/3470496.3527416},
abstract = {Machine learning has made it possible to mount powerful attacks through side channels that have traditionally been seen as challenging to exploit. However, due to the black-box nature of machine learning models, these attacks are often difficult to interpret correctly. Models that detect correlations cannot be used to prove causality or understand an attack's various sources of information leakage.In this paper, we show that a state-of-the-art website-fingerprinting attack powered by machine learning was only partially analyzed. In this attack, an attacker collects cache-sweeping traces, which measure the frequency at which the entire last-level cache can be accessed over time, while a victim loads a website. A neural network is then trained on these traces to predict websites accessed by the victim. The attack's usage of the cache led to a consensus that the attack exploited a cache-based side channel. However, we provide additional analysis contradicting this assumption and clarifying the mechanisms behind this powerful attack.We first replicate the website-fingerprinting attack without making any cache accesses, demonstrating that memory accesses are not crucial to the attack's success and may even inhibit its performance. We then search for the primary source of information leakage in our new attack by analyzing the effects of various isolation mechanisms and by instrumenting the Linux kernel. We ultimately find that this attack's success can be attributed primarily to system interrupts. Finally, we use this analysis to craft highly practical and effective defense mechanisms against our attack.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {204–217},
numpages = {14},
keywords = {website fingerprinting, side channels, security, microarchitecture, deep learning},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527402,
author = {Lenjani, Marzieh and Ahmed, Alif and Stan, Mircea and Skadron, Kevin},
title = {Gearbox: a case for supporting accumulation dispatching and hybrid partitioning in PIM-based accelerators},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527402},
doi = {10.1145/3470496.3527402},
abstract = {Processing-in-memory (PIM) minimizes data movement overheads by placing processing units near each memory segment. Recent PIMs employ processing units with a SIMD architecture. However, kernels with random accesses, such as sparse-matrix-dense-vector (SpMV) and sparse-matrix-sparse-vector (SpMSpV), cannot effectively exploit the parallelism of SIMD units because SIMD's ALUs remain idle until all the operands are collected from local memory segments (memory segment attached to the processing unit) or remote memory segments (other segments of the memory).For SpMV and SpMSpV, properly partitioning the matrix and the vector among the memory segments is also very important. Partitioning determines (i) how much processing load will be assigned to each processing unit and (ii) how much communication is required among the processing units.In this paper, first, we propose a highly parallel architecture that can exploit the available parallelism even in the presence of random accesses. Second, we observed that, in SpMV and SpMSpV, most of the remote accesses become remote accumulations with the right choice of algorithm and partitioning. The remote accumulations could be offloaded to be performed by processing units next to the destination memory segments, eliminating idle time due to remote accesses. Accordingly, we introduce a dispatching mechanism for remote accumulation offloading. Third, we propose Hybrid partitioning and associated hardware support. Our partitioning technique enables (i) replacing remote read accesses with broadcasting (for only a small portion of data that will be read by all processing units), (ii) reducing the number of remote accumulations, and (iii) balancing the load.Our proposed method, Gearbox, with just one memory stack, delivers on average (up to) 15.73X (52X) speedup over a server-class GPU, NVIDIA P100, with three stacks of HBM2 memory.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {218–230},
numpages = {13},
keywords = {sparse, processing in memory, graph, SpMV, SpMSpV, PIM},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527431,
author = {Devic, Alexandar and Rai, Siddhartha Balakrishna and Sivasubramaniam, Anand and Akel, Ameen and Eilert, Sean and Eno, Justin},
title = {To PIM or not for emerging general purpose processing in DDR memory systems},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527431},
doi = {10.1145/3470496.3527431},
abstract = {As Processing-In-Memory (PIM) hardware matures and starts making its way into normal compute platforms, software has an important role to play in determining what to perform where, and when, on such heterogeneous systems. Taking an emerging class of PIM hardware which provisions a general purpose (RISC-V) processor at each memory bank, this paper takes on this challenging problem by developing a software compilation framework. This framework analyzes several application characteristics - parallelizability, vectorizability, data set sizes, and offload costs - to determine what, whether, when and how to offload computations to the PIM engines. In the process, it also proposes a vector engine extension to the bank-level RISC-V cores. Using several off-the-shelf C/C++ applications, we demonstrate that PIM is not always a panacea, and a framework such as ours is essential in carefully selecting what needs to be performed where, when and how. The choice of hardware platforms - number of memory banks, relative speeds and capabilities of host CPU and PIM cores, can further impact the "to PIM or not" question.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {231–244},
numpages = {14},
keywords = {vector processing, processing-in-memory, parallel processing, general purpose processing, compilers, DRAM},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527432,
author = {Feng, Siying and He, Xin and Chen, Kuan-Yu and Ke, Liu and Zhang, Xuan and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
title = {MeNDA: a near-memory multi-way merge solution for sparse transposition and dataflows},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527432},
doi = {10.1145/3470496.3527432},
abstract = {Near-memory processing has been extensively studied to optimize memory intensive workloads. However, none of the proposed designs address sparse matrix transposition, an important building block in sparse linear algebra applications. Prior work shows that sparse matrix transposition does not scale as well as other sparse primitives such as sparse matrix vector multiplication (SpMV) and hence has become a growing bottleneck in common applications. Sparse matrix transposition is highly memory intensive but low in computational intensity, making it a promising candidate for near-memory processing. In this work, we propose MeNDA, a scalable near-DRAM multi-way merge accelerator that eliminates the off-chip memory interface bottleneck and exposes the high internal memory bandwidth to improve performance and reduce energy consumption for sparse matrix transposition. MeNDA adopts a merge sort based algorithm, exploiting spatial locality, and proposes a near-memory processing unit (PU) featuring a high-performance hardware merge tree. Because of the wide application of merge sort in sparse linear algebra, MeNDA is an extensible solution that can be easily adapted to support other sparse primitives such as SpMV. Techniques including seamless back-to-back merge sort, stall reducing prefetching and request coalescing are further explored to take full advantage of the increased system memory bandwidth. Compared to two state-of-the-art implementations of sparse matrix transposition on a CPU and a sparse library on a GPU, MeNDA is able to achieve a speedup of 19.1X, 12.0X, and 7.7x, respectively. MeNDA also shows an efficiency gain of 3.8x over a recent SpMV accelerator integrated with HBM. Incurring a power consumption of only 78.6 mW, a MeNDA PU can be easily accommodated by commodity DIMMs.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {245–258},
numpages = {14},
keywords = {sparse matrix-vector multiplication, sparse matrix transposition, sparse linear algebra, near-memory processing, multi-way merge accelerator, hardware merge tree, hardware accelerator},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527426,
author = {Man, Xingchen and Zhu, Jianfeng and Song, Guihuan and Yin, Shouyi and Wei, Shaojun and Liu, Leibo},
title = {CaSMap: agile mapper for reconfigurable spatial architectures by automatically clustering intermediate representations and scattering mapping process},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527426},
doi = {10.1145/3470496.3527426},
abstract = {Today, reconfigurable spatial architectures (RSAs) have sprung up as accelerators for compute- and data-intensive domains because they deliver energy and area efficiency close to ASICs and still retain sufficient programmability to keep the development cost low. The mapper, which is responsible for mapping algorithms onto RSAs, favors a systematic backtracking methodology because of high portability for evolving RSA designs. However, exponentially scaling compilation time has become the major obstacle. The key observation of this paper is that the key limiting factor to the systematic backtracking mappers is the waterfall mapping model which resolves all mapping variables and constraints at the same time using single-level intermediate representations (IRs).This work proposes CaSMap, an agile mapper framework independent of software and hardware of RSAs. By clustering the lowest-level software and hardware IRs into multi-level IRs, the original mapping process can be scattered as multi-stage decomposed ones and therefore the mapping problem with exponential complexity is mitigated. This paper introduces (a) strategies for clustering low-level hardware and software IRs with static connectivity and critical path analysis. (b) a multi-level scattered mapping model in which the higher-level model carries out the heuristics from IR clustering, endeavors to promote mapping success rate, and reduces the scale of the lower-level model. Our evaluation shows that CaSMap is able to reduce the problem scale (nonzeros) by 80.5\% (23.1\%-94.9\%) and achieve a mapping time speedup of 83X over the state-of-the-art waterfall mapper across four different RSA topologies: MorphoSys, HReA, HyCUBE, and REVEL.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {259–273},
numpages = {15},
keywords = {reconfigurable spatial architecture, integer linear programming, compiler, coarse-grained reconfigurable architecture},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527406,
author = {Xu, Yuanchao and Ye, Chencheng and Solihin, Yan and Shen, Xipeng},
title = {FFCCD: fence-free crash-consistent concurrent defragmentation for persistent memory},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527406},
doi = {10.1145/3470496.3527406},
abstract = {Persistent Memory (PM) is increasingly supplementing or substituting DRAM as main memory. Prior work have focused on reusability and memory leaks of persistent memory but have not addressed a problem amplified by persistence, persistent memory fragmentation, which refers to the continuous worsening of fragmentation of persistent memory throughout its usage. This paper reveals the challenges and proposes the first systematic crash-consistent solution, Fence-Free Crash-consistent Concurrent Defragmentation (FFCCD). FFCCD resues persistent pointer format, root nodes and typed allocation provided by persistent memory programming model to enable concurrent defragmentation on PM. FFCCD introduces architecture support for concurrent defragmentation that enables a fence-free design and fast read barrier, reducing two major overheads of defragmenting persistent memory. The techniques is effective (28--73\% fragmentation reduction) and fast (4.1\% execution time overhead).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {274–288},
numpages = {15},
keywords = {persistent memory, non-volatile memory, memory management, garbage collection, defragmentation},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527397,
author = {Lee, Sangwon and Kwon, Miryeong and Park, Gyuyoung and Jung, Myoungsoo},
title = {LightPC: hardware and software co-design for energy-efficient full system persistence},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527397},
doi = {10.1145/3470496.3527397},
abstract = {We propose LightPC, a lightweight persistence-centric platform to make the system robust against power loss. LightPC consists of hardware and software subsystems, each being referred to as open-channel PMEM (OC-PMEM) and persistence-centric OS (PecOS). OC-PMEM removes physical and logical boundaries in drawing a line between volatile and nonvolatile data structures by unshackling new memory media from conventional PMEM complex. PecOS provides a single execution persistence cut to quickly convert the execution states to persistent information in cases of a power failure, which can eliminate persistent control overhead. We prototype LightPC's computing complex and OC-PMEM using our custom system board. PecOS is implemented based on Linux 4.19 and Berkeley bootloader on the hardware prototype. Our evaluation results show that OC-PMEM can make user-level performance comparable with a DRAM-only non-persistent system, while consuming 73\% lower power and 69\% less energy. LightPC also shortens the execution time of diverse HPC, SPEC, and In-memory DB workloads, compared to traditional persistent systems by 4.3X, on average.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {289–305},
numpages = {17},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527399,
author = {Abulila, Ahmed and Hajj, Izzat El and Jung, Myoungsoo and Kim, Nam Sung},
title = {ASAP: architecture support for asynchronous persistence},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527399},
doi = {10.1145/3470496.3527399},
abstract = {Supporting atomic durability of updates for persistent memories is typically achieved with Write-Ahead Logging (WAL). WAL flushes log entries to persistent memory before making the actual data persistent to ensure that a consistent state can be recovered if a crash occurs. Performing WAL in hardware is attractive because it makes most aspects of log management transparent to software, and it completes log persist operations (LPOs) and data persist operations (DPOs) in the background, overlapping them with the execution of other instructions.Prior hardware logging solutions commit atomic regions synchronously. That is, once the end of a region is reached, all outstanding persist operations required for the region to commit must complete before instruction execution may proceed. For undo logging, LPOs and DPOs are both performed synchronously to ensure that the region commits synchronously. For redo logging, DPOs can be performed asynchronously, but LPOs are performed synchronously to ensure that the region commits synchronously. In both cases, waiting for synchronous persist operations (LPO or DPO) at the end of an atomic region causes atomic regions to incur high latency.To tackle this limitation, we propose ASAP, a hardware logging solution that allows atomic regions to commit asynchronously. That is, once the end of an atomic region is reached, instruction execution may proceed without waiting for outstanding persist operations to complete. As such, both LPOs and DPOs can be performed asynchronously. The challenge with allowing atomic regions to commit asynchronously is that it can lead to control and data dependence violations in the commit order of the atomic regions, leaving data in an unrecoverable state in case of a crash. To address this issue, ASAP tracks and enforces control and data dependencies between atomic regions in hardware to ensure that the regions commit in the proper order.Our evaluation shows that ASAP outperforms the state-of-the-art hardware undo and redo logging techniques by 1.41X and 1.53X, respectively, while achieving 0.96X the ideal performance when no persistence is enforced, at a small hardware cost (&lt; 3\%). ASAP also reduces memory traffic to persistent memory by 38\% and 48\%, compared with the state-of-the-art hardware undo and redo logging techniques, respectively. ASAP is robust against increasing persistent memory latency, making it suitable for both fast and slow persistent memory technologies.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {306–319},
numpages = {14},
keywords = {non-volatile memory, memory persistency, hardware logging},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527442,
author = {Singh, Gagandeep and Nadig, Rakesh and Park, Jisung and Bera, Rahul and Hajinazar, Nastaran and Novo, David and G\'{o}mez-Luna, Juan and Stuijk, Sander and Corporaal, Henk and Mutlu, Onur},
title = {Sibyl: adaptive and extensible data placement in hybrid storage systems using online reinforcement learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527442},
doi = {10.1145/3470496.3527442},
abstract = {Hybrid storage systems (HSS) use multiple different storage devices to provide high and scalable storage capacity at high performance. Data placement across different devices is critical to maximize the benefits of such a hybrid system. Recent research proposes various techniques that aim to accurately identify performance-critical data to place it in a "best-fit" storage device. Unfortunately, most of these techniques are rigid, which (1) limits their adaptivity to perform well for a wide range of workloads and storage device configurations, and (2) makes it difficult for designers to extend these techniques to different storage system configurations (e.g., with a different number or different types of storage devices) than the configuration they are designed for. Our goal is to design a new data placement technique for hybrid storage systems that overcomes these issues and provides: (1) adaptivity, by continuously learning from and adapting to the workload and the storage device characteristics, and (2) easy extensibility to a wide range of workloads and HSS configurations.We introduce Sibyl, the first technique that uses reinforcement learning for data placement in hybrid storage systems. Sibyl observes different features of the running workload as well as the storage devices to make system-aware data placement decisions. For every decision it makes, Sibyl receives a reward from the system that it uses to evaluate the long-term performance impact of its decision and continuously optimizes its data placement policy online.We implement Sibyl on real systems with various HSS configurations, including dual- and tri-hybrid storage systems, and extensively compare it against four previously proposed data placement techniques (both heuristic- and machine learning-based) over a wide range of workloads. Our results show that Sibyl provides 21.6\%/19.9\% performance improvement in a performance-oriented/cost-oriented HSS configuration compared to the best previous data placement technique. Our evaluation using an HSS configuration with three different storage devices shows that Sibyl outperforms the state-of-the-art data placement policy by 23.9\%-48.2\%, while significantly reducing the system architect's burden in designing a data placement mechanism that can simultaneously incorporate three storage devices. We show that Sibyl achieves 80\% of the performance of an oracle policy that has complete knowledge offuture access patterns while incurring a very modest storage overhead of only 124.4 KiB.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {320–336},
numpages = {17},
keywords = {solid-state drives (SSDs), reinforcement learning, machine learning, hybrid systems, hybrid storage systems, data placement},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527381,
author = {Wu, Anbang and Li, Gushu and Zhang, Hezi and Guerreschi, Gian Giacomo and Ding, Yufei and Xie, Yuan},
title = {A synthesis framework for stitching surface code with superconducting quantum devices},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527381},
doi = {10.1145/3470496.3527381},
abstract = {Quantum error correction (QEC) is the central building block of fault-tolerant quantum computation but the design of QEC codes may not always match the underlying hardware. To tackle the discrepancy between the quantum hardware and QEC codes, we propose a synthesis framework that can implement and optimize the surface code onto superconducting quantum architectures. In particular, we divide the surface code synthesis into three key subroutines. The first two optimize the mapping of data qubits and ancillary qubits including syndrome qubits on the connectivity-constrained superconducting architecture, while the last subroutine optimizes the surface code execution by rescheduling syndrome measurements. Our experiments on mainstream superconducting architectures demonstrate the effectiveness of the proposed synthesis framework. Especially, the surface codes synthesized by the proposed automatic synthesis framework can achieve comparable or even better error correction capability than manually designed QEC codes.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {337–350},
numpages = {14},
keywords = {quantum error correction, quantum computing, compiler},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527394,
author = {Lao, Lingling and Browne, Dan E.},
title = {2QAN: a quantum compiler for 2-local qubit hamiltonian simulation algorithms},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527394},
doi = {10.1145/3470496.3527394},
abstract = {Simulating quantum systems is one of the most important potential applications of quantum computers. The high-level circuit defining the simulation needs to be compiled into one that complies with hardware limitations such as qubit architecture (connectivity) and instruction (gate) set. General-purpose quantum compilers work at the gate level and have little knowledge of the mathematical properties of quantum applications, missing further optimization opportunities. Existing application-specific compilers only apply advanced optimizations in the scheduling procedure and are restricted to the CNOT or CZ gate set. In this work, we develop a compiler, named 2QAN, to optimize quantum circuits for 2-local qubit Hamiltonian simulation problems, a framework which includes the important quantum approximate optimization algorithm (QAOA). In particular, we exploit the flexibility of permuting different operators in the Hamiltonian (no matter whether they commute) and propose permutation-aware techniques for qubit routing, gate optimization and scheduling to minimize compilation overhead. 2QAN can target different architectures and different instruction sets. Compilation results on four applications (up to 50 qubits) and three quantum computers (namely, Google Sycamore, IBMQ Montreal and Rigetti Aspen) show that 2QAN outperforms state-of-the-art general-purpose compilers and application-specific compilers. Specifically, 2QAN can reduce the number of inserted SWAP gates by 11.5X, reduce overhead in hardware gate count by 68.5X, and reduce overhead in circuit depth by 21X. Experimental results on the Montreal device demonstrate that benchmarks compiled by 2QAN achieve the highest fidelity.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {351–365},
numpages = {15},
keywords = {quantum simulation, quantum computing, quantum compilation},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527417,
author = {Byun, Ilkwon and Kim, Junpyo and Min, Dongmoon and Nagaoka, Ikki and Fukumitsu, Kosuke and Ishikawa, Iori and Tanimoto, Teruo and Tanaka, Masamitsu and Inoue, Koji and Kim, Jangwoo},
title = {XQsim: modeling cross-technology control processors for 10+K qubit quantum computers},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527417},
doi = {10.1145/3470496.3527417},
abstract = {10+K qubit quantum computer is essential to achieve a true sense of quantum supremacy. With the recent effort towards the large-scale quantum computer, architects have revealed various scalability issues including the constraints in a quantum control processor, which should be holistically analyzed to design a future scalable control processor. However, it has been impossible to identify and resolve the processor's scalability bottleneck due to the absence of a reliable tool to explore an extensive design space including microarchitecture, device technology, and operating temperature.In this paper, we present XQsim, an open-source cross-technology quantum control processor simulator. XQsim can accurately analyze the target control processors' scalability bottlenecks for various device technology and operating temperature candidates. To achieve the goal, we first fully implement a convincing control processor microarchitecture for the Fault-tolerant Quantum Computer (FTQC) systems. Next, on top of the microarchitecture, we develop an architecture-level control processor simulator (XQsim) and thoroughly validate it with post-layout analysis, timing-accurate RTL simulation, and noisy quantum simulation. Lastly, driven by XQsim, we provide the future directions to design a 10+K qubit quantum control processor with several design guidelines and architecture optimizations. Our case study shows that the final control processor architecture can successfully support ~59K qubits with our operating temperature and technology choices.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {366–382},
numpages = {17},
keywords = {single flux quantum (SFQ), simulation, quantum computing, modeling, cryogenic computing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527428,
author = {Patel, Tirthak and Silver, Daniel and Tiwari, Devesh},
title = {Geyser: a compilation framework for quantum computing with neutral atoms},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527428},
doi = {10.1145/3470496.3527428},
abstract = {Compared to widely-used superconducting qubits, neutral-atom quantum computing technology promises potentially better scalability and flexible arrangement of qubits to allow higher operation parallelism and more relaxed cooling requirements. The high performance computing (HPC) and architecture community is beginning to design new solutions to take advantage of neutral-atom quantum architectures and overcome its unique challenges.We propose Geyser, the first work to take advantage of the multi-qubit gates natively supported by neutral-atom quantum computers by appropriately mapping quantum circuits to three-qubit-friendly physical arrangement of qubits. Then, Geyser creates multiple logical blocks in the quantum circuit to exploit quantum parallelism and reduce the number of pulses needed to realize physical gates. These circuit blocks elegantly enable Geyser to compose equivalent circuits with three-qubit gates, even when the original program does not have any multi-qubit gates. Our evaluation results show Geyser reduces the number of operation pulses by 25\%-90\% and improves the algorithm's output fidelity by 25\%-60\% points across different algorithms.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {383–395},
numpages = {13},
keywords = {rydberg atoms, quantum software, quantum computing, quantum compiling, neutral atoms, NISQ computing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527380,
author = {Sedaghati, Ali and Hakimi, Milad and Hojabr, Reza and Shriraman, Arrvindh},
title = {X-cache: a modular architecture for domain-specific caches},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527380},
doi = {10.1145/3470496.3527380},
abstract = {With Dennard scaling ending, architects are turning to domain-specific accelerators (DSAs). State-of-the-art DSAs work with sparse data [37] and indirectly-indexed data structures [18, 30]. They introduce non-affine and dynamic memory accesses [7, 35], and require domain-specific caches. Unfortunately, cache controllers are notorious for being difficult to architect; domain-specialization compounds the problem. DSA caches need to support custom tags, data-structure walks, multiple refills, and preloading. Prior DSAs include ad-hoc cache structures, and do not implement the cache controller. We propose X-Cache, a reusable caching idiom for DSAs. We will be open-sourcing a toolchain for both generating the RTL and programming X-Cache. There are three key ideas: i) DSA-specific Tags (Meta-tag): The designer can use any combination of fields from the DSA-metadata as the tag. Meta-tags eliminate the overhead of walking and translating metadata to global addresses. This saves energy, and improves load-to-use latency. ii) DSA-programmable walkers (X-Actions): We find that a common set of microcode actions can be used to implement the DSA-specific walking, data block, and tag management. We develop a programmable microcode engine that can efficiently realize the data orchestration. iii) DSA-portable controller (X-Routines): We use a portable abstraction, coroutines, to let the designer express walking and orchestration. Coroutines capture the block-level parallelism, remain lightweight, and minimize controller occupancy. We create caches for four different DSA families: Sparse GEMM [35, 37], GraphPulse [30], DASX [22], and Widx [18]. X-Cache outperforms address-based caches by 1.7 \texttimes{} and remains competitive with hardwired DSAs (even 50\% improvement in one case). We demonstrate that meta-tags save 26--79\% energy compared to address-tags. In X-Cache, meta-tags consume 1.5--6.5\% of data RAM energy and the programmable microcode adds a further 7\%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {396–409},
numpages = {14},
keywords = {domain-specific architectures, dataflow architectures, coroutines, caches},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527398,
author = {Shukla, Sudhanshu and Bandishte, Sumeet and Gaur, Jayesh and Subramoney, Sreenivas},
title = {Register file prefetching},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527398},
doi = {10.1145/3470496.3527398},
abstract = {The memory wall continues to limit the performance of modern out-of-order (OOO) processors, despite the expensive provisioning of large multi-level caches and advancements in memory prefetching. In this paper, we put forth an important observation that the memory wall is not monolithic, but is constituted of many latency walls arising due to the latency of each tier of cache/memory. Our results show that even though level-1 (L1) data cache latency is nearly 40X lower than main memory latency, mitigating this latency offers a very similar performance opportunity as the more widely studied, main memory latency.This motivates our proposal Register File Prefetch (RFP) that intelligently utilizes the existing OOO scheduling pipeline and available L1 data cache/Register File bandwidth to successfully prefetch 43.4\% of load requests from the L1 cache to the Register File. Simulation results on 65 diverse workloads show that this translates to 3.1\% performance gain over a baseline with parameters similar to Intel Tiger Lake processor, which further increases to 5.7\% for a futuristic up-scaled core. We also contrast and differentiate register file prefetching from techniques like load value and address prediction that enhance performance by speculatively breaking data dependencies. Our analysis shows that RFP is synergistic with value prediction, with both the features together delivering 4.1\% average performance improvement, which is significantly higher than the 2.2\% performance gain obtained from just doing value prediction.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {410–423},
numpages = {14},
keywords = {value prediction, pipeline prefetching, microarchitecture, load value prefetching, address prediction},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527384,
author = {Lee, Jounghoo and Ha, Yeonan and Lee, Suhyun and Woo, Jinyoung and Lee, Jinho and Jang, Hanhwi and Kim, Youngsok},
title = {GCoM: a detailed GPU core model for accurate analytical modeling of modern GPUs},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527384},
doi = {10.1145/3470496.3527384},
abstract = {Analytical models can greatly help computer architects perform orders of magnitude faster early-stage design space exploration than using cycle-level simulators. To facilitate rapid design space exploration for graphics processing units (GPUs), prior studies have proposed GPU analytical models which capture first-order stall events causing performance degradation; however, the existing analytical models cannot accurately model modern GPUs due to their outdated and highly abstract GPU core microarchitecture assumptions. Therefore, to accurately evaluate the performance of modern GPUs, we need a new GPU analytical model which accurately captures the stall events incurred by the significant changes in the core microarchitectures of modern GPUs.We propose GCoM, an accurate GPU analytical model which faithfully captures the key core-side stall events of modern GPUs. Through detailed microarchitecture-driven GPU core modeling, GCoM accurately models modern GPUs by revealing the following key core-side stalls overlooked by the existing GPU analytical models. First, GCoM identifies the compute structural stall events caused by the limited per-sub-core functional units. Second, GCoM exposes the memory structural stalls due to the limited banks and shared nature of per-core L1 data caches. Third, GCoM correctly predicts the memory data stalls induced by the sectored L1 data caches which split a cache line into a set of sectors sharing the same tag. Fourth, GCoM captures the idle stalls incurred by the inter- and intra-core load imbalances. Our experiments using an NVIDIA RTX 2060 configuration show that GCoM greatly improves the modeling accuracy by achieving a mean absolute error of 10.0\% against Accel-Sim cycle-level simulator, whereas the state-of-the-art GPU analytical model achieves a mean absolute error of 44.9\%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {424–436},
numpages = {13},
keywords = {performance modeling, interval analysis, graphics processing units},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527387,
author = {Posluns, Gilead and Zhu, Yan and Zhang, Guowei and Jeffrey, Mark C.},
title = {A scalable architecture for reprioritizing ordered parallelism},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527387},
doi = {10.1145/3470496.3527387},
abstract = {Many algorithms schedule their work, or tasks, according to a priority order for correctness or faster convergence. While priority schedulers commonly implement task enqueue and dequeueMin operations, some algorithms need a priority update operation that alters the scheduling metadata for a task. Prior software and hardware systems that support scheduling with priority updates compromise on either parallelism, work-efficiency, or both, leading to missed performance opportunities. Moreover, incorrectly navigating these compromises violates correctness in those algorithms that are not resilient to relaxing priority order.We present Hive, a task-based execution model and multicore architecture that extracts abundant fine-grain parallelism from algorithms with priority updates, while retaining their strict priority schedules. Like prior hardware systems for ordered parallelism, Hive uses data- and control-dependence speculation and a large speculative window to execute tasks in parallel and out of order. Hive improves on prior work by (i) directly supporting updates in the interface, (ii) identifying the novel scheduler-carried dependence, and (iii) speculating on such dependences with task versioning, distinct from data versioning. Hive enables safe speculative updates to the schedule and avoids spurious conflicts among tasks to better utilize speculation tracking resources and efficiently uncover more parallelism. Across a suite of nine benchmarks, Hive improves performance at 256 cores by up to 2.8\texttimes{} over the next best hardware solution, and even more over software-only parallel schedulers.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {437–453},
numpages = {17},
keywords = {task-level parallelism, speculative execution, priority updates, priority scheduling, ordered irregular parallelism},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527396,
author = {Bleier, Nathaniel and Mubarik, Muhammad Husnain and Chakraborty, Srijan and Kishore, Shreyas and Kumar, Rakesh},
title = {Rethinking programmable earable processors},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527396},
doi = {10.1145/3470496.3527396},
abstract = {Earables such as earphones [15, 16, 73], hearing aids [28], and smart glasses [2, 14] are poised to be a prominent programmable computing platform in the future. In this paper, we ask the question: what kind of programmable hardware would be needed to support earable computing in future? To understand hardware requirements, we propose EarBench, a suite of representative emerging earable applications with diverse sensor-based inputs and computation requirements. Our analysis of EarBench applications shows that, on average, there is a 13.54\texttimes{}-3.97\texttimes{} performance gap between the computational needs of EarBench applications and the performance of the microprocessors that several of today's programmable earable SoCs are based on; more complex microprocessors have unacceptable energy efficiency for Earable applications. Our analysis also shows that EarBench applications are dominated by a small number of digital signal processing (DSP) and machine learning (ML)-based kernels that have significant computational similarity. We propose SpEaC --- a coarse-grained reconfigurable spatial architecture - as an energy-efficient programmable processor for earable applications. SpEaC targets earable applications efficiently using a) a reconfigurable fixed-point multiply-and-add augmented reduction tree-based substrate with support for vectorized complex operations that is optimized for the earable ML and DSP kernel code and b) a tightly coupled control core for executing other code (including non-matrix computation, or non-multiply or add operations in the earable DSP kernel code). Unlike other CGRAs that typically target general-purpose computations, SpEaC substrate is optimized for energy-efficient execution of the earable kernels at the expense of generality. Across all our kernels, SpEaC outperforms programmable cores modeled after M4, M7, A53, and HiFi4 DSP by 99.3\texttimes{}, 32.5\texttimes{}, 14.8\texttimes{}, and 9.8\texttimes{} respectively. At 63 mW in 28 nm, the energy efficiency benefits are 1.55 \texttimes{}, 9.04\texttimes{}, 68.3 \texttimes{}, and 32.7 \texttimes{} respectively; energy efficiency benefits are 15.7 \texttimes{} -- 1087 \texttimes{} over a low power Mali T628 MP6 GPU.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {454–467},
numpages = {14},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527401,
author = {Wu, Di and Li, Jingjie and Pan, Zhewen and Kim, Younghyun and Miguel, Joshua San},
title = {uBrain: a unary brain computer interface},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527401},
doi = {10.1145/3470496.3527401},
abstract = {Brain computer interfaces (BCIs) have been widely adopted to enhance human perception via brain signals with abundant spatial-temporal dynamics, such as electroencephalogram (EEG). In recent years, BCI algorithms are moving from classical feature engineering to emerging deep neural networks (DNNs), allowing to identify the spatial-temporal dynamics with improved accuracy. However, existing BCI architectures are not leveraging such dynamics for hardware efficiency. In this work, we present uBrain, a unary computing BCI architecture for DNN models with cascaded convolutional and recurrent neural networks to achieve high task capability and hardware efficiency. uBrain co-designs the algorithm and hardware: the DNN architecture and the hardware architecture are optimized with customized unary operations and immediate signal processing after sensing, respectively. Experiments show that uBrain, with negligible accuracy loss, surpasses the CPU, systolic array and stochastic computing baselines in on-chip power efficiency by 9.0\texttimes{}, 6.2\texttimes{} and 2.0\texttimes{}.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {468–481},
numpages = {14},
keywords = {brain computer interface, neural networks, power efficiency, stochastic computing, temporal computing, unary computing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527441,
author = {Lin, Dehui and Tabatabaee, Yasamin and Pote, Yash and Jevdjic, Djordje},
title = {Managing reliability skew in DNA storage},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527441},
doi = {10.1145/3470496.3527441},
abstract = {DNA is emerging as an increasingly attractive medium for data storage due to a number of important and unique advantages it offers, most notably the unprecedented durability and density. While the technology is evolving rapidly, the prohibitive cost of reads and writes, the high frequency and the peculiar nature of errors occurring in DNA storage pose a significant challenge to its adoption.In this work we make a novel observation that the probability of successful recovery of a given bit from any type of a DNA-based storage system highly depends on its physical location within the DNA molecule. In other words, when used as a storage medium, some parts of DNA molecules appear significantly more reliable than others. We show that large differences in reliability between different parts of DNA molecules lead to highly inefficient use of error-correction resources, and that commonly used techniques such as unequal error-correction cannot be used to bridge the reliability gap between different locations in the context of DNA storage. We then propose two approaches to address the problem. The first approach is general and applies to any types of data; it stripes the data and ECC codewords across DNA molecules in a particular fashion such that the effects of errors are spread out evenly across different codewords and molecules, effectively de-biasing the underlying storage medium and improving the resilience against losses of entire molecules. The second approach is application-specific, and seeks to leverage the underlying reliability bias by using application-aware mapping of data onto DNA molecules such that data that requires higher reliability is stored in more reliable locations, whereas data that needs lower reliability is stored in less reliable parts of DNA molecules. We show that the proposed data mapping can be used to achieve graceful degradation in the presence of high error rates, or to implement the concept of approximate storage in DNA. All proposed mechanisms are seamlessly integrated into the state-of-the art DNA storage pipeline at zero storage overhead, validated through wetlab experiments, and evaluated on end-to-end encrypted and compressed data.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {482–494},
numpages = {13},
keywords = {reliability, error correction, approximate storage, DNA storage},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527424,
author = {Hanhan, Robert and Garz\'{o}n, Esteban and Jahshan, Zuher and Teman, Adam and Lanuzza, Marco and Yavits, Leonid},
title = {EDAM: edit distance tolerant approximate matching content addressable memory},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527424},
doi = {10.1145/3470496.3527424},
abstract = {We propose a novel edit distance-tolerant content addressable memory (EDAM) for energy-efficient approximate search applications. Unlike state-of-the-art approximate search solutions that tolerate certain Hamming distance between the query pattern and the stored data, EDAM tolerates edit distance, which makes it especially efficient in applications such as text processing and genome analysis. EDAM was designed using a commercial 65 nm 1.2 V CMOS technology and evaluated through extensive Monte Carlo simulations, while considering different process corners. Simulation results show that EDAM can achieve robust approximate search operation with a wide range of edit distance threshold levels. EDAM is functionally evaluated as a pathogen DNA detection and classification accelerator. EDAM achieves up to 1.7\texttimes{} higher F1 score for high-quality DNA reads and up to 19.55\texttimes{} higher F1 score for DNA reads with 15\% error rate, compared to state-of-the-art DNA classification tool Kraken2. Simulated at 667 MHz, EDAM provides 1, 214\texttimes{} average speedup over Kraken2. This makes EDAM suitable for hardware acceleration of genomic surveillance of outbreaks, such as the ongoing Covid-19 pandemic.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {495–507},
numpages = {13},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527414,
author = {Sharma, Anshujit and Afoakwa, Richard and Ignjatovic, Zeljko and Huang, Michael},
title = {Increasing ising machine capacity with multi-chip architectures},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527414},
doi = {10.1145/3470496.3527414},
abstract = {Nature has inspired a lot of problem solving techniques over the decades. More recently, researchers have increasingly turned to harnessing nature to solve problems directly. Ising machines are a good example and there are numerous research prototypes as well as many design concepts. They can map a family of NP-complete problems and derive competitive solutions at speeds much greater than conventional algorithms and in some cases, at a fraction of the energy cost of a von Neumann computer.However, physical Ising machines are often fixed in its problem solving capacity. Without any support, a bigger problem cannot be solved at all. With a simple divide-and-conquer strategy, it turns out, the advantage of using an Ising machine quickly diminishes. It is therefore desirable for Ising machines to have a scalable architecture where multiple instances can collaborate to solve a bigger problem. We then discuss scalable architecture design issues which lead to a multiprocessor Ising machine architecture. Experimental analyses show that our proposed architectures allow an Ising machine to scale in capacity and maintain its significant performance advantage (about 2200x speedup over a state-of-the-art computational substrate). In the case of communication bandwidth-limited systems, our proposed optimizations in supporting batch mode operation can cut down communication demand by about 4--5x without a significant impact on solution quality.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {508–521},
numpages = {14},
keywords = {scaling, nature-based computing, multi-chip, ising machine},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527419,
author = {Hanson, Edward and Li, Shiyu and Li, Hai 'Helen' and Chen, Yiran},
title = {Cascading structured pruning: enabling high data reuse for sparse DNN accelerators},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527419},
doi = {10.1145/3470496.3527419},
abstract = {Performance and efficiency of running modern Deep Neural Networks (DNNs) are heavily bounded by data movement. To mitigate the data movement bottlenecks, recent DNN inference accelerator designs widely adopt aggressive compression techniques and sparse-skipping mechanisms. These mechanisms avoid transferring or computing with zero-valued weights or activations to save time and energy. However, such sparse-skipping logic involves large input buffers and irregular data access patterns, thus precluding many energy-efficient data reuse opportunities and dataflows. In this work, we propose Cascading Structured Pruning (CSP), a technique that preserves significantly more data reuse opportunities for higher energy efficiency while maintaining comparable performance relative to recent sparse architectures such as SparTen. CSP includes the following two components: At algorithm level, CSP-A induces a predictable sparsity pattern that allows for low-overhead compression of weight data and sequential access to both activation and weight data. At architecture level, CSP-H leverages CSP-A's induced sparsity pattern with a novel dataflow to access unique activation data only once, thus removing the demand for large input buffers. Each CSP-H processing element (PE) employs a novel accumulation buffer design and a counter-based sparse-skipping mechanism to support the dataflow with minimum controller overhead. We verify our approach on several representative models. Our simulated results show that CSP achieves on average 15\texttimes{} energy efficiency improvement over SparTen with comparable or superior speedup under most evaluations.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {522–535},
numpages = {14},
keywords = {model compression, low power microarchitecture, hardware/software co-design, ML acceleration},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527404,
author = {Lew, Jonathan S. and Liu, Yunpeng and Gong, Wenyi and Goli, Negar and Evans, R. David and Aamodt, Tor M.},
title = {Anticipating and eliminating redundant computations in accelerated sparse training},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527404},
doi = {10.1145/3470496.3527404},
abstract = {Deep Neural Networks (DNNs) are the state of art in image, speech, and text processing. To address long training times and high energy consumption, custom accelerators can exploit sparsity, that is zero-valued weights, activations, and gradients. Proposed sparse Convolution Neural Network (CNN) accelerators support training with no more than one dynamic sparse convolution input. Among existing accelerator classes, the only ones supporting two-sided dynamic sparsity are outer-product-based accelerators. However, when mapping a convolution onto an outer product, multiplications occur that do not correspond to any valid output. These Redundant Cartesian Products (RCPs) decrease energy efficiency and performance. We observe that in sparse training, up to 90\% of computations are RCPs resulting from the convolution of large matrices for weight updates during the backward pass of CNN training.In this work, we design a mechanism, ANT, to anticipate and eliminate RCPs, enabling more efficient sparse training when integrated with an outer-product accelerator. By anticipating over 90\% of RCPs, ANT achieves a geometric mean of 3.71\texttimes{} speed up over an SCNN-like accelerator [67] on 90\% sparse training using DenseNet-121 [38], ResNet18 [35], VGG16 [73], Wide ResNet (WRN) [85], and ResNet-50 [35], with 4.40\texttimes{} decrease in energy consumption and 0.0017mm2 of additional area. We extend ANT to sparse matrix multiplication, so that the same accelerator can anticipate RCPs in sparse fully-connected layers, transformers, and RNNs.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {536–551},
numpages = {16},
keywords = {sparse matrix multiplication, sparse CNN training, hardware acceleration},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527411,
author = {Zhang, Yunan and Tsai, Po-An and Tseng, Hung-Wei},
title = {SIMD2: a generalized matrix instruction set for accelerating tensor computation beyond GEMM},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527411},
doi = {10.1145/3470496.3527411},
abstract = {Matrix-multiplication units (MXUs) are now prevalent in every computing platform. The key attribute that makes MXUs so successful is the semiring structure, which allows tiling for both parallelism and data reuse. Nonetheless, matrix-multiplication is not the only algorithm with such attributes. We find that many algorithms share the same structure and differ in only the core operation; for example, using add-minimum instead of multiply-add. Algorithms with a semiring-like structure therefore have potential to be accelerated by a general-purpose matrix operation architecture, instead of common MXUs.In this paper, we propose SIMD2, a new programming paradigm to support generalized matrix operations with a semiring-like structure. SIMD2 instructions accelerate eight more types of matrix operations, in addition to matrix multiplications. Since SIMD2 instructions resemble a matrix-multiplication instruction, we are able to build SIMD2 architecture on top of any MXU architecture with minimal modifications. We developed a framework that emulates and validates SIMD2 using NVIDIA GPUs with Tensor Cores. Across 8 applications, SIMD2 provides up to 38.59\texttimes{} speedup and more than 6.94\texttimes{} on average over optimized CUDA programs, with only 5\% of full-chip area overhead.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {552–566},
numpages = {15},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527405,
author = {Abts, Dennis and Kimmell, Garrin and Ling, Andrew and Kim, John and Boyd, Matt and Bitar, Andrew and Parmar, Sahil and Ahmed, Ibrahim and DiCecco, Roberto and Han, David and Thompson, John and Bye, Michael and Hwang, Jennifer and Fowers, Jeremy and Lillian, Peter and Murthy, Ashwin and Mehtabuddin, Elyas and Tekur, Chetan and Sohmers, Thomas and Kang, Kris and Maresh, Stephen and Ross, Jonathan},
title = {A software-defined tensor streaming multiprocessor for large-scale machine learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527405},
doi = {10.1145/3470496.3527405},
abstract = {We describe our novel commercial software-defined approach for large-scale interconnection networks of tensor streaming processing (TSP) elements. The system architecture includes packaging, routing, and flow control of the interconnection network of TSPs. We describe the communication and synchronization primitives of a bandwidth-rich substrate for global communication. This scalable communication fabric provides the backbone for large-scale systems based on a software-defined Dragonfly topology, ultimately yielding a parallel machine learning system with elasticity to support a variety of workloads, both training and inference. We extend the TSP's producer-consumer stream programming model to include global memory which is implemented as logically shared, but physically distributed SRAM on-chip memory. Each TSP contributes 220 MiBytes to the global memory capacity, with the maximum capacity limited only by the network's scale --- the maximum number of endpoints in the system. The TSP acts as both a processing element (endpoint) and network switch for moving tensors across the communication links. We describe a novel software-controlled networking approach that avoids the latency variation introduced by dynamic contention for network links. We describe the topology, routing and flow control to characterize the performance of the network that serves as the fabric for a large-scale parallel machine learning system with up to 10,440 TSPs and more than 2 TeraBytes of global memory accessible in less than 3 microseconds of end-to-end system latency.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {567–580},
numpages = {14},
keywords = {tensor streaming processor, software scheduling, machine learning, dragonfly},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527382,
author = {Rashidi, Saeed and Won, William and Srinivasan, Sudarshan and Sridharan, Srinivas and Krishna, Tushar},
title = {Themis: a network bandwidth-aware collective scheduling policy for distributed training of DL models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527382},
doi = {10.1145/3470496.3527382},
abstract = {Distributed training is a solution to reduce DNN training time by splitting the task across multiple NPUs (e.g., GPU/TPU). However, distributed training adds communication overhead between the NPUs in order to synchronize the gradients and/or activation, depending on the parallelization strategy. In next-generation platforms for training at scale, NPUs will be connected through multidimensional networks with diverse, heterogeneous bandwidths. This work identifies a looming challenge of keeping all network dimensions busy and maximizing the network BW within the hybrid environment if we leverage scheduling techniques for collective communication on systems today. We propose Themis, a novel collective scheduling scheme that dynamically schedules collectives (divided into chunks) to balance the communication loads across all dimensions, further improving the network BW utilization. Our results show that on average, Themis can improve the network BW utilization of the single All-Reduce by 1.72\texttimes{} (2.70\texttimes{} max), and improve the end-to-end training iteration performance of real workloads such as ResNet-152, GNMT, DLRM, and Transformer-1T by 1.49\texttimes{} (2.25\texttimes{} max), 1.30\texttimes{} (1.78\texttimes{} max), 1.30\texttimes{} (1.77\texttimes{} max), and 1.25\texttimes{} (1.53\texttimes{} max), respectively.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {581–596},
numpages = {16},
keywords = {distributed training, collective communication, bandwidth-aware communication scheduling},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527383,
author = {Bakhshalipour, Mohammad and Ehsani, Seyed Borna and Qadri, Mohamad and Guri, Dominic and Likhachev, Maxim and Gibbons, Phillip B.},
title = {RACOD: algorithm/hardware co-design for mobile robot path planning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527383},
doi = {10.1145/3470496.3527383},
abstract = {RACOD is an algorithm/hardware co-design for mobile robot path planning. It consists of two main components: CODAcc, a hardware accelerator for collision detection; and RASExp, an algorithm extension for runahead path exploration. CODAcc uses a novel MapReduce-style hardware computational model and massively parallelizes individual collision checks. RASExp predicts future path explorations and proactively computes its collision status ahead of time, thereby overlapping multiple collision detections. By affording multiple cheap CODAcc accelerators and overlapping collision detections using RASExp, RACOD significantly accelerates planning for mobile robots operating in arbitrary environments. Evaluations of popular benchmarks show up to 41.4\texttimes{} (self-driving cars) and 34.3\texttimes{} (pilotless drones) speedup with less than 0.3\% area overhead.While the performance is maximized when CODAcc and RASExp are used together, they can also be used individually. To illustrate, we evaluate CODAcc alone in the context of a stationary robotic arm and show that it improves performance by 3.4\texttimes{}--3.8\texttimes{}. Also, we evaluate RASExp alone on commodity many-core CPU and GPU platforms by implementing it purely in software and show that with 32/128 CPU/GPU threads, it accelerates the end-to-end planning time by 8.6\texttimes{}/2.9\texttimes{}.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {597–609},
numpages = {13},
keywords = {speculative parallelism, robotics, path planning, hardware acceleration, collision detection},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527443,
author = {You, Haoran and Wan, Cheng and Zhao, Yang and Yu, Zhongzhi and Fu, Yonggan and Yuan, Jiayi and Wu, Shang and Zhang, Shunyao and Zhang, Yongan and Li, Chaojian and Boominathan, Vivek and Veeraraghavan, Ashok and Li, Ziyun and Lin, Yingyan},
title = {EyeCoD: eye tracking system acceleration via flatcam-based algorithm \&amp; accelerator co-design},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527443},
doi = {10.1145/3470496.3527443},
abstract = {Eye tracking has become an essential human-machine interaction modality for providing immersive experience in numerous virtual and augmented reality (VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and enhanced visual privacy. However, existing eye tracking systems are still limited by their: (1) large form-factor largely due to the adopted bulky lens-based cameras; (2) high communication cost required between the camera and backend processor; and (3) potentially concerned low visual privacy, thus prohibiting their more extensive applications. To this end, we propose, develop, and validate a lensless FlatCambased eye tracking algorithm and accelerator co-design framework dubbed EyeCoD to enable eye tracking systems with a much reduced form-factor and boosted system efficiency without sacrificing the tracking accuracy, paving the way for next-generation eye tracking solutions. On the system level, we advocate the use of lensless FlatCams instead of lens-based cameras to facilitate the small form-factor need in mobile eye tracking systems, which also leaves rooms for a dedicated sensing-processor co-design to reduce the required camera-processor communication latency. On the algorithm level, EyeCoD integrates a predict-then-focus pipeline that first predicts the region-of-interest (ROI) via segmentation and then only focuses on the ROI parts to estimate gaze directions, greatly reducing redundant computations and data movements. On the hardware level, we further develop a dedicated accelerator that (1) integrates a novel workload orchestration between the aforementioned segmentation and gaze estimation models, (2) leverages intra-channel reuse opportunities for depth-wise layers, (3) utilizes input feature-wise partition to save activation memory size, and (4) develops a sequential-write-parallel-read input buffer to alleviate the bandwidth requirement for the activation global buffer. On-silicon measurement and extensive experiments validate that our EyeCoD consistently reduces both the communication and computation costs, leading to an overall system speedup of 10.95\texttimes{}, 3.21\texttimes{}, and 12.85\texttimes{} over general computing platforms including CPUs and GPUs, and a prior-art eye tracking processor called CIS-GEP, respectively, while maintaining the tracking accuracy. Codes are available at https://github.com/RICE-EIC/EyeCoD.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {610–622},
numpages = {13},
keywords = {eye tracking systems, algorithm-hardware co-design, VR/AR},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527435,
author = {Caminal, Helena and Chronis, Yannis and Wu, Tianshu and Patel, Jignesh M. and Mart\'{\i}nez, Jos\'{e} F.},
title = {Accelerating database analytic query workloads using an associative processor},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527435},
doi = {10.1145/3470496.3527435},
abstract = {Database analytic query workloads are heavy consumers of data-center cycles, and there is constant demand to improve their performance. Associative processors (AP) have re-emerged as an attractive architecture that offers very large data-level parallelism that can be used to implement a wide range of general-purpose operations. Associative processing is based primarily on efficient search and bulk update operations. Analytic query workloads benefit from data parallel execution and often feature both search and bulk update operations. In this paper, we investigate how amenable APs are to improving the performance of analytic query workloads. For this study, we use the recently proposed Content-Addressable Processing Engine (CAPE) framework. CAPE is an AP core that is highly programmable via the RISC-V ISA with standard vector extensions. By mapping key database operators to CAPE and introducing AP-aware changes to the query optimizer, we show that CAPE is a good match for database analytic workloads. We also propose a set of database-aware microarchitectural changes to CAPE to further improve performance. Overall, CAPE achieves a 10.8\texttimes{} speedup on average (up to 61.1\texttimes{}) on the SSB benchmark (a suite of 13 queries) compared to an iso-area aggressive out-of-order processor with AVX-512 SIMD support.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {623–637},
numpages = {15},
keywords = {vector architectures, databases, codesign, associative processors, analytics, analytic workloads, acceleration},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527436,
author = {Cali, Damla Senol and Kanellopoulos, Konstantinos and Lindegger, Jo\"{e}l and Bing\"{o}l, Z\"{u}lal and Kalsi, Gurpreet S. and Zuo, Ziyi and Firtina, Can and Cavlak, Meryem Banu and Kim, Jeremie and Ghiasi, Nika Mansouri and Singh, Gagandeep and G\'{o}mez-Luna, Juan and Alserr, Nour Almadhoun and Alser, Mohammed and Subramoney, Sreenivas and Alkan, Can and Ghose, Saugata and Mutlu, Onur},
title = {SeGraM: a universal hardware accelerator for genomic sequence-to-graph and sequence-to-sequence mapping},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527436},
doi = {10.1145/3470496.3527436},
abstract = {A critical step of genome sequence analysis is the mapping of sequenced DNA fragments (i.e., reads) collected from an individual to a known linear reference genome sequence (i.e., sequence-to-sequence mapping). Recent works replace the linear reference sequence with a graph-based representation of the reference genome, which captures the genetic variations and diversity across many individuals in a population. Mapping reads to the graph-based reference genome (i.e., sequence-to-graph mapping) results in notable quality improvements in genome analysis. Unfortunately, while sequence-to-sequence mapping is well studied with many available tools and accelerators, sequence-to-graph mapping is a more difficult computational problem, with a much smaller number of practical software tools currently available.We analyze two state-of-the-art sequence-to-graph mapping tools and reveal four key issues. We find that there is a pressing need to have a specialized, high-performance, scalable, and low-cost algorithm/hardware co-design that alleviates bottlenecks in both the seeding and alignment steps of sequence-to-graph mapping. Since sequence-to-sequence mapping can be treated as a special case of sequence-to-graph mapping, we aim to design an accelerator that is efficient for both linear and graph-based read mapping.To this end, we propose SeGraM, a universal algorithm/hardware co-designed genomic mapping accelerator that can effectively and efficiently support both &lt;u&gt;se&lt;/u&gt;quence-to-&lt;u&gt;gra&lt;/u&gt;ph &lt;u&gt;m&lt;/u&gt;apping and sequence-to-sequence mapping, for both short and long reads. To our knowledge, SeGraM is the first algorithm/hardware co-design for accelerating sequence-to-graph mapping. SeGraM consists of two main components: (1) MinSeed, the first &lt;u&gt;min&lt;/u&gt;imizer-based &lt;u&gt;seed&lt;/u&gt;ing accelerator, which finds the candidate locations in a given genome graph; and (2) BitAlign, the first &lt;u&gt;bit&lt;/u&gt;vector-based sequence-to-graph &lt;u&gt;align&lt;/u&gt;ment accelerator, which performs alignment between a given read and the subgraph identified by MinSeed. We couple SeGraM with high-bandwidth memory to exploit low latency and highly-parallel memory access, which alleviates the memory bottleneck.We demonstrate that SeGraM provides significant improvements for multiple steps of the sequence-to-graph (i.e., S2G) and sequence-to-sequence (i.e., S2S) mapping pipelines. First, SeGraM outperforms state-of-the-art S2G mapping tools by 5.9\texttimes{}/3.9\texttimes{} and 106\texttimes{}/- 742\texttimes{} for long and short reads, respectively, while reducing power consumption by 4.1\texttimes{}/4.4\texttimes{} and 3.0\texttimes{}/3.2\texttimes{}. Second, BitAlign outperforms a state-of-the-art S2G alignment tool by 41\texttimes{}-539\texttimes{} and three S2S alignment accelerators by 1.2\texttimes{}-4.8\texttimes{}. We conclude that SeGraM is a high-performance and low-cost universal genomics mapping accelerator that efficiently supports both sequence-to-graph and sequence-to-sequence mapping pipelines.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {638–655},
numpages = {18},
keywords = {seeding, read mapping, read alignment, minimizer, hardware accelerator, genomics, genome graphs, genome analysis, bitvector, algorithm/hardware co-design},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527422,
author = {Zou, Zhuowen and Chen, Hanning and Poduval, Prathyush and Kim, Yeseong and Imani, Mahdi and Sadredini, Elaheh and Cammarota, Rosario and Imani, Mohsen},
title = {BioHD: an efficient genome sequence search platform using HyperDimensional memorization},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527422},
doi = {10.1145/3470496.3527422},
abstract = {In this paper, we propose BioHD, a novel genomic sequence searching platform based on Hyper-Dimensional Computing (HDC) for hardware-friendly computation. BioHD transforms inherent sequential processes of genome matching to highly-parallelizable computation tasks. We exploit HDC memorization to encode and represent the genome sequences using high-dimensional vectors. Then, it combines the genome sequences to generate an HDC reference library. During the sequence searching, BioHD performs exact or approximate similarity check of an encoded query with the HDC reference library. Our framework simplifies the required sequence matching operations while introducing a statistical model to control the alignment quality. To get actual advantage from BioHD inherent robustness and parallelism, we design a processing in-memory (PIM) architecture with massive parallelism and compatible with the existing crossbar memory. Our PIM architecture supports all essential BioHD operations natively in memory with minimal modification on the array. We evaluate BioHD accuracy and efficiency on a wide range of genomics data, including COVID-19 databases. Our results indicate that PIM provides 102.8\texttimes{} and 116.1\texttimes{} (9.3\texttimes{} and 13.2\texttimes{}) speedup and energy efficiency compared to the state-of-the-art pattern matching algorithm running on GeForce RTX 3060 Ti GPU (state-of-the-art PIM accelerator).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {656–669},
numpages = {14},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527427,
author = {Loughlin, Kevin and Saroiu, Stefan and Wolman, Alec and Manerkar, Yatin A. and Kasikci, Baris},
title = {MOESI-prime: preventing coherence-induced hammering in commodity workloads},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527427},
doi = {10.1145/3470496.3527427},
abstract = {Prior work shows that Rowhammer attacks---which flip bits in DRAM via frequent activations of the same row(s)---are viable. Adversaries typically mount these attacks via instruction sequences that are carefully-crafted to bypass CPU caches. However, we discover a novel form of hammering that we refer to as coherence-induced hammering, caused by Intel's implementations of cache coherent non-uniform memory access (ccNUMA) protocols. We show that this hammering occurs in commodity benchmarks on a major cloud provider's production hardware, the first hammering found to be generated by non-malicious code. Given DRAM's rising susceptibility to bit flips, it is paramount to prevent coherence-induced hammering to ensure reliability and security in the cloud.Accordingly, we introduce MOESI-prime, a ccNUMA coherence protocol that mitigates coherence-induced hammering while retaining Intel's state-of-the-art scalability. MOESI-prime shows that most DRAM reads and writes triggering such hammering are unnecessary. Thus, by encoding additional information in the coherence protocol, MOESI-prime can omit these reads and writes, preventing coherence-induced hammering in non-malicious and malicious workloads. Furthermore, by omitting unnecessary reads and writes, MOESI-prime has negligible effect on average performance (within ±0.61\% of MESI and MOESI) and average DRAM power (0.03\%-0.22\% improvement) across evaluated ccNUMA configurations.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {670–684},
numpages = {15},
keywords = {security, reliability, coherence protocol, Rowhammer},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527429,
author = {Ravichandran, Joseph and Na, Weon Taek and Lang, Jay and Yan, Mengjia},
title = {PACMAN: attacking ARM pointer authentication with speculative execution},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527429},
doi = {10.1145/3470496.3527429},
abstract = {This paper studies the synergies between memory corruption vulnerabilities and speculative execution vulnerabilities. We leverage speculative execution attacks to bypass an important memory protection mechanism, ARM Pointer Authentication, a security feature that is used to enforce pointer integrity. We present PACMAN, a novel attack methodology that speculatively leaks PAC verification results via micro-architectural side channels without causing any crashes. Our attack removes the primary barrier to conducting control-flow hijacking attacks on a platform protected using Pointer Authentication.We demonstrate multiple proof-of-concept attacks of PACMAN on the Apple M1 SoC, the first desktop processor that supports ARM Pointer Authentication. We reverse engineer the TLB hierarchy on the Apple M1 SoC and expand micro-architectural side-channel attacks to Apple processors. Moreover, we show that the PACMAN attack works across privilege levels, meaning that we can attack the operating system kernel as an unprivileged user in userspace.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {685–698},
numpages = {14},
keywords = {security, pointer authentication, micro-architectural side channels, memory corruption attacks},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527421,
author = {Qureshi, Moinuddin and Rohan, Aditya and Saileshwar, Gururaj and Nair, Prashant J.},
title = {Hydra: enabling low-overhead mitigation of row-hammer at ultra-low thresholds via hybrid tracking},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527421},
doi = {10.1145/3470496.3527421},
abstract = {DRAM systems continue to be plagued by the Row-Hammer (RH) security vulnerability. The threshold number of row activations (TRH) required to induce RH has reduced rapidly from 139K in 2014 to 4.8K in 2020, and TRH is expected to reduce further, making RH even more severe for future DRAM. Therefore, solutions for mitigating RH should be effective not only at current TRH but also at future TRH. In this paper, we investigate the mitigation of RH at ultra-low thresholds (500 and below). At such thresholds, state-of-the-art solutions, which rely on SRAM or CAM for tracking row activations, incur impractical storage overheads (340KB or more per rank at TRH of 500), making such solutions unappealing for commercial adoption. Alternative solutions, which store per-row metadata in the addressable DRAM space, incur significant slowdown (25\% on average) due to extra memory accesses, even in the presence of metadata caches. Our goal is to develop scalable RH mitigation while incurring low SRAM and performance overheads.To that end, this paper proposes Hydra, a &lt;u&gt;H&lt;/u&gt;ybri&lt;u&gt;d&lt;/u&gt; T&lt;u&gt;ra&lt;/u&gt;cker for RH mitigation, which combines the best of both SRAM and DRAM to enable low-cost mitigation of RH at ultra-low thresholds. Hydra consists of two structures. First, an SRAM-based structure that tracks aggregated counts at the granularity of a group of rows, and is sufficient for the vast majority of rows that receive only a few activations. Second, a per-row tracker stored in the DRAM-array, which can track an arbitrary number of rows, however, to limit performance overheads, this tracker is used only for the small number of rows that exceed the tracking capability of the SRAM-based structure. We provide a security analysis of Hydra to show that Hydra can reliably issue a mitigation within the specified threshold. Our evaluations show that Hydra enables robust mitigation of RH, while incurring an SRAM overhead of only 28 KB per-rank and an average slowdown of only 0.7\% (at TRH of 500).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {699–710},
numpages = {12},
keywords = {security, row-hammer, reliability, memory system, DRAM},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527415,
author = {Kim, Sangpyo and Kim, Jongmin and Kim, Michael Jaemin and Jung, Wonkyung and Kim, John and Rhu, Minsoo and Ahn, Jung Ho},
title = {BTS: an accelerator for bootstrappable fully homomorphic encryption},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527415},
doi = {10.1145/3470496.3527415},
abstract = {Homomorphic encryption (HE) enables the secure offloading of computations to the cloud by providing computation on encrypted data (ciphertexts). HE is based on noisy encryption schemes in which noise accumulates as more computations are applied to the data. The limited number of operations applicable to the data prevents practical applications from exploiting HE. Bootstrapping enables an unlimited number of operations or fully HE (FHE) by refreshing the ciphertext. Unfortunately, bootstrapping requires a significant amount of additional computation and memory bandwidth as well. Prior works have proposed hardware accelerators for computation primitives of FHE. However, to the best of our knowledge, this is the first to propose a hardware FHE accelerator that supports bootstrapping as a first-class citizen.In particular, we propose BTS --- Bootstrappable, Technology-driven, Secure accelerator architecture for FHE. We identify the challenges of supporting bootstrapping in the accelerator and analyze the off-chip memory bandwidth and computation required. In particular, given the limitations of modern memory technology, we identify the HE parameter sets that are efficient for FHE acceleration. Based on the insights gained from our analysis, we propose BTS, which effectively exploits the parallelism innate in HE operations by arranging a massive number of processing elements in a grid. We present the design and microarchitecture of BTS, including a network-on-chip design that exploits a deterministic communication pattern. BTS shows 5,556\texttimes{} and 1,306\texttimes{} improved execution time on ResNet-20 and logistic regression over a CPU, with a chip area of 373.6mm2 and up to 163.2W of power.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {711–725},
numpages = {15},
keywords = {technology-driven, fully homomorphic encryption, bootstrapping, accelerator, CKKS},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527418,
author = {Hua, Weizhe and Umar, Muhammad and Zhang, Zhiru and Suh, G. Edward},
title = {MGX: near-zero overhead memory protection for data-intensive accelerators},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527418},
doi = {10.1145/3470496.3527418},
abstract = {This paper introduces MGX, a near-zero overhead memory protection scheme for hardware accelerators. MGX minimizes the performance overhead of off-chip memory encryption and integrity verification by exploiting the application-specific properties of the accelerator execution. In particular, accelerators tend to explicitly manage data movement between on-chip and off-chip memories. Therefore, the general memory access pattern of an accelerator can largely be determined for a given application. Exploiting these characteristics, MGX generates version numbers used in memory encryption and integrity verification using on-chip accelerator state rather than storing them in the off-chip memory; it also customizes the granularity of the memory protection to match the granularity used by the accelerator. To demonstrate the efficacy of MGX, we present an in-depth study of MGX for DNN and graph algorithms. Experimental results show that on average, MGX lowers the performance overhead of memory protection from 28\% and 33\% to 4\% and 5\% for DNN and graph processing accelerators in a wide range of benchmarks, respectively.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {726–741},
numpages = {16},
keywords = {version number generation, secure accelerators, off-chip memory protection, neural networks, graph algorithms},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527430,
author = {Song, Shixin and Khan, Tanvir Ahmed and Shahri, Sara Mahdizadeh and Sriraman, Akshitha and Soundararajan, Niranjan K and Subramoney, Sreenivas and Jim\'{e}nez, Daniel A. and Litz, Heiner and Kasikci, Baris},
title = {Thermometer: profile-guided btb replacement for data center applications},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527430},
doi = {10.1145/3470496.3527430},
abstract = {Modern processors employ a decoupled frontend with Fetch Directed Instruction Prefetching (FDIP) to avoid frontend stalls in data center applications. However, the large branch footprint of data center applications precipitates frequent Branch Target Buffer (BTB) misses that prohibit FDIP from eliminating more than 40\% of all frontend stalls. We find that the state-of-the-art BTB optimization techniques (e.g., BTB prefetching and replacement mechanisms) cannot eliminate these misses due to their inadequate understanding of branch reuse behavior in data center applications.In this paper, we first perform a comprehensive characterization of the branch behavior of data center applications, and determine that identifying optimal BTB replacement decisions requires considering both transient and holistic (i.e., across the entire execution) branch behavior. We then present Thermometer, a novel BTB replacement technique that realizes the holistic branch behavior via a profile-guided analysis. Based on the collected profile, Thermometer generates useful BTB replacement hints that the underlying hardware can leverage. We evaluate Thermometer using 13 widely-used data center applications and demonstrate that it provides an average speedup of 8.7\% (0.4\%-64.9\%) while outperforming the state-of-the-art BTB replacement techniques by 5.6\texttimes{} (on average, the best performing prior work achieves 1.5\% speedup). We also demonstrate that Thermometer achieves a performance speedup that is, on average, 83.6\% of the speedup achieved by the optimal BTB replacement policy.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {742–756},
numpages = {15},
keywords = {frontend stalls, data center, cache replacement, branch target buffer},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527390,
author = {Schall, David and Margaritov, Artemiy and Ustiugov, Dmitrii and Sandberg, Andreas and Grot, Boris},
title = {Lukewarm serverless functions: characterization and optimization},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527390},
doi = {10.1145/3470496.3527390},
abstract = {Serverless computing has emerged as a widely-used paradigm for running services in the cloud. In serverless, developers organize their applications as a set of functions, which are invoked on-demand in response to events, such as an HTTP request. To avoid long start-up delays of launching a new function instance, cloud providers tend to keep recently-triggered instances idle (or warm) for some time after the most recent invocation in anticipation of future invocations. Thus, at any given moment on a server, there may be thousands of warm instances of various functions whose executions are interleaved in time based on incoming invocations.This paper observes that (1) there is a high degree of interleaving among warm instances on a given server; (2) the individual warm functions are invoked relatively infrequently, often at the granularity of seconds or minutes; and (3) many function invocations complete within a few milliseconds. Interleaved execution of rarely invoked functions on a server leads to thrashing of each function's microarchitectural state between invocations. Meanwhile, the short execution time of a function impedes amortization of the warm-up latency of the cache hierarchy, causing a 31--114\% increase in CPI compared to execution with warm microarchitectural state. We identify on-chip misses for instructions as a major contributor to the performance loss. In response we propose Jukebox, a record-and-replay instruction prefetcher specifically designed for reducing the start-up latency of warm function instances. Jukebox requires just 32KB of metadata per function instance and boosts performance by an average of 18.7\% for a wide range of functions, which translates into a corresponding throughput improvement.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {757–770},
numpages = {14},
keywords = {serverless, microarchitecture, instruction prefetching, characterization},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527389,
author = {Kasan, Hans and Kim, Gwangsun and Yi, Yung and Kim, John},
title = {Dynamic global adaptive routing in high-radix networks},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527389},
doi = {10.1145/3470496.3527389},
abstract = {Global adaptive routing is a critical component of high-radix networks in large-scale systems and is necessary to fully exploit the path diversity of a high-radix topology. The routing decision in global adaptive routing is made between minimal and non-minimal paths, often based on local information (e.g., queue occupancy) and rely on "approximate" congestion information through backpressure. Different heuristic-based adaptive routing algorithms have been proposed for high-radix topologies; however, heuristic-based routing has performance trade-off for different traffic patterns and leads to inefficient routing decisions. In addition, previously proposed global adaptive routing algorithms are static as the same routing decision algorithm is used, even if the congestion information changes. In this work, we propose a novel global adaptive routing that we refer to as dynamic global adaptive routing that adjusts the routing decision algorithm through a dynamic bias based on the network traffic and congestion to maximize performance. In particular, we propose DGB - Decoupled, Gradient descent-based Bias global adaptive routing algorithm. DGB introduces a dynamic bias to the global adaptive routing decision by leveraging gradient descent to dynamically adjust the adaptive routing bias based on the network congestion. In addition, both the local and global congestion information are decoupled in the routing decision - global information is used for the dynamic bias while local information is used in the routing decision to more accurately estimate the network congestion. Our evaluations show that DGB consistently outperforms previously proposed routing algorithms across diverse range of traffic patterns and workloads. For asymmetric traffic pattern, DGB improves throughput by 65\% compared to the state-of-the-art global adaptive routing algorithm while matching the performance for symmetric traffic patterns. For trace workloads, DGB provides average performance improvement of 26\%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {771–783},
numpages = {13},
keywords = {load-balancing, high-radix networks, global adaptive routing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527408,
author = {Gupta, Udit and Elgamal, Mariam and Hills, Gage and Wei, Gu-Yeon and Lee, Hsien-Hsin S. and Brooks, David and Wu, Carole-Jean},
title = {ACT: designing sustainable computer systems with an architectural carbon modeling tool},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527408},
doi = {10.1145/3470496.3527408},
abstract = {Given the performance and efficiency optimizations realized by the computer systems and architecture community over the last decades, the dominating source of computing's carbon footprint is shifting from operational emissions to embodied emissions. These embodied emissions owe to hardware manufacturing and infrastructure-related activities. Despite the rising embodied emissions, there is a distinct lack of architectural modeling tools to quantify and optimize the end-to-end carbon footprint of computing. This work proposes ACT, an architectural carbon footprint modeling framework, to enable carbon characterization and sustainability-driven early design space exploration. Using ACT we demonstrate optimizing hardware for carbon yields distinct solutions compared to optimizing for performance and efficiency. We construct use cases, based on the three tenets of sustainable design---Reduce, Reuse, Recycle---to highlight future methods that enable strong performance and efficiency scaling in an environmentally sustainable manner.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {784–799},
numpages = {16},
keywords = {sustainable computing, mobile, manufacturing, energy, computer architecture},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527407,
author = {Patterson, Liam and Pigorovsky, David and Dempsey, Brian and Lazarev, Nikita and Shah, Aditya and Steinhoff, Clara and Bruno, Ariana and Hu, Justin and Delimitrou, Christina},
title = {HiveMind: a hardware-software system stack for serverless edge swarms},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527407},
doi = {10.1145/3470496.3527407},
abstract = {Swarms of autonomous devices are increasing in ubiquity and size, making the need for rethinking their hardware-software system stack critical.We present HiveMind, the first swarm coordination platform that enables programmable execution of complex task workflows between cloud and edge resources in a performant and scalable manner. HiveMind is a software-hardware platform that includes a domain-specific language to simplify programmability of cloud-edge applications, a program synthesis tool to automatically explore task placement strategies, a centralized controller that leverages serverless computing to elastically scale cloud resources, and a reconfigurable hardware acceleration fabric for network and remote memory accesses.We design and build the full end-to-end HiveMind system on two real edge swarms comprised of drones and robotic cars. We quantify the opportunities and challenges serverless introduces to edge applications, as well as the trade-offs between centralized and distributed coordination. We show that HiveMind achieves significantly better performance predictability and battery efficiency compared to existing centralized and decentralized platforms, while also incurring lower network traffic. Using both real systems and a validated simulator we show that HiveMind can scale to thousands of edge devices without sacrificing performance or efficiency, demonstrating that centralized platforms can be both scalable and performant.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {800–816},
numpages = {17},
keywords = {serverless, hardware acceleration, edge swarms, domain specific language, cloud computing, IoT},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527400,
author = {Orenes-Vera, Marcelo and Manocha, Aninda and Balkind, Jonathan and Gao, Fei and Arag\'{o}n, Juan L. and Wentzlaff, David and Martonosi, Margaret},
title = {Tiny but mighty: designing and realizing scalable latency tolerance for manycore SoCs},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527400},
doi = {10.1145/3470496.3527400},
abstract = {Modern computing systems employ significant heterogeneity and specialization to meet performance targets at manageable power. However, memory latency bottlenecks remain problematic, particularly for sparse neural network and graph analytic applications where indirect memory accesses (IMAs) challenge the memory hierarchy.Decades of prior art have proposed hardware and software mechanisms to mitigate IMA latency, but they fail to analyze real-chip considerations, especially when used in SoCs and manycores. In this paper, we revisit many of these techniques while taking into account manycore integration and verification.We present the first system implementation of latency tolerance hardware that provides significant speedups without requiring any memory hierarchy or processor tile modifications. This is achieved through a Memory Access Parallel-Load Engine (MAPLE), integrated through the Network-on-Chip (NoC) in a scalable manner. Our hardware-software co-design allows programs to perform long-latency memory accesses asynchronously from the core, avoiding pipeline stalls, and enabling greater memory parallelism (MLP).In April 2021 we taped out a manycore chip that includes tens of MAPLE instances for efficient data supply. MAPLE demonstrates a full RTL implementation of out-of-core latency-mitigation hardware, with virtual memory support and automated compilation targetting it. This paper evaluates MAPLE integrated with a dual-core FPGA prototype running applications with full SMP Linux, and demonstrates geomean speedups of 2.35\texttimes{} and 2.27\texttimes{} over software-based prefetching and decoupling, respectively. Compared to state-of-the-art hardware, it provides geomean speedups of 1.82\texttimes{} and 1.72\texttimes{} over prefetching and decoupling techniques.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {817–830},
numpages = {14},
keywords = {modular RTL, memory, latency tolerance, decoupling},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527410,
author = {Bleier, Nathaniel and Lee, Calvin and Rodriguez, Francisco and Sou, Antony and White, Scott and Kumar, Rakesh},
title = {FlexiCores: low footprint, high yield, field reprogrammable flexible microprocessors},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527410},
doi = {10.1145/3470496.3527410},
abstract = {Flexible electronics is a promising approach to target applications whose computational needs are not met by traditional silicon-based electronics due to their conformality, thinness, or cost requirements. A microprocessor is a critical component for many such applications; however, it is unclear whether it is feasible to build flexible processors at scale (i.e., at high yield), since very few flexible microprocessors have been reported and no yield data or data from multiple chips has been reported. Also, prior manufactured flexible systems were not field-reprogrammable and were evaluated either on a simple set of test vectors or a single program. A working flexible microprocessor chip supporting complex or multiple applications has not been demonstrated. Finally, no prior work performs a design space of flexible microprocessors to optimize area, code size, and energy of such microprocessors.In this work, we fabricate and test hundreds of FlexiCores - flexible 0.8 μm IGZO TFT-based field-reprogrammable 4 and 8-bit microprocessor chips optimized for low footprint and yield. We show that these gate count-optimized processors can have high yield (4-bit FlexiCores have 81\% yield - sufficient to enable sub-cent cost if produced at volume). We evaluate these chips over a suite of representative kernels - the kernels take 4.28 ms to 12.9 ms and 21.0 μJ to 61.4 μJ for execution (at 360 nJ per instruction). We also present the first characterization of process variation for a flexible processor - we observe significant process variation (relative standard deviation of 15.3\% and 21.5\% in terms of current draw of 4-bit and 8-bit FlexiCore chips respectively). Finally, we perform a design space exploration and identify design points much better than FlexiCores - the new cores consume only 45--56\% the energy of the base design, and have code size less than 30\% of the base design, with an area overhead of 9--37\%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {831–846},
numpages = {16},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527444,
author = {Xu, Ceyu and Kjellqvist, Chris and Wills, Lisa Wu},
title = {SNS's not a synthesizer: a deep-learning-based synthesis predictor},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527444},
doi = {10.1145/3470496.3527444},
abstract = {The number of transistors that can fit on one monolithic chip has reached billions to tens of billions in this decade thanks to Moore's Law. With the advancement of every technology generation, the transistor counts per chip grow at a pace that brings about exponential increase in design time, including the synthesis process used to perform design space explorations. Such a long delay in obtaining synthesis results hinders an efficient chip development process, significantly impacting time-to-market. In addition, these large-scale integrated circuits tend to have larger and higher-dimension design spaces to explore, making it prohibitively expensive to obtain physical characteristics of all possible designs using traditional synthesis tools.In this work, we propose a deep-learning-based synthesis predictor called SNS (SNS's not a Synthesizer), that predicts the area, power, and timing physical characteristics of a broad range of designs at two to three orders of magnitude faster than the Synopsys Design Compiler while providing on average a 0.4998 RRSE (root relative square error). We further evaluate SNS via two representative case studies, a general-purpose out-of-order CPU case study using RISC-V Boom open-source design and an accelerator case study using an in-house Chisel implementation of DianNao, to demonstrate the capabilities and validity of SNS.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {847–859},
numpages = {13},
keywords = {neural networks, integrated circuits, RTL-level synthesis},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527386,
author = {Kwon, Youngeun and Rhu, Minsoo},
title = {Training personalized recommendation systems from (GPU) scratch: look forward not backwards},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527386},
doi = {10.1145/3470496.3527386},
abstract = {Personalized recommendation models (RecSys) are one of the most popular machine learning workload serviced by hyperscalers. A critical challenge of training RecSys is its high memory capacity requirements, reaching hundreds of GBs to TBs of model size. In RecSys, the so-called embedding layers account for the majority of memory usage so current systems employ a hybrid CPU-GPU design to have the large CPU memory store the memory hungry embedding layers. Unfortunately, training embeddings involve several memory bandwidth intensive operations which is at odds with the slow CPU memory, causing performance overheads. Prior work proposed to cache frequently accessed embeddings inside GPU memory as means to filter down the embedding layer traffic to CPU memory, but this paper observes several limitations with such cache design. In this work, we present a fundamentally different approach in designing embedding caches for RecSys. Our proposed ScratchPipe architecture utilizes unique properties of RecSys training to develop an embedding cache that not only sees the past but also the "future" cache accesses. ScratchPipe exploits such property to guarantee that the active working set of embedding layers can "always" be captured inside our proposed cache design, enabling embedding layer training to be conducted at GPU memory speed.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {860–873},
numpages = {14},
keywords = {systems for machine learning, recommendation system, neural network, memory architecture, graphics processing unit (GPU)},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527440,
author = {Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
title = {AMOS: enabling &lt;u&gt;a&lt;/u&gt;utomatic &lt;u&gt;m&lt;/u&gt;apping for tensor computations &lt;u&gt;o&lt;/u&gt;n &lt;u&gt;s&lt;/u&gt;patial accelerators with hardware abstraction},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527440},
doi = {10.1145/3470496.3527440},
abstract = {Hardware specialization is a promising trend to sustain performance growth. Spatial hardware accelerators that employ specialized and hierarchical computation and memory resources have recently shown high performance gains for tensor applications such as deep learning, scientific computing, and data mining. To harness the power of these hardware accelerators, programmers have to use specialized instructions with certain hardware constraints. However, these hardware accelerators and instructions are quite new and there is a lack of understanding of the hardware abstraction, performance optimization space, and automatic methodologies to explore the space. Existing compilers use hand-tuned computation implementations and optimization templates, resulting in sub-optimal performance and heavy development costs.In this paper, we propose AMOS, which is an automatic compilation framework for spatial hardware accelerators. Central to this framework is the hardware abstraction that not only clearly specifies the behavior of spatial hardware instructions, but also formally defines the mapping problem from software to hardware. Based on the abstraction, we develop algorithms and performance models to explore various mappings automatically. Finally, we build a compilation framework that uses the hardware abstraction as compiler intermediate representation (IR), explores both compute mappings and memory mappings, and generates high-performance code for different hardware backends. Our experiments show that AMOS achieves more than 2.50\texttimes{} speedup to hand-optimized libraries on Tensor Core, 1.37\texttimes{} speedup to TVM on vector units of Intel CPU for AVX-512, and up to 25.04\texttimes{} speedup to AutoTVM on dot units of Mali GPU. The source code of AMOS is publicly available.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {874–887},
numpages = {14},
keywords = {tensor computations, spatial accelerators, mapping, code generation},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527438,
author = {Zadeh, Ali Hadi and Mahmoud, Mostafa and Abdelhadi, Ameer and Moshovos, Andreas},
title = {Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527438},
doi = {10.1145/3470496.3527438},
abstract = {Increasingly larger and better Transformer models keep advancing state-of-the-art accuracy and capability for Natural Language Processing applications. These models demand more computational power, storage, and energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit floating-point transformer models by quantizing all values to 4-bit indexes into dictionaries of representative 16-bit fixed-point centroids. Mokey does not need fine-tuning, an essential feature as often the training resources or datasets are not available to many. Exploiting the range of values that naturally occur in transformer models, Mokey selects centroid values to also fit an exponential curve. This unique feature enables Mokey to replace the bulk of the original multiply-accumulate operations with narrow 3b fixed-point additions resulting in an area- and energy-efficient hardware accelerator design. Over a set of state-of-the-art transformer models, the Mokey accelerator delivers an order of magnitude improvements in energy efficiency over a Tensor Cores-based accelerator while improving performance by at least 4\texttimes{} and as much as 15\texttimes{} depending on the model and on-chip buffering capacity. Optionally, Mokey can be used as memory compression assist for any other accelerator transparently stashing wide floating-point or fixed-point activations or weights into narrow 4-bit indexes. Mokey proves superior to prior state-of-the-art quantization methods for Transformers.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {888–901},
numpages = {14},
keywords = {transformer models, quantization, natural language processing},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527423,
author = {Li, Zheng and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kang, Mingu},
title = {Accelerating attention through gradient-based learned runtime pruning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527423},
doi = {10.1145/3470496.3527423},
abstract = {Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. This attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention, which is only determined at runtime. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. The main challenge is finding the threshold for the scores below which subsequent computation will be inconsequential. Although such a threshold is discrete, this paper formulates its search through a soft differentiable regularizer integrated into the loss function of the training. This formulation piggy backs on the back-propagation training to analytically co-optimize the threshold and the weights simultaneously, striking a formally optimal balance between accuracy and computation pruning. To best utilize this mathematical innovation, we devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism. We evaluate our design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision transformer models. Post-layout results show that, on average, LeOPArd yields 1.9\texttimes{}and 3.9\texttimes{}speedup and energy reduction, respectively, while keeping the average accuracy virtually intact (&lt; 0.2\% degradation).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {902–915},
numpages = {14},
keywords = {transformer, self-attention, neural processing units, learned pruning, gradient-based optimization, deep learning, attention mechanism, accelerators},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527403,
author = {Gong, Zhangxiaowen and Ji, Houxiang and Yao, Yao and Fletcher, Christopher W. and Hughes, Christopher J. and Torrellas, Josep},
title = {Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527403},
doi = {10.1145/3470496.3527403},
abstract = {Graph Neural Networks (GNNs) are becoming popular because they are effective at extracting information from graphs. To execute GNNs, CPUs are good platforms because of their high availability and terabyte-level memory capacity, which enables full-batch computation on large graphs. However, GNNs on CPUs are heavily memory bound, which limits their performance.In this paper, we address this problem by alleviating the stress of GNNs on memory with cooperative software-hardware techniques. Our software techniques include: (i) layer fusion that overlaps the memory-intensive phase and the compute-intensive phase in a GNN layer, (ii) feature compression that reduces memory traffic by exploiting the sparsity in the vertex feature vectors, and (iii) an algorithm that changes the processing order of vertices to improve temporal locality. On top of the software techniques, we enhance the CPUs' direct memory access (DMA) engines with the capability to execute the GNNs' memory-intensive phase, so that the processor cores can focus on the compute-intensive phase. We call the combination of our software and hardware techniques Graphite.We evaluate Graphite with popular GNN models on large graphs. The result is high-performance full-batch GNN training and inference on CPUs. Our software techniques outperform a state-of-the-art GNN layer implementation by 1.7--1.9x in inference and 1.6--2.6x in training. Our combined software and hardware techniques speedup inference by 1.6--2.0x and training by 1.9--3.1x.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {916–931},
numpages = {16},
keywords = {hardware-software co-design, graph neural networks, DMA, CPU},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527391,
author = {Lee, Yunjae and Chung, Jinha and Rhu, Minsoo},
title = {SmartSAGE: training large-scale graph neural networks using in-storage processing architectures},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527391},
doi = {10.1145/3470496.3527391},
abstract = {Graph neural networks (GNNs) can extract features by learning both the representation of each objects (i.e., graph nodes) and the relationship across different objects (i.e., the edges that connect nodes), achieving state-of-the-art performance in various graph-based tasks. Despite its strengths, utilizing these algorithms in a production environment faces several challenges as the number of graph nodes and edges amount to several billions to hundreds of billions scale, requiring substantial storage space for training. Unfortunately, state-of-the-art ML frameworks employ an in-memory processing model which significantly hampers the productivity of ML practitioners as it mandates the overall working set to fit within DRAM capacity. In this work, we first conduct a detailed characterization on a state-of-the-art, large-scale GNN training algorithm, GraphSAGE. Based on the characterization, we then explore the feasibility of utilizing capacity-optimized NVMe SSDs for storing memory-hungry GNN data, which enables large-scale GNN training beyond the limits of main memory size. Given the large performance gap between DRAM and SSD, however, blindly utilizing SSDs as a direct substitute for DRAM leads to significant performance loss. We therefore develop SmartSAGE, our software/hardware co-design based on an in-storage processing (ISP) architecture. Our work demonstrates that an ISP based large-scale GNN training system can achieve both high capacity storage and high performance, opening up opportunities for ML practitioners to train large GNN datasets without being hampered by the physical limitations of main memory size.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {932–945},
numpages = {14},
keywords = {solid state drives (SSD), near data processing, graph neural network, computational storage device},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527439,
author = {Li, Shuangchen and Niu, Dimin and Wang, Yuhao and Han, Wei and Zhang, Zhe and Guan, Tianchan and Guan, Yijin and Liu, Heng and Huang, Linyong and Du, Zhaoyang and Xue, Fei and Fang, Yuanwei and Zheng, Hongzhong and Xie, Yuan},
title = {Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527439},
doi = {10.1145/3470496.3527439},
abstract = {Graph neural network (GNN) is a promising emerging application for link prediction, recommendation, etc. Existing hardware innovation is limited to single-machine GNN (SM-GNN), however, the enterprises usually adopt huge graph with large-scale distributed GNN (LSD-GNN) that has to be carried out with distributed in-memory storage. The LSD-GNN is very different from SM-GNN in terms of system architecture demand, workflow and operators, and hence characterizations.In this paper, we first quantitively characterize the LSD-GNN with industrial-grade framework and application, summarize that its challenges lie in graph sampling, including distributed graph access, long latency, and underutilized communication and memory bandwidth. These challenges are missing from previous SM-GNN targeted researches. We then propose a customized hardware architecture to solve the challenges, including a fully pipelined access engine architecture for graph access and sampling, a low-latency and bandwidth-efficient customized memory-over-fabric hardware, and a RISC-V centric control system providing good programma-bility. We implement the proposed architecture with full software support in a 4-card FPGA heterogeneous proof-of-concept (PoC) system. Based on the measurement result from the FPGA PoC, we demonstrate a single FPGA can provide up to 894 vCPU's sampling capability. With the goal of being profitable, programmable, and scalable, we further integrate the architecture to FPGA cloud (FaaS) at hyperscale, along with the industrial software framework. We explicitly explore eight FaaS architectures that carry out the proposed accelerator hardware. We finally conclude that off-the-shelf FaaS.base can already provide 2.47\texttimes{} performance per dollar improvement with our hardware. With architecture optimizations, FaaS.comm-opt with customized FPGA fabrics pushes the benefit to 7.78\texttimes{}, and FaaS.mem-opt with FPGA local DRAM and high-speed links to GPU further unleash the benefit to 12.58\texttimes{}.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {946–961},
numpages = {16},
keywords = {graph neural network, accelerator, FPGA-as-a-service},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527395,
author = {Feng, Yu and Hammonds, Gunnar and Gan, Yiming and Zhu, Yuhao},
title = {Crescent: taming memory irregularities for accelerating deep point cloud analytics},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527395},
doi = {10.1145/3470496.3527395},
abstract = {3D perception in point clouds is transforming the perception ability of future intelligent machines. Point cloud algorithms, however, are plagued by irregular memory accesses, leading to massive inefficiencies in the memory sub-system, which bottlenecks the overall efficiency.This paper proposes Crescent, an algorithm-hardware co-design system that tames the irregularities in deep point cloud analytics while achieving high accuracy. To that end, we introduce two approximation techniques, approximate neighbor search and selectively bank conflict elision, that "regularize" the DRAM and SRAM memory accesses. Doing so, however, necessarily introduces accuracy loss, which we mitigate by a new network training procedure that integrates approximation into the network training process. In essence, our training procedure trains models that are conditioned upon a specific approximate setting and, thus, retain a high accuracy. Experiments show that Crescent doubles the performance and halves the energy consumption compared to an optimized baseline accelerator with &lt; 1\% accuracy loss. The code of our paper is available at: https://github.com/horizon-research/crescent.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {962–977},
numpages = {16},
keywords = {point cloud, irregular memory accesses, accelerators, DNN},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533040,
author = {Sankaralingam, Karthikeyan and Nowatzki, Tony and Gangadhar, Vinay and Shah, Preyas and Davies, Michael and Galliher, William and Guo, Ziliang and Khare, Jitu and Vijay, Deepak and Palamuttam, Poly and Punde, Maghawan and Tan, Alex and Thiruvengadam, Vijay and Wang, Rongyi and Xu, Shunmiao},
title = {The Mozart reuse exposed dataflow processor for AI and beyond: industrial product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533040},
doi = {10.1145/3470496.3533040},
abstract = {In this paper we introduce the Mozart Processor, which implements a new processing paradigm called Reuse Exposed Dataflow (RED). RED is a counterpart to existing execution models of Von-Neumann, SIMT, Dataflow, and FPGA. Dataflow and data reuse are the fundamental architecture primitives in RED, implemented with mechanisms for inter-worker communication and synchronization. The paper defines the processor architecture, the details of the microarchitecture, chip implementation, software stack development, and performance results. The architecture's goal is to achieve near-CPU like flexibility while having ASIC-like efficiency for a large-class of data-intensive workloads. An additional goal was software maturity --- have large coverage of applications immediately, avoiding the need for a long-drawn hand-tuning software development phase. The architecture was defined with this software-maturity/compiler friendliness in mind. In short, the goal was to do to GPUs, what GPUs did to CPUs --- i.e. be a better solution for a large range of workloads, while preserving flexibility and programmability. The chip was implemented with HBM and PCIe interfaces and taken to production on a 16nm TSMC FFC process. For ML inference tasks with batch-size=4, Mozart is integer factors better than state-of-the-art GPUs even while being nearly 2 technology nodes behind. We conclude with a set of lessons learned, the unique challenges of a clean-slate architecture in a commercial setting, and pointers for uncovered research problems.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {978–992},
numpages = {15},
keywords = {reuse, multicasting, machine learning, dataflow, chips, accelerator},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533727,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533042,
author = {Lichtenau, Cedric and Buyuktosunoglu, Alper and Bertran, Ramon and Figuli, Peter and Jacobi, Christian and Papandreou, Nikolaos and Pozidis, Haris and Saporito, Anthony and Sica, Andrew and Tzortzatos, Elpida},
title = {AI accelerator on IBM Telum processor: industrial product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533042},
doi = {10.1145/3470496.3533042},
abstract = {IBM Telum is the next generation processor chip for IBM Z and LinuxONE systems. The Telum design is focused on enterprise class workloads and it achieves over 40\% per socket performance growth compared to IBM z15. The IBM Telum is the first server-class chip with a dedicated on-chip AI accelerator that enables clients to gain real time insights from their data as it is getting processed.Seamlessly infusing AI in all enterprise workloads is highly desirable to get real business insight on every transaction as well as to improve IT operation, security, and data privacy. While it would undeniably provide significant additional value, its application in practice is often accompanied by hurdles from low throughput if run on-platform to security concerns and inconsistent latency if run off-platform. The IBM Telum chip introduces an on-chip AI accelerator that provides consistent low latency and high throughput (over 200 TFLOPS in 32 chip system) inference capacity usable by all threads. The accelerator is memory coherent and directly connected to the fabric like any other general-purpose core to support low latency inference while meeting the system's transaction rate. A scalable architecture providing transparent access to AI accelerator functions via a non-privileged general-purpose core instruction further reduces software orchestration and library complexity as well as provides extensibility to the AI functions. On a global bank customer credit card fraud detection model, the AI accelerator achieves 22\texttimes{} speed up in latency compared to a general purpose core utilizing vector execution units. For the same model, the AI accelerator achieves 116k inferences every second with a latency of only 1.1 msec. As the system is scaled up from one chip to 32 chips, it performs more than 3.5 Million inferences/sec and the latency still stays very low at only 1.2 msec.This paper briefly introduces the IBM Telum chip and later describes the integrated AI accelerator. IBM Telum's AI accelerator architecture, microarchitecture, integration into the system stack, performance, and power are covered in detail.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1012–1028},
numpages = {17},
keywords = {z16, on-chip AI accelerator, low-latency in-transaction inference, enterprise workload AI, Telum, AI on server-class processor},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533043,
author = {Chen, Jian and Zhang, Xiaoyu and Wang, Tao and Zhang, Ying and Chen, Tao and Chen, Jiajun and Xie, Mingxu and Liu, Qiang},
title = {Fidas: fortifying the cloud via comprehensive FPGA-based offloading for intrusion detection: industrial product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533043},
doi = {10.1145/3470496.3533043},
abstract = {Network intrusion detection systems (IDS) are crucial for secure cloud computing, but they are also severely constrained by CPU computation capacity as the network bandwidth increases. Therefore, hardware offloading is essential for the IDS servers to support the ever-growing throughput demand for packet processing. Based on the experience of large-scale IDS deployment, we find the existing hardware offloading solutions have fundamental limitations that prevent them from being massively deployed in the production environment. In this paper, we present Fidas, an FPGA-based intrusion detection offload system that avoids the limitations of the existing hardware solutions by comprehensively offloading the primary NIC, rule pattern matching, and traffic flow rate classification. The pattern matching module in Fidas uses a multi-level filter-based approach for efficient regex processing, and the flow rate classification module employs a novel dual-stack memory scheme to identify the hot flows under volumetric attacks. Our evaluation shows that Fidas achieves the state-of-the-art throughput in pattern matching and flow rate classification while freeing up processors for other security-related functionalities. Fidas is deployed in the production data center and has been battle-tested for its performance, cost-effectiveness, and DevOps agility.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1029–1041},
numpages = {13},
keywords = {pattern matching, intrusion detection, flow rate classification, balanced offloading},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533044,
author = {Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu\u{g}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and Narayanan, Sundaram and Langman, Jack and Wilfong, Kevin and Rastogi, Harsha and Wu, Carole-Jean and Kozyrakis, Christos and Pol, Parik},
title = {Understanding data storage and ingestion for large-scale deep recommendation model training: industrial product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533044},
doi = {10.1145/3470496.3533044},
abstract = {Datacenter-scale AI training clusters consisting of thousands of domain-specific accelerators (DSA) are used to train increasingly-complex deep learning models. These clusters rely on a data storage and ingestion (DSI) pipeline, responsible for storing exabytes of training data and serving it at tens of terabytes per second. As DSAs continue to push training efficiency and throughput, the DSI pipeline is becoming the dominating factor that constrains the overall training performance and capacity. Innovations that improve the efficiency and performance of DSI systems and hardware are urgent, demanding a deep understanding of DSI characteristics and infrastructure at scale.This paper presents Meta's end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across geo-distributed datacenters via diverse and continuous training jobs. These training jobs read and heavily filter massive and evolving datasets, resulting in popular features and samples used across training jobs. We measure the intense network, memory, and compute resources required by each training job to preprocess samples during training. Finally, we synthesize key takeaways based on our production infrastructure characterization. These include identifying hardware bottlenecks, discussing opportunities for heterogeneous DSI hardware, motivating research in datacenter scheduling and benchmark datasets, and assimilating lessons learned in optimizing DSI infrastructure.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1042–1057},
numpages = {16},
keywords = {machine learning systems, distributed systems, databases, data storage, data ingestion},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533045,
author = {Lustig, Daniel and Cooksey, Simon and Giroux, Olivier},
title = {Mixed-proxy extensions for the NVIDIA PTX memory consistency model: industrial product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533045},
doi = {10.1145/3470496.3533045},
abstract = {In recent years, there has been a trend towards the use of accelerators and architectural specialization to continue scaling performance in spite of a slowing of Moore's Law. GPUs have always relied on dedicated hardware for graphics workloads, but modern GPUs now also incorporate compute-domain accelerators such as NVIDIA's Tensor Cores for machine learning. For these accelerators to be successfully integrated into a general-purpose programming language such as C++ or CUDA, there must be a forward- and backward-compatible API for the functionality they provide. To the extent that all of these accelerators interact with program threads through memory, they should be incorporated into the GPU's memory consistency model. Unfortunately, the use of accelerators and/or special non-coherent paths into memory produces non-standard memory behavior that existing GPU memory models cannot capture.In this work, we describe the "proxy" extensions added to version 7.5 of NVIDIA's PTX ISA for GPUs. A proxy is an extra tag abstractly applied to every memory or fence operation. Proxies generalize the notion of address translation and specialized non-coherent cache hierarchies into an abstraction that cleanly describes the resulting non-standard behavior. The goal of proxies is to facilitate integration of these specialized memory accesses into the general-purpose PTX programming model in a fully composable manner. This paper characterizes the behaviors that proxies can capture, the microarchitectural intuition behind them, the necessary updates to the formal memory model, and the tooling that we built in order to ensure that the resulting model both is sound and meets the needs of business-critical workloads that they are designed to support.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1058–1070},
numpages = {13},
keywords = {synchronization, memory ordering, memory consistency, GPU},
location = {New York, New York},
series = {ISCA '22}
}

