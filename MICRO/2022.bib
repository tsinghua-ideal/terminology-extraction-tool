@inproceedings{10.1109/MICRO56248.2022.00015,
author = {Bera, Rahul and Kanellopoulos, Konstantinos and Balachandran, Shankar and Novo, David and Olgun, Ataberk and Sadrosadati, Mohammad and Mutlu, Onur},
title = {Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00015},
doi = {10.1109/MICRO56248.2022.00015},
abstract = {Long-latency load requests continue to limit the performance of modern high-performance processors. To increase the latency tolerance of a processor, architects have primarily relied on two key techniques: sophisticated data prefetchers and large on-chip caches. In this work, we show that: (1) even a sophisticated state-of-the-art prefetcher can only predict half of the off-chip load requests on average across a wide range of workloads, and (2) due to the increasing size and complexity of on-chip caches, a large fraction of the latency of an off-chip load request is spent accessing the on-chip cache hierarchy to solely determine that it needs to go off-chip.The goal of this work is to accelerate off-chip load requests by removing the on-chip cache access latency from their critical path. To this end, we propose a new technique called Hermes, whose key idea is to: (1) accurately predict which load requests might go off-chip, and (2) speculatively fetch the data required by the predicted off-chip loads directly from the main memory, while also concurrently accessing the cache hierarchy for such loads.To enable Hermes, we develop a new lightweight, perceptron-based off-chip load prediction technique that learns to identify off-chip load requests using multiple program features (e.g., sequence of program counters, byte offset of a load request). For every load request generated by the processor, the predictor observes a set of program features to predict whether or not the load would go off-chip. If the load is predicted to go off-chip, Hermes issues a speculative load request directly to the main memory controller once the load's physical address is generated. If the prediction is correct, the load eventually misses the cache hierarchy and waits for the ongoing speculative load request to finish, and thus Hermes completely hides the on-chip cache hierarchy access latency from the critical path of the correctly-predicted off-chip load. Our extensive evaluation using a wide range of workloads shows that Hermes provides consistent performance improvement on top of a state-of-the-art baseline system across a wide range of configurations with varying core count, main memory bandwidth, high-performance data prefetchers, and on-chip cache hierarchy access latencies, while incurring only modest storage overhead. The source code of Hermes is freely available at: https://github.com/CMU-SAFARI/Hermes.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1–18},
numpages = {18},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00017,
author = {Khan, Tanvir Ahmed and Ugur, Muhammed and Nathella, Krishnendra and Sunwoo, Dam and Litz, Heiner and Jim\'{e}nez, Daniel A. and Kasikci, Baris},
title = {Whisper: Profile-Guided Branch Misprediction Elimination for Data Center Applications},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00017},
doi = {10.1109/MICRO56248.2022.00017},
abstract = {Modern data center applications experience frequent branch mispredictions - degrading performance, increasing cost, and reducing energy efficiency in data centers. Even the state-of-the-art branch predictor, TAGE-SC-L, suffers from an average branch Mispredictions Per Kilo Instructions (branch-MPKI) of 3.0 (0.5--7.2) for these applications since their large code footprints exhaust TAGE-SC-L's intended capacity.In this work, we propose Whisper, a novel profile-guided mechanism to avoid branch mispredictions. Whisper investigates the in-production profile of data center applications to identify precise program contexts that lead to branch mispredictions. Corresponding prediction hints are then inserted into code to strategically avoid those mispredictions during program execution. Whisper presents three novel profile-guided techniques: (1) hashed history correlation which efficiently encodes hard-to-predict correlations in branch history using lightweight Boolean formulas, (2) randomized formula testing which selects a locally-optimal Boolean formula from a randomly selected subset of possible formulas to predict a branch, and (3) the extension of Read-Once Monotone Boolean Formulas with Implication and Converse Non-Implication to improve the branch history coverage of these formulas with minimal overhead.We evaluate Whisper on 12 widely-used data center applications and demonstrate that Whisper enables traditional branch predictors to achieve a speedup close to that of an ideal branch predictor. Specifically, Whisper achieves an average speedup of 2.8\% (0.4\%-4.6\%) by reducing 16.8\% (1.7\%-32.4\%) of branch mispredictions over TAGE-SC-L and outperforms the state-of-the-art profile-guided branch prediction mechanisms by 7.9\% on average.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {19–34},
numpages = {16},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00018,
author = {Liu, Sihao and Weng, Jian and Kupsh, Dylan and Sohrabizadeh, Atefeh and Wang, Zhengrong and Guo, Licheng and Liu, Jiuyang and Zhulin, Maxim and Mani, Rishabh and Zhang, Lucheng and Cong, Jason and Nowatzki, Tony},
title = {OverGen: Improving FPGA Usability through Domain-Specific Overlay Generation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00018},
doi = {10.1109/MICRO56248.2022.00018},
abstract = {FPGAs have been proven to be powerful computational accelerators across many types of workloads. The mainstream programming approach is high level synthesis (HLS), which maps high-level languages (e.g. C + #pragmas) to hardware. Unfortunately, HLS leaves a significant programmability gap in terms of reconfigurability, customization and versatility: Although HLS compilation is fast, the downstream physical design takes hours to days; FPGA reconfiguration time limits the time-multiplexing ability of hardware, and tools do not reason about cross-workload flexibility. Overlay architectures mitigate the above by mapping a programmable design (e.g. CPU, GPU, etc.) on top of FPGAs. However, the abstraction gap between overlay and FPGA leads to low efficiency/utilization.Our essential idea is to develop a hardware generation framework targeting a highly-customizable overlay, so that the abstraction gap can be lowered by tuning the design instance to applications of interest. We leverage and extend prior work on customizable spatial architectures, SoC generation, accelerator compilers, and design space explorers to create an end-to-end FPGA acceleration system. Our novel techniques address inefficient networks between on-chip memories and processing elements, as well as improving DSE by reducing the amount of recompilation required.Our framework, OverGen, is highly competitive with fixed-function HLS-based designs, even though the generated designs are programmable with fast reconfiguration. We compared to a state-of-the-art DSE-based HLS framework, AutoDSE. Without kernel-tuning for AutoDSE, OverGen gets 1.2\texttimes{} geomean performance, and even with manual kernel-tuning for the baseline, OverGen still gets 0.55\texttimes{} geomean performance - all while providing runtime flexibility across workloads.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {35–56},
numpages = {22},
keywords = {reconfigurable architectures, domain-specific accelerators, FPGA, CGRA, design automation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00016,
author = {Hao, Yifan and Zhao, Yongwei and Liu, Chenxiao and Du, Zidong and Cheng, Shuyao and Li, Xiaqing and Hu, Xing and Guo, Qi and Xu, Zhiwei and Chen, Tianshi},
title = {Cambricon-P: A Bitflow Architecture for Arbitrary Precision Computing},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00016},
doi = {10.1109/MICRO56248.2022.00016},
abstract = {Arbitrary precision computing (APC), where the digits vary from tens to millions of bits, is fundamental for scientific applications, such as mathematics, physics, chemistry, and biology. APC on existing platforms (e.g., CPUs and GPUs) is achieved by decomposing the original data into small pieces to accommodate to the low-bitwidth (e.g., 32-/64-bit) functional units. However, such fine-grained decomposition inevitably introduces large amounts of intermediates, bringing in intensive on-chip data traffic and long, complex dependency chains, so that causing low hardware utilization.To address this issue, we propose Cambricon-P, a bitflow architecture supporting monolithic large and flexible bitwidth operations for efficient APC processing, which avoids generating large amounts of intermediates from decomposition. Cambricon-P features a tightly-integrated computational architecture for processing different bitflows in parallel, where full bit-serial data paths are deployed. The bit-serial scheme still needs to eliminate the dependency chain of APC for exploiting parallelism within one monolithic large-bitwidth operation. For this purpose, Cambricon-P adopts a carry parallel computing mechanism, which enables recursively transforming the multiplication into smaller inner-products that can be performed in parallel between bit-indexed IPUs (Inner-Product Units). Furthermore, to improve the computing efficiency of APC, Cambricon-P employs a bit-indexed inner-product processing scheme, namely BIPS, to eliminate intra-IPU bit-level redundancy. Compared to Intel Xeon 6134 CPU, Cambricon-P achieves 100.98\texttimes{} performance on monolithic long multiplication, and 23.41\texttimes{}/30.16\texttimes{} speedup and energy benefit over four real-world APC applications on average. Compared to NVidia V100 GPU, Cambricon-P also delivers the same throughput, as well as 430\texttimes{}/60.5\texttimes{} lesser area and power, respectively, on batch-processing multiplications.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {57–72},
numpages = {16},
keywords = {arbitrary precision, bitflow architecture, bit-serial},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00020,
author = {Manzhosov, Evgeny and Hastings, Adam and Pancholi, Meghna and Piersma, Ryan and Ziad, Mohamed Tarek Ibn and Sethumadhavan, Simha},
title = {Revisiting Residue Codes for Modern Memories},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00020},
doi = {10.1109/MICRO56248.2022.00020},
abstract = {Residue codes have been traditionally used for compute error correction rather than storage error correction. In this paper, we use these codes for storage error correction with surprising results. We find that adapting residue codes to modern memory systems offers a level of error correction comparable to traditional schemes such as Reed-Solomon with fewer bits of storage. For instance, our adaptation of residue code - MUSE ECC - can offer ChipKill protection using approximately 30\% fewer bits. We show that the storage gains can be used to hold metadata needed for emerging security functionality such as memory tagging or to provide better detection capabilities against Rowhammer attacks. Our evaluation shows that memory tagging in a MUSE-enabled system shows a 12\% reduction in memory bandwidth utilization while providing the same level of error correction as a traditional ECC baseline without a noticeable loss of performance. Thus, our work demonstrates a new, flexible primitive for co-designing reliability with security and performance.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {73–90},
numpages = {18},
keywords = {error correcting codes, memory tagging, metadata, rowhammer},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00021,
author = {Rajat, Rachit and Wang, Yongqin and Annavaram, Murali},
title = {PageORAM : An Efficient DRAM Page Aware ORAM Strategy},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00021},
doi = {10.1109/MICRO56248.2022.00021},
abstract = {Leaking memory access addresses can significantly empower an adversary in several computing usage scenarios - from key extraction to disclosing private information. Oblivious RAM has been proposed as a solution to this problem. Oblivious RAM involves reading multiple blocks instead of a single block for each memory access and changing the address of the data block being read after each access. State-of-the-art ORAMs (PathORAM and RingORAM) consider a tree-based structure for storing the data. However, ORAM designs pay a performance penalty. One reason is the strict requirement to evict stash blocks on a particular path that was previously read from. In tree-based ORAMs, each memory block is assigned a random path number, and when accessing a single block, one must fetch all the blocks in that path. Once the path is fetched into a client, the computation on the block is performed and that block is assigned a new random path number. All the blocks that were fetched into the client should be evicted to the memory. However, the eviction process must place the unmodified blocks on the same path that the prior read has fetched data from. This eviction requirement may cause a block not to be placed back in the ORAM tree due to limited space on a given tree node. As a result, the client must temporarily hold the block in its stash, which is a secure storage. Every fetch request for a block must search the stash before issuing a request to the ORAM. As the stash size grows, the stash search process becomes a substantial latency hurdle. On the other hand, if the stash is small then the client has to issue dummy reads which are useless reads in the tree for the sole purpose of creating more opportunities to place the stash data back in the tree. An alternate approach used in prior works is to embed dummy data blocks to create large bucket sizes at each tree level to enable better stash eviction probability. Neither of the above two solutions is palatable in practice. Dummy reads increase memory access latency, while dummy blocks increase the fetch bandwidth to bring large buckets from each level into the stash. Furthermore, dummy blocks also decrease the effective memory size available.To solve this problem we propose PageORAM, a novel block eviction and placement strategy. PageORAM makes the critical observation that DRAM is accessed at the granularity of a page (also referred to as row buffer), which is at least an order magnitude larger than the tree node size. Thus, a page may hold data blocks from multiple sub-trees. Hence, when fetching a path, PageORAM fetches a few additional sub-paths from the tree that are already present in an open DRAM page. These additional fetches vastly increase stash eviction options by opening up exponentially more data block placement choices. Thus, PageORAM enables a dramatic reduction in stash size without increasing page access counts in DRAM. While this observation may be counter-intuitive, we note that PageORAM reduces the overall bandwidth even after accounting for the increased fetches along the sub-paths. The reason is that by vastly improving stash block placement possibilities, PageORAM can significantly reduce the bucket size of the tree.Our implementation of PageORAM demonstrates an order of magnitude slower stash growth, increased bucket occupancy with useful data, and correspondingly improved memory access latency and reduced memory bandwidth. In our experiments, we find that PageORAM can either reduce the memory space requirement of the tree-based ORAMs by up to 40\% compared to baseline tree-based ORAM or give a performance improvement of up to 7.8x for the same structured tree-based ORAM.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {91–107},
numpages = {17},
keywords = {memory, ORAM, security, memory access pattern},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00022,
author = {Saxena, Anish and Saileshwar, Gururaj and Nair, Prashant J. and Qureshi, Moinuddin},
title = {AQUA: Scalable Rowhammer Mitigation by Quarantining Aggressor Rows at Runtime},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00022},
doi = {10.1109/MICRO56248.2022.00022},
abstract = {Rowhammer allows an attacker to induce bit flips in a row by rapidly accessing neighboring rows. Rowhammer is a severe security threat as it can be used to escalate privilege or break confidentiality. Moreover, the threshold of activations needed to induce Rowhammer continues to reduce and new attacks like Half-Double break existing solutions that refresh victim rows. The recently proposed Randomized Row-Swap (RRS) scheme is resilient to Half-Double as it provides mitigation by swapping an aggressor row with a random row. However, to ensure security, the threshold for triggering a row-swap must be set much lower than the Rowhammer threshold, leading to a significant performance loss of 20\% on average, at a Rowhammer threshold of 1K. Furthermore, the SRAM overhead for storing the indirection table of RRS becomes prohibitively large - 2.4MB per rank at a Rowhammer threshold of 1K. Our goal is to develop a scalable Rowhammer mitigation that incurs negligible performance and storage overheads.To this end, we propose AQUA, a Rowhammer mitigation that breaks the spatial correlation between aggressor and victim rows by dynamically quarantining the aggressor row in a dedicated region of memory. AQUA allows for an effective row migration threshold much higher than in RRS, leading to an order of magnitude less slowdown and SRAM. As the security of AQUA is not reliant on keeping the destination row a secret, we further reduce the SRAM overheads of the indirection table by storing it in DRAM, and accessing it on-demand. We derive the size of the quarantine region required to ensure security for AQUA and show that reserving about 1\% of DRAM is sufficient to mitigate Rowhammer at a threshold of 1K. Our evaluations show that AQUA incurs an average slowdown of 2\% and an SRAM overhead (for mapping and migration) of only 41KB per rank at a Rowhammer threshold of 1K.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {108–123},
numpages = {16},
keywords = {DRAM, security, rowhammer, isolation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00019,
author = {Jiang, Jianyu and Qi, Ji and Shen, Tianxiang and Chen, Xusheng and Zhao, Shixiong and Wang, Sen and Chen, Li and Zhang, Gong and Luo, Xiapu and Cui, Heming},
title = {Cronus: Fault-Isolated, Secure and High-Performance Heterogeneous Computing for Trusted Execution Environment},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00019},
doi = {10.1109/MICRO56248.2022.00019},
abstract = {With the trend of processing a large volume of sensitive data on PaaS services (e.g., DNN training), a TEE architecture that supports general heterogeneous accelerators, enables spatial sharing on one accelerator, and enforces strong isolation across accelerators is highly desirable. However, none of the existing TEE solutions meet all three requirements.In this paper, we propose Cronus, the first TEE architecture that achieves the three crucial requirements. The key idea of Cronus is to partition heterogeneous computation into isolated TEE enclaves, where each enclave encapsulates only one kind of computation (e.g., GPU computation), and multiple enclaves can spatially share an accelerator. Then, Cronus constructs heterogeneous computing using remote procedure calls (RPCs) among enclaves. With Cronus, each accelerator's hardware and its software stack are strongly isolated from others', and each enclave trusts only its own hardware. To tackle the security challenge caused by inter-enclave interactions, we design a new streaming remote procedure call abstraction to enable secure RPCs with high performance. Cronus is software-based, making it general to diverse accelerators. We implemented Cronus on ARM TrustZone. Evaluation on diverse workloads with CPUs, GPUs and NPUs shows that, Cronus achieves less than 7.1\% extra computation time compared to native (unprotected) executions.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {124–143},
numpages = {20},
keywords = {ARM TrustZone, trusted execution environment, accelerator, GPU, fault isolation, security isolation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00023,
author = {Jeong, Ipoom and Lee, Jiwon and Yoon, Myung Kuk and Ro, Won Woo},
title = {Reconstructing Out-of-Order Issue Queue},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00023},
doi = {10.1109/MICRO56248.2022.00023},
abstract = {Out-of-order cores provide high performance at the cost of energy efficiency. Dynamic scheduling is one of the major contributors to this: generating highly optimized issue schedules considering both data dependences and underlying execution resources, but relying heavily on complex wakeup and select operations of an out-of-order issue queue (IQ). For decades, researchers have proposed several complexity-effective dynamic scheduling schemes by leveraging the energy efficiency of an in-order IQ. However, they are either costly or not capable of delivering sufficient performance to substitute for a conventional wide-issue out-of-order IQ.In this work, we revisit two previous designs: one classical dependence-based design and the other state-of-the-art readiness-based design. We observe that they are complementary to each other, and thus their synergistic integration has the potential to be a good alternative to an out-of-order IQ. We first combine these two designs, and further analyze the main architectural bottlenecks that incur the underutilization of aggregate issue capability, thereby limiting the exploitation of instruction-level and memory-level parallelisms: 1) memory dependences not exposed by the register-based dependence analysis and 2) wide and shallow nature of dynamic dependence chains due to the long-latency memory accesses. To this end, we propose Ballerino, a novel microarchitecture that performs balanced and cache-miss-tolerable dynamic scheduling via a complementary combination of cascaded and clustered in-order IQs. Ballerino is built upon three key functionalities: 1) speculatively filtering out ready-at-dispatch instructions, 2) eliminating wasteful wakeup operations via a simple steering technique leveraging the awareness of memory dependences, and 3) reacting to program phase changes by allowing different load-dependent chains to share a single IQ while guaranteeing their out-of-order issue. The net effect is minimal scheduling energy consumption per instruction while providing comparable scheduling performance to a fully out-of-order IQ. In our analysis, Ballerino achieves comparable performance to an 8-wide out-of-order core by using twelve in-order IQs, improving core-wide energy efficiency by 20\%.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {144–161},
numpages = {18},
keywords = {dynamic scheduling, data dependence, steering},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00024,
author = {Moody, Logan and Qi, Wei and Sharifi, Abdolrasoul and Berry, Layne and Rudek, Joey and Gaur, Jayesh and Parkhurst, Jeff and Subramoney, Sreenivas and Skadron, Kevin and Venkat, Ashish},
title = {Speculative Code Compaction: Eliminating Dead Code via Speculative Microcode Transformations},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00024},
doi = {10.1109/MICRO56248.2022.00024},
abstract = {The computing landscape has been increasingly characterized by processor architectures with increasing core counts, while a majority of the software applications remain inherently sequential. Although state-of-the-art compilers feature sophisticated optimizations, a significant chunk of wasteful computation persists due to the presence of data-dependent operations and irregular control-flow patterns that are unpredictable at compile-time. This work presents speculative code compaction (SCC), a novel microarchitectural technique that significantly enhances the capabilities of the microcode engine to aggressively and speculatively eliminate dead code from hot code regions resident in the micro-op cache, and further generate a compact stream of micro-ops, based on dynamically predicted machine code invariants. SCC also extends existing micro-op cache designs to co-host multiple versions of unoptimized and speculatively optimized micro-op sequences, providing the fetch engine with significant flexibility to dynamically choose from and stream the appropriate set of micro-ops, as and when deemed profitable.SCC is a minimally-invasive technique that can be implemented at the processor front-end using a simple ALU and a register context table, and is yet able to substantially accelerate the performance of already compile-time optimized and machine-tuned code by an average of 6\% (and as much as 30\%), with an average of 12\% (and as much as 24\%) savings in energy consumption, while eliminating the need for profiling and offering increased adaptability to changing datasets and workload patterns.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {162–180},
numpages = {19},
keywords = {microarchitecture, speculation, optimization},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00025,
author = {Ta, Tuan and Al-Hawaj, Khalid and Cebry, Nick and Ou, Yanghui and Hall, Eric and Golden, Courtney and Batten, Christopher},
title = {big.VLITTLE: On-Demand Data-Parallel Acceleration for Mobile Systems on Chip},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00025},
doi = {10.1109/MICRO56248.2022.00025},
abstract = {Single-ISA heterogeneous multi-core architectures offer a compelling high-performance and high-efficiency solution to executing task-parallel workloads in mobile systems on chip (SoCs). In addition to task-parallel workloads, many data-parallel applications, such as machine learning, computer vision, and data analytics, increasingly run on mobile SoCs to provide real-time user interactions. Next-generation scalable vector architectures, such as the RISC-V Vector Extension and Arm SVE, have recently emerged as unified vector abstractions for both large- and small-scale systems. In this paper, we propose novel area-efficient high-performance architectures called big.VLITTLE that support next-generation vector architectures to efficiently accelerate data-parallel workloads in conventional big.LITTLE systems. big.VLITTLE architectures reconfigure multiple little cores on demand to work as a decoupled vector engine when executing data-parallel workloads. Our results show that a big.VLITTLE system can achieve 1.6\texttimes{} performance speedup over an area-comparable big.LITTLE system equipped with an integrated vector unit across multiple data-parallel applications and 1.7\texttimes{} speedup compared to an aggressive decoupled vector engine for task-parallel workloads.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {181–198},
numpages = {18},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00026,
author = {Singh, Sawan and Perais, Arthur and Jimborean, Alexandra and Ros, Alberto},
title = {Exploring Instruction Fusion Opportunities in General Purpose Processors},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00026},
doi = {10.1109/MICRO56248.2022.00026},
abstract = {The Complex Instruction Set Computer (CISC) paradigm has led to the introduction of instruction cracking in which an architectural instruction is divided into multiple microarchitectural instructions (μ-ops). However, the dual concept, instruction fusion is also prevalent in modern microarchitectures to maximize resource utilization. In essence, some architectural instructions are too complex to be executed as a unit, so they should be cracked, while others are too simple to waste resources on executing them as a unit, so they should be fused with others.In this paper, we focus on instruction fusion and explore opportunities for fusing additional instructions in a high-performance general purpose pipeline. We show that enabling fusion for common RISC-V idioms improves performance by 7\%. Then, we determine experimentally that enabling fusion only for memory instructions achieves 86\% of the potential of fusion in this particular case. Finally, we propose the Helios microarchitecture, able to fuse non-consecutive and noncontiguous memory instructions, and discuss microarchitectural changes required to do so efficiently while preserving correctness. Helios allows to fuse an additional 5.5\% of dynamic instructions, yielding a 14.2\% performance uplift over no fusion (8.2\% over baseline fusion).},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {199–212},
numpages = {14},
keywords = {general purpose, microarchitecture, instruction fusion},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00028,
author = {Joseph, Diya and Arag\'{o}n, Juan L. and Parcerisa, Joan-Manuel and Gonzalez, Antonio},
title = {DTexL: Decoupled Raster Pipeline for Texture Locality},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00028},
doi = {10.1109/MICRO56248.2022.00028},
abstract = {Contemporary GPU architectures have multiple shader cores and a scheduler that distributes work (threads) among them, focusing on load balancing. These load balancing techniques favor thread distributions that are detrimental to texture memory locality for graphics applications in the L1 Texture Caches. Texture memory accesses make up the majority of the traffic to the memory hierarchy in typical low power graphics architectures. This paper focuses on improving the L1 Texture cache locality by focusing on a new workload scheduler by exploring various methods to group the threads, assign the groups to shader cores and also to reorder threads without violating the correctness of the pipeline. To overcome the resulting load imbalance, we also propose a minor modification in the GPU architecture that helps translate the improvement in cache locality to an improvement in the GPU's performance. We propose DTexL that envelops these ideas and evaluate it over a benchmark suite of ten commercial games, to obtain a 46.8\% decrease in L2 Accesses, a 19.3\% increase in performance and a 6.3\% decrease in total GPU energy. All this with a negligible overhead.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {213–227},
numpages = {15},
keywords = {GPU, caches, graphics, scheduling, texture locality, low-power},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00029,
author = {Darabi, Sina and Sadrosadati, Mohammad and Akbarzadeh, Negar and Lindegger, Jo\"{e}l and Hosseini, Mohammad and Hosseini, Mohammad and Park, Jisung and G\'{o}mez-Luna, Juan and Mutlu, Onur and Sarbazi-Azad, Hamid},
title = {Morpheus: Extending the Last Level Cache Capacity in GPU Systems Using Idle GPU Core Resources},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00029},
doi = {10.1109/MICRO56248.2022.00029},
abstract = {Graphics Processing Units (GPUs) are widely-used accelerators for data-parallel applications. In many GPU applications, GPU memory bandwidth bottlenecks performance, causing underutilization of GPU cores. Hence, disabling many cores does not affect the performance of memory-bound workloads. While simply power-gating unused GPU cores would save energy, prior works attempt to better utilize GPU cores for other applications (ideally compute-bound), which increases the GPU's total throughput.In this paper, we introduce Morpheus, a new hardware/software co-designed technique to boost the performance of memory-bound applications. The key idea of Morpheus is to exploit unused core resources to extend the GPU last level cache (LLC) capacity. In Morpheus, each GPU core has two execution modes: compute mode and cache mode. Cores in compute mode operate conventionally and run application threads. However, for the cores in cache mode, Morpheus invokes a software helper kernel that uses the cores' on-chip memories (i.e., register file, shared memory, and L1) in a way that extends the LLC capacity for a running memory-bound workload. Morpheus adds a controller to the GPU hardware to forward LLC requests to either the conventional LLC (managed by hardware) or the extended LLC (managed by the helper kernel). Our experimental results show that Morpheus improves the performance and energy efficiency of a baseline GPU architecture by an average of 39\% and 58\%, respectively, across several memory-bound workloads. Morpheus' performance is within 3\% of a GPU design that has a quadruple-sized conventional LLC. Morpheus can thus contribute to reducing the hardware dedicated to a conventional LLC by exploiting idle cores' on-chip memory resources as additional cache capacity.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {228–244},
numpages = {17},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00030,
author = {Zhang, Yida and Jung, Changhee},
title = {Featherweight Soft Error Resilience for GPUs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00030},
doi = {10.1109/MICRO56248.2022.00030},
abstract = {This paper presents Flame, a hardware/software co-designed resilience scheme for protecting GPUs against soft errors. For low-cost yet high-performance resilience, Flame uses acoustic sensors and idempotent processing for error detection and recovery, respectively. That is, Flame seeks to correct any sensor-detected errors by re-executing the idempotent region where they occurred. To achieve this, it is essential for each idempotent region to ensure the absence of errors before moving on to the next region. This is so-called soft error verification that takes sensors' worst-case detection latency (WCDL) to verify each region finished. Rather than waiting for WCDL at each region end, which incurs too much performance overhead, Flame proposes WCDL-aware warp scheduling that can hide the error verification delay (i.e., WCDL) with GPU's inherent massive warp-level parallelism. When a warp hits each idempotent region boundary, Flame deschedules the warp and switches to one of the other ready warps---as if the region boundary were a regular long-latency operation triggering the warp switching. By leveraging GPU's inherent ability for the latency hiding, Flame can completely eliminate the verification delay without significant hardware modification. The experimental results demonstrate that the performance overhead of Flame is near zero, i.e., 0.6\% on average for 34 GPU benchmark applications.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {245–262},
numpages = {18},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00027,
author = {Saed, Mohammadreza and Chou, Yuan Hsi and Liu, Lufei and Nowicki, Tyler and Aamodt, Tor M.},
title = {Vulkan-Sim: A GPU Architecture Simulator for Ray Tracing},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00027},
doi = {10.1109/MICRO56248.2022.00027},
abstract = {Ray tracing can generate photorealistic images with more convincing visual effects compared to rasterization. Recent hardware advances have enabled ray tracing to be applied in real-time. Current GPUs feature a dedicated ray tracing acceleration unit, and game developers have started to make use of ray tracing APIs to bring more realistic graphics to their players. Industry cooperatively contributed to Vulkan, which recently introduced an open-standard API for ray tracing. However, little has been disclosed about the mapping of this API to hardware. In this paper, we introduce Vulkan-Sim, a detailed cycle-level simulator for enabling architecture research for ray tracing. We extend GPGPU-Sim, integrating it with Mesa, an open-source graphics library to support the Vulkan API, and add dedicated ray traversal and intersection units. We also demonstrate an explicit mapping of the Vulkan ray tracing pipeline to a modern GPU using a technique we call delayed intersection and any-hit execution. Additionally we evaluate several ray tracing workloads with Vulkan-Sim, identifying bottlenecks and inefficiencies of the ray tracing hardware we model. To demonstrate the utility of Vulkan-Sim we conduct two case studies evaluating techniques recently proposed or deployed by industry targeting enhanced ray tracing performance.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {263–281},
numpages = {19},
keywords = {GPU, ray tracing, computer graphics, modeling and simulation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00031,
author = {Ying, Ziyu and Zhao, Shulin and Bhuyan, Sandeepa and Mishra, Cyan Subhra and Kandemir, Mahmut T. and Das, Chita R.},
title = {Pushing Point Cloud Compression to the Edge},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00031},
doi = {10.1109/MICRO56248.2022.00031},
abstract = {As Point Clouds (PCs) gain popularity in processing millions of data points for 3D rendering in many applications, efficient data compression becomes a critical issue. This is because compression is the primary bottleneck in minimizing the latency and energy consumption of existing PC pipelines. Data compression becomes even more critical as PC processing is pushed to edge devices with limited compute and power budgets. In this paper, we propose and evaluate two complementary schemes, intra-frame compression and inter-frame compression, to speed up the PC compression, without losing much quality or compression efficiency. Unlike existing techniques that use sequential algorithms, our first design, intra-frame compression, exploits parallelism for boosting the performance of both geometry and attribute compression. The proposed parallelism brings around 43.7\texttimes{} performance improvement and 96.6\% energy savings at a cost of 1.01\texttimes{} larger compressed data size. To further improve the compression efficiency, our second scheme, inter-frame compression, considers the temporal similarity among the video frames and reuses the attribute data from the previous frame for the current frame. We implement our designs on an NVIDIA Jetson AGX Xavier edge GPU board. Experimental results with six videos show that the combined compression schemes provide 34.0\texttimes{} speedup compared to a state-of-the-art scheme, with minimal impact on quality and compression ratio.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {282–299},
numpages = {18},
keywords = {point cloud compression, edge computing, video processing, energy-efficiency},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00033,
author = {Krishnan, Srivatsan and Wan, Zishen and Bhardwaj, Kshitij and Whatmough, Paul and Faust, Aleksandra and Neuman, Sabrina and Wei, Gu-Yeon and Brooks, David and Reddi, Vijay Janapa},
title = {Automatic Domain-Specific SoC Design for Autonomous Unmanned Aerial Vehicles},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00033},
doi = {10.1109/MICRO56248.2022.00033},
abstract = {Building domain-specific accelerators is becoming increasingly paramount to meet the high-performance requirements under stringent power and real-time constraints. However, emerging application domains like autonomous vehicles are complex systems with constraints extending beyond the computing stack. Manually selecting and navigating the design space to design custom and efficient domain-specific SoCs (DSSoC) is tedious and expensive. Hence, there is a need for automated DSSoC design methodologies. In this paper, we use agile and autonomous UAVs as a case study to understand how to automate domain-specific SoCs design for autonomous vehicles. Architecting a UAV DSSoC requires consideration of parameters such as sensor rate, compute throughput, and other physical characteristics (e.g., payload weight, thrust-to-weight ratio) that affect overall performance. Iterating over several component choices results in a combinatorial explosion of the number of possible combinations: from tens of thousands to billions, depending on implementation details. To navigate the DSSoC design space efficiently, we introduce AutoPilot, a systematic methodology for automatically designing DSSoC for autonomous UAVs. AutoPilot uses machine learning to navigate the large DSSoC design space and automatically select a combination of autonomy algorithm and hardware accelerator while considering the cross-product effect across different UAV components. AutoPilot consistently outperforms general-purpose hardware selections like Xavier NX and Jetson TX2, as well as dedicated hardware accelerators built for autonomous UAVs. DSSoC designs generated by AutoPilot increase the number of missions on average by up to 2.25\texttimes{}, 1.62\texttimes{}, and 1.43\texttimes{} for nano, micro, and mini-UAVs, respectively, over baselines. Further, we discuss the potential application of AutoPilot methodology to other related autonomous vehicles.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {300–317},
numpages = {18},
keywords = {robotics, domain-specific architectures, ML for systems, autonomous machines, IoT and edge computing, mobile systems},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00034,
author = {Ruppel, Emily and Surbatovich, Milijana and Desai, Harsh and Maeng, Kiwan and Lucia, Brandon},
title = {An Architectural Charge Management Interface for Energy-Harvesting Systems},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00034},
doi = {10.1109/MICRO56248.2022.00034},
abstract = {Energy-harvesting devices eliminate batteries, instead collecting their operating energy from environmental sources. A device stores energy into a capacitor, drawing energy to perform tasks and powering off to recharge when the energy is exhausted. State-of-the-art charge management systems for these devices aim to avoid power failure during task execution by reasoning about task energy cost. We identify that the innate equivalent series resistance (ESR) in energy storage capacitors breaks energy-based systems' guarantees; running high current load on a high-ESR capacitor causes a substantial voltage drop that rebounds once the load is removed. This voltage drop is disregarded by systems that only reason about energy. If the drop lowers the voltage below the system's operating threshold, however, the device powers off while stored energy remains. Though ESR is well understood in hardware design, this is the first work to argue that software for batteryless devices must also be aware of ESR.This work presents Culpeo, a hardware/software mechanism and architectural interface to relay the effect of ESR in the power system to software. We develop static and dynamic implementations of Culpeo and demonstrate on real batteryless devices that considering ESR restores correctness guarantees broken by energy-only charge management. We then demonstrate how to integrate Culpeo's safe voltage into state-of-the-art schedulers, restoring task deadline guarantees for applications with predictable energy harvesting. Finally, we propose an on-chip Culpeo hardware implementation that allows for runtime monitoring of the effects of ESR to respond to changes in harvestable power.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {318–335},
numpages = {18},
keywords = {intermittent computing, energy-harvesting power system, equivalent series resistance},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00032,
author = {Guan, Xiuxian and Sun, Zekai and Deng, Shengliang and Chen, Xusheng and Zhao, Shixiong and Zhang, Zongyuan and Duan, Tianyang and Wang, Yuexuan and Wu, Chenshu and Cui, Yong and Zhang, Libo and Wu, Yanjun and Wang, Rui and Cui, Heming},
title = {ROG: A High Performance and Robust Distributed Training System for Robotic IoT},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00032},
doi = {10.1109/MICRO56248.2022.00032},
abstract = {Critical robotic tasks such as rescue and disaster response are more prevalently leveraging ML (Machine Learning) models deployed on a team of wireless robots, on which data parallel (DP) training over Internet of Things of these robots (robotic IoT) can harness the distributed hardware resources to adapt their models to changing environments as soon as possible. Unfortunately, due to the need for DP synchronization across all robots, the instability in wireless networks (i.e., fluctuating bandwidth due to occlusion and varying communication distance) often leads to severe stall of robots, which affects the training accuracy within a tight time budget and wastes energy stalling. Existing methods to cope with the instability of datacenter networks are incapable of handling such straggler effect. That is because they are conducting model-granulated transmission scheduling, which is much more coarse-grained than the granularity of transient network instability in real-world robotic IoT networks, making a previously reached schedule mismatch with the varying bandwidth during transmission.We present Rog, the first ROw-Granulated distributed training system optimized for ML training over unstable wireless networks. Rog confines the granularity of transmission and synchronization to each row of a layer's parameters and schedules the transmission of each row adaptively to the fluctuating bandwidth. In this way the ML training process can update partial and the most important gradients of a stale robot to avoid triggering stalls, while provably guaranteeing convergence. The evaluation shows that, given the same training time, Rog achieved about 4.9\%~6.5\% training accuracy gain compared with the baselines and saved 20.4\%~50.7\% of the energy to achieve the same training accuracy.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {336–353},
numpages = {18},
keywords = {distributed training, wireless networks, training throughput, robust, energy efficient},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00035,
author = {Zou, Chen and Chien, Andrew A.},
title = {ASSASIN: Architecture Support for Stream Computing to Accelerate Computational Storage},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00035},
doi = {10.1109/MICRO56248.2022.00035},
abstract = {Computational storage adds computing to storage devices, providing potential benefits in offload, data-reduction, and lower energy. Successful computational SSD architectures should match growing flash bandwidth, which in turn requires high SSD DRAM memory bandwidth. This creates a memory wall scaling problem, resulting from SSDs' stringent power and cost constraints.A survey of recent computational SSD research shows that many computational storage offloads are suited to stream computing. To exploit this opportunity, we propose a novel general-purpose computational SSD and core architecture, called ASSASIN (Architecture Support for Stream computing to Accelerate computatIoNal Storage). ASSASIN provides a unified set of compute engines between SSD DRAM and the flash array. This eliminates the SSD DRAM bottleneck by enabling direct computing on flash data streams. ASSASIN further employs a crossbar to achieve performance even when flash data layout is uneven and preserve independence for page layout decisions in the flash translation layer. With stream buffers and scratchpad memories, ASSASIN core's memory hierarchy and instruction set extensions provide superior low-latency access at low-power and effectively keep streaming flash data out of the in-SSD cache-DRAM memory hierarchy, thereby solving the memory wall.Evaluation shows that ASSASIN delivers 1.5x - 2.4x speedup for offloaded functions compared to state-of-the-art computational SSD architectures. Further, ASSASIN's streaming approach yields 2.0x power efficiency and 3.2x area efficiency improvement. And these performance benefits at the level of computational SSDs translate to 1.1x - 1.5x end-to-end speedups on data analytics workloads.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {354–368},
numpages = {15},
keywords = {computational storage, ssd, stream computing, memory hierarhcy, memory wall, general-purpose},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00037,
author = {Alverti, Chloe and Karakostas, Vasileios and Kunati, Nikhita and Goumas, Georgios and Swift, Michael},
title = {DaxVM: Stressing the Limits of Memory as a File Interface},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00037},
doi = {10.1109/MICRO56248.2022.00037},
abstract = {Persistent memory (PMem) is a low-latency storage technology connected to the processor memory bus. The Direct Access (DAX) interface promises fast access to PMem, mapping it directly to processes' virtual address spaces. However, virtual memory operations (e.g., paging) limit its performance and scalability. Through an analysis of Linux/x86 memory mapping, we find that current systems fall short of what hardware can provide due to numerous software inefficiencies stemming from OS assumptions that memory mapping is for DRAM.In this paper we propose DaxVM, a design that extends the OS virtual memory and file system layers leveraging persistent memory attributes to provide a fast and scalable DAX-mmap interface. DaxVM eliminates paging costs through pre-populated file page tables, supports faster and scalable virtual address space management for ephemeral mappings, performs unmappings asynchronously, bypasses kernel-space dirty-page tracking support, and adopts asynchronous block pre-zeroing. We implement DaxVM in Linux and the ext4 file system targeting x86-64 architecture. DaxVM mmap achieves 4.9\texttimes{} higher throughput than default mmap for the Apache webserver and up to 1.5\texttimes{} better performance than read system calls. It provides similar benefits for text search. It also provides fast boot times and up to 2.95\texttimes{} better throughput than default mmap for PMem-optimized key-value stores running on a fragmented ext4 image. Despite designed for direct access to byte-addressable storage, various aspects of DaxVM are relevant for efficient access to other high performant storage mediums.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {369–387},
numpages = {19},
keywords = {virtual memory, persistent memory, file systems},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00038,
author = {Kim, Jiho and Kang, Seokwon and Park, Yongjun and Kim, John},
title = {Networked SSD: Flash Memory Interconnection Network for High-Bandwidth SSD},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00038},
doi = {10.1109/MICRO56248.2022.00038},
abstract = {As the flash memory performance increases with more bandwidth, the flash memory channel or the interconnect is becoming a bigger bottleneck to enable high performance SSD system. However, the bandwidth of the flash memory interconnect is not increasing at the same rate as the flash memory. In addition, current flash memory bus is based on dedicated signaling where separate control signals are used for communication between the flash channel controller and the flash memory chip. In this work, we propose to exploit packetized communication to improve the effective flash memory interconnect bandwidth and propose packetized SSD (pSSD) system architecture. We first show how packetized communication can be exploited and the microarchitectural changes required. We then propose the Omnibus topology for flash memory interconnect to enable a packetized network SSD (pnSSD) among the flash memory - a 2D bus-based organization that maintains a "bus" organization for the interconnect while enabling direct communication between the flash memory chips. The pnSSD architecture enables a new type of garbage collection that we refer to as spatial garbage collection that significantly reduces the interference between I/O requests and garbage collection. Our detailed evaluation of pnSSD shows 82\% improvement in I/O latency with no garbage collection (GC) while improving I/O latency by 9.71\texttimes{} when GC occurs in parallel with I/O operation, through spatial garbage collection.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {388–403},
numpages = {16},
keywords = {solid state drive, interconnection networks, garbage collection},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00036,
author = {B, Pratheek and Jawalkar, Neha and Basu, Arkaprava},
title = {Designing Virtual Memory System of MCM GPUs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00036},
doi = {10.1109/MICRO56248.2022.00036},
abstract = {Multi-Chip Module (MCM) designs have emerged as a key technique to scale up a GPU's compute capabilities in the face of slowing transistor technology. However, the disaggregated nature of MCM GPUs with many chiplets connected via in-package interconnects leads to non-uniformity.We explore the implications of MCM's non-uniformity on the GPU's virtual memory. We quantitatively demonstrate that an MCM-aware virtual memory system should aim to 1 leverage aggregate TLB capacity across chiplets while limiting accesses to L2 TLB on remote chiplets, 2 reduce accesses to page table entries resident on a remote chiplet's memory during page walks. We propose MCM-aware GPU virtual memory (MGvm) that leverages static analysis techniques, previously used for thread and data placement, to map virtual addresses to chiplets and to place the page tables. At runtime, MGvm balances its objective of limiting the number of remote L2 TLB lookups with that of reducing the number of remote page table accesses to achieve good speedups (52\%, on average) across diverse application behaviors.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {404–422},
numpages = {19},
keywords = {graphics processing units, multi-chip module, chiplet, virtual memory, address translation, page table walkers, translation look-aside buffers},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00039,
author = {Zhao, Jiechen and Uwizeyimana, Iris and Ganesan, Karthik and Jeffrey, Mark C. and Jerger, Natalie Enright},
title = {Altocumulus: Scalable Scheduling for Nanosecond-Scale Remote Procedure Calls},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00039},
doi = {10.1109/MICRO56248.2022.00039},
abstract = {Online services in modern datacenters use Remote Procedure Calls (RPCs) to communicate between different software layers. Despite RPCs using just a few small functions, inefficient RPC handling can cause delays to propagate across the system and degrade end-to-end performance. Prior work has reduced RPC processing time to less than 1 μs, which now shifts the bottleneck to the scheduling of RPCs. Existing RPC schedulers suffer from either high overheads, inability to effectively utilize high core-count CPUs or do not adaptively fit different traffic patterns. To address these shortcomings, we present Altocumulus,1 a scalable, software-hardware co-design to schedule RPCs at nanosecond scales. Altocumulus provides a proactive scheduling scheme and low-overhead messaging mechanism on top of a decentralized user runtime. Altocumulus also offers direct access from the user space to a set of simple hardware primitives to quickly migrate long-latency RPCs. We evaluate Altocumulus with synthetic workloads and an end-to-end in-memory key-value store application under real-world traffic patterns. Altocumulus improves throughput by 1.3--24.6\texttimes{} under a 99th percentile latency &lt;300 μs and reduces tail latency by up to 15.8\texttimes{} on 16-core systems over current state-of-the-art software and hardware schedulers. For 256-core systems, integrating Altocumulus with either a hardware-optimized NIC or commodity PCIe NIC can improve throughput by 2.8\texttimes{} or 2.7\texttimes{}, respectively, under 99th percentile latency &lt;8.5 μs.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {423–440},
numpages = {18},
keywords = {remote procedure calls, scheduling, datacenters, networked systems, load balancing, migration, queuing theory},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00040,
author = {Khairy, Mahmoud and Alawneh, Ahmad and Barnes, Aaron and Rogers, Timothy G.},
title = {SIMR: Single Instruction Multiple Request Processing for Energy-Efficient Data Center Microservices},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00040},
doi = {10.1109/MICRO56248.2022.00040},
abstract = {Contemporary data center servers process thousands of similar, independent requests per minute. In the interest of programmer productivity and ease of scaling, workloads in data centers have shifted from single monolithic processes toward a micro and nanoservice software architecture. As a result, single servers are now packed with many threads executing the same, relatively small task on different data.State-of-the-art data centers run these microservices on multi-core CPUs. However, the flexibility offered by traditional CPUs comes at an energy-efficiency cost. The Multiple Instruction Multiple Data execution model misses opportunities to aggregate the similarity in contemporary microservices. We observe that the Single Instruction Multiple Thread execution model, employed by GPUs, provides better thread scaling and has the potential to reduce frontend and memory system energy consumption. However, contemporary GPUs are ill-suited for the latency-sensitive microservice space.To exploit the similarity in contemporary microservices, while maintaining acceptable latency, we propose the Request Processing Unit (RPU). The RPU combines elements of out-of-order CPUs with lockstep thread aggregation mechanisms found in GPUs to execute microservices in a Single Instruction Multiple Request (SIMR) fashion. To complement the RPU, we also propose a SIMR-aware software stack that uses novel mechanisms to batch requests based on their predicted control-flow, split batches based on predicted latency divergence and map per-request memory allocations to maximize coalescing opportunities. Our resulting RPU system processes 5.7\texttimes{} more requests/joule than multi-core CPUs, while increasing single thread latency by only 1.44\texttimes{}.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {441–463},
numpages = {23},
keywords = {SIMT, data center, microservices, GPU},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00041,
author = {Vemmou, Marina and Cho, Albert and Daglis, Alexandros},
title = {Patching up Network Data Leaks with Sweeper},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00041},
doi = {10.1109/MICRO56248.2022.00041},
abstract = {Datacenters have witnessed a staggering evolution in networking technologies, driven by insatiable application demands for larger datasets and inter-server data transfers. Modern NICs can already handle 100s of Gbps of traffic, a bandwidth capability equivalent to several memory channels. Direct Cache Access mechanisms like DDIO that contain network traffic inside the CPU's caches are therefore essential to effectively handle growing network traffic rates. However, a growing body of work reveals instances of a critical DDIO weakness known as "leaky DMA", occurring when a significant fraction of network traffic leaks from the CPU's caches to memory. We find that such network data leaks cap the network bandwidth a server can effectively utilize.We identify that a major culprit for such network data leaks are evictions of already consumed dirty network buffers. Our key insight is that buffers already consumed by the application typically need not be written back to memory, as their next reuse will be a full overwrite with new network data by the NIC. We introduce Sweeper, a hardware extension and API that allows applications to mark such consumed network buffers. Hardware then skips writing marked buffers back to memory, drastically reducing memory bandwidth consumption and mitigating the performance penalty of network data leaks. Sweeper boosts a 24-core server's peak sustainable network bandwidth by up to 2.6\texttimes{} as compared to DDIO-based configurations.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {464–479},
numpages = {16},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00042,
author = {Alian, Mohammad and Agarwal, Siddharth and Shin, Jongmin and Patel, Neel and Yuan, Yifan and Kim, Daehoon and Wang, Ren and Kim, Nam Sung},
title = {IDIO: Network-Driven, Inbound Network Data Orchestration on Server Processors},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00042},
doi = {10.1109/MICRO56248.2022.00042},
abstract = {High-bandwidth network interface cards (NICs), each capable of transferring 100s of Gigabits per second, are making inroads into the servers of next-generation datacenters. Such unprecedented data delivery rates impose immense pressure, especially on the server's memory subsystem, as NICs first transfer network data to DRAM before processing. To alleviate the pressure, the cache hierarchy has evolved, supporting a direct data I/O (DDIO) technology to directly place network data in the last-level cache (LLC). Subsequently, various policies have been explored to manage such LLC and have proven to effectively reduce service latency and memory bandwidth consumption of network applications. However, the more recent evolution of the cache hierarchy decreased the size of LLC per core but significantly increased that of mid-level cache (MLC) with a non-inclusive policy. This calls for a re-examination of the aforementioned DDIO technology and management policies.In this paper, first, we identify three shortcomings of the current static data placement policy placing network data to LLC first and the non-inclusive policy with a commercial server system: (1) ineffectively using large MLC, (2) suffering from high rates of writebacks from MLC to LLC, and (3) breaking the isolation between application and network data enforced by limiting cache ways for DDIO. Second, to tackle the three shortcomings, we propose an intelligent direct I/O (IDIO) technology that extends DDIO to MLC and provides three synergistic mechanisms: (1) self-invalidating I/O buffer, (2) network-driven MLC prefetching, and (3) selective direct DRAM access. Our detailed experiments using a full-system simulator --- capable of running modern DPDK userspace network functions while sustaining 100Gbps+ network bandwidth --- show that IDIO significantly reduces data movement (up to 84\% MLC and LLC writeback reduction), provides LLC isolation (up to 22\% performance improvement), and improves tail latency (up to 38\% reduction in 99th latency) for receive-intensive network applications.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {480–493},
numpages = {14},
keywords = {non-inclusive cache, DDIO, datacenter network},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00043,
author = {Prasad, Ashwin and Rajendra, Sampath and Rajan, Kaushik and Govindarajan, R and Bondhugula, Uday},
title = {Treebeard: An Optimizing Compiler for Decision Tree Based ML Inference},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00043},
doi = {10.1109/MICRO56248.2022.00043},
abstract = {Decision tree ensembles are among the most commonly used machine learning models. These models are used in a wide range of applications and are deployed at scale. Decision tree ensemble inference is usually performed with libraries such as XGBoost, LightGBM, and Sklearn. These libraries incorporate a fixed set of optimizations for the hardware targets they support. However, maintaining these optimizations is prohibitively expensive with the evolution of hardware. Further, they do not specialize the inference code to the model being used, leaving significant performance on the table.This paper presents Treebeard, an optimizing compiler that progressively lowers the inference computation to optimized CPU code through multiple intermediate abstractions. By applying model-specific optimizations at the higher levels, tree walk optimizations at the middle level, and machine-specific optimizations lower down, Treebeard can specialize inference code for each model on each supported CPU target. Treebeard combines several novel optimizations at various abstraction levels to mitigate architectural bottlenecks and enable SIMD vectorization of tree walks.We implement Treebeard using the MLIR compiler infrastructure and demonstrate its utility by evaluating it on a diverse set of benchmarks. Treebeard is significantly faster than state-of-the-art systems, XGBoost, Treelite and Hummingbird, by 2.6\texttimes{}, 4.7\texttimes{} and 5.4\texttimes{} respectively in a single-core execution setting, and by 2.3\texttimes{}, 2.7\texttimes{} and 14\texttimes{} respectively in multi-core settings.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {494–511},
numpages = {18},
keywords = {optimizing compiler, decision tree ensemble, decision tree inference, vectorization, machine learning},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00044,
author = {Niu, Wei and Guan, Jiexiong and Shen, Xipeng and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
title = {GCD2: A Globally Optimizing Compiler for Mapping DNNs to Mobile DSPs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00044},
doi = {10.1109/MICRO56248.2022.00044},
abstract = {More specialized chips are exploiting available high transistor density to expose parallelism at a large scale with more intricate instruction sets. This paper reports on a compilation system GCD2, developed to support complex Deep Neural Network (DNN) workloads on mobile DSP chips. We observe several challenges in fully exploiting this architecture, related to SIMD width, more complex SIMD/vector instructions, and VLIW pipeline with the notion of soft dependencies. GCD2 comprises the following contributions: 1) development of matrix layout formats that support the use of different novel SIMD instructions, 2) formulation and solution of a global optimization problem related to choosing the best instruction (and associated layout) for implementation of each operator in a complete DNN, and 3) SDA, an algorithm for packing instructions with consideration for soft dependencies. These solutions are incorporated in a complete compilation system that is extensively evaluated against other systems using 10 large DNN models. Evaluation results show that GCD2 outperforms two product-level state-of-the-art end-to-end DNN execution frameworks (TFLite and Qualcomm SNPE) that support mobile DSPs by up to 6.0\texttimes{} speedup, and outperforms three established compilers (Halide, TVM, and RAKE) by up to 4.5 \texttimes{}, 3.4 \texttimes{}, and 4.0 \texttimes{} speedup, respectively. GCD2 is also unique in supporting real-time execution of certain DNNs, while its implementation enables two major DNNs to execute on a mobile DSP for the first time.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {512–529},
numpages = {18},
keywords = {VLIW instruction packing, compiler optimization, deep neural network, mobile devices},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00045,
author = {Zhang, Yuxuan and Khan, Tanvir Ahmed and Pokam, Gilles and Kasikci, Baris and Litz, Heiner and Devietti, Joseph},
title = {OCOLOS: Online COde Layout OptimizationS},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00045},
doi = {10.1109/MICRO56248.2022.00045},
abstract = {The processor front-end has become an increasingly important bottleneck in recent years due to growing application code footprints, particularly in data centers. First-level instruction caches and branch prediction engines have not been able to keep up with this code growth, leading to more front-end stalls and lower Instructions Per Cycle (IPC). Profile-guided optimizations performed by compilers represent a promising approach, as they rearrange code to maximize instruction cache locality and branch prediction efficiency along a relatively small number of hot code paths. However, these optimizations require continuous profiling and rebuilding of applications to ensure that the code layout matches the collected profiles. If an application's code is frequently updated, it becomes challenging to map profiling data from a previous version onto the latest version, leading to ignored profiling data and missed optimization opportunities.In this paper, we propose Ocolos, the first online code layout optimization system for unmodified applications written in unmanaged languages. Ocolos allows profile-guided optimization to be performed on a running process, instead of being performed offline and requiring the application to be relaunched. By running online, profile data is always relevant to the current execution and always maps perfectly to the running code. Ocolos demonstrates how to achieve robust online code replacement in complex multithreaded applications like MySQL and MongoDB, without requiring any application changes. Our experiments show that Ocolos can accelerate MySQL by up to 1.41\texttimes{}, the Verilator hardware simulator by up to 2.20\texttimes{}, and a build of the Clang compiler by up to 1.14\texttimes{}.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {530–545},
numpages = {16},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00046,
author = {Gobieski, Graham and Ghosh, Souradip and Heule, Marijn and Mowry, Todd and Nowatzki, Tony and Beckmann, Nathan and Lucia, Brandon},
title = {RipTide: A Programmable, Energy-Minimal Dataflow Compiler and Architecture},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00046},
doi = {10.1109/MICRO56248.2022.00046},
abstract = {Emerging sensing applications create an unprecedented need for energy efficiency in programmable processors. To achieve useful multi-year deployments on a small battery or energy harvester, these applications must avoid off-device communication and instead process most data locally. Recent work has proven coarse-grained reconfigurable arrays (CGRAs) as a promising architecture for this domain. Unfortunately, nearly all prior CGRAs support only computations with simple control flow and no memory aliasing (e.g., affine inner loops), causing an Amdahl efficiency bottleneck as non-trivial fractions of programs must run on an inefficient von Neumann core.RipTide is a co-designed compiler and CGRA architecture that achieves both high programmability and extreme energy efficiency, eliminating this bottleneck. RipTide provides a rich set of control-flow operators that support arbitrary control flow and memory access on the CGRA fabric. RipTide implements these primitives without tagged tokens to save energy; this requires careful ordering analysis in the compiler to guarantee correctness. RipTide further saves energy and area by offloading most control operations into its programmable on-chip network, where they can re-use existing network switches. RipTide's compiler is implemented in LLVM, and its hardware is synthesized in Intel 22FFL. RipTide compiles applications written in C while saving 25\% energy v. the state-of-the-art energy-minimal CGRA and 6.6\texttimes{} energy v. a von Neumann core.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {546–564},
numpages = {19},
keywords = {energy-minimal, ultra-low-power, programmable, general-purpose, reconfigurable, CGRA, dataflow, compiler},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00047,
author = {Singh, Sonali and Sarma, Anup and Lu, Sen and Sengupta, Abhronil and Kandemir, Mahmut T. and Neftci, Emre and Narayanan, Vijaykrishnan and Das, Chita R.},
title = {Skipper: Enabling Efficient SNN Training through Activation-Checkpointing and Time-Skipping},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00047},
doi = {10.1109/MICRO56248.2022.00047},
abstract = {Spiking neural networks (SNNs) are a highly efficient signal processing mechanism in biological systems that have inspired a plethora of research efforts aimed at translating their energy efficiency to computational platforms. Efficient training approaches are critical for the successful deployment of SNNs. Compared to mainstream deep neural networks (ANNs), training SNNs is far more challenging due to complex neural dynamics that evolve with time and their discrete, binary computing paradigm. Back-propagation-through-time (BPTT) with surrogate gradients has recently emerged as an effective technique to train deep SNNs directly. SNN-BPTT, however, has a major drawback in that it has a high memory requirement that increases with the number of timesteps. SNNs generally result from the discretization of Ordinary Differential Equations, due to which the sequence length must be typically longer than RNNs, compounding the time dependence problem. It, therefore, becomes hard to train deep SNNs on a single or multi-GPU setup with sufficiently large batch sizes or time-steps, and extended periods of training are required to achieve reasonable network performance.In this work, we reduce the memory requirements of BPTT in SNNs to enable the training of deeper SNNs with more timesteps (T). For this, we leverage the notion of activation re-computation in the context of SNN training that enables the GPU memory to scale sub-linearly with increasing time-steps. We observe that naively deploying the re-computation based approach leads to a considerable computational overhead. To solve this, we propose a time-skipped BPTT approximation technique, called Skipper, for SNNs, that not only alleviates this computation overhead, but also lowers memory consumption further with little to no loss of accuracy. We show the efficacy of our proposed technique by comparing it against a popular method for memory footprint reduction during training. Our evaluations on 5 state-of-the-art networks and 4 datasets show that for a constant batch size and time-steps, skipper reduces memory usage by 3.3\texttimes{} to 8.4\texttimes{} (6.7\texttimes{} on average) over baseline SNN-BPTT. It also achieves a speedup of 29\% to 70\% over the checkpointed approach and of 4\% to 40\% over the baseline approach. For a constant memory budget, skipper can scale to an order of magnitude higher timesteps compared to baseline SNN-BPTT.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {565–581},
numpages = {17},
keywords = {SNN, BPTT, compute and memory},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00048,
author = {Andri, Renzo and Bussolino, Beatrice and Cipolletta, Antonio and Cavigelli, Lukas and Wang, Zhe},
title = {Going Further with Winograd Convolutions: Tap-Wise Quantization for Efficient Inference on 4x4 Tiles},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00048},
doi = {10.1109/MICRO56248.2022.00048},
abstract = {Most of today's computer vision pipelines are built around deep neural networks, where convolution operations require most of the generally high compute effort. The Winograd convolution algorithm computes convolutions with fewer multiply-accumulate operations (MACs) compared to the standard algorithm, reducing the operation count by a factor of 2.25\texttimes{} for 3\texttimes{}3 convolutions when using the version with 2\texttimes{}2-sized tiles F2. Even though the gain is significant, the Winograd algorithm with larger tile sizes, i.e., F4, offers even more potential in improving throughput and energy efficiency, as it reduces the required MACs by 4\texttimes{}. Unfortunately, the Winograd algorithm with larger tile sizes introduces numerical issues that prevent its use on integer domain-specific accelerators (DSAs) and higher computational overhead to transform input and output data between spatial and Winograd domains.To unlock the full potential of Winograd F4, we propose a novel tap-wise quantization method that overcomes the numerical issues of using larger tiles, enabling integer-only inference. Moreover, we present custom hardware units that process the Winograd transformations in a power- and area-efficient way, and we show how to integrate such custom modules in an industrial-grade, programmable DSA. An extensive experimental evaluation on a large set of state-of-the-art computer vision benchmarks reveals that the tap-wise quantization algorithm makes the quantized Winograd F4 network almost as accurate as the FP32 baseline. The Winograd-enhanced DSA achieves up to 1.85\texttimes{} gain in energy efficiency and up to 1.83\texttimes{} end-to-end speed-up for state-of-the-art segmentation and detection networks.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {582–598},
numpages = {17},
keywords = {machine learning acceleration, winograd convolution, ML system design},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00050,
author = {Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I. and Lee, Royson and Kouris, Alexandros and Luk, Wayne and Lane, Nicholas D. and Abdelfattah, Mohamed S.},
title = {Adaptable Butterfly Accelerator for Attention-Based NNs via Hardware and Algorithm Co-Design},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00050},
doi = {10.1109/MICRO56248.2022.00050},
abstract = {Attention-based neural networks have become pervasive in many AI tasks. Despite their excellent algorithmic performance, the use of the attention mechanism and feedforward network (FFN) demands excessive computational and memory resources, which often compromises their hardware performance. Although various sparse variants have been introduced, most approaches only focus on mitigating the quadratic scaling of attention on the algorithm level, without explicitly considering the efficiency of mapping their methods on real hardware designs. Furthermore, most efforts only focus on either the attention mechanism or the FFNs but without jointly optimizing both parts, causing most of the current designs to lack scalability when dealing with different input lengths. This paper systematically considers the sparsity patterns in different variants from a hardware perspective. On the algorithmic level, we propose FABNet, a hardware-friendly variant that adopts a unified butterfly sparsity pattern to approximate both the attention mechanism and the FFNs. On the hardware level, a novel adaptable butterfly accelerator is proposed that can be configured at runtime via dedicated hardware control to accelerate different butterfly layers using a single unified hardware engine. On the Long-Range-Arena dataset, FABNet achieves the same accuracy as the vanilla Transformer while reducing the amount of computation by 10 ~ 66\texttimes{} and the number of parameters 2 ~ 22\texttimes{}. By jointly optimizing the algorithm and hardware, our FPGA-based butterfly accelerator achieves 14.2 ~ 23.2\texttimes{} speedup over state-of-the-art accelerators normalized to the same computational budget. Compared with optimized CPU and GPU designs on Raspberry Pi 4 and Jetson Nano, our system is up to 273.8\texttimes{} and 15.1\texttimes{} faster under the same power budget.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {599–615},
numpages = {17},
keywords = {adaptable butterfly accelerator, attention-based neural networks, butterfly sparsity, algorithm and hardware co-design},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00051,
author = {Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
title = {DFX: A Low-Latency Multi-FPGA Appliance for Accelerating Transformer-Based Text Generation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00051},
doi = {10.1109/MICRO56248.2022.00051},
abstract = {Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pre-trained Transformer (GPT) has achieved remarkable performance in text generation, or natural language generation (NLG), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. The conventional platforms such as GPU are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. Therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation.In this paper, we present DFX, a multi-FPGA acceleration appliance that executes GPT-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. DFX uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. Its compute cores operate on custom instructions and provide GPT-2 operations end-to-end. We implement the proposed hardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the channels of the high bandwidth memory (HBM) and the maximum number of compute resources for high hardware efficiency. DFX achieves 5.58\texttimes{} speedup and 3.99\texttimes{} energy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is also 8.21\texttimes{} more cost-effective than the GPU appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {616–630},
numpages = {15},
keywords = {natural language processing, GPT, text generation, datacenter, multi-FPGA acceleration, model parallelism},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00049,
author = {Tian, Chunlin and Li, Li and Shi, Zhan and Wang, Jun and Xu, ChengZhong},
title = {HARMONY: Heterogeneity-Aware Hierarchical Management for Federated Learning System},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00049},
doi = {10.1109/MICRO56248.2022.00049},
abstract = {Federated learning (FL) enables multiple devices to collaboratively train a shared model while preserving data privacy. However, despite its emerging applications in many areas, real-world deployment of on-device FL is challenging due to wildly diverse training capability and data distribution across heterogeneous edge devices, which highly impact both model performance and training efficiency.This paper proposes Harmony, a high-performance FL framework with heterogeneity-aware hierarchical management of training devices and training data. Unlike previous work that mainly focuses on heterogeneity in either training capability or data distribution, Harmony adopts a hierarchical structure to jointly handle both heterogeneities in a unified manner. Specifically, the two core components of Harmony are a global coordinator hosted by the central server and a local coordinator deployed on each participating device. Without accessing the raw data, the global coordinator first selects the participants, and then further reorganizes their training samples based on the accurate estimation of the runtime training capability and data distribution of each device. The local coordinator keeps monitoring the local training status and conducts efficient training with guidance from the global coordinator. We conduct extensive experiments to evaluate Harmony using both hardware and simulation testbeds on representative datasets. The experimental results show that Harmony improves the accuracy performance by 1.67\% -- 27.62\%. In addition, Harmony effectively accelerates the training process up to 3.29\texttimes{} and 1.84\texttimes{} on average, and saves energy up to 88.41\% and 28.04\% on average.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {631–645},
numpages = {15},
keywords = {federated learning, heterogeneous systems, mobile device},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00053,
author = {Guo, Yanan and Xin, Xin and Zhang, Youtao and Yang, Jun},
title = {Leaky Way: A Conflict-Based Cache Covert Channel Bypassing Set Associativity},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00053},
doi = {10.1109/MICRO56248.2022.00053},
abstract = {Modern x86 processors feature many prefetch instructions that developers can use to enhance performance. However, with some prefetch instructions, users can more directly manipulate cache states which may result in powerful cache covert channel and side channel attacks.In this work, we reverse-engineer the detailed cache behavior of PREFETCHNTA on various Intel processors. Based on the results, we first propose a new conflict-based cache covert channel named NTP+NTP. Prior conflict-based channels often require priming the cache set in order to cause cache conflicts. In contrast, in NTP+NTP, the data of the sender and receiver can compete for one specific way in the cache set, achieving cache conflicts without cache set priming for the first time. As a result, NTP+NTP has higher bandwidth than prior conflict-based channels such as Prime+Probe. The channel capacity of NTP+NTP is 302 KB/s. Second, we found that PREFETCHNTA can also be used to boost the performance of existing side channel attacks that utilize cache replacement states, making those attacks much more efficient than before.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {646–661},
numpages = {16},
keywords = {side channels, replacement policy, cache security},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00052,
author = {Miao, Chenlu and Bu, Kai and Li, Mengming and Mao, Shaowu and Jia, Jianwei},
title = {SwiftDir: Secure Cache Coherence without Overprotection},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00052},
doi = {10.1109/MICRO56248.2022.00052},
abstract = {Cache coherence states have recently been exploited to leak secrets through timing-channel attacks. The root cause lies in the fact that shared data in state Exclusive (E) and state Shared (S) are served from different cache layers. The state-of-the-art countermeasure---S-MESI---serves both E- and S-state shared data from the last-level cache (LLC) by explicitly synchronizing the Modified (M) state across private caches and the LLC. This has to sacrifice the silent upgrade feature that MESI introduces for speedup. Moreover, it enforces protection to not only exploitable shared data but also unshared data. This further slows down performance, especially for write-after-read intensive applications.In this paper, we propose SwiftDir to efficiently secure cache coherence against cover-channel attacks without overprotection. SwiftDir fundamentally narrows down the protection scope to write-protected data. Such exploitable shared data can be uniquely identified with the write-protection permission in the memory management unit (MMU) and do not necessarily transit to state M. We validate this idea through tracing system calls of shared libraries on Linux. We then investigate all three commercial cache architectures (i.e., PIPT, VIPT, and VIVT) and find it feasible to hitchhike the address translation process to transmit the write-protection information from the MMU to the coherence controller. Then SwiftDir enforces protection over only write-protected data by serving all requests toward them directly from the LLC with a constant latency. This not only simplifies how MESI handles write-protected data but also avoids how S-MESI overprotects them. Meanwhile, SwiftDir still preserves silent upgrade for efficient handling of unshared data. Extensive experiments demonstrate that our SwiftDir can secure cache coherence while outperforming not only secure S-MESI but also unprotected MESI.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {662–677},
numpages = {16},
keywords = {cache coherence, timing-channel attack, shared data, address translation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00055,
author = {Wang, Xin and Talapkaliyev, Daulet and Hicks, Matthew and Jian, Xun},
title = {Self-Reinforcing Memoization for Cryptography Calculations in Secure Memory Systems},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00055},
doi = {10.1109/MICRO56248.2022.00055},
abstract = {Modern memory systems use encryption and message authentication codes to ensure confidentiality and integrity. Encryption and integrity verification rely on cryptography calculations, which are slow. To hide the latency of cryptography calculations, prior works exploit the fact that many cryptography steps only require a memory block's write counter (i.e., a value that increases whenever the block is written to memory), but not the block itself. As such, memory controller (MC) caches counters so that MC can start calculating before missing blocks arrive from memory.Irregular workloads suffer from high counter miss rates, however, just like they suffer from high miss rates of page table entries. Many prior works have looked at the problem of page table entry misses for irregular workloads, but not the problem of counter misses for the irregular workloads.This paper addresses the memory latency overheads that irregular workloads suffer due to their high counter miss rate.We observe many (e.g., unlimited number of) counters can have the same value. As such, we propose memoizing cryptography calculations for hot counter values. When a counter arrives from memory, MC can use the counter value to look up a memoization table to quickly obtain the counter's memoized results instead of slowly recalculating them.To maximize memoization table hit rate, we observe whenever writing a block to memory, increasing its counter to any value higher than the current counter value can satisfy the security requirement of always using different counter values to encrypt the same block. As such, we also propose a memoization-aware counter update: when writing a block to memory, increase its counter to a value whose cryptography calculation is currently memoized.We refer to memoizing the calculation results of counters and the corresponding memoization-aware counter update collectively as Self-Reinforcing Memoization for Cryptography Calculations (RMCC).Our evaluations show that RMCC improves average performance by 6\% compared to the state-of-the-art. On average across the lifetimes of different workloads, RMCC accelerates decryption and verification for 92\% of counter misses.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {678–692},
numpages = {15},
keywords = {memory confidentiality and integrity, counter-mode AES, memory subsystem, memoization},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00054,
author = {Wang, Xin and Kotra, Jagadish B. and Jian, Xun},
title = {Eager Memory Cryptography in Caches},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00054},
doi = {10.1109/MICRO56248.2022.00054},
abstract = {To protect memory values from adversaries with physical access to data centers, secure memory systems ensure memory confidentiality and integrity via memory encryption and verification. The corresponding cryptography calculations require a memory block's write counter as input. As such, CPUs today cache counters in the memory controller (MC).Due to the large memory footprint and irregular access patterns of many real-world applications, MC's counter cache is too small to achieve high hit rate. A promising solution is also caching counters in the much bigger Last Level cache (LLC). As such, many prior works use LLC as a second level cache for counters to back up the smaller counter cache in MC.Caching counters in LLC introduces a new problem, however. Modern server CPUs have a long LLC access latency that not only can diminish the benefit of caching counters in LLC, but also can sometimes significantly increase counter access latency compared to not caching counters in LLC.We note the problem lies with MC sitting behind LLC; due to its physical location, MC can only see LLC misses and, therefore, can only serially access and use counters after data miss in LLC has completed. However, prior designs without caching counters in LLC can access and use counters in parallel with accessing data. If a block's counter misses in MC's counter cache, MC can fetch the counter from DRAM in parallel with data; if the counter hits in MC's counter cache, MC can use counters for cryptography calculation in parallel with data traveling from DRAM to MC.To parallelize the access and use of counters with data access while caching counters in LLC, we observe that in modern CPUs, L2 is typically the first place that caches data from DRAM (i.e., L2 and L3 are non-inclusive); as such, data from DRAM need not be decrypted and verified until they reach L2. So it is possible to offload some decryption and verification tasks from MC to L2. Since L2 sits before L3, L2 can access counter and data in parallel from L3; L2 can also use counters for cryptography calculation in parallel with data traveling from DRAM to L2, instead of just from DRAM to MC. As such, we propose caching and using counters directly in L2 and refer to this idea as Eager Memory Cryptography in Caches (EMCC). Our evaluation shows that when applied to the state-of-the-art baseline, EMCC improves performance of large and/or irregular workloads by 7\%, on average.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {693–709},
numpages = {17},
keywords = {memory encryption and verification, counter-mode AES, cache hierarchy, network-on-chip},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00056,
author = {Mao, Haiyu and Alser, Mohammed and Sadrosadati, Mohammad and Firtina, Can and Baranwal, Akanksha and Cali, Damla Senol and Manglik, Aditya and Alserr, Nour Almadhoun and Mutlu, Onur},
title = {GenPIP: In-Memory Acceleration of Genome Analysis via Tight Integration of Basecalling and Read Mapping},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00056},
doi = {10.1109/MICRO56248.2022.00056},
abstract = {Nanopore sequencing is a widely-used high-throughput genome sequencing technology that can sequence long fragments of a genome into raw electrical signals at low cost. Nanopore sequencing requires two computationally-costly processing steps for accurate downstream genome analysis. The first step, basecalling, translates the raw electrical signals into nucleotide bases (i.e., A, C, G, T). The second step, read mapping, finds the correct location of a read in a reference genome. In existing genome analysis pipelines, basecalling and read mapping are executed separately. We observe in this work that such separate execution of the two most time-consuming steps inherently leads to (1) significant data movement and (2) redundant computations on the data, slowing down the genome analysis pipeline.This paper proposes GenPIP, an in-memory genome analysis accelerator that tightly integrates basecalling and read mapping. GenPIP improves the performance of the genome analysis pipeline with two key mechanisms: (1) in-memory fine-grained collaborative execution of the major genome analysis steps in parallel; (2) a new technique for early-rejection of low-quality and unmapped reads to timely stop the execution of genome analysis for such reads, reducing inefficient computation. Our experiments show that, for the execution of the genome analysis pipeline, GenPIP provides 41.6\texttimes{} (8.4\texttimes{}) speedup and 32.8\texttimes{} (20.8\texttimes{}) energy savings with negligible accuracy loss compared to the state-of-the-art software genome analysis tools executed on a state-of-the-art CPU (GPU). Compared to a design that combines state-of-the-art in-memory basecalling and read mapping accelerators, GenPIP provides 1.39\texttimes{} speedup and 1.37\texttimes{} energy savings.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {710–726},
numpages = {17},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00057,
author = {Huangfu, Wenqin and Malladi, Krishna T. and Chang, Andrew and Xie, Yuan},
title = {BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00057},
doi = {10.1109/MICRO56248.2022.00057},
abstract = {Genome analysis benefits precise medical care, wildlife conservation, pandemic treatment (e.g., COVID-19), and so on. Unfortunately, in genome analysis, the speed of data processing lags far behind the speed of data generation. Thus, hardware acceleration turns out to be necessary.As many applications in genome analysis are memory-bound, Processing-In-Memory (PIM) and Near-Data-Processing (NDP) solutions have been explored to tackle this problem. In particular, the Dual-Inline-Memory-Module (DIMM) based designs are very promising due to their non-invasive feature to the cost-sensitive DRAM dies. However, they have two critical limitations, i.e., performance bottle-necked by communication and the limited potential for memory expansion.In this paper, we address these two limitations by designing novel DIMM based accelerators located near the dis-aggregated memory pool with the support from the Compute Express Link (CXL), aiming to leverage the abundant memory within the memory pool and the high communication bandwidth provided by CXL. We propose BEACON, Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support. BEACON adopts a software-hardware co-design approach to tackle the above two limitations. The BEACON architecture builds the foundation for efficient communication and memory expansion by reducing data movement and leveraging the high communication bandwidth provided by CXL. Based on the BEACON architecture, we propose a memory management framework to enable memory expansion with unmodified CXL-DIMMs and further optimize communication by improving data locality. We also propose algorithm-specific optimizations to further boost the performance of BEACON. In addition, BEACON provides two design choices, i.e., BEACON-D and BEACON-S. BEACON-D and BEACON-S perform the computation within the enhanced CXL-DIMMs and enhanced CXL-Switches, respectively. Experimental results show that compared with state-of-the-art DIMM based NDP accelerators, on average, BEACON-D and BEACON-S improve the performance by 4.70x and 4.13x, respectively.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {727–743},
numpages = {17},
keywords = {genome analysis, near-data-processing, software-hardware co-design, accelerator, memory dis-aggregation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00059,
author = {Yazdanbakhsh, Amir and Moradifirouzabadi, Ashkan and Li, Zheng and Kang, Mingu},
title = {Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00059},
doi = {10.1109/MICRO56248.2022.00059},
abstract = {As its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator, called Sprint1, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner. Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling Sprint to fetch only a small subset of relevant data to on-chip memory. To mitigate potential negative repercussions for model accuracy, Sprint re-computes the attention scores for the few fetched data in digital. The combined in-memory pruning and on-chip recompute of the relevant attention scores enables Sprint to transform quadratic complexity to a merely linear one. In addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches. We evaluate our proposed technique on a wide range of state-of-the-art transformer models. On average, Sprint yields 7.5\texttimes{} speedup and 19.6\texttimes{} energy reduction when total 16KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36\% degradation).},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {744–762},
numpages = {19},
keywords = {transformer, attention mechanism, self-attention, sparsity, model compression, in-memory computing, neural processing units, ReRAM, deep learning, hardware-software co-design},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00058,
author = {Hu, Han-Wen and Wang, Wei-Chen and Chang, Yuan-Hao and Lee, Yung-Chun and Lin, Bo-Rong and Wang, Huai-Mu and Lin, Yen-Po and Huang, Yu-Ming and Lee, Chong-Ying and Su, Tzu-Hsiang and Hsieh, Chih-Chang and Hu, Chia-Ming and Lai, Yi-Ting and Chen, Chung-Kuang and Chen, Han-Sung and Li, Hsiang-Pang and Kuo, Tei-Wei and Chang, Meng-Fan and Wang, Keh-Chung and Hung, Chun-Hsiung and Lu, Chih-Yuan},
title = {ICE: An Intelligent Cognition Engine with 3D NAND-Based In-Memory Computing for Vector Similarity Search Acceleration},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00058},
doi = {10.1109/MICRO56248.2022.00058},
abstract = {Vector similarity search (VSS) for unstructured vectors generated via machine learning methods is a promising solution for many applications, such as face search. With increasing awareness and concern about data security requirements, there is a compelling need to store data and process VSS applications locally on edge devices rather than send data to servers for computation. However, the explosive amount of data movement from NAND storage to DRAM across memory hierarchy and data processing of the entire dataset consume enormous energy and require long latency for VSS applications. Specifically, edge devices with insufficient DRAM capacity will trigger data swap and deteriorate the execution performance. To overcome this crucial hurdle, we propose an intelligent cognition engine (ICE) with cognitive 3D NAND, featuring non-volatile in-memory computing (nvIMC) to accelerate the processing, suppress the data movement, and reduce data swap between the processor and storage. This cognitive 3D NAND features digital nvIMC techniques (i.e., ADC/DAC-free approach), high-density 3D NAND, and compatibility with standard 3D NAND products with minor modifications. To facilitate parallel INT8/INT4 vector-vector multiplication (VVM) and mitigate the reliability issue of 3D NAND, we develop a bit-error-tolerance data encoding and a two's complement-based digital accumulator. VVM can support similarity computations (e.g., cosine similarity and Euclidean distance), which are required to search "the most similar data" right where they are stored. In addition, the proposed solution can be realized on edge storage products, e.g., embedded MultiMedia Card (eMMC). The measured and simulated results on real 3D NAND chips show that ICE enhances the system execution time by 17\texttimes{} to 95\texttimes{} and energy efficiency by 11\texttimes{} to 140\texttimes{}, compared to traditional von Neumann approaches using state-of-the-art edge systems with MobileFaceNet on CASIA-WebFace dataset. To the best of our knowledge, this work demonstrates the first 3D NAND-based digital nvIMC technique with measured silicon data.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {763–783},
numpages = {21},
keywords = {3D NAND, in-memory computing, vector similarity search, unstructured data search},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00060,
author = {Ollivier, Sebastien and Longofono, Stephen and Dutta, Prayash and Hu, Jingtong and Bhanja, Sanjukta and Jones, Alex K.},
title = {CORUSCANT: Fast Efficient Processing-in-Racetrack Memories},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00060},
doi = {10.1109/MICRO56248.2022.00060},
abstract = {The growth in data needs of modern applications has created significant challenges for modern systems leading to a "memory wall." Spintronic Domain-Wall Memory (DWM), provides near-SRAM read/write performance, energy savings and non-volatility, potential for extremely high storage density, and does not have significant endurance limitations. However, DWM's benefits cannot directly address data access latency and throughput limitations of memory bus bandwidth. Processing-in-memory (PIM) is a popular solution to reduce the demands of memory-to-processor communication by offloading computation directly to the memory. PIM has been proposed in multiple technologies including DRAM, Phase-change memory (PCM), resistive memory (ReRAM), and Spin-Transfer Torque Memory (STT-MRAM). DRAM PIM provides solutions for a restricted set of two operand bulk-bitwise operations. PIM in PCM and ReRAM raise concerns about their effective endurance and PIM in STT-MRAM has insufficient density for main-memory applications.We propose CORUSCANT, a DWM-based in-memory computing solution that leverages the properties of DWM nanowires and allows them to serve as polymorphic gates. While normally DWM is accessed by applying spin polarized currents orthogonal to the nanowire at access points to read individual bits, transverse access along the DWM nanowire allows the differentiation of the aggregate resistance of multiple bits in the nanowire, akin to a multi-level cell. CORUSCANT leverages this transverse reading to directly provide multi-operand bulk-bitwise logic. Leveraging this multi-operand concept enabled by transverse access, CORUS-CANT provides techniques to conduct multi-operand addition and two operand multiplication much more efficiently than prior digital PIM solutions. CORUSCANT provides a 1.6\texttimes{} speedup compared to the leading DRAM PIM technique for query applications that leverage bulk bitwise operations. Compared to the leading PIM technique for DWM, CORUSCANT improves performance by 6.9\texttimes{}, 2.3\texttimes{} and energy by 5.5\texttimes{}, 3.4\texttimes{} for 8-bit addition and multiplication, respectively. For arithmetic heavy benchmarks, CORUSCANT reduces access latency by 2.1\texttimes{}, while decreasing energy consumption by 25.2\texttimes{} for a 10\% area overhead versus non-PIM DWM.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {784–798},
numpages = {15},
keywords = {processing-in-memory, domain-wall memory, novel memories, machine learning},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00061,
author = {Sazeides, Yiannakis and Gerber, Alex and Gabor, Ron and Bramnik, Arkady and Papadimitriou, George and Gizopoulos, Dimitris and Nicopoulos, Chrysostomos and Dimitrakopoulos, Giorgos and Patsidis, Karyofyllis},
title = {IDLD: Instantaneous Detection of Leakage and Duplication of Identifiers Used for Register Renaming},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00061},
doi = {10.1109/MICRO56248.2022.00061},
abstract = {In this paper, we propose a cost-effective microarchitectural technique capable of Instantaneously Detecting the Leakage and Duplication (IDLD) of the physical register identifiers used for register renaming in modern out-of-order processor cores. Leakage occurs when a physical register identifier disappears, whereas duplication occurs when the physical register identifier appears twice throughout the renaming logic. IDLD checks each cycle that a code calculated by xoring the physical register identifiers read from and written to arrays, used for managing physical registers allocation, renaming and reclamation, is zero. This invariance is intrinsic to the register renaming subsystem functionality and allows detecting an identifier leakage and duplication instantaneously. Detection of bugs in the complex register renaming subsystem is challenging, since: (a) its operation is not directly observable in program or architectural visible locations, (b) it lies in time-critical paths in the heart of every modern out-of-order core, and (c) it is often the target for optimizations in new core designs, and thus, more susceptible to bugs than legacy subsystems. We demonstrate that bugs in the renaming logic can be very difficult to root cause because, for numerous cases, it takes excessive time, e.g., millions of cycles, for a duplication or leakage to become an architecturally observable error. Even worse, activations of such bugs, depending on microarchitectural state, are often masked by subsequent hardware operations. Hence, an activation of a rarely occurring leakage or duplication bug during post-silicon validation can go undetected and escape in the field. The difficulty of root-causing register identifier duplication and leakage without IDLD is demonstrated using detailed bug modeling at the microarchitecture level, whereas the low overhead of IDLD is confirmed using RTL design analysis.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {799–814},
numpages = {16},
keywords = {post-silicon validation, register renaming, merged register file, microarchitecture, pipeline, design bugs},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00062,
author = {Ya\u{g}l\i{}k\c{c}\i{}, A. Giray and Olgun, Ataberk and Patel, Minesh and Luo, Haocong and Hassan, Hasan and Orosa, Lois and Ergin, O\u{g}uz and Mutlu, Onur},
title = {HiRA: Hidden Row Activation for Reducing Refresh Latency of Off-the-Shelf DRAM Chips},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00062},
doi = {10.1109/MICRO56248.2022.00062},
abstract = {DRAM is the building block of modern main memory systems. DRAM cells must be periodically refreshed to prevent data loss. Refresh operations degrade system performance by interfering with memory accesses. As DRAM chip density increases with technology node scaling, refresh operations also increase because: 1) the number of DRAM rows in a chip increases; and 2) DRAM cells need additional refresh operations to mitigate bit failures caused by RowHammer, a failure mechanism that becomes worse with technology node scaling. Thus, it is critical to enable refresh operations at low performance overhead. To this end, we propose a new operation, Hidden Row Activation (HiRA), and the HiRA Memory Controller (HiRA-MC) to perform HiRA operations.HiRA hides a refresh operation's latency by refreshing a row concurrently with accessing or refreshing another row within the same bank. Unlike prior works, HiRA achieves this parallelism without any modifications to off-the-shelf DRAM chips. To do so, it leverages the new observation that two rows in the same bank can be activated without data loss if the rows are connected to different charge restoration circuitry. We experimentally demonstrate on 56 real off-the-shelf DRAM chips that HiRA can reliably parallelize a DRAM row's refresh operation with refresh or activation of any of the 32\% of the rows within the same bank. By doing so, HiRA reduces the overall latency of two refresh operations by 51.4\%.HiRA-MC modifies the memory request scheduler to perform HiRA when a refresh operation can be performed concurrently with a memory access or another refresh. Our system-level evaluations show that HiRA-MC increases system performance by 12.6\% and 3.73\texttimes{} as it reduces the performance degradation due to periodic refreshes and refreshes for RowHammer protection (preventive refreshes), respectively, for future DRAM chips with increased density and RowHammer vulnerability.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {815–834},
numpages = {20},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00063,
author = {Yahya, Jawad Haj and Volos, Haris and Bartolini, Davide B. and Antoniou, Georgia and Kim, Jeremie S. and Wang, Zhe and Kalaitzidis, Kleovoulos and Rollet, Tom and Chen, Zhirui and Geng, Ye and Mutlu, Onur and Sazeides, Yiannakis},
title = {AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for Latency-Sensitive Server Applications},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00063},
doi = {10.1109/MICRO56248.2022.00063},
abstract = {User-facing applications running in modern datacenters exhibit irregular request patterns and are implemented using a multitude of services with tight latency requirements (30--250μs). These characteristics render existing energy-conserving techniques ineffective when processors are idle due to the long transition time (order of 100μs) from a deep CPU core idle power state (C-state). While prior works propose management techniques to mitigate this inefficiency, we tackle it at its root with AgileWatts (AW): a new deep CPU core C-state architecture optimized for datacenter server processors targeting latency-sensitive applications.AW drastically reduces the transition latency from deep CPU core idle power states while retaining most of their power savings based on three key ideas. First, AW eliminates the latency (several microseconds) of saving/restoring the core context when powering-off/-on the core in a deep idle state by i) implementing medium-grained power-gates, carefully distributed across the CPU core, and ii) retaining context in the power-ungated domain. Second, AW eliminates the flush latency (several tens of microseconds) of the L1/L2 caches when entering a deep idle state by keeping L1/L2 content power-ungated. A small control logic also remains ungated to serve cache coherence traffic. AW implements cache sleep-mode and leakage reduction for the power-ungated domain by lowering a core's voltage to the minimum operational level. Third, using a state-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock generator, AW keeps the PLL active and locked during the idle state, cutting microseconds of wake-up latency at negligible power cost.Our evaluation with an accurate industrial-grade simulator calibrated against an Intel Skylake server shows that AW reduces the energy consumption of Memcached by up to 71\% (35\% on average) with &lt;1\% end-to-end performance degradation. We observe similar trends for other evaluated services (MySQL and Kafka). AW's new deep C-states C6A and C6AE reduce transition-time by up to 900\texttimes{} as compared to the deepest existing idle state C6, while consuming only 7\% and 5\% of the active state (C0) power, respectively.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {835–850},
numpages = {16},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00065,
author = {Antoniou, Georgia and Volos, Haris and Bartolini, Davide B. and Rollet, Tom and Sazeides, Yiannakis and Yahya, Jawad Haj},
title = {AgilePkgC: An Agile System Idle State Architecture for Energy Proportional Datacenter Servers},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00065},
doi = {10.1109/MICRO56248.2022.00065},
abstract = {Modern user-facing applications deployed in data-centers use a distributed system architecture that exacerbates the latency requirements of their constituent microservices (30--250μs). Existing CPU power-saving techniques degrade the performance of these applications due to the long transition latency (order of 100μs) to wake up from a deep CPU idle state (C-state). For this reason, server vendors recommend only enabling shallow core C-states (e.g., CC1) for idle CPU cores, thus preventing the system from entering deep package C-states (e.g., PC6) when all CPU cores are idle. This choice, however, impairs server energy proportionality since power-hungry resources (e.g., IOs, uncore, DRAM) remain active even when there is no active core to use them. As we show, it is common for all cores to be idle due to the low average utilization (e.g., 5 -- 20\%) of datacenter servers running user-facing applications.We propose to reap this opportunity with AgilePkgC (APC), a new package C-state architecture that improves the energy proportionality of server processors running latency-critical applications. APC implements PC1A (package C1 agile), a new deep package C-state that a system can enter once all cores are in a shallow C-state (i.e., CC1) and has a nanosecond-scale transition latency. PC1A is based on four key techniques. First, a hardware-based agile power management unit (APMU) rapidly detects when all cores enter a shallow core C-state (CC1) and triggers the system-level power savings control flow. Second, an IO Standby Mode (IOSM) places IO interfaces (e.g., PCIe, DMI, UPI, DRAM) in shallow (nanosecond-scale transition latency) low-power modes. Third, a CLM Retention (CLMR) mode rapidly reduces the CLM (Cache-and-home-agent, Last-level-cache, and Mesh network-on-chip) domain's voltage to its retention level, drastically reducing its power consumption. Fourth, APC keeps all system PLLs active in PC1A to allow nanosecond-scale exit latency by avoiding PLL re-locking overhead.Combining these techniques enables significant power savings while requiring less than 200ns transition latency, &gt;250\texttimes{} faster than existing deep package C-states (e.g., PC6), making PC1A practical for datacenter servers. Our evaluation based on an Intel Skylake-based server shows that APC reduces the energy consumption of Memcached by up to 41\% (25\% on average) with &lt;0.1\% performance degradation. APC provides similar benefits for other representative workloads.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {851–867},
numpages = {17},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00064,
author = {Li, Xueliang and Shi, Zhuobin and Chen, Junyang and Liu, Yepang},
title = {Realizing Emotional Interactions to Learn User Experience and Guide Energy Optimization for Mobile Architectures},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00064},
doi = {10.1109/MICRO56248.2022.00064},
abstract = {In the age of AI, mobile architectures such as smartphones are still "cold machines"; machines do not feel. If the architecture is able to feel users' feelings and runtime user experience (UX), it will accordingly adapt performance/energy to find the optimal system-operating state that consumes the least energy to satisfy users. In this paper, we will utilize users' facial expressions (FEs) to learn their runtime UX. We know that FEs are the natural and direct way for humans to convey their emotions and feelings. Our study reveals that FEs also reflect UX. Our research for the first time quantifies the link between FEs and UX. Leveraging this link, the architecture will be able to use the front camera to see FEs and feel users' UX. Based on UX, the architecture can appropriately provision computing resources. We propose Vi-energy system to realize the above idea. Our evaluation shows that Vi-energy reduces energy consumption by 52.9\% at maximum and secures UX.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {868–884},
numpages = {17},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00066,
author = {Gao, Fei and Tziantzioulis, Georgios and Wentzlaff, David},
title = {FracDRAM: Fractional Values in Off-the-Shelf DRAM},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00066},
doi = {10.1109/MICRO56248.2022.00066},
abstract = {As one of the cornerstones of computing, dynamic random-access memory (DRAM) is prevalent across digital systems. Over the years, researchers have proposed modifications to DRAM macros or explored alternative uses of existing DRAM chips to extend the functionality of this ubiquitous media. This work expands on the latter, providing new insights and demonstrating new functionalities in unmodified, commodity DRAM. FracDRAM is the first work to show how fractional values can be stored in off-the-shelf DRAM. We propose two primitive operations built with specially timed DRAM command sequences, to either store fractional values to the entire DRAM row or to masked bits in a row. Utilizing fractional values, this work enables more modules to perform the in-memory majority operation, increases the stability of the existing in-memory majority operation, and builds a state-of-the-art DRAM-based PUF with unmodified DRAM. In total, 582 DDR3 chips from seven major vendors are evaluated and characterized under different environments in this work. FracDRAM breaks through the conventional binary abstraction of DRAM logic, and brings new functions to the existing DRAM macro.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {885–899},
numpages = {15},
keywords = {DRAM, PIM, processing-with-memory, memory controller, PUF},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00067,
author = {Ferreira, Jo\~{a}o Dinis and Falcao, Gabriel and G\'{o}mez-Luna, Juan and Alser, Mohammed and Orosa, Lois and Sadrosadati, Mohammad and Kim, Jeremie S. and Oliveira, Geraldo F. and Shahroodi, Taha and Nori, Anant and Mutlu, Onur},
title = {pLUTo: Enabling Massively Parallel Computation in DRAM via Lookup Tables},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00067},
doi = {10.1109/MICRO56248.2022.00067},
abstract = {Data movement between the main memory and the processor is a key contributor to execution time and energy consumption in memory-intensive applications. This data movement bottleneck can be alleviated using Processing-in-Memory (PiM). One category of PiM is Processing-using-Memory (PuM), in which computation takes place inside the memory array by exploiting intrinsic analog properties of the memory device. PuM yields high performance and energy efficiency, but existing PuM techniques support a limited range of operations. As a result, current PuM architectures cannot efficiently perform some complex operations (e.g., multiplication, division, exponentiation) without large increases in chip area and design complexity.To overcome these limitations of existing PuM architectures, we introduce pLUTo (processing-using-memory with lookup table (LUT) operations), a DRAM-based PuM architecture that leverages the high storage density of DRAM to enable the massively parallel storing and querying of lookup tables (LUTs). The key idea of pLUTo is to replace complex operations with low-cost, bulk memory reads (i.e., LUT queries) instead of relying on complex extra logic.We evaluate pLUTo across 11 real-world workloads that showcase the limitations of prior PuM approaches and show that our solution outperforms optimized CPU and GPU baselines by an average of 713\texttimes{} and 1.2\texttimes{}, respectively, while simultaneously reducing energy consumption by an average of 1855\texttimes{} and 39.5\texttimes{}. Across these workloads, pLUTo outperforms state-of-the-art PiM architectures by an average of 18.3\texttimes{}. We also show that different versions of pLUTo provide different levels of flexibility and performance at different additional DRAM area overheads (between 10.2\% and 23.1\%). pLUTo's source code and all scripts required to reproduce the results of this paper are openly and fully available at https://github.com/CMU-SAFARI/pLUTo.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {900–919},
numpages = {20},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00068,
author = {Fujiki, Daichi and Khadem, Alireza and Mahlke, Scott and Das, Reetuparna},
title = {Multi-Layer In-Memory Processing},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00068},
doi = {10.1109/MICRO56248.2022.00068},
abstract = {In-memory computing provides revolutionary changes to computer architecture by fusing memory and computation, allowing data-intensive computations to reduce data communications. Despite promising results of in-memory computing in each layer of the memory hierarchy, an integrated approach to a system with multiple computable memories has not been examined. This paper presents a holistic and application-driven approach to building Multi-Layer In-Memory Processing (MLIMP) systems, enabling applications with variable computation demands to reap the benefits of heterogeneous compute resources in an integrated MLIMP system. By introducing concurrent task scheduling to MLIMP, we achieve improved performance and energy efficiency for graph neural networks and multiprogramming of data parallel applications.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {920–936},
numpages = {17},
keywords = {in-memory computing, processing in memory, accelerator, GNN},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00069,
author = {Park, Jisung and Azizi, Roknoddin and Oliveira, Geraldo F. and Sadrosadati, Mohammad and Nadig, Rakesh and Novo, David and G\'{o}mez-Luna, Juan and Kim, Myungsuk and Mutlu, Onur},
title = {Flash-Cosmos: In-Flash Bulk Bitwise Operations Using Inherent Computation Capability of NAND Flash Memory},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00069},
doi = {10.1109/MICRO56248.2022.00069},
abstract = {Bulk bitwise operations, i.e., bitwise operations on large bit vectors, are prevalent in a wide range of important application domains, including databases, graph processing, genome analysis, cryptography, and hyper-dimensional computing. In conventional systems, the performance and energy efficiency of bulk bitwise operations are bottlenecked by data movement between the compute units (e.g., CPUs and GPUs) and the memory hierarchy. In-flash processing (i.e., processing data inside NAND flash chips) has a high potential to accelerate bulk bitwise operations by fundamentally reducing data movement through the entire memory hierarchy, especially when the processed data does not fit into main memory.We identify two key limitations of the state-of-the-art in-flash processing technique for bulk bitwise operations; (i) it falls short of maximally exploiting the bit-level parallelism of bulk bitwise operations that could be enabled by leveraging the unique cell-array architecture and operating principles of NAND flash memory; (ii) it is unreliable because it is not designed to take into account the highly error-prone nature of NAND flash memory.We propose Flash-Cosmos (Flash Computation with One-Shot Multi-Operand Sensing), a new in-flash processing technique that significantly increases the performance and energy efficiency of bulk bitwise operations while providing high reliability. Flash-Cosmos introduces two key mechanisms that can be easily supported in modern NAND flash chips: (i) Multi-Wordline Sensing (MWS), which enables bulk bitwise operations on a large number of operands (tens of operands) with a single sensing operation, and (ii) Enhanced SLC-mode Programming (ESP), which enables reliable computation inside NAND flash memory. We demonstrate the feasibility of performing bulk bitwise operations with high reliability in Flash-Cosmos by testing 160 real 3D NAND flash chips. Our evaluation shows that Flash-Cosmos improves average performance and energy efficiency by 3.5\texttimes{}/32\texttimes{} and 3.3\texttimes{}/95\texttimes{}, respectively, over the state-of-the-art in-flash/outside-storage processing techniques across three real-world applications.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {937–955},
numpages = {19},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00070,
author = {Vavouliotis, Georgios and Chacon, Gino and Alvarez, Lluc and Gratz, Paul V. and Jim\'{e}nez, Daniel A. and Casas, Marc},
title = {Page Size Aware Cache Prefetching},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00070},
doi = {10.1109/MICRO56248.2022.00070},
abstract = {The increase in working set sizes of contemporary applications outpaces the growth in cache sizes, resulting in frequent main memory accesses that deteriorate system performance due to the disparity between processor and memory speeds. Prefetching data blocks into the cache hierarchy ahead of demand accesses has proven successful at attenuating this bottleneck. However, spatial cache prefetchers operating in the physical address space leave significant performance on the table by limiting their pattern detection within 4KB physical page boundaries when modern systems use page sizes larger than 4KB to mitigate the address translation overheads.This paper exploits the high usage of large pages in modern systems to increase the effectiveness of spatial cache prefetching. We design and propose the Page-size Propagation Module (PPM), a μarchitectural scheme that propagates the page size information to the lower-level cache prefetchers, enabling safe prefetching beyond 4KB physical page boundaries when the accessed blocks reside in large pages, at the cost of augmenting the first-level caches' Miss Status Holding Register (MSHR) entries with one additional bit. PPM is compatible with any cache prefetcher without implying design modifications. We capitalize on PPM's benefits by designing a module that consists of two page size aware prefetchers that inherently use different page sizes to drive prefetching. The composite module uses adaptive logic to dynamically enable the most appropriate page size aware prefetcher. Finally, we show that the proposed designs are transparent to which cache prefetcher is used.We apply the proposed page size exploitation techniques to four state-of-the-art spatial cache prefetchers. Our evaluation shows that our proposals improve single-core geomean performance by up to 8.1\% (2.1\% at minimum) over the original implementation of the considered prefetchers, across 80 memory-intensive workloads. In multi-core contexts, we report geomean speedups up to 7.7\% across different cache prefetchers and core configurations.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {956–974},
numpages = {19},
keywords = {cache hierarchy, prefetching, spatial correlation, microarchitecture, hardware, virtual memory, address translation, large pages, memory management, memory wall},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00072,
author = {Navarro-Torres, Agust\'{\i}n and Panda, Biswabandan and Alastruey-Bened\'{e}, Jes\'{u}s and Ib\'{a}\~{n}ez, Pablo and Vi\~{n}als-Y\'{u}fera, V\'{\i}ctor and Ros, Alberto},
title = {Berti: An Accurate Local-Delta Data Prefetcher},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00072},
doi = {10.1109/MICRO56248.2022.00072},
abstract = {Data prefetching is a technique that plays a crucial role in modern high-performance processors by hiding long latency memory accesses. Several state-of-the-art hardware prefetchers exploit the concept of deltas, defined as the difference between the cache line addresses of two demand accesses. Existing delta prefetchers, such as best offset prefetching (BOP) and multi-lookahead prefetching (MLOP), train and predict future accesses based on global deltas. We observed that the use of global deltas results in missed opportunities to anticipate memory accesses.In this paper, we propose Berti, a first-level data cache prefetcher that selects the best local deltas, i.e., those that consider only demand accesses issued by the same instruction. Thanks to a high-confidence mechanism that precisely detects the timely local deltas with high coverage, Berti generates accurate prefetch requests. Then, it orchestrates the prefetch requests to the memory hierarchy, using the selected deltas.Our empirical results using ChampSim and SPEC CPU2017 and GAP workloads show that, with a storage overhead of just 2.55 KB, Berti improves performance by 8.5\% compared to a baseline IP-stride and 3.5\% compared to IPCP, a state-of-the-art prefetcher. Our evaluation also shows that Berti reduces dynamic energy at the memory hierarchy by 33.6\% compared to IPCP, thanks to its high prefetch accuracy.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {975–991},
numpages = {17},
keywords = {data prefetching, hardware prefetching, first-level cache, local deltas, accuracy, timeliness},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00073,
author = {Panwar, Gagandeep and Laghari, Muhammad and Bears, David and Liu, Yuqing and Jearls, Chandler and Choukse, Esha and Cameron, Kirk W. and Butt, Ali R. and Jian, Xun},
title = {Translation-Optimized Memory Compression for Capacity},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00073},
doi = {10.1109/MICRO56248.2022.00073},
abstract = {The demand for memory is ever increasing. Many prior works have explored hardware memory compression to increase effective memory capacity. However, prior works compress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead. As such, this additional block-level translation exacerbates the well-known address translation problem for large and/or irregular workloads.A promising solution is to only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed); this avoids block-level translation overhead for hot pages. However, it still faces two challenges. First, after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. Second, only compressing cold data require compressing them very aggressively to achieve high overall memory savings; decompressing very aggressively compressed data is very slow (e.g., &gt; 800ns assuming the latest Deflate ASIC in industry).This paper presents Translation-optimized Memory Compression for Capacity (TMCC) to tackle the two challenges above. To address the first challenge, we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk. To address the second challenge, we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory; for memory pages, it is 4X as fast as the state-of-the art ASIC Deflate, with little to no sacrifice in compression ratio.Our evaluations show that for large and/or irregular workloads, TMCC can either improve performance by 14\% without sacrificing effective capacity or provide 2.2x the effective capacity without sacrificing performance compared to a state-of-the-art hardware memory compression for capacity.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {992–1011},
numpages = {20},
keywords = {memory, hardware memory compression, address translation, memory subsystem, compression ASIC},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00071,
author = {Jiang, Shizhi and Yang, Qiusong and Ci, Yiwei},
title = {Merging Similar Patterns for Hardware Prefetching},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00071},
doi = {10.1109/MICRO56248.2022.00071},
abstract = {One critical challenge of designing an efficient prefetcher is to strike a balance between performance and hardware overhead. Some state-of-the-art prefetchers achieve very high performance at the price of a very large storage requirement, which makes them not amenable to hardware implementations in commercial processors.We argue that merging memory access patterns can be a feasible solution to reducing storage overhead while obtaining high performance, although no existing prefetchers, to the best of our knowledge, have succeeded in doing so because of the difficulty of designing an effective merging strategy. After analysis of a large number of patterns, we find that the address offset of the first access in a certain memory region is a good feature for clustering highly similar patterns. Based on this observation, we propose a novel hardware data prefetcher, named Pattern Merging Prefetcher (PMP), which achieves high performance at a low cost. The storage requirement for storing patterns is largely reduced and, at the same time, the prefetch accuracy is guaranteed by merging similar patterns in the training process. In the prefetching process, a strategy based on access frequencies of prefetch candidates is applied to accurately extract prefetch targets from merged patterns. According to the experimental results on a wide range of various workloads, PMP outperforms the enhanced Bingo by 2.6\% with 30\texttimes{} lesser storage overhead and Pythia by 8.2\% with 6\texttimes{} lesser storage overhead.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1012–1026},
numpages = {15},
keywords = {cache, hardware data prefetching},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00074,
author = {Wu, Anbang and Zhang, Hezi and Li, Gushu and Shabani, Alireza and Xie, Yuan and Ding, Yufei},
title = {AutoComm: A Framework for Enabling Efficient Communication in Distributed Quantum Programs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00074},
doi = {10.1109/MICRO56248.2022.00074},
abstract = {Distributed quantum computing (DQC) is a promising approach to extending the computational power of near-term quantum hardware. However, the non-local quantum communication between quantum nodes is much more expensive and error-prone than the local quantum operation within each quantum device. Previous DQC compilers focus on optimizing the implementation of each non-local gate and adopt similar compilation designs to single-node quantum compilers. The communication patterns in distributed quantum programs remain unexplored, leading to a far-from-optimal communication cost. In this paper, we identify burst communication, a specific qubit-node communication pattern that widely exists in various distributed quantum programs and can be leveraged to guide communication overhead optimization. We then propose AutoComm, an automatic compiler framework to extract burst communication patterns from input programs and then optimize the communication steps of burst communication discovered. Compared to state-of-the-art DQC compilers, experimental results show that our proposed AutoComm can reduce the communication resource consumption and the program latency by 72.9\% and 69.2\% on average, respectively.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1027–1041},
numpages = {15},
keywords = {quantum computing, quantum compiler},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00075,
author = {Lin, Sophia Fuhui and Sussman, Sara and Duckering, Casey and Mundada, Pranav S. and Baker, Jonathan M. and Kumar, Rohan S. and Houck, Andrew A. and Chong, Frederic T.},
title = {Let Each Quantum Bit Choose Its Basis Gates},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00075},
doi = {10.1109/MICRO56248.2022.00075},
abstract = {Near-term quantum computers are primarily limited by errors in quantum operations (or gates) between two quantum bits (or qubits). A physical machine typically provides a set of basis gates that include primitive 2-qubit (2Q) and 1-qubit (1Q) gates that can be implemented in a given technology. 2Q entangling gates, coupled with some 1Q gates, allow for universal quantum computation. In superconducting technologies, the current state of the art is to implement the same 2Q gate between every pair of qubits (typically an XX-or XY-type gate). This strict hardware uniformity requirement for 2Q gates in a large quantum computer has made scaling up a time and resource-intensive endeavor in the lab.We propose a radical idea - allow the 2Q basis gate(s) to differ between every pair of qubits, selecting the best entangling gates that can be calibrated between given pairs of qubits. This work aims to give quantum scientists the ability to run meaningful algorithms with qubit systems that are not perfectly uniform. Scientists will also be able to use a much broader variety of novel 2Q gates for quantum computing. We develop a theoretical framework for identifying good 2Q basis gates on "nonstandard" Cartan trajectories that deviate from "standard" trajectories like XX. We then introduce practical methods for calibration and compilation with nonstandard 2Q gates, and discuss possible ways to improve the compilation. To demonstrate our methods in a case study, we simulated both standard XY-type trajectories and faster, nonstandard trajectories using an entangling gate architecture with far-detuned transmon qubits. We identify efficient 2Q basis gates on these nonstandard trajectories and use them to compile a number of standard benchmark circuits such as QFT and QAOA. Our results demonstrate an 8x improvement over the baseline 2Q gates with respect to speed and coherence-limited gate fidelity.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1042–1058},
numpages = {17},
keywords = {quantum computing, two-qubit gates},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00076,
author = {Maurya, Satvik and Tannu, Swamit},
title = {COMPAQT: Compressed Waveform Memory Architecture for Scalable Qubit Control},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00076},
doi = {10.1109/MICRO56248.2022.00076},
abstract = {On superconducting architectures, the state of a qubit is manipulated by using microwave pulses. Typically, the pulses are stored in the waveform memory and then streamed to the Digital-to-Analog Converter (DAC) to synthesize the gate operations. The waveform memory requires tens of Gigabytes per second of bandwidth to manipulate the qubit. Unfortunately, the required memory bandwidth grows linearly with the number of qubits. As a result, the bandwidth demand limits the number of qubits we can control concurrently. For example, on current RFSoCs-based qubit control platforms, we can control less than 40 qubits. In addition, the high memory bandwidth for cryogenic ASIC controllers designed to operate within a tight power budget translates to significant power dissipation, thus limiting scalability.In this paper, we show that waveforms are highly compressible, and we leverage this property to enable a scalable and efficient microarchitecture COMPAQT - Compressed Waveform Memory Architecture for Qubit Control. Waveform memory is read-only and COMPAQT leverages this to compress waveforms at compile time and store the compressed waveform in the on-chip memory. To generate the pulse, COMPAQT decompresses the waveform at runtime and then streams the decompressed waveform to the DACs. Using the hardware-efficient discrete cosine transform, COMPAQT can achieve, on average, 5x increase in the waveform memory bandwidth, which can enable 5x increase in the total number of qubits controlled in an RFSoC setup. Moreover, COMPAQT microarchitecture for cryogenic CMOS ASIC controllers can result in a 2.5x power reduction over uncompressed baseline. We also propose an adaptive compression scheme to further reduce the power consumed by the decompression engine, enabling up to 4x power reduction.Qubits are sensitive, and even a slight change in the control waveform can increase the gate error rate. We evaluate the impact of COMPAQT on the gate and circuit fidelity using IBM quantum computers. We see less than 0.1\% degradation in fidelity when using COMPAQT.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1059–1077},
numpages = {19},
keywords = {qubit control, quantum computer architecture, quantum control hardware},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00077,
author = {Molavi, Abtin and Xu, Amanda and Diges, Martin and Pick, Lauren and Tannu, Swamit and Albarghouthi, Aws},
title = {Qubit Mapping and Routing via MaxSAT},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00077},
doi = {10.1109/MICRO56248.2022.00077},
abstract = {Near-term quantum computers will operate in a noisy environment, without error correction. A critical problem for near-term quantum computing is laying out a logical circuit onto a physical device with limited connectivity between qubits. This is known as the qubit mapping and routing (QMR) problem, an intractable combinatorial problem. It is important to solve QMR as optimally as possible to reduce the amount of added noise, which may render a quantum computation useless. In this paper, we present a novel approach for optimally solving the QMR problem via a reduction to maximum satisfiability (MAXSAT). Additionally, we present two novel relaxation ideas that shrink the size of the MAXSAT constraints by exploiting the structure of a quantum circuit. Our thorough empirical evaluation demonstrates (1) the scalability of our approach compared to state-of-the-art optimal QMR techniques (solves more than 3x benchmarks with 40x speedup), (2) the significant cost reduction compared to state-of-the-art heuristic approaches (an average of ~5x swap reduction), and (3) the power of our proposed constraint relaxations.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1078–1091},
numpages = {14},
keywords = {quantum computing, qubit mapping},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00078,
author = {Smith, Kaitlin N. and Ravi, Gokul Subramanian and Baker, Jonathan M. and Chong, Frederic T.},
title = {Scaling Superconducting Quantum Computers with Chiplet Architectures},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00078},
doi = {10.1109/MICRO56248.2022.00078},
abstract = {Fixed-frequency transmon quantum computers (QCs) have advanced in coherence times, addressability, and gate fidelities. Unfortunately, these devices are restricted by the number of on-chip qubits, capping processing power and slowing progress toward fault-tolerance. Although emerging transmon devices feature over 100 qubits, building QCs large enough for meaningful demonstrations of quantum advantage requires overcoming many design challenges. For example, today's transmon qubits suffer from significant variation due to limited precision in fabrication. As a result, barring significant improvements in current fabrication techniques, scaling QCs by building ever larger individual chips with more qubits is hampered by device variation. Severe device variation that degrades QC performance is referred to as a defect. Here, we focus on a specific defect known as a frequency collision.When transmon frequencies collide, their difference falls within a range that limits two-qubit gate fidelity. Frequency collisions occur with greater probability on larger QCs, causing collision-free yields to decline as the number of on-chip qubits increases. As a solution, we propose exploiting the higher yields associated with smaller QCs by integrating quantum chiplets within quantum multi-chip modules (MCMs). Yield, gate performance, and application-based analysis show the feasibility of QC scaling through modularity. Our results demonstrate that chiplet architectures, relative to monolithic designs, benefit from average yield improvements ranging from 9.6 -- 92.6\texttimes{} for ≲500 qubit machines. In addition, our simulations explore the design space of chiplet systems and discover configurations that demonstrate average two-qubit gate infidelity reductions that are at best 0.815\texttimes{} their monolithic counterpart. Finally, we observe that carefully-selected modular systems achieve fidelity improvements on a range of benchmark circuits.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1092–1109},
numpages = {18},
keywords = {quantum computing, quantum architecture, superconducting quantum computers},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00079,
author = {Suzuki, Yasunari and Sugiyama, Takanori and Arai, Tomochika and Liao, Wang and Inoue, Koji and Tanimoto, Teruo},
title = {Q3DE: A Fault-Tolerant Quantum Computer Architecture for Multi-Bit Burst Errors by Cosmic Rays},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00079},
doi = {10.1109/MICRO56248.2022.00079},
abstract = {Demonstrating small error rates by integrating quantum error correction (QEC) into an architecture of quantum computing is the next milestone towards scalable fault-tolerant quantum computing (FTQC). Encoding logical qubits with superconducting qubits and surface codes is considered a promising candidate for FTQC architectures. In this paper, we propose an FTQC architecture, which we call Q3DE, that enhances the tolerance to multi-bit burst errors (MBBEs) by cosmic rays with moderate changes and overhead. There are three core components in Q3DE: in-situ anomaly DEtection, dynamic code DEformation, and optimized error DEcoding. In this architecture, MBBEs are detected only from syndrome values for error correction. The effect of MBBEs is immediately mitigated by dynamically increasing the encoding level of logical qubits and re-estimating probable recovery operation with the rollback of the decoding process. We investigate the performance and overhead of the Q3DE architecture with quantum-error simulators and demonstrate that Q3DE effectively reduces the period of MBBEs by 1000 times and halves the size of their region. Therefore, Q3DE significantly relaxes the requirement of qubit density and qubit chip size to realize FTQC. Our scheme is versatile for mitigating MBBEs, i.e., temporal variations of error properties, on a wide range of physical devices and FTQC architectures since it relies only on the standard features of topological stabilizer codes.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1110–1125},
numpages = {16},
keywords = {quantum computing, quantum error correction, fault-tolerant quantum computing},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00081,
author = {Solt, Flavien and Jattke, Patrick and Razavi, Kaveh},
title = {RemembERR: Leveraging Microprocessor Errata for Design Testing and Validation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00081},
doi = {10.1109/MICRO56248.2022.00081},
abstract = {Microprocessors are constantly increasing in complexity, but to remain competitive, their design and testing cycles must be kept as short as possible. This trend inevitably leads to design errors that eventually make their way into commercial products. Major microprocessor vendors such as Intel and AMD regularly publish and update errata documents describing these errata after their microprocessors are launched. The abundance of errata suggests the presence of significant gaps in the design testing of modern microprocessors.We argue that while a specific erratum provides information about only a single issue, the aggregated information from the body of existing errata can shed light on existing design testing gaps. Unfortunately, errata documents are not systematically structured. We formalize that each erratum describes, in human language, a set of triggers that, when applied in specific contexts, cause certain observations that pertain to a particular bug. We present RemembERR, the first large-scale database of microprocessor errata collected among all Intel Core and AMD microprocessors since 2008, comprising 2,563 individual errata. Each RemembERR entry is annotated with triggers, contexts, and observations, extracted from the original erratum. To generalize these properties, we classify them on multiple levels of abstraction that describe the underlying causes and effects.We then leverage RemembERR to study gaps in design testing by making the key observation that triggers are conjunctive, while observations are disjunctive: to detect a bug, it is necessary to apply all triggers and sufficient to observe only a single deviation. Based on this insight, one can rely on partial information about triggers across the entire corpus to draw consistent conclusions about the best design testing and validation strategies to cover the existing gaps. As a concrete example, our study shows that we need testing tools that exert power level transitions under MSR-determined configurations while operating custom features.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1126–1143},
numpages = {18},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00082,
author = {Lee, Hyun Ryong and Sanchez, Daniel},
title = {Datamime: Generating Representative Benchmarks by Automatically Synthesizing Datasets},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00082},
doi = {10.1109/MICRO56248.2022.00082},
abstract = {Benchmarks that closely match the behavior of production workloads are crucial to design and provision computer systems. However, current approaches fall short: First, open-source benchmarks use public datasets that cause different behavior from production workloads. Second, black-box workload cloning techniques generate synthetic code that imitates the target workload, but the resulting program fails to capture most workload characteristics, such as microarchitectural bottlenecks or time-varying behavior.Generating code that mimics a complex application is an extremely hard problem. Instead, we propose a different and easier approach to benchmark synthesis. Our key insight is that, for many production workloads, the program is publicly available or there is a reasonably similar open-source program. In this case, generating the right dataset is sufficient to produce an accurate benchmark.Based on this observation, we present Datamime, a profile-guided approach to generate representative benchmarks for production workloads. Datamime uses the performance profiles of a target workload to generate a dataset that, when used by a benchmark program, behaves very similarly to the target workload in terms of its microarchitectural characteristics.We evaluate Datamime on several datacenter workloads. Datamime generates synthetic benchmarks that closely match the microarchitectural features of these workloads, with a mean absolute percentage error of 3.2\% on IPC. Microarchitectural behavior stays close across processor types. Finally, time-varying behaviors are also replicated, making these benchmarks useful to e.g. characterize and optimize tail latency.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1144–1159},
numpages = {16},
keywords = {benchmarking, workload generation},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00083,
author = {Baskaran, Saambhavi and Kandemir, Mahmut Taylan and Sampson, Jack},
title = {An Architecture Interface and Offload Model for Low-Overhead, Near-Data, Distributed Accelerators},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00083},
doi = {10.1109/MICRO56248.2022.00083},
abstract = {The performance and energy costs of coordinating and performing data movement have led to proposals adding compute units and/or specialized access units to the memory hierarchy. However, current on-chip offload models are restricted to fixed compute and access pattern types, which limits software-driven optimizations and the applicability of such an offload interface to heterogeneous accelerator resources. This paper presents a computation offload interface for multi-core systems augmented with distributed on-chip accelerators. With energy-efficiency as the primary goal, we define mechanisms to identify offload partitioning, create a low-overhead execution model to sequence these fine-grained operations, and evaluate a set of workloads to identify the complexity needed to achieve distributed near-data execution.We demonstrate that our model and interface, combining features of dataflow in parallel with near-data processing engines, can be profitably applied to memory hierarchies augmented with either specialized compute substrates or lightweight near-memory cores. We differentiate the benefits stemming from each of elevating data access semantics, near-data computation, inter-accelerator coordination, and compute/access logic specialization. Experimental results indicate a geometric mean (energy efficiency improvement; speedup; data movement reduction) of (3.3; 1.59; 2.4)\texttimes{}, (2.46; 1.43; 3.5)\texttimes{} and (1.46; 1.65; 1.48)\texttimes{} compared to an out-of-order processor, monolithic accelerator with centralized accesses and monolithic accelerator with decentralized accesses, respectively. Evaluating both lightweight core and CGRA fabric implementations highlights model flexibility and quantifies the benefits of compute specialization for energy efficiency and speedup at 1.23\texttimes{} and 1.43\texttimes{}, respectively.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1160–1177},
numpages = {18},
keywords = {distributed accelerator, near-data offload, energy efficiency, heterogeneous architecture interface},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00080,
author = {Xu, Yinan and Yu, Zihao and Tang, Dan and Chen, Guokai and Chen, Lu and Gou, Lingrui and Jin, Yue and Li, Qianruo and Li, Xin and Li, Zuojun and Lin, Jiawei and Liu, Tong and Liu, Zhigang and Tan, Jiazhan and Wang, Huaqiang and Wang, Huizhe and Wang, Kaifan and Zhang, Chuanqi and Zhang, Fawang and Zhang, Linjuan and Zhang, Zifei and Zhao, Yangyang and Zhou, Yaoyang and Zhou, Yike and Zou, Jiangrui and Cai, Ye and Huan, Dandan and Li, Zusong and Zhao, Jiye and Chen, Zihao and He, Wei and Quan, Qiyuan and Liu, Xingwu and Wang, Sa and Shi, Kan and Sun, Ninghui and Bao, Yungang},
title = {Towards Developing High Performance RISC-V Processors Using Agile Methodology},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00080},
doi = {10.1109/MICRO56248.2022.00080},
abstract = {While research has shown that the agile chip design methodology is promising to sustain the scaling of computing performance in a more efficient way, it is still of limited usage in actual applications due to two major obstacles: 1) Lack of tool-chain and developing framework supporting agile chip design, especially for large-scale modern processors. 2) The conventional verification methods are less agile and become a major bottleneck of the entire process. To tackle both issues, we propose MinJie, an open-source platform supporting agile processor development flow. MinJie integrates a broad set of tools for logic design, functional verification, performance modelling, pre-silicon validation and debugging for better development efficiency of state-of-the-art processor designs. We demonstrate the usage and effectiveness of MinJie by building two generations of an open-source superscalar out-of-order RISC-V processor code-named XiangShan using agile methodologies. We quantify the performance of XiangShan using SPEC CPU2006 benchmarks and demonstrate that XiangShan achieves industry-competitive performance.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1178–1199},
numpages = {22},
keywords = {agile development, open-source hardware, microarchitecture},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00084,
author = {Park, Beomsik and Hwang, Ranggi and Yoon, Dongho and Choi, Yoonhyuk and Rhu, Minsoo},
title = {DiVa: An Accelerator for Differentially Private Machine Learning},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00084},
doi = {10.1109/MICRO56248.2022.00084},
abstract = {The widespread deployment of machine learning (ML) is raising serious concerns on protecting the privacy of users who contributed to the collection of training data. Differential privacy (DP) is rapidly gaining momentum in the industry as a practical standard for privacy protection. Despite DP's importance, however, little has been explored within the computer systems community regarding the implication of this emerging ML algorithm on system designs. In this work, we conduct a detailed workload characterization on a state-of-the-art differentially private ML training algorithm named DP-SGD. We uncover several unique properties of DP-SGD (e.g., its high memory capacity and computation requirements vs. non-private ML), root-causing its key bottlenecks. Based on our analysis, we propose an accelerator for differentially private ML named DiVa, which provides a significant improvement in compute utilization, leading to 2.6\texttimes{} higher energy-efficiency vs. conventional systolic arrays.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1200–1217},
numpages = {18},
keywords = {differential privacy, accelerator, machine learning, deep learning},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00085,
author = {Ajorpaz, Samira Mirbagher and Moghimi, Daniel and Collins, Jeffrey Neal and Pokam, Gilles and Abu-Ghazaleh, Nael and Tullsen, Dean},
title = {Evax: Towards a Practical, Pro-Active \&amp; Adaptive Architecture for High Performance \&amp; Security},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00085},
doi = {10.1109/MICRO56248.2022.00085},
abstract = {This paper provides an end-to-end solution to defend against known microarchitectural attacks such as speculative execution attacks, fault-injection attacks, covert and side channel attacks, and unknown or evasive versions of these attacks. Current defenses are attack specific and can have unacceptably high performance overhead. We propose an approach that reduces the overhead of state-of-art defenses by over 95\%, by applying defenses only when attacks are detected. Many current proposed mitigations are not practical for deployment; for example, InvisiSpec has 27\% overhead and Fencing has 74\% overhead while protecting against only Spectre attacks. Other mitigations carry similar performance penalties. We reduce the overhead for InvisiSpec to 1.26\% and for Fencing to 3.45\% offering performance and security for not only spectre attacks but other known transient attacks as well, including the dangerous class of LVI and Rowhammer attacks, as well as covering a large set of future evasive and zero-day attacks.Critical to our approach is an accurate detector that is not fooled by evasive attacks and that can generalize to novel zero-day attacks. We use a novel Generative framework, Evasion Vaccination (Evax) for training ML models and engineering new security-centric performance counters. Evax significantly increases sensitivity to detect and classify attacks in time for mitigation to be deployed with low false positives (4 FPs in every 1M instructions in our experiments). Such performance enables efficient and timely mitigations, enabling the processor to automatically switch between performance and security as needed.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1218–1236},
numpages = {19},
keywords = {hardware security, side channel, generative adversarial networks, automatic attack sample generation, adversarial machine learning attacks, automated hardware performance counter engineering, microarchitectural attack detection, linearized neural network, ML interpretability, zero day attack defense},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00086,
author = {Kim, Jongmin and Lee, Gwangho and Kim, Sangpyo and Sohn, Gina and Rhu, Minsoo and Kim, John and Ahn, Jung Ho},
title = {ARK: Fully Homomorphic Encryption Accelerator with Runtime Data Generation and Inter-Operation Key Reuse},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00086},
doi = {10.1109/MICRO56248.2022.00086},
abstract = {Homomorphic Encryption (HE) is one of the most promising post-quantum cryptographic schemes that enable privacy-preserving computation on servers. However, noise accumulates as we perform operations on HE-encrypted data, restricting the number of possible operations. Fully HE (FHE) removes this restriction by introducing the bootstrapping operation, which refreshes the data; however, FHE schemes are highly memory-bound. Bootstrapping, in particular, requires loading GBs of evaluation keys and plaintexts from off-chip memory, which makes FHE acceleration fundamentally bottlenecked by the off-chip memory bandwidth.In this paper, we propose ARK, an Accelerator for FHE with Runtime data generation and inter-operation Key reuse. ARK enables practical FHE workloads with a novel algorithm-architecture co-design to accelerate bootstrapping. We first eliminate the off-chip memory bandwidth bottleneck through runtime data generation and inter-operation key reuse. This approach enables ARK to fully exploit on-chip memory by substantially reducing the size of the working set. On top of such algorithmic enhancements, we build ARK microarchitecture that minimizes on-chip data movement through an efficient, alternating data distribution policy based on the data access patterns and a streamlined dataflow organization of the tailored functional units ---including base conversion, number-theoretic transform, and automorphism units. Overall, our co-design effectively handles the heavy computation and data movement overheads of FHE, drastically reducing the cost of HE operations, including bootstrapping.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1237–1254},
numpages = {18},
keywords = {fully homomorphic encryption, domain-specific architecture, algorithm-architecture co-design},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00087,
author = {Han, Xijing and Tuck, James and Awad, Amro},
title = {Horus: Persistent Security for Extended Persistence-Domain Memory Systems},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00087},
doi = {10.1109/MICRO56248.2022.00087},
abstract = {Persistent memory presents a great opportunity for crash-consistent computing in large-scale computing systems. The ability to recover data upon power outage or crash events can significantly improve the availability of large-scale systems, while improving the performance of persistent data applications (e.g., database applications). However, persistent memory suffers from high write latency and requires specific programming model (e.g., Intel's PMDK) to guarantee crash consistency, which results in long latency to persist data. To mitigate these problems, recent standards advocate for sufficient back-up power that can flush the whole cache hierarchy to the persistent memory upon detection of an outage, i.e., extending the persistence domain to include the cache hierarchy.In the secure NVM with extended persistent domain(EPD), in addition to flushing the cache hierarchy, extra actions need to be taken to protect the flushed cache data. These extra actions of secure operation could cause significant burden on energy costs and battery size. We demonstrate that naive implementations could lead to significantly expanding the required power holdup budget (e.g., 10.3x more operations than EPD system without secure memory support). The significant overhead is caused by memory accesses of secure metadata. In this paper, we present Horus, a novel EPD-aware secure memory implementation. Horus reduces the overhead during draining period of EPD system by reducing memory accesses of secure metadata. Experiment result shows that Horus reduces the draining time by 5x, compared with the naive baseline design.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1255–1269},
numpages = {15},
keywords = {non-volatile memory, eADR, secure memory},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00089,
author = {Talati, Nishil and Ye, Haojie and Vedula, Sanketh and Chen, Kuan-Yu and Chen, Yuhan and Liu, Daniel and Yuan, Yichao and Blaauw, David and Bronstein, Alex and Mudge, Trevor and Dreslinski, Ronald},
title = {Mint: An Accelerator for Mining Temporal Motifs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00089},
doi = {10.1109/MICRO56248.2022.00089},
abstract = {A variety of complex systems, including social and communication networks, financial markets, biology, and neuroscience are modeled using temporal graphs that contain a set of nodes and directed timestamped edges. Temporal motifs in temporal graphs are generalized from subgraph patterns in static graphs in that they also account for edge ordering and time duration, in addition to the graph structure. Mining temporal motifs is a fundamental problem used in several application domains. However, existing software frameworks offer sub-optimal performance due to high algorithmic complexity and irregular memory accesses of temporal motif mining.This paper presents Mint---a novel accelerator architecture and a programming model for mining temporal motifs efficiently. We first divide this workload into three fundamental tasks: search, book-keeping, and backtracking. Based on this, we propose a task-centric programming model that enables decoupled, asynchronous execution. This model unlocks massive opportunities for parallelism, and allows storing task context information on-chip. To best utilize the proposed programming model, we design a domain-specific hardware accelerator using its data path and memory subsystem design to cater to the unique workload characteristics of temporal motif mining. To further improve performance, we propose a novel optimization called search index memoization that significantly reduces memory traffic. We comprehensively compare the performance of Mint with state-of-the-art temporal motif mining software frameworks (both approximate and exact) running on both CPU and GPU, and show 9\texttimes{}--2576\texttimes{} benefit in performance.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1270–1287},
numpages = {18},
keywords = {hardware accelerator, programming model, temporal motif mining},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00090,
author = {Shah, Nimish and Meert, Wannes and Verhelst, Marian},
title = {DPU-v2: Energy-Efficient Execution of Irregular Directed Acyclic Graphs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00090},
doi = {10.1109/MICRO56248.2022.00090},
abstract = {A growing number of applications like probabilistic machine learning, sparse linear algebra, robotic navigation, etc., exhibit irregular data flow computation that can be modeled with directed acyclic graphs (DAGs). The irregularity arises from the seemingly random connections of nodes, which makes the DAG structure unsuitable for vectorization on CPU or GPU. Moreover, the nodes usually represent a small number of arithmetic operations that cannot amortize the overhead of launching tasks/kernels for each node, further posing challenges for parallel execution.To enable energy-efficient execution, this work proposes DAG processing unit (DPU) version 2, a specialized processor architecture optimized for irregular DAGs with static connectivity. It consists of a tree-structured datapath for efficient data reuse, a customized banked register file, and interconnects tuned to support irregular register accesses. DPU-v2 is utilized effectively through a targeted compiler that systematically maps operations to the datapath, minimizes register bank conflicts, and avoids pipeline hazards. Finally, a design space exploration identifies the optimal architecture configuration that minimizes the energy-delay product. This hardware-software co-optimization approach results in a speedup of 1.4\texttimes{}, 3.5\texttimes{}, and 14\texttimes{} over a state-of-the-art DAG processor ASIP, a CPU, and a GPU, respectively, while also achieving a lower energy-delay product. In this way, this work takes an important step towards enabling an embedded execution of emerging DAG workloads.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1288–1307},
numpages = {20},
keywords = {graphs, irregular computation graphs, parallel processor, hardware-software codesign, DAG processing unit, probabilistic circuits, sparse matrix triangular solve, spatial datapath, interconnection network, bank conflicts, design space exploration},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00091,
author = {Wang, Rui and He, Shuibing and Zong, Weixu and Li, Yongkun and Xu, Yinlong},
title = {XPGraph: XPline-Friendly Persistent Memory Graph Stores for Large-Scale Evolving Graphs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00091},
doi = {10.1109/MICRO56248.2022.00091},
abstract = {Traditional in-memory graph storage systems have limited scalability due to the limited capacity and volatility of DRAM. Emerging persistent memory (PMEM), with large capacity and non-volatility, provides us an opportunity to realize the scalable and high-performance graph stores. However, directly moving existing DRAM-based graph storage systems to PMEM would cause serious PMEM access inefficiency issues, including high read and write amplification in PMEM and costly remote PMEM accesses across NUMA nodes, thus leading to the performance bottleneck. In this paper, we propose XPGraph, a PMEM-based graph storage system for managing large-scale evolving graphs, by developing an XPLine-friendly graph access model with vertex-centric graph buffering, hierarchical vertex buffer managing, and NUMA-friendly graph accessing. Experimental results show that XPGraph achieves 3.01\texttimes{} to 3.95\texttimes{} higher update performance and up to 4.46\texttimes{} higher query performance, compared with the state-of-the-art in-memory graph storage system implemented on a PMEM-based system.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1308–1325},
numpages = {18},
keywords = {graph processing, persistent/non-volatile memory, storage systems},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00088,
author = {Wang, Qinggang and Zheng, Long and Hu, Ao and Huang, Yu and Yao, Pengcheng and Gui, Chuangyi and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {A Data-Centric Accelerator for High-Performance Hypergraph Processing},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00088},
doi = {10.1109/MICRO56248.2022.00088},
abstract = {Hypergraph processing has emerged as a powerful approach for analyzing complex multilateral relationships among multiple entities. Past research on building hypergraph systems suggests that changing the scheduling order of bipartite edge tasks can improve the overlap-induced data locality in hypergraph processing. However, due to the complex intertwined connections between vertices and hyperedges, it is almost impossible to find a locality-optimal scheduling order. Thus, these task-centric hypergraph systems often suffer from substantial off-chip communications.In this paper, we first propose a novel data-centric Load-Trigger-Reduce (LTR) execution model to exploit fully the locality in hypergraph processing. Unlike a task-centric model that loads the required data along with a task, our LTR model invokes tasks as per the data used. Specifically, once the hypergraph data is loaded into the on-chip memory, all of its relevant computation tasks will be triggered simultaneously to output intermediate results, which are finally reduced to update the final results. Our LTR model enables all hypergraph data to be accessed once in each iteration. To fully exploit the LTR performance potential, we further architect an LTR-driven hypergraph accelerator, XuLin, which features with an adaptive data loading mechanism to minimize the loading cost via chunk merging at runtime. XuLin is also equipped with a priority-based differential data reduction scheme to reduce the impact of conflicting updates on performance. We have implemented XuLin both on a Xilinx Alveo U250 FPGA card and using a cycle-accurate simulator. The results show that XuLin outperforms the state-of-the-art hypergraph processing solutions Hygra and ChGraph by 20.47\texttimes{} and 8.77\texttimes{} on average, respectively.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1326–1341},
numpages = {16},
keywords = {hypergraph, data-centric, accelerator},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00092,
author = {Chen, Xinyu and Chen, Yao and Cheng, Feng and Tan, Hongshi and He, Bingsheng and Wong, Weng-fai},
title = {ReGraph: Scaling Graph Processing on HBM-Enabled FPGAs with Heterogeneous Pipelines},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00092},
doi = {10.1109/MICRO56248.2022.00092},
abstract = {The use of FPGAs for efficient graph processing has attracted significant interest. Recent memory subsystem upgrades including the introduction of HBM in FPGAs promise to further alleviate memory bottlenecks. However, modern multi-channel HBM requires much more processing pipelines to fully utilize its bandwidth potential. Due to insufficient resource efficiency, existing designs do not scale well, resulting in underutilization of the HBM facilities even when all other resources are fully consumed.In this paper, we propose ReGraph1, which customizes heterogeneous pipelines for diverse workloads in graph processing, achieving better resource efficiency, instantiating more pipelines and improving performance. We first identify workload diversity exists in processing graph partitions and classify them into two types: dense partitions established with good locality and sparse partitions with poor locality. Subsequently, we design two types of pipelines: Little pipelines with burst memory access technique to process dense partitions and Big pipelines tolerating random memory access latency to handle sparse partitions. Unlike existing monolithic pipeline designs, our heterogeneous pipelines are tailored for more specific workload characteristics and hence more lightweight, allowing the architecture to scale up more effectively with limited resources. We also present a graph-aware task scheduling method that schedules partitions to the right pipeline types, generates the most efficient pipeline combination and balances workloads. ReGraph surpasses state-of-the-art FPGA accelerators by 1.6\texttimes{}--5.9\texttimes{} in performance and 2.5\texttimes{}--12.3\texttimes{} in resource efficiency.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1342–1358},
numpages = {17},
keywords = {graph processing, FPGA, HBM, heterogeneity},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00093,
author = {Lee, Hunjun and Kim, Minseop and Min, Dongmoon and Kim, Joonsung and Back, Jongwon and Yoo, Honam and Lee, Jong-Ho and Kim, Jangwoo},
title = {3D-FPIM: An Extreme Energy-Efficient DNN Acceleration System Using 3D NAND Flash-Based In-Situ PIM Unit},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00093},
doi = {10.1109/MICRO56248.2022.00093},
abstract = {The crossbar structure of the nonvolatile memory enables highly parallel and energy-efficient analog matrix-vector-multiply (MVM) operations. To exploit its efficiency, existing works design a mixed-signal deep neural network (DNN) accelerator, which offloads low-precision MVM operations to the memory array. However, they fail to accurately and efficiently support the low-precision networks due to their naive ADC designs. In addition, they cannot be applied to the latest technology nodes due to their premature RRAM-based memory array.In this work, we present 3D-FPIM, an energy-efficient and robust mixed-signal DNN acceleration system. 3D-FPIM is a full-stack 3D NAND flash-based architecture to accurately deploy low-precision networks. We design the hardware stack by carefully architecting a specialized analog-to-digital conversion method and utilizing the three-dimensional structure to achieve high accuracy, energy efficiency, and robustness. To accurately and efficiently deploy the networks, we provide a DNN retraining framework and a customized compiler. For evaluation, we implement an industry-validated circuit-level simulator. The result shows that 3D-FPIM achieves an average of 2.09x higher performance per area and 13.18x higher energy efficiency compared to the baseline 2D RRAM-based accelerator.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1359–1376},
numpages = {18},
keywords = {DNN, 3D NAND flash, mixed-signal accelerator},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00096,
author = {Wu, Yannan Nellie and Tsai, Po-An and Parashar, Angshuman and Sze, Vivienne and Emer, Joel S.},
title = {Sparseloop: An Analytical Approach to Sparse Tensor Accelerator Modeling},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00096},
doi = {10.1109/MICRO56248.2022.00096},
abstract = {In recent years, many accelerators have been proposed to efficiently process sparse tensor algebra applications (e.g., sparse neural networks). However, these proposals are single points in a large and diverse design space. The lack of systematic description and modeling support for these sparse tensor accelerators impedes hardware designers from efficient and effective design space exploration.This paper first presents a unified taxonomy to systematically describe the diverse sparse tensor accelerator design space. Based on the proposed taxonomy, it then introduces Sparseloop, the first fast, accurate, and flexible analytical modeling framework to enable early-stage evaluation and exploration of sparse tensor accelerators. Sparseloop comprehends a large set of architecture specifications, including various dataflows and sparse acceleration features (e.g., elimination of zero-based compute). Using these specifications, Sparseloop evaluates a design's processing speed and energy efficiency while accounting for data movement and compute incurred by the employed dataflow, including the savings and overhead introduced by the sparse acceleration features using stochastic density models.Across representative accelerator designs and workloads, Sparseloop achieves over 2000\texttimes{} faster modeling speed than cycle-level simulations, maintains relative performance trends, and achieves 0.1\% to 8\% average error. The paper also presents example use cases of Sparseloop in different accelerator design flows to reveal important design insights.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1377–1395},
numpages = {19},
keywords = {tensor computation, hardware accelerator, analytical modeling},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00094,
author = {Cai, Xuyi and Wang, Ying and Ma, Xiaohan and Han, Yinhe and Zhang, Lei},
title = {DeepBurning-SEG: Generating DNN Accelerators of Segment-Grained Pipeline Architecture},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00094},
doi = {10.1109/MICRO56248.2022.00094},
abstract = {The growing complexity and diversity of deep neural network (DNN) applications have inspired intensive research on specialized DNN accelerators and also the design automation frameworks. Previous specialized NN acceleratos roughly fall into two categories of implementation, either the no-pipelined architecture that relies on a generic processing unit (PU) to sequentially execute the DNN layers in a layer-wise way, or the fully-pipelined architecture that dedicates interconnected customized PUs to the corresponding DNN layers in the model. Thus, such designs often suffer from either the resource under-utilization issue faced by no-pipelined accelerators or the resource scalability problem brought by the over-deep pipeline designs.In this work, we propose a novel class of design solution for DNN acceleration, segment-grained pipeline architecture (SPA). In the SPA accelerator, the targeted workload of DNN models will be divided into many segments and each segment will be sequentially executed on the shared interconnected PUs in a pipeline manner, so that they will benefit from both the efficiency of pipelined execution and also the flexibility of sharing PUs across different model layers. Particularly, we found that the efficiency of the implemented SPA accelerator significantly depends on the segmentation strategies of the models and the hardware resources assignment policy for PUs. Therefore, we introduce an automated design framework, AutoSeg, that includes a parameterized SPA accelerator template and a co-design engine that will generate the efficient model segmentation solution and hardware pipeline design parameters for the acceleration workload. Experimental results show that the SPA solutions generated by the AutoSeg framework achieve 1.2\texttimes{} to 6.3\texttimes{} speedup when compared to ASIC-based general DNN processors, and the FPGA designs implemented by AutoSeg also achieve as high as 3.4\texttimes{} DSP efficiency and 3.6\texttimes{} throughput improvement.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1396–1413},
numpages = {18},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00095,
author = {Guo, Cong and Zhang, Chen and Leng, Jingwen and Liu, Zihan and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao},
title = {ANT: Exploiting Adaptive Numerical Data Type for Low-Bit Deep Neural Network Quantization},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00095},
doi = {10.1109/MICRO56248.2022.00095},
abstract = {Quantization is a technique to reduce the computation and memory cost of DNN models, which are getting increasingly large. Existing quantization solutions use fixed-point integer or floating-point types, which have limited benefits, as both require more bits to maintain the accuracy of original models. On the other hand, variable-length quantization uses low-bit quantization for normal values and high-precision for a fraction of outlier values. Even though this line of work brings algorithmic benefits, it also introduces significant hardware overheads due to variable-length encoding and decoding.In this work, we propose a fixed-length adaptive numerical data type called ANT to achieve low-bit quantization with tiny hardware overheads. Our data type ANT leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, we propose a particular data type, flint, that combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, we propose an adaptive framework that selects the best type for each tensor according to its distribution characteristics. We design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Our design results in 2.8\texttimes{} speedup and 2.5\texttimes{} energy efficiency improvement over the state-of-the-art quantization accelerators.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1414–1433},
numpages = {20},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1109/MICRO56248.2022.00097,
author = {Li, Gang and Xu, Weixiang and Song, Zhuoran and Jing, Naifeng and Cheng, Jian and Liang, Xiaoyao},
title = {Ristretto: An Atomized Processing Architecture for Sparsity-Condensed Stream Flow in CNN},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00097},
doi = {10.1109/MICRO56248.2022.00097},
abstract = {Low-precision quantization and sparsity have been widely explored in CNN acceleration due to their effectiveness in reducing computational complexity and memory requirements. However, to support variable numerical precision and sparse computation, prior accelerators design flexible multipliers or sparse dataflow separately. A uniform solution that simultaneously exploits mixed-precision and dual-sided irregular sparsity for CNN acceleration is still lacking. Through an in-depth review of existing precision-scalable and sparse accelerators, we observe that a direct combination of low-level multipliers and high-level sparse dataflow from both sides is challenging due to their orthogonal design spaces. To this end, in this paper, we propose condensed streaming computation. By representing non-zero weights and activations as atomized streams, the low-level mixed-precision multiplication and high-level sparse convolution can be unified into a shared dataflow through hierarchical data reuse. Based on the condensed streaming computation, we propose Ristretto, an atomized architecture that exploits both mixed-precision and dual-sided irregular sparsity for CNN inference. We implement Ristretto in a 28nm technology node. Extensive evaluations show that Ristretto consistently outperforms three state-of-the-art CNN accelerators, including Bit Fusion, Laconic, and SparTen, in terms of performance and energy efficiency.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1434–1450},
numpages = {17},
keywords = {mixed precision, dual-sided sparsity, condensed streaming computation, atomized processing architecture},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

