@inproceedings{10.1145/3637179,
author = {Bartolini, Davide Basilio},
title = {Session details: Best Paper Session},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637179},
doi = {10.1145/3637179},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614272,
author = {Koizumi, Toru and Shioya, Ryota and Sugita, Shu and Amano, Taichi and Degawa, Yuya and Kadomoto, Junichiro and Irie, Hidetsugu and Sakai, Shuichi},
title = {Clockhands: Rename-free Instruction Set Architecture for Out-of-order Processors},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614272},
doi = {10.1145/3613424.3614272},
abstract = {Out-of-order superscalar processors are currently the only architecture that speeds up irregular programs, but they suffer from poor power efficiency. To tackle this issue, we focused on how to specify register operands. Specifying operands by register names, as conventional RISC does, requires register renaming, resulting in poor power efficiency and preventing an increase in the front-end width. In contrast, a recently proposed architecture called STRAIGHT specifies operands by inter-instruction distance, thereby eliminating register renaming. However, STRAIGHT has strong constraints on instruction placement, which generally results in a large increase in the number of instructions. We propose Clockhands, a novel instruction set architecture that has multiple register groups and specifies a value as “the value written in this register group k times before.” Clockhands does not require register renaming as in STRAIGHT. In contrast, Clockhands has much looser constraints on instruction placement than STRAIGHT, allowing programs to be written with almost the same number of instructions as Conventional RISC. We implemented a cycle-accurate simulator, FPGA implementation, and first-step compiler for Clockhands and evaluated benchmarks including SPEC CPU. On a machine with an eight-fetch width, the evaluation results showed that Clockhands consumes 7.4\% less energy than RISC while having performance comparable to RISC. This energy reduction increases significantly to 24.4\% when simulating a futuristic up-scaled processor with a 16-fetch width, which shows that Clockhands enables a wider front-end.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1–16},
numpages = {16},
keywords = {Compiler, Instruction set architecture, Out-of-order execution, Power efficiency, Register lifetime, Register renaming, Superscalar processor},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614255,
author = {Naithani, Ajeya and Roelandts, Jaime and Ainsworth, Sam and Jones, Timothy M. and Eeckhout, Lieven},
title = {Decoupled Vector Runahead},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614255},
doi = {10.1145/3613424.3614255},
abstract = {We present Decoupled Vector Runahead (DVR), an in-core prefetching technique, executing separately to the main application thread, that exploits massive amounts of memory-level parallelism to improve the performance of applications featuring indirect memory accesses. DVR dynamically infers loop bounds at run-time, recognizing striding loads, and vectorizing subsequent instructions that are part of an indirect chain. It proactively issues memory accesses for the resulting loads far into the future, even when the out-of-order core has not yet stalled, bringing their data into the L1 cache, and thus providing timely prefetches for the main thread. DVR can adjust the degree of vectorization at run-time, vectorize the same chain of indirect memory accesses across multiple invocations of an inner loop, and efficiently handle branch divergence along the vectorized chain. DVR runs as an on-demand, speculative, in-order, lightweight hardware subthread alongside the main thread within the core and incurs a minimal hardware overhead of only 1139 bytes. Relative to a large superscalar 5-wide out-of-order baseline and Vector Runahead — a recent microarchitectural technique to accelerate indirect memory accesses on out-of-order processors — DVR delivers 2.4 \texttimes{} and 2 \texttimes{} higher performance, respectively, for a set of graph analytics, database, and HPC workloads.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {17–31},
numpages = {15},
keywords = {CPU microarchitecture, graph processing, prefetching, runahead, speculative vectorization},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614311,
author = {Alam, Faiz and Lee, Hyokeun and Bhattacharjee, Abhishek and Awad, Amro},
title = {CryptoMMU: Enabling Scalable and Secure Access Control of Third-Party Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614311},
doi = {10.1145/3613424.3614311},
abstract = {Due to increasing energy and performance gaps between general-purpose processors and hardware accelerators (e.g., FPGA or ASIC), clear trends for leveraging accelerators arise in various fields or workloads, such as edge devices, cloud systems, and data centers. Moreover, system integrators desire higher flexibility to deploy custom accelerators based on their performance, power, and cost constraints, where such integration can be as early as (1) at the design time when third-party intellectual properties (IPs) are used, (2) at integration/upgrade time when third-party discrete chip accelerators are used, or (3) during runtime as in reconfigurable logic. A malicious third-party accelerator can compromise the entire system by accessing other processes’ data, overwriting OS data structures, etc. To eliminate these security ramifications, a unit similar to a memory management unit (MMU), namely IOMMU, is typically used to scrutinize memory accesses from I/O devices, including accelerators. Still, IOMMU incurs significant performance overhead because it resides on the critical path of each I/O memory access. In this paper, we propose a novel scheme, CryptoMMU, to delegate the translation processes to accelerators, whereas the authentication of the targeted address is elegantly performed using a cryptography-based approach. As a result, CryptoMMU facilitates the private caching of translation in each accelerator, providing better scalability. Our evaluation results show that CryptoMMU improves system throughput by an average of 2.97 \texttimes{} and 1.13 \texttimes{} compared to the conventional IOMMU and the state-of-the-art solution, respectively. Importantly, CryptoMMU can be implemented without any software changes.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {32–48},
numpages = {17},
keywords = {IOMMU, accelerator-rich architecture, access control, cryptography},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614275,
author = {Wikner, Johannes and Trujillo, Dani\"{e}l and Razavi, Kaveh},
title = {Phantom: Exploiting Decoder-detectable Mispredictions},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614275},
doi = {10.1145/3613424.3614275},
abstract = {Violating the Von Neumann sequential processing principle at the microarchitectural level is commonplace to reach high performing CPU hardware — violations are safe as long as software executes correctly at the architectural interface. Speculative execution attacks exploit these violations and queue up secret-dependent memory accesses allowed by long speculation windows due to the late detection of these violations in the pipeline. In this paper, we show that recent AMD and Intel CPUs speculate very early in their pipeline, even before they decode the current instruction. This mechanism enables new sources of speculation to be triggered from almost any instruction, enabling a new class of attacks that we refer to as Phantom. Unlike Spectre, Phantom speculation windows are short since the violations are detected early. Nonetheless, Phantom allows for transient fetch and transient decode on all recent x86-based microarchitectures, and transient execution on AMD Zen 1 and 2. We build a number of exploits using these new Phantom primitives and discuss why mitigating them is difficult in practice.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {49–61},
numpages = {13},
keywords = {Branch target injection, Side-channel attack, Spectre, Speculative execution},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637180,
author = {Pellauer, Michael},
title = {Session details: Session 1A: Accelerators Based on HW/SW Co-Design Accelerators for Matrix Processing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637180},
doi = {10.1145/3637180},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614280,
author = {Kim, Seah and Zhao, Jerry and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},
title = {AuRORA: Virtualized Accelerator Orchestration for Multi-Tenant Workloads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614280},
doi = {10.1145/3613424.3614280},
abstract = {With the widespread adoption of deep neural networks (DNNs) across applications, there is a growing demand for DNN deployment solutions that can seamlessly support multi-tenant execution. This involves simultaneously running multiple DNN workloads on heterogeneous architectures with domain-specific accelerators. However, existing accelerator interfaces directly bind the accelerator’s physical resources to user threads, without an efficient mechanism to adaptively re-partition available resources. This leads to high programming complexities and performance overheads due to sub-optimal resource allocation, making scalable many-accelerator deployment impractical. To address this challenge, we propose AuRORA, a novel accelerator integration methodology that enables scalable accelerator deployment for multi-tenant workloads. In particular, AuRORA supports virtualized accelerator orchestration via co-designing the hardware-software stack of accelerators to allow adaptively binding current workloads onto available accelerators. We demonstrate that AuRORA achieves 2.02 higher overall SLA satisfaction, 1.33 overall system throughput, and 1.34 overall fairness compared to existing accelerator integration solutions with less than 2.7\% area overhead.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {62–76},
numpages = {15},
keywords = {Accelerators, Machine Learning, Microarchitecture, Multi-core, Multi-tenant system, Resource Management, SoC Integration},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614282,
author = {Rashidi, Bahador and Gao, Chao and Lu, Shan and Wang, Zhisheng and Zhou, Chunhua and Niu, Di and Sun, Fengyu},
title = {UNICO: Unified Hardware Software Co-Optimization for Robust Neural Network Acceleration},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614282},
doi = {10.1145/3613424.3614282},
abstract = {Specialized hardware has become an indispensable component to deep neural network (DNN) acceleration. To keep up with the rapid evolution of neural networks, holistic and automated solutions for jointly optimizing both hardware (HW) architectures and software (SW) mapping have been studied. These studies face two major challenges. First, the combined HW-SW design space is vast, which hinders the finding of optimal or near-optimal designs. This issue is exacerbated for industrial cases when cycle accurate models are used for design evaluation in the joint optimization. Second, HW design is prone to overfitting to the input DNNs used in the HW-SW co-optimization. To address these issues, in this paper, we propose UNICO, an efficient Unified Co-Optimization framework with a novel Robustness metric for better HW generalization. Guided by a high-fidelity surrogate model, UNICO employs multi-objective Bayesian optimization to effectively explore the HW design space, and conducts adaptive, parallel and scalable software mapping search based on successive halving. To reduce HW overfitting, we propose a HW robustness metric by relating a HW configuration’s quality to its sensitivity in software mapping search, and quantitatively incorporate this metric to search for more robust HW design(s). We implement UNICO in open source accelerator platform, and compare it with the state-of-the-art solution HASCO. Experiments show that UNICO significantly outperforms HASCO; it finds design(s) with similar quality to HASCO up to 4 \texttimes{} faster, and eventually converges to better and more robust designs. Finally, we deploy UNICO for optimizing an industrial accelerator, and show that it generates enhanced HW design(s) for key real-world DNNs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {77–90},
numpages = {14},
keywords = {HW Robustness, HW-SW Co-Design, Multi-Level Optimization, Neural Network Accelerator},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623783,
author = {Feldmann, Axel and Sanchez, Daniel},
title = {Spatula: A Hardware Accelerator for Sparse Matrix Factorization},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623783},
doi = {10.1145/3613424.3623783},
abstract = {Solving sparse systems of linear equations is a crucial component in many science and engineering problems, like simulating physical systems. Sparse matrix factorization dominates a large class of these solvers. Efficient factorization algorithms have two key properties that make them challenging for existing architectures: they consist of small tasks that are structured and compute-intensive, and sparsity induces long chains of data dependences among these tasks. Data dependences make GPUs struggle, while CPUs and prior sparse linear algebra accelerators also suffer from low compute throughput. We present Spatula, an architecture for accelerating sparse matrix factorization algorithms. Spatula hardware combines systolic processing elements that execute structured tasks at high throughput with a flexible scheduler that handles challenging data dependences. Spatula enables a novel scheduling algorithm that avoids stalls and load imbalance while reducing data movement, achieving high compute utilization. As a result, Spatula outperforms a GPU running the state-of-the-art sparse Cholesky and LU factorization implementations by gmean 47 \texttimes{} across a wide range of matrices, and by up to thousands of times on some challenging matrices.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {91–104},
numpages = {14},
keywords = {Cholesky, Hardware accelerators, LU., matrix factorization, sparse linear algebra},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637181,
author = {Ghose, Saugata},
title = {Session details: Session 1B: Architectural Support/ Programming Languages, Case Study},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637181},
doi = {10.1145/3637181},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614256,
author = {Sun, Yan and Yuan, Yifan and Yu, Zeduo and Kuper, Reese and Song, Chihun and Huang, Jinghan and Ji, Houxiang and Agarwal, Siddharth and Lou, Jiaqi and Jeong, Ipoom and Wang, Ren and Ahn, Jung Ho and Xu, Tianyin and Kim, Nam Sung},
title = {Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614256},
doi = {10.1145/3613424.3614256},
abstract = {The ever-growing demands for memory with larger capacity and higher bandwidth have driven recent innovations on memory expansion and disaggregation technologies based on Compute eXpress Link (CXL). Especially, CXL-based memory expansion technology has recently gained notable attention for its ability not only to economically expand memory capacity and bandwidth but also to decouple memory technologies from a specific memory interface of the CPU. However, since CXL memory devices have not been widely available, they have been emulated using DDR memory in a remote NUMA node. In this paper, for the first time, we comprehensively evaluate a true CXL-ready system based on the latest 4th-generation Intel Xeon CPU with three CXL memory devices from different manufacturers. Specifically, we run a set of microbenchmarks not only to compare the performance of true CXL memory with that of emulated CXL memory but also to analyze the complex interplay between the CPU and CXL memory in depth. This reveals important differences between emulated CXL memory and true CXL memory, some of which will compel researchers to revisit the analyses and proposals from recent work. Next, we identify opportunities for memory-bandwidth-intensive applications to benefit from the use of CXL memory. Lastly, we propose a CXL-memory-aware dynamic page allocation policy, Caption to more efficiently use CXL memory as a bandwidth expander. We demonstrate that Caption can automatically converge to an empirically favorable percentage of pages allocated to CXL memory, which improves the performance of memory-bandwidth-intensive applications by up to 24\% when compared to the default page allocation policy designed for traditional NUMA systems.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {105–121},
numpages = {17},
keywords = {Compute eXpress Link, measurement, tiered-memory management},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623795,
author = {Wang, Ziqi and Zhao, Kaiyang and Li, Pei and Jacob, Andrew and Kozuch, Michael and Mowry, Todd and Skarlatos, Dimitrios},
title = {Memento: Architectural Support for Ephemeral Memory Management in Serverless Environments},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623795},
doi = {10.1145/3613424.3623795},
abstract = {Serverless computing is an increasingly attractive paradigm in the cloud due to its ease of use and fine-grained pay-for-what-you-use billing. However, serverless computing poses new challenges to system design due to its short-lived function execution model. Our detailed analysis reveals that memory management is responsible for a major amount of function execution cycles. This is because functions pay the full critical-path costs of memory management in both userspace and the operating system without the opportunity to amortize these costs over their short lifetimes. To address this problem, we propose Memento, a new hardware-centric memory management design based upon our insights that memory allocations in serverless functions are typically small, and either quickly freed after allocation or freed when the function exits. Memento alleviates the overheads of serverless memory management by introducing two key mechanisms: (i) a hardware object allocator that performs in-cache memory allocation and free operations based on arenas, and (ii) a hardware page allocator that manages a small pool of physical pages used to replenish arenas of the object allocator. Together these mechanisms alleviate memory management overheads and bypass costly userspace and kernel operations. Memento naturally integrates with existing software stacks through a set of ISA extensions that enable seamless integration with multiple languages runtimes. Finally, Memento leverages the newly exposed memory allocation semantics in hardware to introduce a main memory bypass mechanism and avoid unnecessary DRAM accesses for newly allocated objects. We evaluate Memento with full-system simulations across a diverse set of containerized serverless workloads and language runtimes. The results show that Memento achieves function execution speedups ranging between 8–28\% and 16\% on average. Furthermore, Memento hardware allocators and main memory bypass mechanisms drastically reduce main memory traffic by 30\% on average. The combined effects of Memento reduce the pricing cost of function execution by 29\%. Finally, we demonstrate the applicability of Memento beyond functions, to major serverless platform operations and long-running data processing applications.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {122–136},
numpages = {15},
keywords = {Cloud computing, Function-as-a-Service, Memory Management, Serverless},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614285,
author = {Hsu, Kuan-Chieh and Tseng, Hung-Wei},
title = {Simultaneous and Heterogenous Multithreading},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614285},
doi = {10.1145/3613424.3614285},
abstract = {The landscape of modern computers is undoubtedly heterogeneous, as all computing platforms integrate multiple types of processing units and hardware accelerators. However, the entrenched programming models focus on using only the most efficient processing units for each code region, underutilizing the processing power within heterogeneous computers. This paper simultaneous and heterogenous multithreading (SHMT), a programming and execution model that enables opportunities for “real” parallel processing using heterogeneous processing units. In contrast to conventional models, SHMT can utilize heterogeneous types of processing units concurrently for the same code region. Furthermore, SHMT presents an abstraction and a runtime system to facilitate parallel execution. More importantly, SHMT needs to additionally address the heterogeneity in data precision that various processing units support to ensure the quality of the result. This paper implements and evaluates SHMT on an embedded system platform with a GPU and an Edge TPU. SHMT achieves up to 1.95 \texttimes{} speedup and &nbsp;51.0\% energy reduction compared to GPU baseline.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {137–152},
numpages = {16},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637182,
author = {Jeffrey, Mark},
title = {Session details: Session 1C: Design Automation, Synthesis, Hardware Generation},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637182},
doi = {10.1145/3637182},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614257,
author = {Elsabbagh, Fares and Sheikhha, Shabnam and Ying, Victor A. and Nguyen, Quan M. and Emer, Joel S and Sanchez, Daniel},
title = {Accelerating RTL Simulation with Hardware-Software Co-Design},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614257},
doi = {10.1145/3613424.3614257},
abstract = {Fast simulation of digital circuits is crucial to build modern chips. But RTL (Register-Transfer-Level) simulators are slow, as they cannot exploit multicores well. Slow simulation lengthens chip design time and makes bugs more frequent. We present ASH, a parallel architecture tailored to simulation workloads. ASH consists of a tightly codesigned hardware architecture and compiler for RTL simulation. ASH exploits two key opportunities. First, it performs dataflow execution of small tasks to leverage the fine-grained parallelism in simulation workloads. Second, it performs selective event-driven execution to run only the fraction of the design exercised each cycle, skipping ineffectual tasks. ASH hardware provides a novel combination of dataflow and speculative execution, and ASH’s compiler features several novel techniques to automatically leverage this hardware. We evaluate ASH in simulation using large Verilog designs. An ASH chip with 256 simple cores is gmean 1,485 \texttimes{} faster than 1-core Verilator, and it is 32 \texttimes{} faster than parallel Verilator on a server CPU with 32 complex cores, while using 3 \texttimes{} less area.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {153–166},
numpages = {14},
keywords = {dataflow execution, domain-specific architectures., hardware acceleration, register-transfer-level, simulation, speculative execution},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623794,
author = {Xu, Ceyu and Sharma, Pragya and Wang, Tianshu and Wills, Lisa Wu},
title = {Fast, Robust and Transferable Prediction for Hardware Logic Synthesis},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623794},
doi = {10.1145/3613424.3623794},
abstract = {The increasing complexity of computer chips and the slow logic synthesis process have become major bottlenecks in the hardware design process, also hindering the ability of hardware generators to make informed design decisions while considering hardware costs. While various models have been proposed to predict physical characteristics of hardware designs, they often suffer from limited domain adaptability and open-source hardware design data scarcity. In this paper, we present SNS v2, a fast, robust, and transferable hardware synthesis predictor based on deep learning models. Inspired by modern natural language processing models, SNS v2 adopts a three-phase training approach encompassing pre-training, fine-tuning, and domain adaptation, enabling it to leverage more abundant unlabeled and off-domain training data. Additionally, we propose a novel contrastive learning approach based on circuit equivalence to enhance model robustness. Our experiments demonstrate that SNS v2 achieves two to three orders of magnitude faster speed compared to conventional EDA tools, while maintaining state-of-the-art prediction accuracy. We also show that SNS v2 can be seamlessly integrated into hardware generator frameworks for real-time cost estimation, resulting in higher quality design recommendations in a significantly reduced time frame.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {167–179},
numpages = {13},
keywords = {Integrated Circuits, Logic Synthesis Prediction, Neural Networks, RTL-level Synthesis},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614301,
author = {Zhou, Kexing and Liang, Yun and Lin, Yibo and Wang, Runsheng and Huang, Ru},
title = {Khronos: Fusing Memory Access for Improved Hardware RTL Simulation},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614301},
doi = {10.1145/3613424.3614301},
abstract = {The use of register transfer level (RTL) simulation is critical for hardware design in various aspects including verification, debugging, and design space exploration. Among various RTL simulation techniques, cycle-accurate software RTL simulation is the most prevalent approach due to its easy accessibility and high flexibility. The current state-of-the-art cycle-accurate simulators mainly use full-cycle RTL simulation that models RTL as a directed acyclic computational graph and traverses the graph in each simulation cycle. However, the adoption of full-cycle simulation makes them mainly focus on optimizing the logic evaluation within one simulation cycle, neglecting temporal optimization opportunities. In this work, we propose Khronos, a cycle-accurate software RTL simulation tool that optimizes the memory accesses to improve simulation speed. RTL simulation often involves a large number of register buffers, making memory access one of the performance bottlenecks. The key insight of Khronos is that a large number of memory accesses across consecutive clock cycles exhibit temporal localities, by fusing those accesses we can reduce the memory traffic and thus improve the overall performance. In order to do this, we first propose a queue-connected operation graph to capture temporal data dependencies. After that, we reschedule the operations and fuse the state access across cycles, reducing the pressure on the host memory hierarchy. To minimize the number of memory accesses, we formulate a linear-constraint non-linear objective integer programming problem and solve it by linearizing to a minimum-cost flow problem iteratively. Experiments show that Khronos can save up to 88\% of cache access and achieve an average acceleration of 2.0x (up to 4.3x) for various hardware designs compared to state-of-the-art simulators.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {180–193},
numpages = {14},
keywords = {Hardware simulation and emulation, Memory access optimization, Register transfer level simulation},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637183,
author = {Krishna, Tushar},
title = {Session details: Session 2A: ML Design Space ExplorationGeneration},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637183},
doi = {10.1145/3637183},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614273,
author = {Lee, Kyungmi and Yan, Mengjia and Emer, Joel and Chandrakasan, Anantha},
title = {SecureLoop: Design Space Exploration of Secure DNN Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614273},
doi = {10.1145/3613424.3614273},
abstract = {Deep neural networks (DNNs) are gaining popularity in a wide range of domains, ranging from speech and video recognition to healthcare. With this increased adoption comes the pressing need for securing DNN execution environments on CPUs, GPUs, and ASICs. While there are active research efforts in supporting a trusted execution environment (TEE) on CPUs, the exploration in supporting TEEs on accelerators is limited, with only a few solutions available [18, 19, 27]. A key limitation along this line of work is that these secure DNN accelerators narrowly consider a few specific architectures. The design choices and the associated cost for securing these architectures do not transfer to other diverse architectures. This paper strives to address this limitation by developing a design space exploration tool for supporting TEEs on diverse DNN accelerators. We target secure DNN accelerators equipped with cryptographic engines where the cryptographic operations are closely coupled with the data movement in the accelerators. These operations significantly complicate the scheduling for DNN accelerators, as the scheduling needs to account for the extra on-chip computation and off-chip memory accesses introduced by these cryptographic operations, and even needs to account for potential interactions across DNN layers. We tackle these challenges in our tool, called SecureLoop, by introducing a scheduling search engine with the following attributes: 1) considers the cryptographic overhead associated with every off-chip data access, 2) uses an efficient modular arithmetic technique to compute the optimal authentication block assignment for each individual layer, and 3) uses a simulated annealing algorithm to perform cross-layer optimizations. Compared to the conventional schedulers, our tool finds the schedule for secure DNN designs with up to 33.2\% speedup and 50.2\% improvement of energy-delay-product.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {194–208},
numpages = {15},
keywords = {Trusted execution environment, accelerator scheduling, neural networks},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623797,
author = {Hong, Charles and Huang, Qijing and Dinh, Grace and Subedar, Mahesh and Shao, Yakun Sophia},
title = {DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623797},
doi = {10.1145/3613424.3623797},
abstract = {In the hardware design space exploration process, it is critical to optimize both hardware parameters and algorithm-to-hardware mappings. Previous work has largely approached this simultaneous optimization problem by separately exploring the hardware design space and the mapspace—both individually large and highly nonconvex spaces—independently. The resulting combinatorial explosion has created significant difficulties for optimizers. In this paper, we introduce DOSA, which consists of differentiable performance models and a gradient descent-based optimization technique to simultaneously explore both spaces and identify high-performing design points. Experimental results demonstrate that DOSA outperforms random search and Bayesian optimization by 2.80 \texttimes{} and 12.59 \texttimes{}, respectively, in improving DNN model energy-delay product, given a similar number of samples. We also demonstrate the modularity and flexibility of DOSA by augmenting our analytical model with a learned model, allowing us to optimize buffer sizes and mappings of a real DNN accelerator and attain a 1.82 \texttimes{} improvement in energy-delay product.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {209–224},
numpages = {16},
keywords = {Design space exploration, Machine learning accelerators},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614303,
author = {Tang, Haotian and Yang, Shang and Liu, Zhijian and Hong, Ke and Yu, Zhongming and Li, Xiuyu and Dai, Guohao and Wang, Yu and Han, Song},
title = {TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614303},
doi = {10.1145/3613424.3614303},
abstract = {Sparse convolution plays a pivotal role in emerging workloads, including point cloud processing in AR/VR, autonomous driving, and graph understanding in recommendation systems. Since the computation pattern is sparse and irregular, specialized high-performance kernels are required. Existing GPU libraries offer two dataflow types for sparse convolution. The gather-GEMM-scatter dataflow is easy to implement but not optimal in performance, while the dataflows with overlapped computation and memory access (e.g. implicit GEMM) are highly performant but have very high engineering costs. In this paper, we introduce TorchSparse++, a new GPU library that achieves the best of both worlds. We create a highly efficient Sparse Kernel Generator that generates performant sparse convolution kernels at less than one-tenth of the engineering cost of the current state-of-the-art system. On top of this, we design the Sparse Autotuner, which extends the design space of existing sparse convolution libraries and searches for the best dataflow configurations for training and inference workloads. Consequently, TorchSparse++ achieves 2.9 \texttimes{} , 3.3 \texttimes{} , 2.2 \texttimes{} and 1.7 \texttimes{} measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is 1.2-1.3 \texttimes{} faster than SpConv v2 in mixed precision training across seven representative autonomous driving benchmarks. It also seamlessly supports graph convolutions, achieving 2.6-7.6 \texttimes{} faster inference speed compared with state-of-the-art graph deep learning libraries. Our code is publicly released at https://github.com/mit-han-lab/torchsparse.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {225–239},
numpages = {15},
keywords = {GPU, graph, high-performance computing, neural network, point cloud, sparse convolution},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637184,
author = {Sorin, Daniel},
title = {Session details: Session 2B: Microarchitecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637184},
doi = {10.1145/3637184},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623774,
author = {Perais, Arthur and Sheikh, Rami},
title = {Branch Target Buffer Organizations},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623774},
doi = {10.1145/3613424.3623774},
abstract = {To accommodate very large instruction footprints, modern high-performance processors rely on fetch directed instruction prefetching through huge branch predictors and a hierarchy of Branch Target Buffers (BTBs). Recently, significant effort has been undertaken to reduce the footprint of each branch in the BTB, in order to either minimize the storage occupied by the BTB on die, or to increase the number of tracked branches at iso-storage. However, designing for branch density, while necessary, is only one dimension of BTB efficacy. In particular, BTB entry organization plays a significant role in improving instruction fetch throughput, which is a necessary step towards increased performance. In this paper, we first revisit the advantages and drawbacks of three classical BTB organizations in the context of multi-level BTB hierarchies. We then consider three possible improvements to increase the fetch PC throughput of the Region BTB and Block BTB organizations, bridging most of the performance gap with the impractical but highly storage-efficient Instruction BTB organization, thus paving the way for future very high fetch throughput machines.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {240–253},
numpages = {14},
keywords = {BTB, Branch Target Buffers, Instruction fetch},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614258,
author = {Schall, David and Sandberg, Andreas and Grot, Boris},
title = {Warming Up a Cold Front-End with Ignite},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614258},
doi = {10.1145/3613424.3614258},
abstract = {Serverless computing is a popular software deployment model for the cloud, in which applications are designed as a collection of stateless tasks. Developers are charged for the CPU time and memory footprint during the execution of each serverless function, which incentivizes them to reduce both runtime and memory usage. As a result, functions tend to be short (often on the order of a few milliseconds) and compact (128–256&nbsp;MB). Cloud providers can pack thousands of such functions on a server, resulting in frequent context switches and a tremendous degree of interleaving. As a result, when a given memory-resident function is re-invoked, it commonly finds its on-chip microarchitectural state completely cold due to thrashing by other functions&nbsp;— a phenomenon termed lukewarm invocation. Our analysis shows that the cold microarchitectural state due to lukewarm invocations is highly detrimental to performance, which corroborates prior work. The main source of performance degradation is the front-end, composed of instruction delivery, branch identification via the BTB and the conditional branch prediction. State-of-the-art front-end prefetchers show only limited effectiveness on lukewarm invocations, falling considerably short of an ideal front-end. We demonstrate that the reason for this is the cold microarchitectural state of the branch identification and prediction units. In response, we introduce Ignite, a comprehensive restoration mechanism for front-end microarchitectural state targeting instructions, BTB and branch predictor via unified metadata. Ignite records an invocation’s control flow graph in compressed format and uses that to restore the front-end structures the next time the function is invoked. Ignite outperforms state-of-the-art front-end prefetchers, improving performance by an average of 43\% by significantly reducing instruction, BTB and branch predictor MPKI.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {254–267},
numpages = {14},
keywords = {Microarchitecture, front-end prefetching and serverless, instruction delivery},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614289,
author = {Bai, Chen and Huang, Jiayi and Wei, Xuechao and Ma, Yuzhe and Li, Sicheng and Zheng, Hongzhong and Yu, Bei and Xie, Yuan},
title = {ArchExplorer: Microarchitecture Exploration Via Bottleneck Analysis},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614289},
doi = {10.1145/3613424.3614289},
abstract = {Design space exploration (DSE) for microarchitecture parameters is an essential stage in microprocessor design to explore the trade-offs among performance, power, and area (PPA). Prior work either employs excessive expert efforts to guide microarchitecture parameter tuning or demands high computing resources to prepare datasets and train black-box prediction models for DSE. In this work, we aim to circumvent the domain knowledge requirements through automated bottleneck analysis and propose ArchExplorer, which reveals microarchitecture bottlenecks to guide DSE with much fewer simulations. ArchExplorer consists of a new graph formulation of microexecution, an optimal critical path construction algorithm, and hardware resource reassignment strategies. Specifically, the critical path is constructed from the microexecution to uncover the performance-critical microarchitecture bottlenecks, which facilitates ArchExplorer to reclaim the hardware budgets of performance-insensitive structures that consume unnecessary power and area. These budgets are then reassigned to the microarchitecture bottlenecks for performance boost while maintaining the power and area constraints under the total budget envelope. Experiments show that ArchExplorer can find better PPA Pareto-optimal designs, achieving an average of higher Pareto hypervolume using at most fewer simulations compared to the state-of-the-art approaches.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {268–282},
numpages = {15},
keywords = {Design Space Exploration, Microarchitecture, Microprocessor},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637185,
author = {Neuman, Sabrina},
title = {Session details: Session 2C: Accelerators for Graphs, Robotics},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637185},
doi = {10.1145/3637185},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614292,
author = {Zeng, Shulin and Zhu, Zhenhua and Liu, Jun and Zhang, Haoyu and Dai, Guohao and Zhou, Zixuan and Li, Shuangchen and Ning, Xuefei and Xie, Yuan and Yang, Huazhong and Wang, Yu},
title = {DF-GAS: a Distributed FPGA-as-a-Service Architecture towards Billion-Scale Graph-based Approximate Nearest Neighbor Search},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614292},
doi = {10.1145/3613424.3614292},
abstract = {Embedding retrieval is a crucial task for recommendation systems. Graph-based approximate nearest neighbor search (GANNS) is the most commonly used method for retrieval, and achieves the best performance on billion-scale datasets. Unfortunately, the existing CPU- and GPU-based GANNS systems are difficult to optimize the throughput under the latency constraints on billion-scale datasets, due to the underutilized local memory bandwidth (5-45\%) and the expensive remote data access overhead (∼ 85\% of the total latency). In this paper, we first introduce a practically ideal GANNS architecture for billion-scale datasets, which facilitates a detailed analysis of the challenges and characteristics of distributed GANNS systems. Then, at the architecture level, we propose DF-GAS, a Distributed FPGA-as-a-Service (FPaaS) architecture for accelerating billion-scale Graph-based Approximate nearest neighbor Search. DF-GAS uses a feature-packing memory access engine and a data prefetching and delayed processing scheme to increase local memory bandwidth by 36-42\% and reduce remote data access overhead by 76.2\%, respectively. At the system level, we exploit the “full-graph + sub-graph” hybrid parallel search scheme on distributed FPaaS system. It achieves million-level query-per-second with sub-millisecond latency on billion-scale GANNS for the first time. Extensive evaluations on million-scale and billion-scale datasets show that DF-GAS achieves an average of 55.4 \texttimes{}, 32.2 \texttimes{}, 5.4 \texttimes{}, and 4.4 \texttimes{} better latency-bounded throughput than CPUs, GPUs, and two state-of-the-art ANNS architectures, i.e., ANNA [23] and Vstore [27], respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {283–296},
numpages = {14},
keywords = {Approximate Nearest Neighbor Search, Distributed Architecture, Embedding Retrieval, FPGA, Graph},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614298,
author = {Yang, Yuxin and Chen, Xiaoming and Han, Yinhe},
title = {Dadu-RBD: Robot Rigid Body Dynamics Accelerator with Multifunctional Pipelines},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614298},
doi = {10.1145/3613424.3614298},
abstract = {Rigid body dynamics is a core technology in the robotics field. In trajectory optimization and model predictive control algorithms, there are usually a large number of rigid body dynamics computing tasks. Using CPUs to process these tasks consumes a lot of time, which will affect the real-time performance of robots. To this end, we propose a multifunctional robot rigid body dynamics accelerator, named Dadu-RBD, to address the performance bottleneck. By analyzing different functions commonly used in robot dynamics calculations, we summarize their relationships and characteristics, then optimize them according to the hardware. Based on this, Dadu-RBD can fully reuse common hardware modules when processing different computing tasks. By dynamically switching the dataflow path, Dadu-RBD can accelerate various dynamics functions without reconfiguring the hardware. We design the Round-Trip Pipeline and Structure-Adaptive Pipelines for Dadu-RBD, which can greatly improve the throughput of the accelerator. Robots with different structures and parameters can be optimized specifically. Compared with the state-of-the-art CPU, GPU dynamics libraries and FPGA accelerator, Dadu-RBD can significantly improve the performance.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {297–309},
numpages = {13},
keywords = {Accelerator, Dataflow, Multifunctional, Pipeline, Rigid Body Dynamics, Robotics},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614260,
author = {Gao, Chao and Afarin, Mahbod and Rahman, Shafiur and Abu-Ghazaleh, Nael and Gupta, Rajiv},
title = {MEGA Evolving Graph Accelerator},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614260},
doi = {10.1145/3613424.3614260},
abstract = {Graph Processing is an emerging workload for applications working with unstructured data, such as social network analysis, transportation networks, bioinformatics and operations research. We examine the problem of graph analytics over evolving graphs, which are graphs that change over time. The problem is challenging because it requires evaluation of a graph query on a sequence of graph snapshots over a time window, typically to track the progression of a property over time. In this paper, we introduce MEGA, a hardware accelerator designed for efficiently evaluating queries over evolving graphs. MEGA leverages CommonGraph, a recently proposed software approach for incrementally processing evolving graphs that gains efficiency by avoiding the need to process expensive deletions by converting them into additions. MEGA supports incremental event-based streaming of edge additions as well as execution of multiple snapshots concurrently to support evolving graphs. We propose Batch-Oriented-Execution (BOE), a novel batch-update scheduling technique that activates snapshots that share batches simultaneously to achieve both computation and data reuse. We introduce optimizations that pack compatible batches together, and pipeline batch processing. To the best of our knowledge, MEGA is the first graph accelerator for evolving graphs that evaluates graph queries over multiple snapshots simultaneously. MEGA achieves 24 \texttimes{} -120 \texttimes{} speedup over CommonGraph. It also achieves speedups ranging from 4.08 \texttimes{} to 5.98 \texttimes{} over JetStream, a state-of-the-art streaming graph accelerator.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {310–323},
numpages = {14},
keywords = {batch oriented execution, common graph, evolving graphs, iterative graph algorithms, redundancy removal, temporal locality},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637186,
author = {Panda, Biswabandan},
title = {Session details: Session 3A: ML Sparsity},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637186},
doi = {10.1145/3637186},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614312,
author = {Gondimalla, Ashish and Thottethodi, Mithuna and Vijaykumar, T. N.},
title = {Eureka: Efficient Tensor Cores for One-sided Unstructured Sparsity in DNN Inference},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614312},
doi = {10.1145/3613424.3614312},
abstract = {Deep neural networks (DNNs), while enormously popular, continue to place ever higher compute demand for which GPUs provide specialized matrix multipliers called tensor cores. To reduce the compute demand via sparsity, Nvidia Ampere’s tensor cores support 2:4 structured sparsity in the filters (i.e., two non-zeros out of four values) which provides uniform 50\% sparsity without any load imbalance issues. Consequently, the sparse tensor cores maintain (input or output) operand stationarity, which is fundamental for avoiding high-overhead hardware, requiring only one extra 4-1 multiplexer per multiply-accumulate unit (MAC). However, 2:4 sparsity is limited to 2x improvements in performance and energy without loss of accuracy, whereas unstructured sparsity provides 5-6x opportunity albeit while causing load imbalance. Previous papers on unstructured sparsity incur high hardware overhead (e.g., buffering, crossbars, scatter-gather networks, and address calculators) mainly due to sacrificing operand stationarity in favor of load balance. To avoid adding high overheads to the highly-efficient tensor cores, we propose Eureka, an efficient tensor core for unstructured sparsity. Eureka addresses load imbalance via three contributions: (1) Our key insight is that a slight weakening of output stationarity achieves load balance most of the time while incurring only a modest hardware overhead. Accordingly, we propose single-step uni-directional displacement (SUDS), where a filter element’s multiplication can either occur in its original position or be displaced to a vacant MAC in the adjacent row below while the accumulation occurs in the original row to restore output stationarity. SUDS is an offline technique for inference. (2) We provide an optimal algorithm for work assignment for SUDS. (3) To achieve fewer bubbles in the tensor core’s systolic pipeline due to the irregularity of unstructured sparsity, we propose offline systolic scheduling to group together the sparse filters with similar, statically-known execution times (based on the number of non-zeros). Our evaluation shows that Eureka achieves 4.8x and 2.4x speedups, and 3.1x and 1.8x energy reductions over dense and 2:4 sparse (Ampere) implementations, respectively, and incurs area and power overheads of 6\% and 11.5\%, respectively, over Ampere.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {324–337},
numpages = {14},
keywords = {Tensor cores, deep neural network (DNN) inference. one-sided sparsity, unstructured sparsity},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623775,
author = {Huang, Guyue and Wang, Zhengyang and Tsai, Po-An and Zhang, Chen and Ding, Yufei and Xie, Yuan},
title = {RM-STC: Row-Merge Dataflow Inspired GPU Sparse Tensor Core for Energy-Efficient Sparse Acceleration},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623775},
doi = {10.1145/3613424.3623775},
abstract = {This paper proposes RM-STC, a novel GPU tensor core architecture designed for sparse Deep Neural Networks (DNNs) with two key innovations: (1) native support for both training and inference and (2) high efficiency for all sparsity degrees. To achieve the first goal, RM-STC employs a uniform sparse encoding scheme that natively supports all operations holistically in forward and backward passes, thereby eliminating the need for costly sparse encoding transformation in between. For the second goal, RM-STC takes inspiration from the row-merge dataflow and combines the input-gathering and output-scattering hardware features to minimize the energy overhead. Experiments show that RM-STC achieves significant speedups and energy efficiency improvements over dense tensor cores and previous sparse tensor cores.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {338–352},
numpages = {15},
keywords = {DNN Acceleration, GPU, Sparse Tensor Core},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614263,
author = {Fan, Hongxiang and Venieris, Stylianos I. and Kouris, Alexandros and Lane, Nicholas},
title = {Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614263},
doi = {10.1145/3613424.3614263},
abstract = {Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10\% decrease in latency constraint violation rate and nearly 4 \texttimes{} reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {353–366},
numpages = {14},
keywords = {Algorithm and Hardware Co-Design, Dynamic and Static Approach, Sparse Multi-DNN Scheduling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637187,
author = {Vijaykumar, Nandita and Neuman, Sabrina},
title = {Session details: Session 3B: GPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637187},
doi = {10.1145/3637187},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614247,
author = {Sung, Seunghwan and Hur, Sujin and Kim, Sungwoo and Ha, Dongho and Oh, Yunho and Ro, Won Woo},
title = {MAD MAcce: Supporting Multiply-Add Operations for Democratizing Matrix-Multiplication Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614247},
doi = {10.1145/3613424.3614247},
abstract = {Modern GPUs commonly employ specialized matrix multiplication units (MXUs) to accelerate matrix multiplication, the core computation of deep learning workloads. However, it is challenging to exploit the MXUs for GPGPU applications whose fundamental algorithms do not rely on matrix multiplication. Furthermore, an additional programming effort is necessary to tailor existing code or algorithms using dedicated APIs or libraries to utilize MXUs. Therefore, MXUs are often underutilized even when GPUs hunger for higher throughput. We observe that the intensive multiply-and-add (MAD) instructions often become bottlenecks in compute-intensive applications. Furthermore, such MAD instructions create computations similar to the dot-product operations of MXUs when they have data dependency. By leveraging these observations, we propose a novel MXU architecture called MAD MAcce that can handle both matrix multiplication and MAD operations. In our design, GPU compiler detects target MAD instructions by analyzing the instruction stream and generates new instructions for MAD Macce in a programmer-transparent manner. Then, MAD MAcce executes the newly generated instructions. By offloading MAD operations to the MXUs, GPUs can exploit the high throughput of MXUs for various domains without significant hardware modification or additional programming efforts. In our evaluation, MAD MAcce achieves up to 2.13 \texttimes{} speedup and 1.65 \texttimes{} average speedup in compute-intensive applications.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {367–379},
numpages = {13},
keywords = {GPU, High Performance Computing, Tensor Cores},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614277,
author = {Li, Ying and Sun, Yifan and Jog, Adwait},
title = {Path Forward Beyond Simulators: Fast and Accurate GPU Execution Time Prediction for DNN Workloads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614277},
doi = {10.1145/3613424.3614277},
abstract = {Today, DNNs’ high computational complexity and sub-optimal device utilization present a major roadblock to democratizing DNNs. To reduce the execution time and improve device utilization, researchers have been proposing new system design solutions, which require performance models (especially GPU models) to help them with pre-product concept validation. Currently, researchers have been utilizing simulators to predict execution time, which provides high flexibility and acceptable accuracy, but at the cost of a long simulation time. Simulators are becoming increasingly impractical to model today’s large-scale systems and DNNs, urging us to find alternative lightweight solutions. To solve this problem, we propose using a data-driven method for modeling DNNs system performance. We first build a dataset that includes the execution time of numerous networks/layers/kernels. After identifying the relationships of directly known information (e.g., network structure, hardware theoretical computing capabilities), we discuss how to build a simple, yet accurate, performance model for DNNs execution time. Our observations on the dataset demonstrate prevalent linear relationships between the GPU kernel execution times, operation counts, and input/output parameters of DNNs layers. Guided by our observations, we develop a fast, linear-regression-based DNNs execution time predictor. Our evaluation using various image classification models suggests our method can predict new DNNs performance with a 7\% error and new GPU performance with a 15.2\% error. Our case studies also demonstrate how the performance model can facilitate future DNNs system research.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {380–394},
numpages = {15},
keywords = {Deep Neural Networks, Graphics Processing Units, Performance Model},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614309,
author = {Zhang, Haoyang and Zhou, Yirui and Xue, Yuqi and Liu, Yiqi and Huang, Jian},
title = {G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614309},
doi = {10.1145/3613424.3614309},
abstract = {To break the GPU memory wall for scaling deep learning workloads, a variety of architecture and system techniques have been proposed recently. Their typical approaches include memory extension with flash memory and direct storage access. However, these techniques still suffer from suboptimal performance and introduce complexity to the GPU memory management, making them hard to meet the scalability requirement of deep learning workloads today. In this paper, we present a unified GPU memory and storage architecture named G10 driven by the fact that the tensor behaviors of deep learning workloads are highly predictable. G10 integrates the host memory, GPU memory, and flash memory into a unified memory space, to scale the GPU memory capacity while enabling transparent data migrations. Based on this unified GPU memory and storage architecture, G10 utilizes compiler techniques to characterize the tensor behaviors in deep learning workloads. Therefore, it can schedule data migrations in advance by considering the available bandwidth of flash memory and host memory. The cooperative mechanism between deep learning compilers and the unified memory architecture enables G10 to hide data transfer overheads in a transparent manner. We implement G10 based on an open-source GPU simulator. Our experiments demonstrate that G10 outperforms state-of-the-art GPU memory solutions by up to 1.75 \texttimes{}, without code modifications to deep learning workloads. With the smart data migration mechanism, G10 can reach 90.3\% of the performance of the ideal case assuming unlimited GPU memory.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {395–410},
numpages = {16},
keywords = {Deep Learning Compiler, GPU Memory, GPUDirect Storage, Solid State Drives, Unified Virtual Memory},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637188,
author = {Tsai, Po-An},
title = {Session details: Session 4A: ML Architecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637188},
doi = {10.1145/3637188},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614268,
author = {Fan, Renhao and Cui, Yikai and Chen, Qilin and Wang, Mingyu and Zhang, Youhui and Zheng, Weimin and Li, Zhaolin},
title = {MAICC : A Lightweight Many-core Architecture with In-Cache Computing for Multi-DNN Parallel Inference},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614268},
doi = {10.1145/3613424.3614268},
abstract = {The growing complexity and diversity of neural networks in the fields of autonomous driving and intelligent robots have facilitated the research of many-core architectures, which can offer sufficient programming flexibility to simultaneously support multi-DNN parallel inference with different network structures and sizes compared to domain-specific architectures. However, due to the tight constraints of area and power consumption, many-core architectures typically use lightweight scalar cores without vector units and are almost unable to meet the high-performance computing needs of multi-DNN parallel inference. To solve the above problem, we design an area- and energy-efficient many-core architecture by integrating large amounts of lightweight processor cores with RV32IMA ISA. The architecture leverages the emerging SRAM-based computing-in-memory technology to implement vector instruction extensions by reusing memory cells in the data cache instead of conventional logic circuits. Thus, the data cache in each core can be reconfigured as the memory part and the computing part with the latter tightly coupled with the core pipeline, enabling parallel execution of the basic RISC-V instructions and the extended multi-cycle vector instructions. Furthermore, a corresponding execution framework is proposed to effectively map DNN models onto the many-core architecture by using intra-layer and inter-layer pipelining, which potentially supports multi-DNN parallel inference. Experimental results show that the proposed MAICC architecture obtains a 4.3 \texttimes{} throughput and 31.6 \texttimes{} energy efficiency over CPU (Intel i9-13900k). MAICC also achieves a 1.8 \texttimes{} energy efficiency over GPU (RTX 4090) with only 4MB on-chip memory and 28 mm2 area.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {411–423},
numpages = {13},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614286,
author = {Guo, Hongrui and Zhao, Yongwei and Li, Zhangmai and Hao, Yifan and Liu, Chang and Song, Xinkai and Li, Xiaqing and Du, Zidong and Zhang, Rui and Guo, Qi and Chen, Tianshi and Xu, Zhiwei},
title = {Cambricon-U: A Systolic Random Increment Memory Architecture for Unary Computing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614286},
doi = {10.1145/3613424.3614286},
abstract = {Unary computing, whose arithmetics require only one logic gate, has enabled efficient DNN processing, especially on strictly power-constrained devices. However, unary computing still confronts the power efficiency bottleneck for buffering unary bitstreams. The buffering of unary bitstreams requires accumulating bits into large bitwidth binary numbers. The large bitwidth binary number needs to activate all bits per cycle in case of carry propagation. As a result, the accumulation process accounts for 32\%-70\% of the power budget. To push the boundary of power efficiency, we propose Cambricon-U, a systolic random increment memory architecture featuring efficient accumulation. By leveraging skew number data format, Cambricon-U only activates no more than three bits (instead of all bits) from each number per accumulating cycle. Experimental results show that Cambricon-U reduces 51\% power on unary accumulation, and improves 1.18-1.45 \texttimes{} energy efficiency over uSystolic, the SOTA unary computing scheme baseline, with -1.9\% ∼ +0.77\% area overhead.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {424–437},
numpages = {14},
keywords = {skew number, systolic array, unary computing;},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614299,
author = {Kim, Jungwoo and Na, Seonjin and Lee, Sanghyeon and Lee, Sunho and Huh, Jaehyuk},
title = {Improving Data Reuse in NPU On-chip Memory with Interleaved Gradient Order for DNN Training},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614299},
doi = {10.1145/3613424.3614299},
abstract = {During training tasks for machine learning models with neural processing units (NPUs), the most time-consuming part is the backward pass, which incurs significant overheads due to off-chip memory accesses. For NPUs, to mitigate the long latency and limited bandwidth of such off-chip DRAM accesses, the software-managed on-chip scratchpad memory (SPM) plays a crucial role. As the backward pass computation must be optimized to improve the effectiveness of SPM, this study identifies a new data reuse pattern specific to the backward computation. The backward pass includes independent input and weight gradient computations sharing the same output gradient in each layer. Conventional sequential processing does not exploit the potential inter-operation data reuse opportunity within SPM. With this new opportunity of data reuse in the backward pass, this study proposes a novel data flow transformation scheme called interleaved gradient order, consisting of three techniques to enhance the utilization of NPU scratchpad memory. The first technique shuffles the input and weight gradient computations by interleaving two operations into a single fused operation to reduce redundant output gradient accesses. The second technique adjusts the tile access order for the interleaved gradient computations to maximize the potential data locality. However, since the best order is not fixed for all tensors, we propose a selection algorithm to find the most suitable order based on the tensor dimensions. The final technique further improves data reuse chances by using the best partitioning and mapping scheme for two gradient computations for single-core and multi-core NPUs. The simulation-based evaluation with single-core edge and server NPUs shows that the combined techniques can improve performance by 29.3\% and 14.5\% for edge and server NPUs respectively. Furthermore, with a quad-core server NPU, the proposed techniques reduce the execution time by 23.7\%.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {438–451},
numpages = {14},
keywords = {DNN training, accelerators, on-chip memory, scheduling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614305,
author = {Qu, Zheng and Niu, Dimin and Li, Shuangchen and Zheng, Hongzhong and Xie, Yuan},
title = {TT-GNN: Efficient On-Chip Graph Neural Network Training via Embedding Reformation and Hardware Optimization},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614305},
doi = {10.1145/3613424.3614305},
abstract = {Training Graph Neural Networks on large graphs is challenging due to the need to store graph data and move them along the memory hierarchy. In this work, we tackle this by effectively compressing graph embedding matrix such that the model training can be fully enabled with on-chip compute and memory resources. Specifically, we leverage the graph homophily property and consider using Tensor-train to represent the graph embedding. This allows nodes with similar neighborhoods to partially share the feature representation. While applying Tensor-train reduces the size of the graph embedding, it imposes several challenges to hardware design. On one hand, utilizing low-rank representation requires the features to be decompressed before being sent to GNN models, which introduces extra computation overhead. On the other hand, the decompressed features might still exceed on-chip memory capacity even with the minibatch setting, causing inefficient off-chip memory access. Thus, we propose the TT-GNN hardware accelerator with a specialized dataflow tailored for on-chip Tensor-train GNN learning. Based on the on-chip memory capacity and training configuration, TT-GNN adaptively breaks down a minibatch into smaller microbatches that can be fitted on-chip. The microbatch composition and scheduling order are designed to maximize data reuse and reduce redundant computations both across and within microbatches. To mitigate TT computation overhead, we further propose a unified algorithm to jointly handle TT decompression during forward propagation and TT gradient derivation during backward propagation. Evaluated on a series of benchmarks, the proposed software-hardware solution is able to outperform existing CPU-GPU training systems on both training performance (1.55 ∼ 4210 \texttimes{}) and energy efficiency (2.83 ∼ 2254 \texttimes{}). We believe TT-GNN introduces a new perspective to address large-scale GNN training and enables possibilities to train GNN models even under a significantly constrained resource budget.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {452–464},
numpages = {13},
keywords = {Graph Neural Networks, Hardware Accelerator, Tensor-train Decomporition},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614315,
author = {Vengalam, Uday Kumar Reddy and Liu, Yongchao and Geng, Tong and Wu, Hui and Huang, Michael},
title = {SUPPORTING ENERGY-BASED LEARNING WITH AN ISING MACHINE SUBSTRATE: A CASE STUDY ON RBM},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614315},
doi = {10.1145/3613424.3614315},
abstract = {Nature apparently does a lot of computation constantly. If we can harness some of that computation at an appropriate level, we can potentially perform certain type of computation (much) faster and more efficiently than we can do with a von Neumann computer. Indeed, many powerful algorithms are inspired by nature and are thus prime candidates for nature-based computation. One particular branch of this effort that has seen some recent rapid advances is Ising machines. Some Ising machines are already showing better performance and energy efficiency for optimization problems. Through design iterations and co-evolution between hardware and algorithm, we expect more benefits from nature-based computing systems in the future. In this paper, we make a case for an augmented Ising machine suitable for both training and inference using an energy-based machine learning algorithm. We show that with a small change, the Ising substrate accelerates key parts of the algorithm and achieves non-trivial speedup and efficiency gain. With a more substantial change, we can turn the machine into a self-sufficient gradient follower to virtually complete training entirely in hardware. This can bring about 29x speedup and about 1000x reduction in energy compared to a Tensor Processing Unit (TPU) host.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {465–478},
numpages = {14},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637189,
author = {Kobayashi, Hiroaki},
title = {Session details: Session 4B: Quantum},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637189},
doi = {10.1145/3637189},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614253,
author = {Wu, Anbang and Ding, Yufei and Li, Ang},
title = {QuComm: Optimizing Collective Communication for Distributed Quantum Computing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614253},
doi = {10.1145/3613424.3614253},
abstract = {Distributed quantum computing (DQC) is a scalable way to build a large-scale quantum computing system. Previous compilers for DQC focus on either qubit-to-qubit inter-node gates or qubit-to-node nonlocal circuit blocks, missing opportunities of optimizing collective communication which consists of nonlocal gates over multiple nodes. In this paper, we observe that by utilizing patterns of collective communication, we can greatly reduce the amount of inter-node communication required to implement a group of nonlocal gates. We propose QuComm, the first compiler framework which unveils and analyzes collective communication patterns hidden in distributed quantum programs and efficiently routes inter-node gates on any DQC architecture based on discovered patterns, cutting down the overall communication cost of the target program. We also provide the first formalization of the communication buffer concept in DQC compiling. The communication buffer utilizes data qubits to store remote entanglement so that we can ensure enough communication resources on any DQC architecture to support the proposed optimizations for collective communication. Experimental results show that, compared to the state-of-the-art baseline, QuComm reduces the amount of inter-node communication by 54.9\% on average, over various distributed quantum programs and DQC hardware configurations.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {479–493},
numpages = {15},
keywords = {quantum compiler, quantum computing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614274,
author = {Tan, Siwei and Lang, Congliang and Xiang, Liang and Wang, Shudi and Jia, Xinghui and Tan, Ziqi and Li, Tingting and Yin, Jieming and Shang, Yongheng and Python, Andre and Lu, Liqiang and Yin, Jianwei},
title = {QuCT: A Framework for Analyzing Quantum Circuit by Extracting Contextual and Topological Features},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614274},
doi = {10.1145/3613424.3614274},
abstract = {In the current Noisy Intermediate-Scale Quantum era, quantum circuit analysis is an essential technique for designing high-performance quantum programs. Current analysis methods exhibit either accuracy limitations or high computational complexity for obtaining precise results. To reduce this tradeoff, we propose QuCT, a unified framework for extracting, analyzing, and optimizing quantum circuits. The main innovation of QuCT is to vectorize each gate with each element, quantitatively describing the degree of the interaction with neighboring gates. Extending from the vectorization model, we propose two representative downstream models for fidelity prediction and unitary decomposition. The fidelity prediction model performs a linear transformation on all gate vectors and aggregates the results to estimate the overall circuit fidelity. By identifying critical weights in the transformation matrix, we propose two optimizations to improve the circuit fidelity. In the unitary decomposition model, we significantly reduce the search space by bridging the gap between unitary and circuit via gate vectors. Experiments show that QuCT improves the accuracy of fidelity prediction by 4.2 \texttimes{} on 5-qubit and 18-qubit quantum devices and achieves 2.5 \texttimes{} fidelity improvement compared to existing quantum compilers&nbsp;[19, 55]. In unitary decomposition, QuCT achieves 46.3 \texttimes{} speedup for 5-qubit unitary and more than hundreds of speedup for 8-qubit unitary, compared to the state-of-the-art method&nbsp;[87].},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {494–508},
numpages = {15},
keywords = {quantum circuit synthesis, quantum computing, quantum error correction},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614251,
author = {Vittal, Suhas and Das, Poulami and Qureshi, Moinuddin},
title = {ERASER: Towards Adaptive Leakage Suppression for Fault-Tolerant Quantum Computing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614251},
doi = {10.1145/3613424.3614251},
abstract = {Quantum error correction (QEC) codes can tolerate hardware errors by encoding fault-tolerant logical qubits using redundant physical qubits and detecting errors using parity checks. Leakage errors occur in quantum systems when a qubit leaves its computational basis and enters higher energy states. These errors severely limit the performance of QEC due to two reasons. First, they lead to erroneous parity checks that obfuscate the accurate detection of errors. Second, the leakage spreads to other qubits and creates a pathway for more errors over time. Prior works tolerate leakage errors by using leakage reduction circuits (LRCs) that modify the parity check circuitry of QEC codes. Unfortunately, naively using LRCs always throughout a program is sub-optimal because LRCs incur additional two-qubit operations that (1)&nbsp;facilitate leakage transport, and (2)&nbsp;serve as new sources of errors. Ideally, LRCs should only be used if leakage occurs, so that errors from both leakage as well as additional LRC operations are simultaneously minimized. However, identifying leakage errors in real-time is challenging. To enable the robust and efficient usage of LRCs, we propose ERASER that speculates the subset of qubits that may have leaked and only uses LRCs for those qubits. Our studies show that the majority of leakage errors typically impact the parity checks. We leverage this insight to identify the leaked qubits by analyzing the patterns in the failed parity checks. We propose ERASER+M that enhances ERASER by detecting leakage more accurately using qubit measurement protocols that can classify qubits into |0⟩, |1⟩ and |L⟩ states. ERASER and ERASER+M improve the logical error rate by up to 4.3 \texttimes{} and 23 \texttimes{} respectively compared to always using LRC.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {509–525},
numpages = {17},
keywords = {Leakage Suppression, Quantum Error Correction},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614270,
author = {Xu, Shifan and Hann, Connor T. and Foxman, Ben and Girvin, Steven M. and Ding, Yongshan},
title = {Systems Architecture for Quantum Random Access Memory},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614270},
doi = {10.1145/3613424.3614270},
abstract = {Operating on the principles of quantum mechanics, quantum algorithms hold the promise for solving problems that are beyond the reach of the best-available classical algorithms. An integral part of realizing such speedup is the implementation of quantum queries, which read data into forms that quantum computers can process. Quantum random access memory (QRAM) is a promising architecture for realizing quantum queries. However, implementing QRAM in practice poses significant challenges, including query latency, memory capacity and fault-tolerance. In this paper, we propose the first end-to-end system architecture for QRAM. First, we introduce a novel QRAM that hybridizes two existing implementations and achieves asymptotically superior scaling in space (qubit number) and time (circuit depth). Like in classical virtual memory, our construction enables queries to a virtual address space larger than what is actually available in hardware. Second, we present a compilation framework to synthesize, map, and schedule QRAM circuits on realistic hardware. For the first time, we demonstrate how to embed large-scale QRAM on a 2D Euclidean space, such as a 2D square grid layout, with minimal routing overhead. Third, we show how to leverage the intrinsic biased-noise resilience of the proposed QRAM for implementation on either Noisy Intermediate-Scale Quantum (NISQ) or Fault-Tolerant Quantum Computing (FTQC) hardware. Finally, we validate these results numerically via both classical simulation and quantum hardware experimentation. Our novel Feynman-path-based simulator allows for efficient simulation of noisy QRAM circuits at a larger scale than previously possible. Collectively, our results outline the set of software and hardware controls needed to implement practical QRAM.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {526–538},
numpages = {13},
keywords = {Quantum Computing, Quantum Random Access Memory},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614300,
author = {Stein, Samuel and Sussman, Sara and Tomesh, Teague and Guinn, Charles and Tureci, Esin and Lin, Sophia Fuhui and Tang, Wei and Ang, James and Chakram, Srivatsan and Li, Ang and Martonosi, Margaret and Chong, Fred and Houck, Andrew A. and Chuang, Isaac L. and Demarco, Michael},
title = {HetArch: Heterogeneous Microarchitectures for Superconducting Quantum Systems},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614300},
doi = {10.1145/3613424.3614300},
abstract = {Noisy Intermediate-Scale Quantum Computing (NISQ) has dominated headlines in recent years, with the longer-term vision of Fault-Tolerant Quantum Computation (FTQC) offering significant potential albeit at currently intractable resource costs and quantum error correction (QEC) overheads. For problems of interest, FTQC will require millions of physical qubits with long coherence times, high-fidelity gates, and compact sizes to surpass classical systems. Just as heterogeneous specialization has offered scaling benefits in classical computing, it is likewise gaining interest in FTQC. However, systematic use of heterogeneity in either hardware or software elements of FTQC systems remains a serious challenge due to the vast design space and variable physical constraints. This paper meets the challenge of making heterogeneous FTQC design practical by introducing HetArch, a toolbox for designing heterogeneous quantum systems, and using it to explore heterogeneous design scenarios. Using a hierarchical approach, we successively break quantum algorithms into smaller operations (akin to classical application kernels), thus greatly simplifying the design space and resulting tradeoffs. Specializing to superconducting systems, we then design optimized heterogeneous hardware composed of varied superconducting devices, abstracting physical constraints into design rules that enable devices to be assembled into standard cells optimized for specific operations. Finally, we provide a heterogeneous design space exploration framework which reduces the simulation burden by a factor of 104 or more and allows us to characterize optimal design points. We use these techniques to design superconducting quantum modules for entanglement distillation, error correction, and code teleportation, reducing error rates by 2.6 \texttimes{}, 10.7 \texttimes{}, and 3.0 \texttimes{} compared to homogeneous systems.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {539–554},
numpages = {16},
keywords = {Quantum Computing, Quantum Computing Architecture, Superconducting Quantum Systems},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637190,
author = {Inoue, Koji},
title = {Session details: Session 4C: Emerging Technologies: Superconducting, Photonics, DNA},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637190},
doi = {10.1145/3637190},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614308,
author = {Sharma, Puru and Lim, Cheng-Kai and Lin, Dehui and Pote, Yash and Jevdjic, Djordje},
title = {Efficiently Enabling Block Semantics and Data Updates in DNA Storage},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614308},
doi = {10.1145/3613424.3614308},
abstract = {We propose a novel and flexible DNA-storage architecture, which divides the storage space into fixed-size units (blocks) that can be independently and efficiently accessed at random for both read and write operations, and further allows efficient sequential access to consecutive data blocks. In contrast to prior work, in our architecture a pair of random-access PCR primers of length 20 does not define a single object, but an independent storage partition, which is internally blocked and managed independently of other partitions. We expose the flexibility and constraints with which the internal address space of each partition can be managed, and incorporate them into our design to provide rich and functional storage semantics, such as block-storage organization, efficient implementation of data updates, and sequential access. To leverage the full power of the prefix-based nature of PCR addressing, we define a methodology for transforming the internal addressing scheme of a partition into an equivalent that is PCR-compatible. This allows us to run PCR with primers that can be variably elongated to include a desired part of the internal address, and thus narrow down the scope of the reaction to retrieve a specific block or a range of blocks within the partition with sufficiently high accuracy. Our wetlab evaluation demonstrates the practicality of the proposed ideas and a 140x reduction in sequencing cost and latency for retrieval of individual blocks within the partition.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {555–568},
numpages = {14},
keywords = {Block Storage, DNA Storage, Data Updates in DNA Storage},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623798,
author = {Li, Shurui and Yang, Hangbo and Wong, Chee Wei and Sorger, Volker J. and Gupta, Puneet},
title = {ReFOCUS: Reusing Light for Efficient Fourier Optics-Based Photonic Neural Network Accelerator},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623798},
doi = {10.1145/3613424.3623798},
abstract = {In recent years, there has been a significant focus on achieving low-latency and high-throughput convolutional neural network (CNN) inference. Integrated photonics offers the potential to substantially expedite neural networks due to its inherent low-latency properties. Recently, on-chip Fourier optics-based neural network accelerators have been demonstrated and achieved superior energy efficiency for CNN acceleration. By incorporating Fourier optics, computationally intensive convolution operations can be performed instantaneously through on-chip lenses at a significantly lower cost compared to other on-chip photonic neural network accelerators. This is thanks to the complexity reduction offered by the convolution theorem and the passive Fourier transforms computed by on-chip lenses. However, conversion overhead between optical and digital domains and memory access energy still hinder overall efficiency. We introduce ReFOCUS, a Joint Transform Correlator (JTC) based on-chip neural network accelerator that efficiently reuses light through optical buffers. By incorporating optical delay lines, wavelength-division multiplexing, dataflow, and memory hierarchy optimization, ReFOCUS minimizes both conversion overhead and memory access energy. As a result, ReFOCUS achieves 2 \texttimes{} throughput, 2.2 \texttimes{} energy efficiency, and 1.36 \texttimes{} area efficiency compared to state-of-the-art photonic neural network accelerators.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {569–583},
numpages = {15},
keywords = {4F system, Fourier optics, Photonic neural network, deep learning, neural network accelerator, on-chip photonics},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623771,
author = {Li, Zhengang and Yuan, Geng and Yamauchi, Tomoharu and Masoud, Zabihi and Xie, Yanyue and Dong, Peiyan and Tang, Xulong and Yoshikawa, Nobuyuki and Tiwari, Devesh and Wang, Yanzhi and Chen, Olivia},
title = {SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623771},
doi = {10.1145/3613424.3623771},
abstract = {Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with extremely high energy efficiency. By employing the distinct polarity of current to denote logic ‘0’ and ‘1’, AQFP devices serve as excellent carriers for binary neural network (BNN) computations. Although recent research has made initial strides toward developing an AQFP-based BNN accelerator, several critical challenges remain, preventing the design from being a comprehensive solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN acceleration framework that leverages software-hardware co-optimization to eventually make the AQFP devices a feasible solution for BNN acceleration. Specifically, we investigate the randomized behavior of the AQFP devices and analyze the impact of crossbar size on current attenuation, subsequently formulating the current amplitude into the values suitable for use in BNN computation. To tackle the accumulation problem and improve overall hardware performance, we propose a stochastic computing-based accumulation module and a clocking scheme adjustment-based circuit optimization method. To effectively train the BNN models that are compatible with the distinctive characteristics of AQFP devices, we further propose a novel randomized BNN training solution that utilizes algorithm-hardware co-optimization, enabling simultaneous optimization of hardware configurations. In addition, we propose implementing batch normalization matching and the weight rectified clamp method to further improve the overall performance. We validate our SupeRBNN framework across various datasets and network architectures, comparing it with implementations based on different technologies, including CMOS, ReRAM, and superconducting RSFQ/ERSFQ. Experimental results demonstrate that our design achieves an energy efficiency of approximately 7.8 \texttimes{} 104 times higher than that of the ReRAM-based BNN framework while maintaining a similar level of model accuracy. Furthermore, when compared with superconductor-based counterparts, our framework demonstrates at least two orders of magnitude higher energy efficiency.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {584–598},
numpages = {15},
keywords = {AQFP, BNN, Deep Learning, Stochastic Computing, Superconducting},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614267,
author = {Zha, Haipeng and Tannu, Swamit and Annavaram, Murali},
title = {SuperBP: Design Space Exploration of Perceptron-Based Branch Predictors for Superconducting CPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614267},
doi = {10.1145/3613424.3614267},
abstract = {Single Flux Quantum (SFQ) superconducting technology has a considerable advantage over CMOS in power and performance. SFQ CPUs can also help scale quantum computing technologies, as SFQ circuits can be integrated with qubits due to their amenability to a cryogenic environment. Recently, there have been significant developments in VLSI design automation tools, making it feasible to design pipelined SFQ CPUs. SFQ technology, however, is constrained by the number of Josephson Junctions (JJs) integrated into a single chip. Prior works focused on JJ-efficient SFQ datapath designs. Pipelined SFQ CPUs also require branch predictors that provide the best prediction accuracy for a given JJ budget. In this paper, we design and evaluate the original Perceptron branch predictor and a later variant named the Hashed Perceptron predictor in terms of their accuracy and JJ usage. Since branch predictors, to date, have not been designed for SFQ CPUs, we first design a baseline predictor built using non-destructive readout (NDRO) cells for storing the perceptron weights. Given that NDRO cells are JJ intensive, we propose an enhanced JJ-efficient design, called SuperBP, that uses high-capacity destructive readout (HC-DRO) cells to store perceptron weights. HC-DRO is a recently introduced multi-bit fluxon storage cell that stores 2 bits per cell. HC-DRO cells double the weight storage density over basic DRO cells to improve prediction accuracy for a given JJ count. However, naive integration of HC-DRO with SFQ logic is inefficient as HC-DRO cells store multiple fluxons in a single cell, which needs a decoding step on a read and an encoding step on a write. SuperBP presents novel inference and prediction update circuits for the Perceptron predictor that can directly operate on the native 2-bit HC-DRO weights without decoding and encoding, thereby reducing the JJ use. SuperBP reduces the JJ count by 39\% compared to the NDRO-based design. We evaluate the performance of Perceptron and its hashed variants with the HC-DRO cell design using a range of benchmarks, including several SPEC CPU 2017, mobile, and server traces from the 5th Championship Branch Predictor competition. Our evaluation shows that for a given JJ count, the basic Perceptron variant of SuperBP provides better accuracy than the hashed variant. The hashed variant uses multiple weight tables, each of which needs its own access decoder, and decoder designs in SFQ consume a significant number of JJs. Thus, the hashed variant of SuperBP wastes the JJ budget for accessing multiple tables, leaving a smaller weight storage capacity, which compromises prediction accuracy. The basic Perceptron variant of SuperBP improves prediction accuracy by 13.6\% over the hashed perceptron variant for an exemplar 30K JJ budget.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {599–613},
numpages = {15},
keywords = {Branch Prediction, Perceptron, SFQ, Single Flux Quantum},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623787,
author = {Liu, Zeshi and Chen, Shuo and Qu, Peiyao and Liu, Huanli and Niu, Minghui and Ying, Liliang and Ren, Jie and Tang, Guangming and You, Haihang},
title = {SUSHI: Ultra-High-Speed and Ultra-Low-Power Neuromorphic Chip Using Superconducting Single-Flux-Quantum Circuits},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623787},
doi = {10.1145/3613424.3623787},
abstract = {The rapid single-flux-quantum (RSFQ) superconducting technology is highly promising due to its ultra-high-speed computation with ultra-low-power consumption, making it an ideal solution for the post-Moore era. In superconducting technology, information is encoded and processed based on pulses that resemble the neuronal pulses present in biological neural systems. This has led to a growing research focus on implementing neuromorphic processing using superconducting technology. However, current research on superconducting neuromorphic processing does not fully leverage the advantages of superconducting circuits due to incomplete neuromorphic design and approach. Although they have demonstrated the benefits of using superconducting technology for neuromorphic hardware, their designs are mostly incomplete, with only a few components validated, or based solely on simulation. This paper presents SUSHI (Superconducting neUromorphic proceSsing cHIp) to fully leverage the potential of superconducting neuromorphic processing. Based on three guiding principles and our architectural and methodological designs, we address existing challenges and enables the design of verifiable and fabricable superconducting neuromorphic chips. We fabricate and verify a chip of SUSHI using superconducting circuit technology. Successfully obtaining the correct inference results of a complete neural network on the chip, this is the first instance of neural networks being completely executed on a superconducting chip to the best of our knowledge. Our evaluation shows that using approximately 105 Josephson junctions, SUSHI achieves a peak neuromorphic processing performance of 1,355 giga-synaptic operations per second (GSOPS) and a power efficiency of 32,366 GSOPS per Watt (GSOPS/W). This power efficiency outperforms the state-of-the-art neuromorphic chips TrueNorth and Tianjic by 81 and 50 times, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {614–627},
numpages = {14},
keywords = {Neuromorphic, Single-Flux-Quantum, Spiking Neural Networks, Superconducting},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637191,
author = {Saileshwar, Gururaj},
title = {Session details: Session 5A: Security Encryption/Confidentiality Support},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637191},
doi = {10.1145/3637191},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614297,
author = {Luo, Yukui and Xu, Nuo and Peng, Hongwu and Wang, Chenghong and Duan, Shijin and Mahmood, Kaleel and Wen, Wujie and Ding, Caiwen and Xu, Xiaolin},
title = {AQ2PNN: Enabling Two-party Privacy-Preserving Deep Neural Network Inference with Adaptive Quantization},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614297},
doi = {10.1145/3613424.3614297},
abstract = {The growing prevalence of Machine Learning as a Service (MLaaS) enables a wide range of applications but simultaneously raises numerous security and privacy concerns. A key issue involves the potential privacy exposure of involved parties, such as the customer’s input data and the vendor’s model. Consequently, two-party computing (2PC) has emerged as a promising solution to safeguard the privacy of different parties during deep neural network (DNN) inference. However, the state-of-the-art (SOTA) 2PC-DNN techniques are tailored explicitly to traditional instruction set architecture (ISA) systems like CPUs and CPU+GPU. This reliance on ISA systems significantly constrains their energy efficiency, as these architectures typically employ 32- or 64-bit instruction sets. In contrast, the possibilities of harnessing dynamic and adaptive quantization to build high-performance 2PC-DNNs remain largely unexplored due to the lack of compatible algorithms and hardware accelerators. To mitigate the bottleneck of SOTA solutions and fill the existing research gaps, this work investigates the construction of 2PC-DNNs on field programmable gate arrays (FPGAs). We introduce AQ2PNN, an end-to-end framework that effectively employs adaptive quantization schemes to develop high-performance 2PC-DNNs on FPGAs. From an algorithmic perspective, AQ2PNN introduces an innovative 2PC-ReLU method to replace Yao’s Garbled Circuits (GC). Regarding hardware, AQ2PNN employs an extensive set of building blocks for linear operators, non-linear operators, and a specialized Oblivious Transfer (OT) module for secure data exchange, respectively. These algorithm-hardware co-designed modules extremely utilize the fine-grained reconfigurability of FPGAs, to adapt the data bit-width of different DNN layers in the ciphertext domain, thereby reducing communication overhead between parties without compromising DNN performance, such as accuracy. We thoroughly assess AQ2PNN using widely adopted DNN architectures, including ResNet18, ResNet50, and VGG16, all trained on ImageNet and producing quantized models. Experimental results demonstrate that AQ2PNN outperforms SOTA solutions, achieving significantly reduced communication overhead by , improved energy efficiency by 26.3 \texttimes{}, and comparable or even superior throughput and accuracy.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {628–640},
numpages = {13},
keywords = {Deep learning, FPGA, Privacy-Preserving machine learning, Quantization, Two-party computing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614266,
author = {Amar, Saar and Chisnall, David and Chen, Tony and Filardo, Nathaniel Wesley and Laurie, Ben and Liu, Kunyan and Norton, Robert and Moore, Simon W. and Tao, Yucong and Watson, Robert N. M. and Xia, Hongyan},
title = {CHERIoT: Complete Memory Safety for Embedded Devices},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614266},
doi = {10.1145/3613424.3614266},
abstract = {The ubiquity of embedded devices is apparent. The desire for increased functionality and connectivity drives ever larger software stacks, with components from multiple vendors and entities. These stacks should be replete with isolation and memory safety technologies, but existing solutions impinge upon development, unit cost, power, scalability, and/or real-time constraints, limiting their adoption and production-grade deployments. As memory safety vulnerabilities mount, the situation is clearly not tenable and a new approach is needed. To slake this need, we present a novel adaptation of the CHERI capability architecture, co-designed with a green-field, security-centric RTOS. It is scaled for embedded systems, is capable of fine-grained software compartmentalization, and provides affordances for full inter-compartment memory safety. We highlight central design decisions and offloads and summarize how our prototype RTOS uses these to enable memory-safe, compartmentalized applications. Unlike many state-of-the-art schemes, our solution deterministically (not probabilistically) eliminates memory safety vulnerabilities while maintaining source-level compatibility. We characterize the power, performance, and area microarchitectural impacts, run microbenchmarks of key facilities, and exhibit the practicality of an end-to-end IoT application. The implementation shows that full memory safety for compartmentalized embedded systems is achievable without violating resource constraints or real-time guarantees, and that hardware assists need not be expensive, intrusive, or power-hungry.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {641–653},
numpages = {13},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614293,
author = {Du, Dong and Yang, Bicheng and Xia, Yubin and Chen, Haibo},
title = {Accelerating Extra Dimensional Page Walks for Confidential Computing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614293},
doi = {10.1145/3613424.3614293},
abstract = {To support highly scalable and fine-grained computing paradigms such as microservices and serverless computing better, modern hardware-assisted confidential computing systems, such as Intel TDX and ARM CCA, introduce permission table to achieve fine-grained and scalable memory isolation among different domains. However, it also adds an extra dimension to page walks besides page tables, leading to significantly more memory references (e.g., 4 → 12 for RISC-V Sv39)1. We observe that most costs (about 75\%) caused by the extra dimension of page walks are used to validate page table pages. Based on this observation, this paper proposes HPMP (Hybrid Physical Memory Protection), a hardware-software co-design (on RISC-V) that protects page table pages using segment registers and normal pages using permission tables to balance scalability and performance. We have implemented HPMP and Penglai-HPMP (a TEE system based on HPMP) on FPGA with two RISC-V cores (both in-order and out-of-order). Evaluation results show that HPMP can reduce costs by 23.1\%–73.1\% on BOOM and significantly improve performance on real-world applications, including serverless computing (FunctionBench) and Redis.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {654–669},
numpages = {16},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614279,
author = {Shivdikar, Kaustubh and Bao, Yuhui and Agrawal, Rashmi and Shen, Michael and Jonatan, Gilbert and Mora, Evelio and Ingare, Alexander and Livesay, Neal and Abell\'{A}N, Jos\'{E} L. and Kim, John and Joshi, Ajay and Kaeli, David},
title = {GME: GPU-based Microarchitectural Extensions to Accelerate Homomorphic Encryption},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614279},
doi = {10.1145/3613424.3614279},
abstract = {Fully Homomorphic Encryption (FHE) enables the processing of encrypted data without decrypting it. FHE has garnered significant attention over the past decade as it supports secure outsourcing of data processing to remote cloud services. Despite its promise of strong data privacy and security guarantees, FHE introduces a slowdown of up to five orders of magnitude as compared to the same computation using plaintext data. This overhead is presently a major barrier to the commercial adoption of FHE. In this work, we leverage GPUs to accelerate FHE, capitalizing on a well-established GPU ecosystem available in the cloud. We propose GME, which combines three key microarchitectural extensions along with a compile-time optimization to the current AMD CDNA GPU architecture. First, GME integrates a lightweight on-chip compute unit (CU)-side hierarchical interconnect to retain ciphertext in cache across FHE kernels, thus eliminating redundant memory transactions. Second, to tackle compute bottlenecks, GME introduces special MOD-units that provide native custom hardware support for modular reduction operations, one of the most commonly executed sets of operations in FHE. Third, by integrating the MOD-unit with our novel pipelined 64-bit integer arithmetic cores (WMAC-units), GME further accelerates FHE workloads by . Finally, we propose a Locality-Aware Block Scheduler (LABS) that exploits the temporal locality available in FHE primitive blocks. Incorporating these microarchitectural features and compiler optimizations, we create a synergistic approach achieving average speedups of 796 \texttimes{}, 14.2 \texttimes{}, and 2.3 \texttimes{} over Intel Xeon CPU, NVIDIA V100 GPU, and Xilinx FPGA implementations, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {670–684},
numpages = {15},
keywords = {CU-side interconnects, Custom accelerators, Fully Homomorphic Encryption (FHE), Modular reduction, Zero-trust frameworks},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614302,
author = {Agrawal, Rashmi and De Castro, Leo and Juvekar, Chiraag and Chandrakasan, Anantha and Vaikuntanathan, Vinod and Joshi, Ajay},
title = {MAD: Memory-Aware Design Techniques for Accelerating Fully Homomorphic Encryption},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614302},
doi = {10.1145/3613424.3614302},
abstract = {Cloud computing has made it easier for individuals and companies to get access to large compute and memory resources. However, it has also raised privacy concerns about the data that users share with the remote cloud servers. Fully homomorphic encryption (FHE) offers a solution to this problem by enabling computations over encrypted data. Unfortunately, all known constructions of FHE require a noise term for security, and this noise grows during computation. To perform unlimited computations on the encrypted data, we need to perform a periodic noise reduction step known as bootstrapping. This bootstrapping operation is memory-bound as it requires several GBs of data. This leads to orders of magnitude increase in the time required for operating on encrypted data as compared to unencrypted data. In this work, we first present an in-depth analysis of the bootstrapping operation in the CKKS FHE scheme. Similar to other existing works, we observe that CKKS bootstrapping exhibits a low arithmetic intensity (&lt; 1 Op/byte). We then propose memory-aware design (MAD) techniques to accelerate the bootstrapping operation of the CKKS FHE scheme. Our proposed MAD techniques are agnostic of the underlying compute platform and can be equally applied to GPUs, CPUs, FPGAs, and ASICs. Our MAD techniques make use of several caching optimizations that enable maximal data reuse and perform reordering of operations to reduce the amount of data that needs to be transferred to/from the main memory. In addition, our MAD techniques include several algorithmic optimizations that reduce the number of data access pattern switches and the expensive NTT operations. Applying our MAD optimizations for FHE improves bootstrapping arithmetic intensity by 3 \texttimes{}. For Logistic Regression (LR) training, by leveraging our MAD optimizations, the existing GPU design can get up to 3.5 \texttimes{} improvement in performance for the same on-chip memory size. Similarly, the existing ASIC designs can get up to 27 \texttimes{} and 57 \texttimes{} improvement in performance for LR training and ResNet-20 inference, respectively, while reducing the on-chip memory requirement by 16 \texttimes{}, which proportionally reduces the cost of the solution.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {685–697},
numpages = {13},
keywords = {Bootstrapping, CKKS Scheme, Cache Optimizations, Fully Homomorphic Encryption, Hardware Acceleration, Memory Bottleneck Analysis, SimFHE},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637192,
author = {Peled, Leeor},
title = {Session details: Session 5B: Prefetching},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637192},
doi = {10.1145/3637192},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623780,
author = {Gerogiannis, Gerasimos and Torrellas, Josep},
title = {Micro-Armed Bandit: Lightweight \&amp; Reusable Reinforcement Learning for Microarchitecture Decision-Making},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623780},
doi = {10.1145/3613424.3623780},
abstract = {Online Reinforcement Learning (RL) has been adopted as an effective mechanism in various decision-making problems in microarchitecture. Its high adaptability and the ability to learn at runtime are attractive characteristics in microarchitecture settings. However, although hardware RL agents are effective, they suffer from two main problems. First, they have high complexity and storage overhead. This complexity stems from decomposing the environment into a large number of states and then, for each of these states, bookkeeping many action values. Second, many RL agents are engineered for a specific application and are not reusable. In this work, we tackle both of these shortcomings by designing an RL agent that is both lightweight and reusable across different microarchitecture decision-making problems. We find that, in some of these problems, only a small fraction of the action space is useful in a given time window. We refer to this property as temporal homogeneity in the action space. Motivated by this property, we design an RL agent based on Multi-Armed Bandit algorithms, the simplest form of RL. We call our agent Micro-Armed Bandit. We showcase our agent in two use cases: data prefetching and instruction fetch in simultaneous multithreaded (SMT) processors. For prefetching, our agent outperforms non-RL prefetchers Bingo and MLOP by 2.6\% and 2.3\% (geometric mean), respectively, and attains similar performance as the state-of-the-art RL prefetcher Pythia—with the dramatically lower storage requirement of only 100 bytes. For SMT instruction fetch, our agent outperforms the Hill Climbing method by 2.2\% (geometric mean).},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {698–713},
numpages = {16},
keywords = {Machine Learning for Architecture, Microarchitecture, Multi-Armed Bandits, Prefetching, Reinforcement Learning, Simultaneous Multithreading},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614245,
author = {Panda, Biswabandan},
title = {CLIP: Load Criticality based Data Prefetching for Bandwidth-constrained Many-core Systems},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614245},
doi = {10.1145/3613424.3614245},
abstract = {Hardware prefetching is a latency-hiding technique that hides the costly off-chip DRAM accesses. However, state-of-the-art prefetchers fail to deliver performance improvement in the case of many-core systems with constrained DRAM bandwidth. For SPEC CPU2017 homogeneous workloads, the state-of-the-art Berti L1 prefetcher, on a 64-core system with four and eight DRAM channels, incurs performance slowdowns of 24\% and 16\%, respectively. However, Berti improves performance by 35\% if we use an unrealistic configuration of 64 DRAM channels for a 64-core system (one DRAM channel per core). Prior approaches such as prefetch throttling and critical load prefetching are not effective in the presence of state-of-the-art prefetchers. Existing load criticality predictors fail to detect loads that are critical in the presence of hardware prefetching and the best predictor provides an average critical load prediction accuracy of 41\%. Existing prefetch throttling techniques use prefetch accuracy as one of the primary metrics. However, these techniques offer limited benefits for state-of-the-art prefetchers that deliver high prefetch accuracy and use prefetcher-specific throttling and filtering. We propose CLIP, a novel load criticality predictor for hardware prefetching with constrained DRAM bandwidth. Our load criticality predictor provides an average accuracy of more than 93\% and as high as 100\%. CLIP also filters out the critical loads that lead to accurate prefetching. For a 64-core system with eight DRAM channels, CLIP improves the effectiveness of state-of-the-art Berti prefetcher by 24\% and 9\% for 45 and 200 64-core homogeneous and heterogeneous workload mixes, respectively. We show that CLIP is equally effective in the presence of other state-of-the-art L1 and L2 prefetchers. Overall, CLIP incurs a storage overhead of 1.56KB/core.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {714–727},
numpages = {14},
keywords = {Cache, DRAM, Instruction criticality, Prefetching},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623782,
author = {Mostofi, Saba and Falahati, Hajar and Mahani, Negin and Lotfi-Kamran, Pejman and Sarbazi-Azad, Hamid},
title = {Snake: A Variable-length Chain-based Prefetching for GPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623782},
doi = {10.1145/3613424.3623782},
abstract = {Graphics Processing Units (GPUs) utilize memory hierarchy and Thread-Level Parallelism (TLP) to tolerate off-chip memory latency, which is a significant bottleneck for memory-bound applications. However, parallel threads generate a large number of memory requests, which increases the average memory latency and degrades cache performance due to high contention. Prefetching is an effective technique to reduce memory access latency, and prior research shows the positive impact of stride-based prefetching on GPU performance. However, existing prefetching methods only rely on fixed strides. To address this limitation, this paper proposes a new prefetching technique, Snake, which is built upon chains of variable strides, using throttling and memory decoupling strategies. Snake&nbsp;achieves 80\% coverage and 75\% accuracy in prefetching demand memory requests, resulting in a 17\% improvement in total GPU performance and energy consumption for memory-bound General-Purpose Graphics Processing Unit (GPGPU) applications.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {728–741},
numpages = {14},
keywords = {GPU, On-Chip Memory, Performance., Prefetching},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614288,
author = {Chou, Yuan Hsi and Nowicki, Tyler and Aamodt, Tor M.},
title = {Treelet Prefetching For Ray Tracing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614288},
doi = {10.1145/3613424.3614288},
abstract = {Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive. Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications. These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree. However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods. To address this, we propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree. When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal. Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1\% while maintaining the same power consumption.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {742–755},
numpages = {14},
keywords = {GPU, graphics, hardware accelerator, prefetching, ray tracing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637193,
author = {Skarlatos, Dimitrios},
title = {Session details: Session 5C: Processing-In-Memory},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637193},
doi = {10.1145/3637193},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614265,
author = {Wan, Qiyu and Wang, Lening and Wang, Jing and Song, Shuaiwen Leon and Fu, Xin},
title = {NAS-SE: Designing A Highly-Efficient In-Situ Neural Architecture Search Engine for Large-Scale Deployment},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614265},
doi = {10.1145/3613424.3614265},
abstract = {The emergence of Neural Architecture Search (NAS) enables an automated neural network development process that potentially replaces manually-enabled machine learning expertise. A state-of-the-art NAS method, namely One-Shot NAS, has been proposed to drastically reduce the lengthy search time for a wide spectrum of conventional NAS methods. Nevertheless, the search cost is still prohibitively expensive for practical large-scale deployment with real-world applications. In this paper, we reveal that the fundamental cause for inefficient deployment of One-Shot NAS in both single-device and large-scale scenarios originates from the massive redundant off-chip weight access during the numerous DNN inference in sequential searching. Inspired by its algorithmic characteristics, we depart from the traditional CMOS-based architecture designs and propose a promising processing-in-memory design alternative to perform in-situ architecture search, which helps fundamentally address the redundancy issue. Moreover, we further discovered two major performance challenges of directly porting the searching process onto the existing PIM-based accelerators: severe pipeline contention and resource under-utilization. By leveraging these insights, we propose the first highly-efficient in-situ One-Shot NAS search engine design, named NAS-SE, for both single-device and large-scale deployment scenarios. NAS-SE is equipped with a two-phased network diversification strategy for eliminating resource contention, and a novel hardware mapping scheme for boosting the resource utilization by an order of magnitude. Our extensive evaluation demonstrates that NAS-SE significantly outperforms the state-of-the-art digital-based customized NAS accelerator (NASA) with an average speedup of 8.8 \texttimes{} and energy-efficiency improvement of 2.05 \texttimes{}.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {756–768},
numpages = {13},
keywords = {Evolutionary algorithm, Hardware accelerator, In-memory computing, Neural architecture search},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623776,
author = {Patel, Neel and Mamandipoor, Amin and Quinn, Derrick and Alian, Mohammad},
title = {XFM: Accelerated Software-Defined Far Memory},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623776},
doi = {10.1145/3613424.3623776},
abstract = {DRAM constitutes over 50\% of server cost and 75\% of the embodied carbon footprint of a server. To mitigate DRAM cost, far memory architectures have emerged. They can be separated into two broad categories: software-defined far memory (SFM) and disaggregated far memory (DFM). In this work, we compare the cost of SFM and DFM in terms of their required capital investment, operational expense, and carbon footprint. We show that, for applications whose data sets are compressible and have predictable memory access patterns, it takes several years for a DFM to break even with an equivalent capacity SFM in terms of cost and sustainability. We then introduce XFM, a near-memory accelerated SFM architecture, which exploits the coldness of data during SFM-initiated swap ins and outs. XFM leverages refresh cycles to seamlessly switch the access control of DRAM between the CPU and near-memory accelerator. XFM parallelizes near-memory accelerator accesses with row refreshes and removes the memory interference caused by SFM swap ins and outs. We modify an open source far memory implementation to implement a full-stack, user-level XFM. Our experimental results use a combination of an FPGA implementation, simulation, and analytical modeling to show that XFM eliminates memory bandwidth utilization when performing compression and decompression operations with SFM s of capacities up to 1TB. The memory and cache utilization reductions translate to 5 ∼ 27\% improvement in the combined performance of co-running applications.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {769–783},
numpages = {15},
keywords = {Accelerator, Compression, Near-Memory Processing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623778,
author = {Wang, Zhengrong and Liu, Christopher and Beckmann, Nathan and Nowatzki, Tony},
title = {Affinity Alloc: Taming Not-So Near-Data Computing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623778},
doi = {10.1145/3613424.3623778},
abstract = {To mitigate the data movement bottleneck on large multicore systems, the near-data computing paradigm (NDC) offloads computation to where the data resides on-chip. The benefit of NDC heavily depends on spatial affinity, where all relevant data are in the same location, e.g. same cache bank. However, existing NDC works lack a general and systematic solution: they either ignore the problem and abort NDC when there is no spatial affinity, or rely on error-prone manual data placement. Our insight is that the essential affinity relationship, i.e. data A should be close to data B, is orthogonal to microarchitecture details and input sizes. By co-optimizing the data structure and capturing this general affinity information in the data allocation interface, the allocator can automatically optimize for data affinity and load balance to make NDC computations truly near data. With this insight, we propose affinity alloc, a general framework to optimize data layout for near-data computing. It comprises an extended allocator runtime, co-optimized data structures, and lightweight extensions to the OS and microarchitecture. Evaluated on parallel workloads across broad domains, affinity alloc achieves 2.26 \texttimes{} speedup and 1.76 \texttimes{} energy efficiency over a state-of-the-art near-data computing technique with 72\% traffic reduction.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {784–799},
numpages = {16},
keywords = {Data Layout, Data Placement, Data Structure Co-Design, Memory Allocation, Near-Data Computing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623784,
author = {Fujiki, Daichi},
title = {MVC: Enabling Fully Coherent Multi-Data-Views through the Memory Hierarchy with Processing in Memory},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623784},
doi = {10.1145/3613424.3623784},
abstract = {Fusing computation and memory through Processing-in-Memory (PIM) provides a radical solution to the memory wall problem by minimizing communication overheads for data-intensive tasks, leading to a revolutionary shift in computer architecture. Although PIM has demonstrated promising results at different layers of the memory hierarchy, few studies have explored integrating compute memories into the memory management system, specifically in relation to coherence protocol. This paper presents MVC, a framework that leverages existing coherence protocols to enable fully coherent views throughout the memory hierarchy. By introducing coherent views, which are user-defined compact representations of conventional data structures, MVC can minimize data movement and harness the reusability of PIM output. The locality-aware MVC views significantly enhance the performance and energy efficiency of various irregular workloads.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {800–814},
numpages = {15},
keywords = {Cache Coherence Protocol, Caches, Processing-in-Memory},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614314,
author = {Kal, Hongju and Yoo, Chanyoung and Ro, Won Woo},
title = {AESPA: Asynchronous Execution Scheme to Exploit Bank-Level Parallelism of Processing-in-Memory},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614314},
doi = {10.1145/3613424.3614314},
abstract = {This paper presents an asynchronous execution scheme to leverage the bank-level parallelism of near-bank processing-in-memory (PIM). We observe that performing memory operations underutilizes the parallelism of PIM computation because near-bank PIMs are designated to operate all banks synchronously. The all-bank computation can be delayed when one of the banks performs the basic memory commands, such as read/write requests and activation/precharge operations. We aim to mitigate the throughput degradation and especially focus on execution delay caused by activation/precharge operations. For all-bank execution accessing the same row of all banks, a large number of activation/precharge operations inevitably occur. Considering the timing parameter limiting the rate of row-open operations (tFAW), the throughput might decrease even further. To resolve this activation/precharge overhead, we propose AESPA, a new parallel execution scheme that operates banks asynchronously. AESPA is different from the previous synchronous execution in that (1) the compute command of AESPA targets a single bank, and (2) each processing unit computes data stored in multiple DRAM columns. By doing so, while one bank computes multiple DRAM columns, the memory controller issues activation/precharge or PIM compute commands to other banks. Thus, AESPA hides the activation latency of PIM computation and fully utilizes the aggregated bandwidth of the banks. For this, we modify hardware and software to support vector and matrix computation of previous near-bank PIM architectures. In particular, we change the matrix-vector multiplication based on an inner product to fit it on AESPA PIM. Previous matrix-vector multiplication requires data broadcasting and simultaneous computation across all processing units. By changing the matrix-vector multiplication method, AESPA PIM can transfer data to respective processing units and start computation asynchronously. As a result, the near-bank PIMs adopting AESPA achieve 33.5\% and 59.5\% speedup compared to two different state-of-the-art PIMs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {815–827},
numpages = {13},
keywords = {Execution Method, Matrix-Vector Multiplication, Processing-in-Memory},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637194,
author = {Ajorpaz, Samira Mirbagher},
title = {Session details: Session 6A: Security Hardware},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637194},
doi = {10.1145/3637194},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623770,
author = {Aimoniotis, Pavlos and Kvalsvik, Amund Bergland and Chen, Xiaoyue and Sj\"{a}lander, Magnus and Kaxiras, Stefanos},
title = {ReCon: Efficient Detection, Management, and Use of Non-Speculative Information Leakage},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623770},
doi = {10.1145/3613424.3623770},
abstract = {In a speculative side-channel attack, a secret is improperly accessed and then leaked by passing it to a transmitter instruction. Several proposed defenses effectively close this security hole by either delaying the secret from being loaded or propagated, or by delaying dependent transmitters (e.g., loads) from executing when fed with tainted input derived from an earlier speculative load. This results in a loss of memory-level parallelism and performance. A security definition proposed recently, in which data already leaked in non-speculative execution need not be considered secret during speculative execution, can provide a solution to the loss of performance. However, detecting and tracking non-speculative leakage carries its own cost, increasing complexity. The key insight of our work that enables us to exploit non-speculative leakage as an optimization to other secure speculation schemes is that the majority of non-speculative leakage is simply due to pointer dereferencing (or base-address indexing) — essentially what many secure speculation schemes prevent from taking place speculatively. We present ReCon that: i)&nbsp;efficiently detects non-speculative leakage by limiting detection to pairs of directly-dependent loads that dereference pointers (or index a base-address); and ii)&nbsp;piggybacks non-speculative leakage information on the coherence protocol. In ReCon, the coherence protocol remembers and propagates the knowledge of what has leaked and therefore what is safe to dereference under speculation. To demonstrate the effectiveness of ReCon, we show how two state-of-the-art secure speculation schemes, Non-speculative Data Access (NDA) and speculative Taint Tracking (STT), leverage this information to enable more memory-level parallelism both in a single core scenario and in a multicore scenario: NDA with ReCon reduces the performance loss by 28.7\% for SPEC2017, 31.5\% for SPEC2006, and 46.7\% for PARSEC; STT with ReCon reduces the loss by 45.1\%, 39\%, and 78.6\%, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {828–842},
numpages = {15},
keywords = {Speculation, load pair, non-speculative leakage, side-channels},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614259,
author = {Guo, Yanan and Cao, Dingyuan and Xin, Xin and Zhang, Youtao and Yang, Jun},
title = {Uncore Encore: Covert Channels Exploiting Uncore Frequency Scaling},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614259},
doi = {10.1145/3613424.3614259},
abstract = {Modern processors dynamically adjust clock frequencies and voltages to reduce energy consumption. Recent Intel processors separate the uncore frequency from the core frequency, using Uncore Frequency Scaling (UFS) to adapt the uncore frequency to various workloads. While UFS improves power efficiency, it also introduces security vulnerabilities. In this paper, we study the feasibility of covert channels exploiting UFS. First, we conduct a series of experiments to understand the details of UFS, such as the factors that can cause uncore frequency variations. Then, based on the results, we build the first UFS-based covert channel, UF-variation, which works both across-cores and across-processors. Finally, we analyze the robustness of UF-variation under known defense mechanisms against uncore covert channels, and show that UF-variation remains functional even with those defenses in place.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {843–855},
numpages = {13},
keywords = {Cache, Security, Side channel},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623796,
author = {Miao, Yuanqing and Kandemir, Mahmut Taylan and Zhang, Danfeng and Zhang, Yingtian and Tan, Gang and Wu, Dinghao},
title = {Hardware Support for Constant-Time Programming},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623796},
doi = {10.1145/3613424.3623796},
abstract = {Side-channel attacks are one of the rising security concerns in modern computing platforms. Observing this, researchers have proposed both hardware-based and software-based strategies to mitigate side-channel attacks, targeting not only on-chip caches but also other hardware components like memory controllers and on-chip networks. While hardware-based solutions to side-channel attacks are usually costly to implement as they require modifications to the underlying hardware, software-based solutions are more practical as they can work on unmodified hardware. One of the recent software-based solutions is constant-time programming, which tries to transform an input program to be protected against side-channel attacks such that an operation working on a data element/block to be protected would execute in an amount of time that is independent of the input. Unfortunately, while quite effective from a security angle, constant-time programming can lead to severe performance penalties. Motivated by this observation, in this paper, we explore novel hardware support to make constant-time programming much more efficient than its current implementations. Specifically, we present a new hardware component that can greatly improve the performance of constant-time programs with large memory footprints. The key idea in our approach is to add a small structure into the architecture and two accompanying instructions, which collectively expose the existence/dirtiness information of multiple cache lines to the application program, so that the latter can perform more efficient side-channel mitigation. Our experimental evaluation using three benchmark programs with secret data clearly show the effectiveness of the proposed approach over a state-of-the-art implementation of constant-time programming. Specifically, in the three benchmark programs tested, our approach leads to about 7x reduction in performance overheads over the state-of-the-art approach.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {856–870},
numpages = {15},
keywords = {Cache, Constant time programming, Side channel leakage},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614254,
author = {Orenes-Vera, Marcelo and Yun, Hyunsung and Wistoff, Nils and Heiser, Gernot and Benini, Luca and Wentzlaff, David and Martonosi, Margaret},
title = {AutoCC: Automatic Discovery of Covert Channels in Time-Shared Hardware},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614254},
doi = {10.1145/3613424.3614254},
abstract = {Covert channels enable information leakage between security domains that should be isolated by observing execution differences in shared hardware. These channels can appear in any stateful shared resource, including caches, predictors, and accelerators. Previous works have identified many vulnerable components, demonstrating and defending against attacks via reverse engineering. However, this approach requires much human effort and reasoning. With the Cambrian explosion of specialized hardware, it is becoming increasingly difficult to identify all vulnerabilities manually. To tackle this challenge, we propose AutoCC, a methodology that leverages formal property verification (FPV) to automatically discover covert channels in hardware that is shared between processes. AutoCC operates at the register-transfer level (RTL) to exhaustively examine any machine state left by a process after a context switch that creates an execution difference. Upon finding such a difference, AutoCC provides a precise execution trace showing how the information was encoded into the machine state and recovered. Leveraging AutoCC’s flow to generate FPV testbenches that apply our methodology, we evaluated it on four open-source hardware projects, including two RISC-V cores and two accelerators. Without hand-written code or directed tests, AutoCC uncovered known covert channels (within minutes instead of many hours of test-driven emulations) and unknown ones. Although AutoCC is primarily intended to find covert channels, our evaluation has also found RTL bugs, demonstrating that AutoCC is an effective tool to test both the security and reliability of hardware designs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {871–885},
numpages = {15},
keywords = {FPV, covert channel, data leak, flush., formal, information flow, microarchitectural, temporal partitioning, timing channel, verification},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637195,
author = {Carlson, Trevor E.},
title = {Session details: Session 6B: Datacenter Networks},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637195},
doi = {10.1145/3637195},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623769,
author = {Rashelbach, Alon and de Paula, Igor and Silberstein, Mark},
title = {NeuroLPM - Scaling Longest Prefix Match Hardware with Neural Networks},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623769},
doi = {10.1145/3613424.3623769},
abstract = {Longest Prefix Match engines (LPM) are broadly used in computer systems and especially in modern network devices such as Network Interface Cards (NICs), switches and routers. However, existing LPM hardware fails to scale to millions of rules required by modern systems, is often optimized for specific applications, and thus is performance-sensitive to the structure of LPM rules. We describe NeuroLPM, a new architecture for multi-purpose LPM hardware that replaces queries in traditional memory-intensive trie- and hash-table data structures with inference in a lightweight Neural Network-based model, called RQRMI. NeuroLPM scales to millions of rules under small on-die SRAM budget and achieves stable, rule-structure-agnostic performance, allowing its use in a variety of applications. We solve several unique challenges when implementing RQRMI inference in hardware, including minimizing the amount of floating point computations while maintaining query correctness, and scaling the rule-set size while ensuring small, deterministic off-chip memory bandwidth. We prototype NeuroLPM in Verilog and evaluate it on real-world packet forwarding rule-sets and network traces. NeuroLPM offers substantial scalability benefits without any application-specific optimizations. For example, it is the only algorithm that can serve a 950K-large rule-set at an average of 196M queries per second with 4.5MB of SRAM, only within 2\% of the best-case throughput of the state-of-the-art Tree Bitmap and SAIL on smaller rule-sets. With 2MB of SRAM, it reduces the DRAM bandwidth per query, the dominant performance factor, by up to 9 \texttimes{} and 3 \texttimes{} compared to the state-of-the-art.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {886–899},
numpages = {14},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614271,
author = {Bleier, Nathaniel and Mubarik, Muhammad Husnain and Swenson, Gary R and Kumar, Rakesh},
title = {Space Microdatacenters},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614271},
doi = {10.1145/3613424.3614271},
abstract = {Earth observation (EO) has been a key task for satellites since the first time a satellite was put into space. The temporal and spatial resolution at which EO satellites take pictures has been increasing to support space-based applications, but this increases the amount of data each satellite generates. We observe that future EO satellites will generate so much data that this data cannot be transmitted to Earth due to the limited capacity of communication that exists between space and Earth. We show that conventional data reduction techniques such as compression&nbsp;[126] and early discard&nbsp;[41] do not solve this problem, nor does a direct enhancement of today’s RF-based infrastructure&nbsp;[133, 153] for space-Earth communication. We explore an unorthodox solution instead - moving to space the computation that would have happened on the ground. This alleviates the need for data transfer to Earth. We analyze ten non-longitudinal RGB and hyperspectral image processing Earth observation applications for their computation and power requirements and discover that these requirements cannot be met by the small satellites that dominate today’s EO missions. We make a case for space microdatacenters - large computational satellites whose primary task is to support in-space computation of EO data. We show that one 4KW space microdatacenter can support the computation need of a majority of applications, especially when used in conjunction with early discard. We do find, however, that communication between EO satellites and space microdatacenters becomes a bottleneck. We propose three space microdatacenter-communication co-design strategies – k − list-based network topology, microdatacenter splitting, and moving space microdatacenters to geostationary orbit – that alleviate the bottlenecks and enable effective usage of space microdatacenters.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {900–915},
numpages = {16},
keywords = {Computational satellite, Compute in space, Micro datacenter},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614291,
author = {Guo, Zerui and Lin, Jiaxin and Bai, Yuebin and Kim, Daehyeok and Swift, Michael and Akella, Aditya and Liu, Ming},
title = {LogNIC: A High-Level Performance Model for SmartNICs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614291},
doi = {10.1145/3613424.3614291},
abstract = {SmartNICs have become an indispensable communication fabric and computing substrate in today’s data centers and enterprise clusters, providing in-network computing capabilities for traversed packets and benefiting a range of applications across the system stack. Building an efficient SmartNIC-assisted solution is generally non-trivial and tedious as it requires programmers to understand the SmartNIC architecture, refactor application logic to match the device’s capabilities and limitations, and correlate an application execution with traffic characteristics. A high-level SmartNIC performance model can decouple the underlying SmartNIC hardware device from its offloaded software implementations and execution contexts, thereby drastically simplifying and facilitating the development process. However, prior architectural models can hardly be applied due to their limited capabilities in dissecting the SmartNIC-offloaded program’s complexity, capturing the nondeterministic overlapping between computation and I/O, and perceiving diverse traffic profiles. This paper presents the LogNIC model that systematically analyzes the performance characteristics of a SmartNIC-offloaded program. Unlike conventional execution flow-based modeling, LogNIC employs a packet-centric approach that examines SmartNIC execution based on how packets traverse heterogeneous computing domains, on-/off-chip interconnects, and memory subsystems. It abstracts away the low-level device details, represents a deployed program as an execution graph, retains a handful of configurable parameters, and generates latency/throughput estimation for a given traffic profile. It further exposes a couple of extensions to handle multi-tenancy, traffic interleaving, and accelerator peculiarity. We demonstrate the LogNIC model’s capabilities using both commodity SmartNICs and an academic prototype under five application scenarios. Our evaluations show that LogNIC can estimate performance bounds, explore software optimization strategies, and provide guidelines for new hardware designs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {916–929},
numpages = {14},
keywords = {Architectural Modeling, Programmable Networks, SmartNIC},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614310,
author = {Feng, Yinxiao and Xiang, Dong and Ma, Kaisheng},
title = {Heterogeneous Die-to-Die Interfaces: Enabling More Flexible Chiplet Interconnection Systems},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614310},
doi = {10.1145/3613424.3614310},
abstract = {The chiplet architecture is one of the emerging methodologies and is believed to be scalable and economical. However, most current multi-chiplet systems are based on one uniform die-to-die interface, which severely limits flexibility. First, any interface has specific applicable workloads/scales/scenarios; therefore, chiplets with a uniform interface cannot be freely reused in different systems. Second, since modern computing systems must deal with complex and mixed tasks, the uniform interface does not cope well with flexible workloads, especially for large-scale systems. To deal with these inflexibilities, we propose the idea of Heterogeneous Interface (Hetero-IF), which allows chiplets to use two different interfaces (parallel IF and serial IF) at the same time. Hetero-IF can combine the advantages of different interfaces and cover up the disadvantages of each, thus improving flexibility and performance. However, adopting hetero-IF-based multi-chiplet interconnection systems still faces many challenges. The microarchitecture, scheduling, interconnection, and routing issues have not been discussed so far. In this paper, we put forward two typical hetero-IF implementations: Hetero-PHY and Hetero-Channel. Based on these two implementations, detailed usages and scheduling methods are discussed. We also present the interconnection methods for hetero-IF-based multi-chiplet systems and show how to apply deadlock-free routing algorithms. Extensive evaluations, including simulation and circuit verification, are made on these systems. The experiment results show that hetero-IF provides more flexible interconnection and scheduling possibilities to achieve better performance and energy metrics under various workloads.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {930–943},
numpages = {14},
keywords = {Chiplet, Interconnection, Interface, Network-on-Chip, Routing},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637196,
author = {Gabbay, Freddy},
title = {Session details: Session 6C: Reliability, Availability},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637196},
doi = {10.1145/3637196},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614294,
author = {Jung, Jeageun and Erez, Mattan},
title = {Predicting Future-System Reliability with a Component-Level DRAM Fault Model},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614294},
doi = {10.1145/3613424.3614294},
abstract = {We introduce a new fault model for recent and future DRAM systems that uses empirical analysis to derive DRAM internal-component level fault models. This modeling level offers higher fidelity and greater predictive capability than prior models that rely on logical-address based characterization and modeling. We show how to derive the model, overcoming several challenges of using a publicly-available dataset of memory error logs. We then demonstrate the utility of our model by scaling it and analyzing the expected reliability of DDR5, HBM3, and LPDDR5 based systems. In addition to the novelty of the analysis and the model itself, we draw several insights regarding on-die ECC design and tradeoffs and the efficacy of repair/retirement mechanisms.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {944–956},
numpages = {13},
keywords = {Memory reliability},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614304,
author = {Agiakatsikas, Dimitris and Papadimitriou, George and Karakostas, Vasileios and Gizopoulos, Dimitris and Psarakis, Mihalis and Belanger-Champagne, Camille and Blackmore, Ewart},
title = {Impact of Voltage Scaling on Soft Errors Susceptibility of Multicore Server CPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614304},
doi = {10.1145/3613424.3614304},
abstract = {Microprocessor power consumption and dependability are both crucial challenges that designers have to cope with due to shrinking feature sizes and increasing transistor counts in a single chip. These two challenges are mutually destructive: microprocessor reliability deteriorates at lower supply voltages that save power. An important dependability metric for microprocessors is their radiation-induced soft error rate (SER). This work goes beyond state-of-the-art by assessing the trade-offs between voltage scaling and soft error rate (SER) on a microprocessor system executing workloads on real hardware and a full software stack setup. We analyze data from accelerated neutron radiation testing for nominal and reduced microprocessor operating voltages. We perform our experiments on a 64-bit Armv8 multicore microprocessor built on 28 nm process technology. We show that the SER of SRAM arrays can increase up to 40.4\% when the device operates at reduced supply voltage levels. To put our findings into context, we also estimate the radiation-induced Failures in Time (FIT) rate of various workloads for all the studied voltage levels. Our results show that the total and the Silent Data Corruptions (SDC) FIT of the microprocessor operating at voltage-scaled conditions can be 6.6 \texttimes{} and 16 \texttimes{} larger than at the nominal voltage, respectively. Moreover, changes in the microprocessor’s clock frequency do not have a noticeable impact on its soft error susceptibility. The findings of this work can aid computer architects in striking a balance between power and dependability, thus, designing more robust and efficient microprocessors.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {957–971},
numpages = {15},
keywords = {Microprocessor reliability, energy efficiency, error resilience, neutron radiation testing, power consumption, silent data corruptions, soft errors, voltage and frequency scaling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614278,
author = {Hanson, Edward and Li, Shiyu and Zhou, Guanglei and Cheng, Feng and Wang, Yitu and Bose, Rohan and Li, Hai and Chen, Yiran},
title = {Si-Kintsugi: Towards Recovering Golden-Like Performance of Defective Many-Core Spatial Architectures for AI},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614278},
doi = {10.1145/3613424.3614278},
abstract = {The growing demand for higher compute and memory capacity driven by artificial intelligence (AI) applications pushes higher core counts in modern systems. Many-core architectures exhibiting spatial interconnects with high on-chip bandwidth are ideal for these workloads due to their data movement flexibility and sheer parallelism. However, the size of such platforms makes them particularly susceptible to manufacturing defects, prompting a need for designs and mechanisms that improve yield. Despite these techniques, nonfunctional cores and links are unavoidable. Although prior works address defective cores by disabling them and only scheduling workload to functional ones, communication latency through spatial interconnects is tightly associated with the locations of defective cores and cores with assigned work. Based on this observation, we present Si-Kintsugi, a defect-aware workload scheduling framework for spatial architectures with mesh topology. First, we design a novel and generalizable workload mapping representation and cost function that integrates defect pattern information. The mapping representation is formed into a 1D vector with simple constraints, making it an ideal candidate for open source heuristic-based optimization algorithms. After a communication latency optimized workload mapping is found, dataflow between the mapped cores is automatically generated to balance communication and computation cost. Si-Kintsugi is extensively evaluated on various workloads (i.e., BERT, ResNet, GEMM) across a wide range of defect patterns and rates. Experiment results show that Si-Kintsugi generates a workload schedule that is on average 1.34 \texttimes{} faster than the industry standard layer-pipelined schedule on defective platforms.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {972–985},
numpages = {14},
keywords = {AI acceleration, defective cores, multi-core architectures, spatial network-on-chip, workload scheduling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623777,
author = {Kim, Michael Jaemin and Wi, Minbok and Park, Jaehyun and Ko, Seoyoung and Choi, Jaeyoung and Nam, Hwayoung and Kim, Nam Sung and Ahn, Jung Ho and Lee, Eojin},
title = {How to Kill the Second Bird with One ECC: The Pursuit of Row Hammer Resilient DRAM},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623777},
doi = {10.1145/3613424.3623777},
abstract = {Error-correcting code (ECC) has been widely used in DRAM-based memory systems to address the exacerbating random errors following the fabrication process scaling. However, ECCs including the strong form of Chipkill have not been so effective against Row Hammer (RH), which incurs bursts of errors discretely corrupting the whole row beyond the ECC correction capability. We propose Cube, a novel chip-wise physical to DRAM address randomization scheme that leverages the abundant detection capability of on-die-ECC (OECC) and correction capability of Chipkill against RH. Cube allows for synergistic cooperation between ECC, probabilistic RH-protection schemes, and the system with minimal to no modification for each. First, Cube scrambles the rows of each chip using a boot-time key in a way that distributes RH victims to multiple Chipkill codewords. Second, Cube utilizes the newly observed distinct RH error characteristics from real DRAM chips, to swiftly diagnose the RH victim rows using the error profile from OECC scrubbing, and even correct it leveraging Chipkill. When combined, Cube decreases the failure probability of PARA and a state-of-the-art RH protection scheme SRS by up to 10− 25. At a target failure probability of 10− 10 per year on a DDR5 rank under the RH threshold of 2K, Cube reduces the performance and table size overhead of SRS by up to 24.3\% and 39.9\%, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {986–1001},
numpages = {16},
keywords = {DRAM, Memory system, main memory, row-hammer, security},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637197,
author = {Jones, Alex K.},
title = {Session details: Session 7A: Accelerators Various},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637197},
doi = {10.1145/3637197},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614249,
author = {Lo, Yun-Chen and Liu, Ren-Shuo},
title = {Bucket Getter: A Bucket-based Processing Engine for Low-bit Block Floating Point (BFP) DNNs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614249},
doi = {10.1145/3613424.3614249},
abstract = {Block floating point (BFP), an efficient numerical system for deep neural networks (DNNs), achieves a good trade-off between dynamic range and hardware costs. Specifically, prior works have demonstrated that BFP format with 3 ∼ 5-bit mantissa can achieve FP32-comparable accuracy for various DNN workloads. We find that the floating-point adder (FP-Acc), which contains modules for normalization, alignment, addition, and fixed-point-to-floating-point (FXP2FP) conversion, dominates the power and area overheads, hence hindering the hardware efficiency of state-of-the-art low-bit BFP processing engines (BFP-PE). To mitigate the identified issue, we propose Bucket Getter, a novel architecture implemented with the following techniques for improving the energy efficiency and area efficiency: 1) we propose a bucket-based accumulation unit prior to FP-Acc, which uses multiple small accumulators (buckets) that are responsible for a small range of exponent values where intermediate results are distributed accordingly, and b) accumulate in FXP domain. This reduces the activities of power-hungry a) alignment and b) format conversion units. 2) We propose inter-bucket carry propagation, which allows each bucket to transmit overflow to an adjacent bucket and further reduces the activity of FP-Acc. 3) We propose an out-of-bound-aware, adaptive and circular bucket accumulator to significantly reduce the overhead for the bucket-based accumulator. 4) We further propose shared FP-Acc, which exploits the low activity of FP-Acc in the bucket-based architecture and shares an FP-Acc across several MAC engines to reduce the area overhead of FP-Acc. The experimental results based on TSMC 40 nm demonstrate that our proposed Bucket Getter architecture reduces the computational energy by up to 57\% and improves the area efficiency by up to 1.4 \texttimes{}, compared to state-of-the-art BFP engines across seven representative DNN models. Furthermore, our proposed approach helps state-of-the-art floating-point engines reduce up to 32\% of the PE area and 81\% of the PE power.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1002–1015},
numpages = {14},
keywords = {Bucket-based accumulation, Deep learning, Floating-point architecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623788,
author = {McCrabb, Andrew and Ahmed, Aymen and Bertacco, Valeria},
title = {ACRE: Accelerating Random Forests for Explainability},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623788},
doi = {10.1145/3613424.3623788},
abstract = {As machine learning models become more widespread, they are being increasingly applied in applications that heavily impact people’s lives (e.g., medical diagnoses, judicial system sentences, etc.). Several communities are thus calling for ML models to be not only accurate, but also explainable. To achieve this, recommendations must be augmented with explanations summarizing how each recommendation outcome is derived. Explainable Random Forest (XRF) models are popular choices in this space, as they are both very accurate and can be augmented with explainability functionality, allowing end-users to learn how and why a specific outcome was reached. However, the limitations of XRF models hamper their adoption, the foremost being the high computational demands associated with training such models to support high-accuracy classifications, while also annotating them with explainability meta-data. In response, we present ACRE, a hardware accelerator to support XRF model training. ACRE accelerates key operations that bottleneck performance, while maintaining meta-data critical to support explainability. It leverages a novel Processing-in-Memory hardware unit, co-located with banks of a 3D-stacked High-Bandwidth Memory (HBM). The unit locally accelerates the execution of key training computations, boosting effective data-transfer bandwidth. Our evaluation shows that, when ACRE augments HBM3 memory, it yields an average system-level training performance improvement of 26.6x, compared to a baseline multicore processor solution with DDR4 memory. Further, ACRE yields a 2.5x improvement when compared to an HBM3 architecture baseline, increasing to 5x when not bottlenecked by a 16k-thread limit in the host. Finally, due to much higher performance, we observe that ACRE provides a 16.5x energy reduction overall, over a DDR baseline.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1016–1028},
numpages = {13},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614261,
author = {Taranco, Ra\'{u}l and Arnau, Jos\'{e}-Mar\'{\i}a and Gonz\'{a}lez, Antonio},
title = {δLTA: Decoupling Camera Sampling from Processing to Avoid Redundant Computations in the Vision Pipeline},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614261},
doi = {10.1145/3613424.3614261},
abstract = {Continuous Vision (CV) systems are essential for emerging applications like Autonomous Driving (AD) and Augmented/Virtual Reality (AR/VR). A standard CV System-on-a-Chip (SoC) pipeline includes a frontend for image capture and a backend for executing vision algorithms. The frontend typically captures successive similar images with gradual positional and orientational variations. As a result, many regions between consecutive frames yield nearly identical results when processed in the backend. Despite this, current systems process every image region at the camera’s sampling rate, overlooking the fact that the actual rate of change in these regions could be significantly lower. In this work, we introduce δ LTA (δont’t Look Twice, it’s Alright), a novel frontend that decouples camera frame sampling from backend processing by extending the camera with the ability to discard redundant image regions before they enter subsequent CV pipeline stages. δ LTA informs the backend about the image regions that have notably changed, allowing it to focus solely on processing these distinctive areas and reusing previous results to approximate the outcome for similar ones. As a result, the backend processes each image region using different processing rates based on its temporal variation. δ LTA features a new Image Signal Processing (ISP) design providing similarity filtering functionality, seamlessly integrated with other ISP stages to incur zero-latency overhead in the worst-case scenario. It also offers an interface for frontend-backend collaboration to fine-tune similarity filtering based on the application requirements. To illustrate the benefits of this novel approach, we apply it to a state-of-the-art CV localization application, typically employed in AD and AR/VR. We show that δ LTA removes a significant fraction of unneeded frontend and backend memory accesses and redundant backend computations, which reduces the application latency by 15.22\% and its energy consumption by 17\%.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1029–1043},
numpages = {15},
keywords = {Computation Reuse, Image Signal Processor, Image Similarity},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637198,
author = {Ausavarungnirun, Rachata},
title = {Session details: Session 7B: Caches, Intermitent Computing, Persistency},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637198},
doi = {10.1145/3637198},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614295,
author = {Kwon, Jaewon and Lee, Yongju and Kal, Hongju and Kim, Minjae and Kim, Youngsok and Ro, Won Woo},
title = {McCore: A Holistic Management of High-Performance Heterogeneous Multicores},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614295},
doi = {10.1145/3613424.3614295},
abstract = {Heterogeneous multicore systems have emerged as a promising approach to scale performance in high-end desktops within limited power and die size constraints. Despite their advantages, these systems face three major challenges: memory bandwidth limitation, shared cache contention, and heterogeneity. Small cores in these systems tend to occupy a significant portion of shared LLC and memory bandwidth, despite their lower computational capabilities, leading to performance degradation of up to 18\% in memory-intensive workloads. Therefore, it is crucial to address these challenges holistically, considering shared resources and core heterogeneity while managing shared cache and bandwidth. To tackle these issues, we propose McCore, a comprehensive solution that reorganizes the heterogeneous multicore memory hierarchy and effectively leverages this structure through a hardware-based reinforcement learning (RL) scheduler. The McCore structure aims to enhance performance by partitioning the shared LLC based on each cluster’s asymmetric computing power and conditionally enabling fine-grained access in small cores. The McCore RL agent holistically controls these structures, incorporating a hardware-based online RL scheduler that accounts for bandwidth utilization and caching effectiveness to consider the heterogeneity in McCore structures. By implementing the RL agent module as hardware that cooperates with existing hardware monitors and performance counters, low-latency scheduling is enabled without burdening the OS kernel. McCore achieves a 25.1\% performance gain compared to the baseline and significantly outperforms existing state-of-the-art cache partitioning, sparse access managing schemes, and heterogeneous multicore schedulers, providing a comprehensive solution for high-performance heterogeneous multicore systems.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1044–1058},
numpages = {15},
keywords = {cache partitioning, hardware-based scheduling, heterogeneous computing, memory hierarchy, multi-core architectures, reinforcement learning},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623781,
author = {Zhou, Yuchen and Zeng, Jianping and Jeong, Jungi and Choi, Jongouk and Jung, Changhee},
title = {SweepCache: Intermittence-Aware Cache on the Cheap},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623781},
doi = {10.1145/3613424.3623781},
abstract = {This paper presents SweepCache, a new compiler/architecture co-design scheme that can equip energy harvesting systems with a volatile cache in a performant yet lightweight way. Unlike prior just-in-time checkpointing designs that persists volatile data just before power failure and thus dedicates additional energy, SweepCache partitions program into a series of recoverable regions and persists stores at region granularity to fully utilize harvested energy for computation. In particular, SweepCache introduces persist buffer—as a redo buffer resident in nonvolatile memory (NVM)—to keep the main memory consistent across power failure while persisting region’s stores in a failure-atomic manner. Specifically, for writebacks during region execution, SweepCache saves their cachelines to the persist buffer. At each region end, SweepCache first flushes dirty cachelines to the buffer, allowing the next region to start with a clean cache, and then moves all buffered cachelines to the corresponding NVM locations. In this way, no matter when power failure occurs, the buffer contents or their memory locations always remain intact, which serves as a basis for correct recovery. To hide the persistence delay, SweepCache speculatively starts a region right after the prior region finishes its execution—as if its stores were already persisted—with the two regions having their own persist buffer, i.e., dual-buffering. This region-level parallelism helps SweepCache to achieve the full potential of a high-performance data cache. The experimental results show that compared to the original cache-free nonvolatile processor, SweepCache delivers speedups of 14.60x and 14.86x—outperforming the state-of-the-art work by 3.47x and 3.49x—for two representative energy harvesting power traces, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1059–1074},
numpages = {16},
keywords = {compiler/architecture co-design, energy harvesting, failure-atomic},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623772,
author = {Zeng, Jianping and Jeong, Jungi and Jung, Changhee},
title = {Persistent Processor Architecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623772},
doi = {10.1145/3613424.3623772},
abstract = {This paper presents PPA (Persistent Processor Architecture), simple microarchitectural support for lightweight yet performant whole-system persistence. PPA offers fully transparent crash consistency to all sorts of program covering the entire computing stack and even legacy applications without any source code change or recompilation. As a basis for crash consistency, PPA leverages so-called store integrity that preserves store operands during program execution, persists them on impending power failure, and replays the stores when power comes back. In particular, PPA realizes the store integrity via hardware by keeping the operands in a physical register file (PRF), though the stores are committed. Such store integrity enforcement leads to region-level persistence, i.e., whenever PRF runs out, PPA starts a new region after ensuring that all stores of the prior region have already been written to persistent memory. To minimize the pipeline stall across regions, PPA writes back the stores of each region asynchronously, overlapping their persistence latency with the execution of other instructions in the region. The experimental results with 41 applications from SPEC CPU2006/2017, SPLASH3, STAMP, WHISPER, and DOE Mini-apps show that PPA incurs only a 2\% average run-time overhead and a 0.005\% areal cost, while the state-of-the-art work suffers a 26\% overhead along with prohibitively high hardware and energy costs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1075–1091},
numpages = {17},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637199,
author = {Clemons, Jason},
title = {Session details: Session 8A: Accelerators for Neural Nets Accelerators for Matrix Processing},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637199},
doi = {10.1145/3637199},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623779,
author = {Janfaza, Vahid and Mandal, Shantanu and Mahmud, Farabi and Muzahid, Abdullah},
title = {ADA-GP: Accelerating DNN Training By Adaptive Gradient Prediction},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623779},
doi = {10.1145/3613424.3623779},
abstract = {Neural network training is inherently sequential where the layers finish the forward propagation in succession, followed by the calculation and back-propagation of gradients (based on a loss function) starting from the last layer. The sequential computations significantly slow down neural network training, especially the deeper ones. Prediction has been successfully used in many areas of computer architecture to speed up sequential processing. Therefore, we propose ADA-GP, which uses gradient prediction adaptively to speed up deep neural network (DNN) training while maintaining accuracy. ADA-GP works by incorporating a small neural network to predict gradients for different layers of a DNN model. ADA-GP uses a novel tensor reorganization method to make it feasible to predict a large number of gradients. ADA-GP alternates between DNN training using backpropagated gradients and DNN training using predicted gradients. ADA-GP adaptively adjusts when and for how long gradient prediction is used to strike a balance between accuracy and performance. Last but not least, we provide a detailed hardware extension in a typical DNN accelerator to realize the speed up potential from gradient prediction. Our extensive experiments with fifteen DNN models show that ADA-GP can achieve an average speed up of 1.47 \texttimes{} with similar or even higher accuracy than the baseline models. Moreover, it consumes, on average, 34\% less energy due to reduced off-chip memory accesses compared to the baseline accelerator.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1092–1105},
numpages = {14},
keywords = {Hardware accelerators, Prediction, Systolic arrays, Training},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623786,
author = {Wu, Yannan Nellie and Tsai, Po-An and Muralidharan, Saurav and Parashar, Angshuman and Sze, Vivienne and Emer, Joel},
title = {HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623786},
doi = {10.1145/3613424.3623786},
abstract = {Due to complex interactions among various deep neural network (DNN) optimization techniques, modern DNNs can have weights and activations that are dense or sparse with diverse sparsity degrees. To offer a good trade-off between accuracy and hardware performance, an ideal DNN accelerator should have high flexibility to efficiently translate DNN sparsity into reductions in energy and/or latency without incurring significant complexity overhead. This paper introduces hierarchical structured sparsity (HSS), with the key insight that we can systematically represent diverse sparsity degrees by having them hierarchically composed from multiple simple sparsity patterns. As a result, HSS simplifies the underlying hardware since it only needs to support simple sparsity patterns; this significantly reduces the sparsity acceleration overhead, which improves efficiency. Motivated by such opportunities, we propose a simultaneously efficient and flexible accelerator, named HighLight, to accelerate DNNs that have diverse sparsity degrees (including dense). Due to the flexibility of HSS, different HSS patterns can be introduced to DNNs to meet different applications’ accuracy requirements. Compared to existing works, HighLight achieves a geomean of up to 6.4 \texttimes{} better energy-delay product (EDP) across workloads with diverse sparsity degrees, and always sits on the EDP-accuracy Pareto frontier for representative DNNs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1106–1120},
numpages = {15},
keywords = {Deep learning accelerator, computer architecture, hardware-software co-design, structured sparsity},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614287,
author = {Lee, Hyunwuk and Jang, Hyungjun and Kim, Sungbin and Kim, Sungwoo and Cho, Wonho and Ro, Won Woo},
title = {Exploiting Inherent Properties of Complex Numbers for Accelerating Complex Valued Neural Networks},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614287},
doi = {10.1145/3613424.3614287},
abstract = {Since conventional Deep Neural Networks (DNNs) use real numbers as their data, they are unable to capture the imaginary values and the correlations between real and imaginary values in applications that use complex numbers. To address this limitation, Complex Valued Neural Networks (CVNNs) have been introduced, enabling to capture the context of complex numbers for various applications such as Magnetic Resonance Imaging (MRI), radar, and sensing. CVNNs handle their data with complex numbers and adopt complex number arithmetic to their layer operations, so they exhibit distinct design challenges with real-valued DNNs. The first challenge is the data representation of the complex number, which requires two values for a single data, doubling the total data size of the networks. Moreover, due to the unique operations of the complex-valued layers, CVNNs require a specialized scheduling policy to fully utilize the hardware resources and achieve optimal performance. To mitigate the design challenges, we propose software and hardware co-design techniques that effectively resolves the memory and compute overhead of CVNNs. First, we propose Polar Form Aware Quantization (PAQ) that utilizes the characteristics of the complex number and their unique value distribution on CVNNs. Then, we propose our hardware accelerator that supports PAQ and CVNN operations. Lastly, we design a CVNN-aware scheduling scheme that optimizes the performance and resource utilization of an accelerator by aiming at the special layer operations of CVNN. PAQ achieves 62.5\% data compression over CVNNs using FP16 while retaining a similar error with INT8 quantization, and our hardware support PAQ with only 2\% area overhead over conventional systolic array architecture. In our evaluation, PAQ hardware with the scheduling scheme achieves a 32\% lower latency and 30\% lower energy consumption than other accelerators.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1121–1134},
numpages = {14},
keywords = {Accelerators, Complex Valued Neural Networks, Quantization},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614290,
author = {Chen, Cen and Zou, Xiaofeng and Shao, Hongen and Li, Yangfan and Li, Kenli},
title = {Point Cloud Acceleration by Exploiting Geometric Similarity},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614290},
doi = {10.1145/3613424.3614290},
abstract = {Deep learning on point clouds has attracted increasing attention for various emerging 3D computer vision applications, such as autonomous driving, robotics, and virtual reality. These applications interact with people in real-time on edge devices and thus require low latency and low energy. To accelerate the execution of deep neural networks (DNNs) on point clouds, some customized accelerators have been proposed, which achieved a significantly higher performance with reduced energy consumption than GPUs and existing DNN accelerators. In this work, we reveal that DNNs execution on geometrically adjacent points exhibits similar values and relations, and exhibits a large amount of redundant computation and communication due to the correlations. To address this issue, we propose GDPCA, a geometry-aware differential point cloud accelerator, which can exploit geometric similarity to reduce these redundancies for point cloud neural networks. GDPCA is supported by an algorithm and architecture co-design. Our proposed algorithm can discover and reduce computation and communication redundancies with geometry-aware and differential execution mechanisms. Then a novel architecture is designed to support the proposed algorithm and transform the redundancy reduction into performance improvement. GDPCA performs the same computations and gives the same accuracy as traditional point cloud neural networks. To the best of our knowledge, GDPCA is the first accelerator that can reduce execution redundancies for point cloud neural networks by exploiting geometric similarity. Our proposed GDPCA system gains an average of 2.9 \texttimes{} speedup and 2.7 \texttimes{} energy efficiency over state-of-the-art accelerators for point cloud neural networks.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1135–1147},
numpages = {13},
keywords = {Hardware Accelerator, Point cloud, Redundancy-Aware Computation, Software-Hardware Co-Design},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623790,
author = {Kim, Jinkwon and Jang, Myeongjae and Nam, Haejin and Kim, Soontae},
title = {HARP: Hardware-Based Pseudo-Tiling for Sparse Matrix Multiplication Accelerator},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623790},
doi = {10.1145/3613424.3623790},
abstract = {General sparse matrix-matrix multiplication (SpGEMM) is a memory-bound workload, due to the compression format used. To minimize data movements for input matrices, outer product accelerators have been proposed. Since these accelerators access input matrices only once and then generate numerous partial products, managing the generated partial products is the key optimization factor. To reduce the number of partial products handled, the state-of-the-art accelerator uses software to tile an input matrix. However, the software-based tiling has three limitations. First, a user manually executes the tiling software and manages the tiles. Second, generating a compression format for each tile incurs memory-intensive operations. Third, an accelerator that uses the compression format cannot skip ineffectual accesses for input matrices. To overcome these limitations, this paper proposes hardware-based pseudo-tiling (HARP), which enables logical tiling of the original compressed matrix without generating a compression format for each tile. To this end, HARP utilizes our proposed Runtime Operand Descriptor to point to an effectual column-row pair in a pseudo-tile. Consequently, HARP enables a user to use the accelerator as a normal SpGEMM operation does without tiling. Furthermore, HARP does not require a compression format for each tile and can skip ineffectual accesses for input matrices. To further improve the efficiency of pseudo-tiling, HARP performs super-tiling to combine pseudo-tiles and sub-tiling to further tile a pseudo-tile. Experiment results show that HARP achieves 4 \texttimes{}, 35 \texttimes{}, and 33 \texttimes{} speedup and 5 \texttimes{}, 664 \texttimes{}, and 233 \texttimes{} energy efficiency on average compared to the state-of-the-art outer product accelerator, CPU (MKL), and GPU (cuSPARSE), respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1148–1162},
numpages = {15},
keywords = {Application-specific hardware, Hardware accelerator, SpGEMM, Spare matrix tiling, Sparse Matrix, Sparse matrix multiplication, Tiling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637200,
author = {Alian, Mohammad},
title = {Session details: Session 8B: Virtual Memory (Translation)},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637200},
doi = {10.1145/3637200},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614269,
author = {Li, Bingyao and Guo, Yanan and Wang, Yueqi and Jaleel, Aamer and Yang, Jun and Tang, Xulong},
title = {IDYLL: Enhancing Page Translation in Multi-GPUs via Light Weight PTE Invalidations},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614269},
doi = {10.1145/3613424.3614269},
abstract = {Multi-GPU systems have emerged as a desirable platform to deliver high computing capabilities and large memory capacity to accommodate large dataset sizes. However, naively employing multi-GPU incurs non-scalable performance. One major reason is that execution efficiency suffers expensive address translations in multi-GPU systems. The data-sharing nature of GPU applications requires page migration between GPUs to mitigate non-uniform memory access overheads. Unfortunately, frequent page migration incurs substantial page table invalidation overheads to ensure translation coherence. A comprehensive investigation of multi-GPU address translation efficiency identifies two significant bottlenecks caused by page table invalidation requests: (i) increased latency for demand TLB miss requests and (ii) increased waiting latency for performing page migrations. Based on observations, we propose IDYLL, which reduces the number of page table invalidations by maintaining an “in-PTE" directory and reduces invalidation latency by batching multiple invalidation requests to exploit spatial locality. We show that IDYLL&nbsp;improves overall performance by 69.9\% on average.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1163–1177},
numpages = {15},
keywords = {multi-GPU, page sharing, page table invalidation},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614276,
author = {Kanellopoulos, Konstantinos and Nam, Hong Chul and Bostanci, Nisa and Bera, Rahul and Sadrosadati, Mohammad and Kumar, Rakesh and Bartolini, Davide Basilio and Mutlu, Onur},
title = {Victima: Drastically Increasing Address Translation Reach by Leveraging Underutilized Cache Resources},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614276},
doi = {10.1145/3613424.3614276},
abstract = {Address translation is a performance bottleneck in data-intensive workloads due to large datasets and irregular access patterns that lead to frequent high-latency page table walks (PTWs). PTWs can be reduced by using (i) large hardware TLBs or (ii) large software-managed TLBs. Unfortunately, both solutions have significant drawbacks: increased access latency, power and area (for hardware TLBs), and costly memory accesses, the need for large contiguous memory blocks, and complex OS modifications (for software-managed TLBs). We present Victima, a new software-transparent mechanism that drastically increases the translation reach of the processor by leveraging the underutilized resources of the cache hierarchy. The key idea of Victima is to repurpose L2 cache blocks to store clusters of TLB entries, thereby providing an additional low-latency and high-capacity component that backs up the last-level TLB and thus reduces PTWs. Victima has two main components. First, a PTW cost predictor (PTW-CP) identifies costly-to-translate addresses based on the frequency and cost of the PTWs they lead to. Leveraging the PTW-CP, Victima uses the valuable cache space only for TLB entries that correspond to costly-to-translate pages, reducing the impact on cached application data. Second, a TLB-aware cache replacement policy prioritizes keeping TLB entries in the cache hierarchy by considering (i) the translation pressure (e.g., last-level TLB miss rate) and (ii) the reuse characteristics of the TLB entries. Our evaluation results show that in native (virtualized) execution environments Victima improves average end-to-end application performance by 7.4\% (28.7\%) over the baseline four-level radix-tree-based page table design and by 6.2\% (20.1\%) over a state-of-the-art software-managed TLB, across 11 diverse data-intensive workloads. Victima delivers similar performance as a system that employs an optimistic 128K-entry L2 TLB, while avoiding the associated area and power overheads. Victima (i) is effective in both native and virtualized environments, (ii) is completely transparent to application and system software, (iii) unlike large software-managed TLBs, does not require contiguous physical allocations, (iv) is compatible with modern large page mechanisms and (iv) incurs very small area and power overheads of and , respectively, on a modern high-end CPU. The source code of Victima is freely available at https://github.com/CMU-SAFARI/Victima.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1178–1195},
numpages = {18},
keywords = {Address Translation, Cache, Memory Hierarchy, Memory Systems, Microarchitecture, TLB, Virtual Memory, Virtualization},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623789,
author = {Kanellopoulos, Konstantinos and Bera, Rahul and Stojiljkovic, Kosta and Bostanci, F. Nisa and Firtina, Can and Ausavarungnirun, Rachata and Kumar, Rakesh and Hajinazar, Nastaran and Sadrosadati, Mohammad and Vijaykumar, Nandita and Mutlu, Onur},
title = {Utopia: Fast and Efficient Address Translation via Hybrid Restrictive \&amp; Flexible Virtual-to-Physical Address Mappings},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623789},
doi = {10.1145/3613424.3623789},
abstract = {Conventional virtual memory (VM) frameworks enable a virtual address to flexibly map to any physical address. This flexibility necessitates large data structures to store virtual-to-physical mappings, which leads to high address translation latency and large translation-induced interference in the memory hierarchy, especially in data-intensive workloads. On the other hand, restricting the address mapping so that a virtual address can only map to a specific set of physical addresses can significantly reduce address translation overheads by making use of compact and efficient translation structures. However, restricting the address mapping flexibility across the entire main memory severely limits data sharing across different processes and increases data accesses to the swap space of the storage device even in the presence of free memory. We propose Utopia, a new hybrid virtual-to-physical address mapping scheme that allows both flexible and restrictive hash-based address mapping schemes to harmoniously co-exist in the system. The key idea of Utopia is to manage physical memory using two types of physical memory segments: restrictive segments and flexible segments. A restrictive segment uses a restrictive, hash-based address mapping scheme that maps virtual addresses to only a specific set of physical addresses and enables faster address translation using compact translation structures. A flexible segment employs the conventional fully-flexible address mapping scheme. By mapping data to a restrictive segment, Utopia enables faster address translation with lower translation-induced interference. At the same time, Utopia retains the ability to use the flexible address mapping to (i) support conventional VM features such as data sharing and (ii) avoid storing data in the swap space of the storage device when program data does not fit inside a restrictive segment. Our evaluation using 11 diverse data-intensive workloads shows that Utopia improves performance by 24\% in a single-core system over the baseline conventional four-level radix-tree page table design, whereas the best prior state-of-the-art contiguity-aware translation scheme improves performance by 13\%. Utopia provides 95\% of the performance benefits of an ideal address translation scheme where every translation request hits in the first-level TLB. All of Utopia’s benefits come at a modest cost of 0.64\% area overhead and 0.72\% power overhead compared to a modern high-end CPU. The source code of Utopia is freely available at https://github.com/CMU-SAFARI/Utopia.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1196–1212},
numpages = {17},
keywords = {Address Translation, Cache, Memory Systems, Microarchitecture, TLB, Virtual Memory, Virtualization},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614296,
author = {Manocha, Aninda and Yan, Zi and Tureci, Esin and Arag\'{o}n, Juan L. and Nellans, David and Martonosi, Margaret},
title = {Architectural Support for Optimizing Huge Page Selection Within the OS},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614296},
doi = {10.1145/3613424.3614296},
abstract = {Irregular, memory-intensive applications often incur high translation lookaside buffer (TLB) miss rates that result in significant address translation overheads. Employing huge pages is an effective way to reduce these overheads, however in real systems the number of available huge pages can be limited when system memory is nearly full and/or fragmented. Thus, huge pages must be used selectively to back application memory. This work demonstrates that choosing memory regions that incur the most TLB misses for huge page promotion best reduces address translation overheads. We call these regions High reUse TLB-sensitive data (HUBs). Unlike prior work which relies on expensive per-page software counters to identify promotion regions, we propose new architectural support to identify these regions dynamically at application runtime. We propose a promotion candidate cache (PCC) that identifies HUB candidates based on hardware page table walks after a last-level TLB miss. This small, fixed-size structure tracks huge page-aligned regions (consisting of N base pages), ranks them based on observed page table walk frequency, and only keeps the most frequently accessed ones. Evaluated on applications of various memory intensity, our approach successfully identifies application pages incurring the highest address translation overheads. Our approach demonstrates that with the help of a PCC, the OS only needs to promote of the application footprint to achieve more than of the peak achievable performance, yielding 1.19-1.33 \texttimes{} speedups over 4KB base pages alone. In real systems where memory is typically fragmented, the PCC outperforms Linux’s page promotion policy by (when 50\% of total memory is fragmented) and (when 90\% of total memory is fragmented) respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1213–1226},
numpages = {14},
keywords = {cache architectures, graph processing, hardware-software co-design, memory management, operating systems, virtual memory},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637201,
author = {Moret\'{o}, Miquel},
title = {Session details: Session 8C: Benchmarking and Methodology},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637201},
doi = {10.1145/3637201},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623773,
author = {Liu, Changxi and Sun, Yifan and Carlson, Trevor E.},
title = {Photon: A Fine-grained Sampled Simulation Methodology for GPU Workloads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623773},
doi = {10.1145/3613424.3623773},
abstract = {GPUs, due to their massively-parallel computing architectures, provide high performance for data-parallel applications. However, existing GPU simulators are too slow to enable architects to quickly evaluate their hardware designs and software analysis studies. Sampled simulation methodologies are one common way to speed up CPU simulation. However, GPUs apply drastically different execution models that challenge the sampled simulation methods designed for CPU simulations. Recent GPU sampled simulation methodologies do not fully take advantage of the GPU’s special architecture features, such as limited types of basic blocks or warps. Moreover, these methods depend on up-front analysis via profiling tools or functional simulation, making them difficult to use. To address this, we extensively studied the execution patterns of a variety of GPU workloads and propose Photon, a sampled simulation methodology tailored to GPUs. Photon incorporates methodologies that automatically consider different levels of GPU execution, such as kernels, warps, and basic blocks. Photon does not require up-front profiling of GPU workloads and utilizes a light-weight online analysis method based on the identification of highly repetitive software behavior. We evaluate Photon using a variety of GPU workloads, including real-world applications like VGG and ResNet. The final result shows that Photon reduces the simulation time needed to perform one inference of ResNet-152 with batch size 1 from 7.05 days to just 1.7 hours with a low sampling error of 10.7\%.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1227–1241},
numpages = {15},
keywords = {GPU, Simulation, Workload sampling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623785,
author = {Mazurek, Filip and Tschand, Arya and Wang, Yu and Pajic, Miroslav and Sorin, Daniel},
title = {Rigorous Evaluation of Computer Processors with Statistical Model Checking},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623785},
doi = {10.1145/3613424.3623785},
abstract = {Experiments with computer processors must account for the inherent variability in executions. Prior work has shown that real systems exhibit variability, and random effects must be injected into simulators to account for it. Thus, we can run multiple executions of a given benchmark and generate a distribution of results. Prior work uses standard statistical techniques that are not suitable. While the result distributions may take any forms that are unknown a priori, many works naively assume they are Gaussian, which can be far from the truth. To allow rigorous evaluation for arbitrary result distributions, we introduce statistical model checking (SMC) to the world of computer architecture. SMC is a statistical technique that is used in research communities that depend heavily on statistical guarantees. SMC provides a rigorous mathematical methodology that employs experimental sampling for probabilistic evaluation of properties of interest, such that one can determine with a desired confidence whether a property (e.g., System X is 1.1x faster than System Y) is true or not. SMC alone is not enough for computer architects to draw conclusions based on their data. We create an end-to-end framework called SMC for Processor Analysis (SPA) which utilizes SMC techniques to provide insightful conclusions given experimental data.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1242–1254},
numpages = {13},
keywords = {confidence intervals, evaluation, statistical model checking},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623791,
author = {Nayak, Nandeeka and Odemuyiwa, Toluwanimi O. and Ugare, Shubham and Fletcher, Christopher and Pellauer, Michael and Emer, Joel},
title = {TeAAL: A Declarative Framework for Modeling Sparse Tensor Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623791},
doi = {10.1145/3613424.3623791},
abstract = {Over the past few years, the explosion in sparse tensor algebra workloads has led to a corresponding rise in domain-specific accelerators to service them. Due to the irregularity present in sparse tensors, these accelerators employ a wide variety of novel solutions to achieve good performance. At the same time, prior work on design-flexible sparse accelerator modeling does not express this full range of design features, making it difficult to understand the impact of each design choice and compare or extend the state-of-the-art. To address this, we propose TeAAL: a language and simulator generator for the concise and precise specification and evaluation of sparse tensor algebra accelerators. We use TeAAL to represent and evaluate four disparate state-of-the-art accelerators—ExTensor, Gamma, OuterSPACE, and SIGMA—and verify that it reproduces their performance with high accuracy. Finally, we demonstrate the potential of TeAAL as a tool for designing new accelerators by showing how it can be used to speed up vertex-centric programming accelerators—achieving 1.9 \texttimes{} on BFS and 1.2 \texttimes{} on SSSP over GraphDynS.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1255–1270},
numpages = {16},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623792,
author = {Zheng, Size and Chen, Siyuan and Gao, Siyuan and Jia, Liancheng and Sun, Guangyu and Wang, Runsheng and Liang, Yun},
title = {TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623792},
doi = {10.1145/3613424.3623792},
abstract = {With the increasing size of DNN models and the growing discrepancy between compute performance and memory bandwidth, fusing multiple layers together to reduce off-chip memory access has become a popular approach in dataflow design. However, designing such dataflows requires flexible and accurate performance models to facilitate evaluation, architecture analysis, and design space exploration. Unfortunately, current state-of-the-art performance models are limited to the dataflows of single operator acceleration, making them inapplicable to operator fusion dataflows. In this paper, we propose a framework called TileFlow that models dataflows for operator fusion. We first characterize the design space of fusion dataflows as a 3D space encompassing compute ordering, resource binding, and loop tiling. We then introduce a tile-centric notation to express dataflow designs within this space. Inspired by the tiling structure of fusion dataflows, we present a tree-based approach to analyze two critical performance metrics: data movement volume within the accelerator memory hierarchy and accelerator compute/memory resource usage. Finally, we leverage these metrics to calculate latency and energy consumption. Our evaluation validates TileFlow’s modeling accuracy against both real hardware and state-of-the-art performance models. We use TileFlow to aid in fusion dataflow design and analysis, and it helps us discover fusion dataflows that achieve an average runtime speedup of 1.85 \texttimes{} for self-attention and 1.28 \texttimes{} for convolution chains compared to the state-of-the-art dataflow.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1271–1288},
numpages = {18},
keywords = {Accelerator, Fusion, Simulation and modeling, Tensor Programs},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614281,
author = {Li, Daixuan and Sun, Jinghan and Huang, Jian},
title = {Learning to Drive Software-Defined Solid-State Drives},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614281},
doi = {10.1145/3613424.3614281},
abstract = {Thanks to the mature manufacturing techniques, flash-based solid-state drives (SSDs) are highly customizable for applications today, which brings opportunities to further improve their storage performance and resource utilization. However, the SSD efficiency is usually determined by many hardware parameters, making it hard for developers to manually tune them and determine the optimized SSD hardware configurations. In this paper, we present an automated learning-based SSD hardware configuration framework, named AutoBlox, that utilizes both supervised and unsupervised machine learning (ML) techniques to drive the tuning of hardware configurations for SSDs. AutoBlox automatically extracts the unique access patterns of a new workload using its block I/O traces, maps the workload to previous workloads for utilizing the learned experiences, and recommends an optimized SSD configuration based on the validated storage performance. AutoBlox accelerates the development of new SSD devices by automating the hardware parameter configurations and reducing the manual efforts. We develop AutoBlox with simple yet effective learning algorithms that can run efficiently on multi-core CPUs. Given a target storage workload, our evaluation shows that AutoBlox can deliver an optimized SSD configuration that can improve the performance of the target workload by 1.30 \texttimes{} on average, compared to commodity SSDs, while satisfying specified constraints such as SSD capacity, device interfaces, and power budget. And this configuration will maximize the performance improvement for both target workloads and non-target workloads.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1289–1304},
numpages = {16},
keywords = {Learning-Based Storage, Machine Learning for Systems, Software-Defined Hardware, Solid State Drive},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637202,
author = {Liu, Sihang},
title = {Session details: Session 9A: Accelerators in Processors},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637202},
doi = {10.1145/3637202},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614250,
author = {Song, Xinkai and Wen, Yuanbo and Hu, Xing and Liu, Tianbo and Zhou, Haoxuan and Han, Husheng and Zhi, Tian and Du, Zidong and Li, Wei and Zhang, Rui and Zhang, Chen and Gao, Lin and Guo, Qi and Chen, Tianshi},
title = {Cambricon-R: A Fully Fused Accelerator for Real-Time Learning of Neural Scene Representation},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614250},
doi = {10.1145/3613424.3614250},
abstract = {Neural scene representation (NSR) initiates a new methodology of encoding a 3D scene with neural networks by learning from dozens of photos taken from different camera positions. NSR not only achieves significant improvement in the quality of novel view synthesis and 3D reconstruction but also reduces the camera cost from the expensive laser cameras to the cheap color cameras on the shelf. However, performing 3D scene encoding using NSR is far from real-time due to the extremely low hardware utilization (only utilization of hardware peak performance), which greatly limits its applications in real-time AR/VR interactions In this paper, we propose Cambricon-R, a fully fused on-chip processing architecture for real-time NSR learning. Initially, by performing a thorough characterization of the computing model of NSR on a GPU, we find that the extremely low hardware utilization is mainly caused by the fragmentary stages and heavy irregular memory accesses. To address these issues, we propose Cambricon-R architecture with a novel fully-fused ray-based execution model to eliminate the computing and memory inefficiencies for real-time NSR learning. Concretely, Cambricon-R features a ray-level fused architecture that not only eliminates the intermediate memory traffics but also leverages the point sparsity in scenes to eliminate unnecessary computations. Additionally, a high throughput on-chip memory system based on Auto-Interpolation Bank Array (AIBA) is proposed to efficiently handle a large volume of irregular memory accesses. We evaluate Cambricon-R on 12 commonly-used datasets with the state-of-the-art representative algorithm, instant-ngp. The result shows that Cambricon-R achieves PE utilization of , on average. Compared to the state-of-the-art solution on A100 GPU, &nbsp;Cambricon-R achieves 373.8 \texttimes{} speedup and 256.6 \texttimes{} energy saving, on average. More importantly, it enables real-time NSR learning with 69.0 scenes per second, on average.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1305–1318},
numpages = {14},
keywords = {hardware accelerator;, neural scene representation},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614264,
author = {Putra, Adiwena and Prasetiyo and Chen, Yi and Kim, John and Kim, Joo-Young},
title = {Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614264},
doi = {10.1145/3613424.3614264},
abstract = {Homomorphic encryption (HE) is a type of cryptography that allows computations to be performed on encrypted data. The technique relies on learning with errors problem, where data is hidden under noise for security. To avoid excessive noise, bootstrapping is used to reset the noise level in the ciphertext, but it requires a large key and is computationally expensive. The fully homomorphic encryption over the torus (TFHE) scheme offers a faster and programmable bootstrapping (PBS) algorithm, which is crucial for many privacy-focused applications. Nonetheless, the current TFHE scheme does not support ciphertext packing, resulting in low-throughput performance. To the best of our knowledge, this is the first work that thoroughly analyzes TFHE bootstrapping, identifies the TFHE acceleration bottleneck in GPUs, and proposes a hardware TFHE accelerator to solve the bottleneck. We begin by identifying the TFHE acceleration bottleneck in GPUs due to the blind rotation fragmentation problem. This can be improved by increasing the batch size in PBS. We propose a two-level batching approach to enhance the batch size in PBS. To implement this solution efficiently, we introduce Strix, utilizing a streaming and fully pipelined architecture with specialized units to accelerate ciphertext processing in TFHE. Specifically, we propose a novel microarchitecture for decomposition in TFHE, suitable for processing streaming data at high throughput. We also employ a fully-pipelined FFT microarchitecture to address the memory access bottleneck and improve its performance through a folding scheme, achieving 2 \texttimes{} throughput improvement and 1.7 \texttimes{} area reduction. Strix achieves over 1, 067 \texttimes{} and 37 \texttimes{} higher throughput in running TFHE with PBS than the state-of-the-art implementation on CPU and GPU, respectively, surpassing the state-of-the-art TFHE accelerator, MATCHA, by 7.4 \texttimes{}.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1319–1331},
numpages = {13},
keywords = {accelerator, ciphertext batching, fully homomorphic encryption, programmable bootstrapping},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614284,
author = {Siracusa, Marco and Soria-Pardos, V\'{\i}ctor and Sgherzi, Francesco and Randall, Joshua and Joseph, Douglas J. and Moret\'{o} Planas, Miquel and Armejach, Adri\`{a}},
title = {A Tensor Marshaling Unit for Sparse Tensor Algebra on General-Purpose Processors},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614284},
doi = {10.1145/3613424.3614284},
abstract = {This paper proposes the Tensor Marshaling Unit (TMU), a near-core programmable dataflow engine for multicore architectures that accelerates tensor traversals and merging, the most critical operations of sparse tensor workloads running on today’s computing infrastructures. The TMU leverages a novel multi-lane design that enables parallel tensor loading and merging, which naturally produces vector operands that are marshaled into the core for efficient SIMD computation. The TMU supports all the necessary primitives to be tensor-format and tensor-algebra complete. We evaluate the TMU on a simulated multicore system using a broad set of tensor algebra workloads, achieving 3.6 \texttimes{}, 2.8 \texttimes{}, and 4.9 \texttimes{} speedups over memory-intensive, compute-intensive, and merge-intensive vectorized software implementations, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1332–1346},
numpages = {15},
keywords = {Dataflow accelerator, parallel tensor traversal, sparse tensor algebra, tensor merging, vectorization},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3623793,
author = {Xue, Zi Yu and Wu, Yannan Nellie and Emer, Joel S. and Sze, Vivienne},
title = {Tailors: Accelerating Sparse Tensor Algebra by Overbooking Buffer Capacity},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623793},
doi = {10.1145/3613424.3623793},
abstract = {Sparse tensor algebra is a challenging class of workloads to accelerate due to low arithmetic intensity and varying sparsity patterns. Prior sparse tensor algebra accelerators have explored tiling sparse data to increase exploitable data reuse and improve throughput, but typically allocate tile size in a given buffer for the worst-case data occupancy. This severely limits the utilization of available memory resources and reduces data reuse. Other accelerators employ complex tiling during preprocessing or at runtime to determine the exact tile size based on its occupancy. This paper proposes a speculative tensor tiling approach, called overbooking, to improve buffer utilization by taking advantage of the distribution of nonzero elements in sparse tensors to construct larger tiles with greater data reuse. To ensure correctness, we propose a low-overhead hardware mechanism, Tailors, that can tolerate data overflow by design while ensuring reasonable data reuse. We demonstrate that Tailors can be easily integrated into the memory hierarchy of an existing sparse tensor algebra accelerator. To ensure high buffer utilization with minimal tiling overhead, we introduce a statistical approach, Swiftiles, to pick a tile size so that tiles usually fit within the buffer’s capacity, but can potentially overflow, i.e., it overbooks the buffers. Across a suite of 22 sparse tensor algebra workloads, we show that our proposed overbooking strategy introduces an average speedup of 52.7 \texttimes{} and 2.3 \texttimes{} and an average energy reduction of 22.5 \texttimes{} and 2.5 \texttimes{} over ExTensor without and with optimized tiling, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1347–1363},
numpages = {17},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637203,
author = {Huang, Jian},
title = {Session details: Session 9B: ML Compiler Optimizations/ Reconfigurable Architectures},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637203},
doi = {10.1145/3637203},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614248,
author = {Zheng, Bojian and Yu, Cody Hao and Wang, Jie and Ding, Yaoyao and Liu, Yizhi and Wang, Yida and Pekhimenko, Gennady},
title = {Grape: Practical and Efficient Graphed Execution for Dynamic Deep Neural Networks on GPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614248},
doi = {10.1145/3613424.3614248},
abstract = {Achieving high performance in machine learning workloads is a crucial yet difficult task. To achieve high runtime performance on hardware platforms such as GPUs, graph-based executions such as CUDA graphs are often used to eliminate CPU runtime overheads by submitting jobs in the granularity of multiple kernels. However, many machine learning workloads, especially dynamic deep neural networks (DNNs) with varying-sized inputs or data-dependent control flows, face challenges when directly using CUDA graphs to achieve optimal performance. We observe that the use of graph-based executions poses three key challenges in terms of efficiency and even practicability: (1) Extra data movements when copying input values to graphs’ placeholders. (2) High GPU memory consumption due to the numerous CUDA graphs created to efficiently support dynamic-shape workloads. (3) Inability to handle data-dependent control flows. To address those challenges, we propose Grape, a new graph compiler that enables practical and efficient graph-based executions for dynamic DNNs on GPUs. Grape comprises three key components: (1) an alias predictor that automatically removes extra data movements by leveraging code positions at the Python frontend, (2) a metadata compressor that efficiently utilizes the data redundancy in CUDA graphs’ memory regions by compressing them, and (3) a predication rewriter that safely replaces control flows with predication contexts while preserving programs’ semantics. The three components improve the efficiency and broaden the optimization scope of graph-based executions while allowing machine learning practitioners to program dynamic DNNs at the Python level with minimal source code changes. We evaluate Grape on state-of-the-art text generation (GPT-2, GPT-J) and speech recognition (Wav2Vec2) workloads, which include both training and inference, using real systems with modern GPUs. Our evaluation shows that Grape achieves up to 36.43 \texttimes{} less GPU memory consumption and up to 1.26 \texttimes{} better performance than prior works on graph-based executions that directly use CUDA graphs. Furthermore, Grape can optimize workloads that are impractical for prior works due to the three key challenges, achieving 1.78 \texttimes{} and 1.82 \texttimes{} better performance on GPT-J and Wav2Vec2 respectively than the original implementations that do not use graph-based executions.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1364–1380},
numpages = {17},
keywords = {CUDA graphs, dynamic neural networks, machine learning compilers},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614307,
author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
title = {PockEngine: Sparse and Efficient Fine-tuning in a Pocket},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614307},
doi = {10.1145/3613424.3614307},
abstract = {On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of training graph optimizations, thus can further accelerate the training cost, including operator reordering and backend switching. PockEngine supports diverse applications, frontends and hardware backends: it flexibly compiles and tunes models defined in PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We evaluated PockEngine on both vision models and large language models. PockEngine achieves up to 15 \texttimes{} speedup over off-the-shelf TensorFlow (Raspberry Pi), 5.6 \texttimes{} memory saving back-propagation (Jetson AGX Orin). Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin at 550 tokens/s, 7.9 \texttimes{} faster than the PyTorch.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1381–1394},
numpages = {14},
keywords = {efficient finetuning, neural network, on-device training, sparse update},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614246,
author = {Deng, Jinyi and Tang, Xinru and Zhang, Jiahao and Li, Yuxuan and Zhang, Linyun and Han, Boxiao and He, Hongjun and Tu, Fengbin and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
title = {Towards Efficient Control Flow Handling in Spatial Architecture via Architecting the Control Flow Plane},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614246},
doi = {10.1145/3613424.3614246},
abstract = {Spatial architecture is a high-performance architecture that uses control flow graphs and data flow graphs as the computational model and producer/consumer models as the execution models. However, existing spatial architectures suffer from control flow handling challenges. Upon categorizing their PE execution models, we find that they lack autonomous, peer-to-peer, and temporally loosely-coupled control flow handling capability. This leads to limited performance in intensive control programs. A spatial architecture, Marionette, is proposed, with an explicit-designed control flow plane. The Control Flow Plane enables autonomous, peer-to-peer and temporally loosely-coupled control flow handling. The Proactive PE Configuration ensures computation-overlapped and timely configuration to improve handling Branch Divergence. The Agile PE Assignment enhance the pipeline performance of Imperfect Loops. We develop full stack of Marionette (ISA, compiler, simulator, RTL) and demonstrate that in a variety of challenging intensive control programs, compared to state-of-the-art spatial architectures, Marionette outperforms Softbrain, TIA, REVEL, and RipTide by geomean 2.88\texttimes{}, 3.38\texttimes{}, 1.55\texttimes{}, and 2.66\texttimes{}.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1395–1408},
numpages = {14},
keywords = {coarse-grained reconfigurable array, control flow, control plane, spatial architecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614283,
author = {Serafin, Nathan and Ghosh, Souradip and Desai, Harsh and Beckmann, Nathan and Lucia, Brandon},
title = {Pipestitch: An energy-minimal dataflow architecture with lightweight threads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614283},
doi = {10.1145/3613424.3614283},
abstract = {Computing at the extreme edge allows systems with high-resolution sensors to be pushed well outside the reach of traditional communication and power delivery, requiring high-performance, high-energy-efficiency architectures to run complex ML, DSP, image processing, etc. Recent work has demonstrated the suitability of CGRAs for energy-minimal computation, but has focused strictly on energy optimization, neglecting performance. Pipestitch is an energy-minimal CGRA architecture that adds lightweight hardware threads to ordered dataflow, exploiting abundant, untapped parallelism in the complex workloads needed to meet the demands of emerging sensing applications. Pipestitch introduces a programming model, control-flow operator, and synchronization network to allow lightweight hardware threads to pipeline on the CGRA fabric. Across 5 important sparse workloads, Pipestitch achieves a 3.49 \texttimes{} increase in performance over RipTide, the state-of-the-art, at a cost of a 1.10 \texttimes{} increase in area and a 1.05 \texttimes{} increase in energy.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1409–1422},
numpages = {14},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3637204,
author = {Bose, Pradip},
title = {Session details: Session 9C: Domain Specific Genomics},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637204},
doi = {10.1145/3637204},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614313,
author = {Huang, Yi and Kong, Lingkun and Chen, Dibei and Chen, Zhiyu and Kong, Xiangyu and Zhu, Jianfeng and Mamouras, Konstantinos and Wei, Shaojun and Yang, Kaiyuan and Liu, Leibo},
title = {CASA: An Energy-Efficient and High-Speed CAM-based SMEM Seeding Accelerator for Genome Alignment},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614313},
doi = {10.1145/3613424.3614313},
abstract = {Genome analysis is a critical tool in medical and bioscience research, clinical diagnostics and treatment, and disease control and prevention. Seed and extension-based alignment is the main approach in the genome analysis pipeline, and BWA-MEM2, a widely acknowledged tool for genome alignment, performs seeding by searching for super maximal exact match (SMEM). The computation of SMEM searching requires high memory bandwidth and energy consumption, which becomes the main performance bottleneck in BWA-MEM2. State-of-the-Art designs like ERT and GenAx have achieved impressive speed-ups of SMEM-based genome alignment. However, they are constrained by frequent DRAM fetches or computationally intensive intersection calculations for all possible k-mers at every read position. We present a CAM-based SMEM seeding accelerator for genome alignment (CASA), which circumvents the major throughput and power bottlenecks brought by data fetches and frequent position intersections through the co-design of a novel CAM-based computing architecture and a new SMEM search algorithm. CASA mainly consists of a pre-seeding filter table and a SMEM computing unit. The former expands the k-mer size to 19 using limited on-chip memory, which enables the efficient filtration of non-SMEM pivots. The latter applies a new algorithm to filter out disposable SMEMs that are already contained in other SMEMs. We evaluated a 28nm CASA implementation using the human and mouse genome references. CASA achieves 1.2 \texttimes{} and 5.47 \texttimes{} throughput improvement over ERT and GenAx while only requiring less than 30GB/s DRAM bandwidth and keeping the same alignment results as BWA-MEM2. Moreover, CASA provides 2.57 \texttimes{} and 6.69 \texttimes{} higher energy efficiency than ERT and GenAx.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1423–1436},
numpages = {14},
keywords = {CAM, Filtering, Genome Alignment, SMEM Seeding},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614252,
author = {Shahroodi, Taha and Singh, Gagandeep and Zahedi, Mahdi and Mao, Haiyu and Lindegger, Joel and Firtina, Can and Wong, Stephan and Mutlu, Onur and Hamdioui, Said},
title = {Swordfish: A Framework for Evaluating Deep Neural Network-based Basecalling using Computation-In-Memory with Non-Ideal Memristors},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614252},
doi = {10.1145/3613424.3614252},
abstract = {Basecalling, an essential step in many genome analysis studies, relies on large Deep Neural Network s (DNN s) to achieve high accuracy. Unfortunately, these DNN s are computationally slow and inefficient, leading to considerable delays and resource constraints in the sequence analysis process. A Computation-In-Memory (CIM) architecture using memristors can significantly accelerate the performance of DNN s. However, inherent device non-idealities and architectural limitations of such designs can greatly degrade the basecalling accuracy, which is critical for accurate genome analysis. To facilitate the adoption of memristor-based CIM designs for basecalling, it is important to (1) conduct a comprehensive analysis of potential CIM architectures and (2) develop effective strategies for mitigating the possible adverse effects of inherent device non-idealities and architectural limitations.&nbsp; This paper proposes Swordfish, a novel hardware/software co-design framework that can effectively address the two aforementioned issues. Swordfish incorporates seven circuit and device restrictions or non-idealities from characterized real memristor-based chips. Swordfish leverages various hardware/software co-design solutions to mitigate the basecalling accuracy loss due to such non-idealities. To demonstrate the effectiveness of Swordfish, we take Bonito, the state-of-the-art (i.e., accurate and fast), open-source basecaller as a case study. Our experimental results using Swordfish show that a CIM architecture can realistically accelerate Bonito for a wide range of real datasets by an average of 25.7 \texttimes{}, with an accuracy loss of 6.01\%.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1437–1452},
numpages = {16},
keywords = {basecalling, computation in memory (CIM), deep neural networks (DNN s), genome analysis, memory systems, memristors, non-ideality, processing in memory (PIM)},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614262,
author = {Jahshan, Zuher and Merlin, Itay and Garz\'{o}n, Esteban and Yavits, Leonid},
title = {DASH-CAM: Dynamic Approximate SearcH Content Addressable Memory for genome classification},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614262},
doi = {10.1145/3613424.3614262},
abstract = {We propose a novel dynamic storage-based approximate search content addressable memory (DASH-CAM) for computational genomics applications, particularly for identification and classification of viral pathogens of epidemic significance. DASH-CAM provides 5.5 \texttimes{} better density compared to state-of-the-art SRAM-based approximate search CAM. This allows using DASH-CAM as a portable classifier that can be applied to pathogen surveillance in low-quality field settings during pandemics, as well as to pathogen diagnostics at points of care. DASH-CAM approximate search capabilities allow a high level of flexibility when dealing with a variety of industrial sequencers with different error profiles. DASH-CAM achieves up to 30\% and 20\% higher F1 score when classifying DNA reads with 10\% error rate, compared to state-of-the-art DNA classification tools MetaCache-GPU and Kraken2 respectively. Simulated at 1GHz, DASH-CAM provides 1, 178 \texttimes{} and 1, 040 \texttimes{} average speedup over MetaCache-GPU and Kraken2 respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1453–1465},
numpages = {13},
keywords = {Approximate search, Content Addressable Memory, Dynamic approximate search, GC-eDRAM, Pathogen classification, Pathogen detection},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3613424.3614306,
author = {Doblas, Max and Lostes-Cazorla, Oscar and Aguado-Puig, Quim and Cebry, Nick and Fontova-Must\'{e}, Pau and Batten, Christopher Frances and Marco-Sola, Santiago and Moret\'{o}, Miquel},
title = {GMX: Instruction Set Extensions for Fast, Scalable, and Efficient Genome Sequence Alignment},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614306},
doi = {10.1145/3613424.3614306},
abstract = {Sequence alignment remains a fundamental problem in computer science with practical applications ranging from pattern matching to computational biology. The ever-increasing volumes of genomic data produced by modern DNA sequencers motivate improved software and hardware sequence alignment accelerators that scale with longer sequence lengths and high error rates without losing accuracy. Furthermore, the wide variety of use cases requiring sequence alignment demands flexible and efficient solutions that can match or even outperform expensive application-specific accelerators. To address these challenges, we propose GMX, a set of ISA extensions that enable efficient sequence alignment computations based on dynamic programming (DP). GMX extensions provide the basic building-block operations to perform fast tile-wise computations of the DP matrix, reducing the memory footprint and allowing easy integration into widely-used algorithms and tools. Furthermore, we provide an efficient hardware implementation that integrates GMX extensions in a RISC-V-based edge system-on-chip (SoC). Compared to widely-used software implementations, our hardware-software co-design leveraging GMX extensions obtains speed-ups from 25–265 \texttimes{}, scaling to megabyte-long sequences. Compared to domain-specific accelerators (DSA), we demonstrate that GMX-accelerated implementations demand significantly less memory bandwidth, requiring less area per processing element (PE). As a result, a single GMX-enabled core achieves a throughput per area between 0.35-0.52 \texttimes{} that of state-of-the-art DSAs while being more flexible and reusing the core’s resources. Post-place-and-route results for a GMX-enhanced SoC in 22nm technology shows that GMX extensions only account for 1.7\% of the overall area while consuming just 8.47mW. We conclude that GMX extensions represent versatile and scalable ISA additions to improve the performance of genome analysis tools and other use cases that require fast and efficient sequence alignment.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1466–1480},
numpages = {15},
keywords = {ISA extensions, bioinformatics, edit-distance, genomics, hardware acceleration, microarchitecture, sequence alignment},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

