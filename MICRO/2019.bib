@inproceedings{10.1145/3352460.3358316,
author = {Gudaparthi, Sumanth and Narayanan, Surya and Balasubramonian, Rajeev and Giacomin, Edouard and Kambalasubramanyam, Hari and Gaillardon, Pierre-Emmanuel},
title = {Wire-Aware Architecture and Dataflow for CNN Accelerators},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358316},
doi = {10.1145/3352460.3358316},
abstract = {In spite of several recent advancements, data movement in modern CNN accelerators remains a significant bottleneck. Architectures like Eyeriss implement large scratchpads within individual processing elements, while architectures like TPU v1 implement large systolic arrays and large monolithic caches. Several data movements in these prior works are therefore across long wires, and account for much of the energy consumption. In this work, we design a new wire-aware CNN accelerator, WAX, that employs a deep and distributed memory hierarchy, thus enabling data movement over short wires in the common case. An array of computational units, each with a small set of registers, is placed adjacent to a subarray of a large cache to form a single tile. Shift operations among these registers allow for high reuse with little wire traversal overhead. This approach optimizes the common case, where register fetches and access to a few-kilobyte buffer can be performed at very low cost. Operations beyond the tile require traversal over the cache's H-tree interconnect, but represent the uncommon case. For high reuse of operands, we introduce a family of new data mappings and dataflows. The best dataflow, WAXFlow-3, achieves a 2\texttimes{} improvement in performance and a 2.6-4.4\texttimes{} reduction in energy, relative to Eyeriss. As more WAX tiles are added, performance scales well until 128 tiles.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1–13},
numpages = {13},
keywords = {CNN, DNN, accelerator, near memory, neural networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358302,
author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
title = {Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358302},
doi = {10.1145/3352460.3358302},
abstract = {Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarse-grained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with fine-grained chiplets for deep learning inference, an application area with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deep-learning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16\% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with batch size of one, delivering inference latency of 0.50 ms.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {14–27},
numpages = {14},
keywords = {Multi-chip module, accelerator architecture, neural networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358295,
author = {Lascorz, Alberto Delm\'{a}s and Sharify, Sayeh and Edo, Isak and Stuart, Dylan Malone and Awad, Omar Mohamed and Judd, Patrick and Mahmoud, Mostafa and Nikolic, Milos and Siu, Kevin and Poulos, Zissis and Moshovos, Andreas},
title = {ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358295},
doi = {10.1145/3352460.3358295},
abstract = {We show that selecting a data width for all values in Deep Neural Networks, quantized or not and even if that width is different per layer, amounts to worst-case design. Much shorter data widths can be used if we target the common case by adjusting the data type width at a much finer granularity. We propose ShapeShifter, where we group weights and activations and encode them using a width specific to each group and where typical group sizes vary from 16 to 256 values. The per group widths are selected statically for the weights and dynamically by hardware for the activations. We present two applications of ShapeShifter. In the first, that is applicable to any system, ShapeShifter reduces off- and on-chip storage and communication. This ShapeShifter-based memory compression is simple and low cost yet reduces off-chip traffic to 33\% and 36\% for 8-bit and 16-bit models respectively. This makes it possible to sustain higher performance for a given off-chip memory interface while also boosting energy efficiency. In the second application, we show how ShapeShifter can be implemented as a surgical extension over designs that exploit variable precision in time.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {28–41},
numpages = {14},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358310,
author = {Bourgeat, Thomas and Lebedev, Ilia and Wright, Andrew and Zhang, Sizhuo and Arvind and Devadas, Srinivas},
title = {MI6: Secure Enclaves in a Speculative Out-of-Order Processor},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358310},
doi = {10.1145/3352460.3358310},
abstract = {Recent attacks have broken process isolation by exploiting microarchitectural side channels that allow indirect access to shared microarchitectural state. Enclaves strengthen the process abstraction to restore isolation guarantees.We propose MI6, an aggressively speculative out-of-order processor capable of providing secure enclaves under a threat model that includes an untrusted OS and an attacker capable of mounting any software attack currently considered practical, including those utilizing control flow mis-speculation. MI6 is inspired by Sanctum [16] and extends its isolation guarantee to more realistic memory hierarchy. It also introduces a purge instruction, which is used only when a secure process is (de)scheduled, and implements it for a complex processor microarchitecture. We model the performance impact of enclaves in MI6 through FPGA emulation on AWS F1 FPGAs by running SPEC CINT2006 benchmarks as enclaves within an untrusted Linux OS. Security comes at the cost of approximately 16.4\% average slowdown for protected programs.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {42–56},
numpages = {15},
keywords = {architectural isolation, microarchitectural isolation, secure processors},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358273,
author = {Harris, Austin and Wei, Shijia and Sahu, Prateek and Kumar, Pranav and Austin, Todd and Tiwari, Mohit},
title = {Cyclone: Detecting Contention-Based Cache Information Leaks Through Cyclic Interference},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358273},
doi = {10.1145/3352460.3358273},
abstract = {Micro-architecture units like caches are notorious for leaking secrets across security domains. An attacker program can contend for on-chip state or bandwidth and can even use speculative execution in processors to drive this contention; and protecting against all contention-driven attacks is exceptionally challenging. Prior works can mitigate contention channels through caches by partitioning the larger, lower-level caches or by looking for anomalous performance or contention behavior. Neither scales to large number of fine-grained domains as required by browsers and web-services that place many domains within the same address space.We observe that cache contention channels have a unique property - contention leaks information only when it is cyclic, i.e., domain A interferes with domain B, followed by interference from B to A. We propose to use this cyclic interference property to detect micro-architectural attacks as anomalous cyclic interference. Unlike partitioning, our detection approach scales to many concurrent domains in a single address space; and unlike prior anomaly detectors, cyclic interference is robust to noise from benign interference.We track cyclic interference using non-intrusive detectors in an out-of-order core and stress test our prototype, Cyclone, with fine-grained isolation in browsers (against speculation-driven attacks) and coarse-grained isolation of cores (against covert-channels embedded in database and machine learning workloads). Full-system simulations on an ARM micro-architecture show close to perfect detection rates and 260 - 1000\texttimes{} lower false positives than using (state-of-the-art) contention alone, with slowdowns of only ~3.6\%.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {57–72},
numpages = {16},
keywords = {anomaly detection, secure architectures, side-channel defenses},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358314,
author = {Saileshwar, Gururaj and Qureshi, Moinuddin K.},
title = {CleanupSpec: An "Undo" Approach to Safe Speculation},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358314},
doi = {10.1145/3352460.3358314},
abstract = {Speculation-based attacks affect hundreds of millions of computers. These attacks typically exploit caches to leak information, using speculative instructions to cause changes to the cache state. Hardware-based solutions that protect against such forms of attacks try to prevent any speculative changes to the cache sub-system by delaying them. For example, InvisiSpec, a recent work, splits the load into two operations: the first operation is speculative and obtains the value and the second operation is non-speculative and changes the state of the cache. Unfortunately, such a "Redo" based approach typically incurs slowdown due to the requirement of extra operations for correctly speculated loads, that form the large majority of loads.In this work, we propose CleanupSpec, an "Undo"-based approach to safe speculation. CleanupSpec is a hardware-based solution that mitigates these attacks by undoing the changes to the cache sub-system caused by speculative instructions, in the event they are squashed on a mis-speculation. As a result, CleanupSpec prevents information leakage on the correct path of execution due to any mis-speculated load and is secure against speculation-based attacks exploiting caches (we demonstrate a proof-of-concept defense on Spectre Variant-1 PoC). Unlike a Redo-based approach which incurs overheads for correct-path loads, CleanupSpec incurs overheads only for the wrong-path loads that are less frequent. As a result, CleanupSpec only incurs an average slowdown of 5.1\% compared to a non-secure baseline. Moreover, CleanupSpec incurs a modest storage overhead of less than 1 kilobyte per core, for tracking and undoing the speculative changes to the caches.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {73–86},
numpages = {14},
keywords = {Caches, Side-channel Attacks, Transient-Execution Attacks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358324,
author = {Sadredini, Elaheh and Rahimi, Reza and Verma, Vaibhav and Stan, Mircea and Skadron, Kevin},
title = {eAP: A Scalable and Efficient In-Memory Accelerator for Automata Processing},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358324},
doi = {10.1145/3352460.3358324},
abstract = {Accelerating finite automata processing benefits regular-expression workloads and a wide range of other applications that do not map obviously to regular expressions, including pattern mining, bioinformatics, and machine learning. Existing in-memory automata processing accelerators suffer from inefficient routing architectures. They are either incapable of efficiently place-and-route a highly connected automaton or require an excessive amount of hardware resources.In this paper, first, we propose a compact, low-overhead, and yet flexible interconnect architecture that efficiently implements routing for next-state activation, and can be applied to the existing in-memory automata processing architectures. Then, we present eAP (embedded Automata Processor), a high-throughput and scalable in-memory automata processing accelerator. Performance benefits of eAP are achieved by (1) exploiting subarray-level parallelism in memory, (2) a compact memory-based interconnect architecture, (3) an optimal pipeline for state matching and state transition, and (4) efficiently mapping to appropriate memory technologies.Overall, eAP achieves 5.1\texttimes{} and 207\texttimes{} better throughput per unit area compared to Cache Automaton and Micron's Automata Processor, respectively, as well as lower power consumption and better scaling.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {87–99},
numpages = {13},
keywords = {automata processing, embedded DRAM, interconnect, processing-in-memory, reconfigurable computing},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358260,
author = {Gao, Fei and Tziantzioulis, Georgios and Wentzlaff, David},
title = {ComputeDRAM: In-Memory Compute Using Off-the-Shelf DRAMs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358260},
doi = {10.1145/3352460.3358260},
abstract = {In-memory computing has long been promised as a solution to the "Memory Wall" problem. Recent work has proposed using chargesharing on the bit-lines of a memory in order to compute in-place and with massive parallelism, all without having to move data across the memory bus. Unfortunately, prior work has required modification to RAM designs (e.g. adding multiple row decoders) in order to open multiple rows simultaneously. So far, the competitive and low-margin nature of the DRAM industry has made commercial DRAM manufacturers resist adding any additional logic into DRAM. This paper addresses the need for in-memory computation with little to no change to DRAM designs. It is the first work to demonstrate in-memory computation with off-the-shelf, unmodified, commercial, DRAM. This is accomplished by violating the nominal timing specification and activating multiple rows in rapid succession, which happens to leave multiple rows open simultaneously, thereby enabling bit-line charge sharing. We use a constraint-violating command sequence to implement and demonstrate row copy, logical OR, and logical AND in unmodified, commodity, DRAM. Subsequently, we employ these primitives to develop an architecture for arbitrary, massively-parallel, computation. Utilizing a customized DRAM controller in an FPGA and commodity DRAM modules, we characterize this opportunity in hardware for all major DRAM vendors. This work stands as a proof of concept that in-memory computation is possible with unmodified DRAM modules and that there exists a financially feasible way for DRAM manufacturers to support in-memory compute.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {100–113},
numpages = {14},
keywords = {DRAM, bit-serial, in-memory computing, main memory},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358328,
author = {Chou, Teyuh and Tang, Wei and Botimer, Jacob and Zhang, Zhengya},
title = {CASCADE: Connecting RRAMs to Extend Analog Dataflow In An End-To-End In-Memory Processing Paradigm},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358328},
doi = {10.1145/3352460.3358328},
abstract = {Processing in memory (PIM) is a concept to enable massively parallel dot products while keeping one set of operands in memory. PIM is ideal for computationally demanding deep neural networks (DNNs) and recurrent neural networks (RNNs). Processing in resistive RAM (RRAM) is particularly appealing due to RRAM's high density and low energy. A key limitation of PIM is the cost of multibit analog-to-digital (A/D) conversions that can defeat the efficiency and performance benefits of PIM. In this work, we demonstrate the CASCADE architecture that connects multiply-accumulate (MAC) RRAM arrays with buffer RRAM arrays to extend the processing in analog and in memory: dot products are followed by partial-sum buffering and accumulation to implement a complete DNN or RNN layer. Design choices are made and the interface is designed to enable a variation-tolerant, robust analog dataflow. A new memory mapping scheme named R-Mapping is devised to enable the in-RRAM accumulation of partial sums; and an analog summation scheme is used to reduce the number of A/D conversions required to obtain the final sum. CASCADE is compared with recent in-RRAM computation architectures using state-of-the-art DNN and RNN benchmarks. The results demonstrate that CASCADE improves the energy efficiency by 3.5\texttimes{} while maintaining a competitive throughput.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {114–125},
numpages = {12},
keywords = {Neural network accelerator, Process in memory, Resistive RAM},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358305,
author = {Akin, Berkin and Chishti, Zeshan A. and Alameldeen, Alaa R.},
title = {ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358305},
doi = {10.1145/3352460.3358305},
abstract = {Deep Neural Networks (DNNs) are becoming the prevalent approach in computer vision, machine learning, natural language processing, and speech recognition applications. Although DNNs are perceived as compute-intensive tasks, they also apply intense pressure on the capacity and bandwidth of the memory hierarchy, primarily due to the large intermediate data communicated across network layers. Prior work on hardware DNN accelerators leverages the cross-layer data sparsity via fully-customized datapaths. However, dynamically compressing/expanding such data is a challenging task for general-purpose multi-processors with virtual memory and hardware-managed coherent cache hierarchies.In this paper, we observe that the DNN intermediate data is either sequentially streamed or reshaped with a regular transformation between layers. Hence, accesses to this data can tolerate a sequential or block sequential compression/expansion without requiring random element retrieval. Based on this insight, we propose ZCOMP, a CPU vector ISA extension tailored for DNN cross-layer communication. ZCOMP compactly represents zero value compression/expansion and fully automates the metadata generation, storage and retrieval which eliminates the need for several extra instruction executions and register usage. ZCOMP can be targeted both for inference and training to dynamically compress/expand cross-layer data before being written to memory. Our evaluations for individual layers and end-to-end DNN networks demonstrate that ZCOMP offers substantial data traffic reduction, both on-chip across cache-hierarchy and off-chip to DRAM, and performance improvements over no compression and existing AVX512 compression approaches.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {126–138},
numpages = {13},
keywords = {CPU, Deep learning, ISA, compression, memory system, sparsity},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358283,
author = {Hua, Weizhe and Zhou, Yuan and De Sa, Christopher and Zhang, Zhiru and Suh, G. Edward},
title = {Boosting the Performance of CNN Accelerators with Dynamic Fine-Grained Channel Gating},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358283},
doi = {10.1145/3352460.3358283},
abstract = {This paper proposes a new fine-grained dynamic pruning technique for CNN inference, named channel gating, and presents an accelerator architecture that can effectively exploit the dynamic sparsity. Intuitively, channel gating identifies the regions in the feature map of each CNN layer that contribute less to the classification result and turns off a subset of channels for computing the activations in these less important regions. Unlike static network pruning, which removes redundant weights or neurons prior to inference, channel gating exploits dynamic sparsity specific to each input at run time and in a structured manner. To maximize compute savings while minimizing accuracy loss, channel gating learns the gating thresholds together with weights automatically through training. Experimental results show that the proposed approach can significantly speed up state-of-the-art networks with a marginal accuracy loss, and enable a trade-off between performance and accuracy. This paper also shows that channel gating can be supported with a small set of extensions to a CNN accelerator, and implements a prototype for quantized ResNet-18 models. The accelerator shows an average speedup of 2.3\texttimes{} for ImageNet when the theoretical FLOP reduction is 2.8\texttimes{}, indicating that the hardware can effectively exploit the dynamic sparsity exposed by channel gating.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {139–150},
numpages = {12},
keywords = {algorithm-hardware co-design, dynamic pruning, neural networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358291,
author = {Gondimalla, Ashish and Chesnut, Noah and Thottethodi, Mithuna and Vijaykumar, T. N.},
title = {SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358291},
doi = {10.1145/3352460.3358291},
abstract = {Convolutional neural networks (CNNs) are emerging as powerful tools for image processing. Recent machine learning work has reduced CNNs' compute and data volumes by exploiting the naturally-occurring and actively-transformed zeros in the feature maps and filters. While previous semi-sparse architectures exploit one-sided sparsity either in the feature maps or the filters, but not both, a recent fully-sparse architecture, called Sparse CNN (SCNN), exploits two-sided sparsity to improve performance and energy over dense architectures. However, sparse vector-vector dot product, a key primitive in sparse CNNs, would be inefficient using the representation adopted by SCNN. The dot product requires finding and accessing non-zero elements in matching positions in the two sparse vectors -- an inner join using the position as the key with a single value field. SCNN avoids the inner join by performing a Cartesian product capturing the relevant multiplications. However, SCNN's approach incurs several considerable overheads and is not applicable to non-unit-stride convolutions. Further, exploiting reuse in sparse CNNs fundamentally causes systematic load imbalance not addressed by SCNN. We propose SparTen which achieves efficient inner join by providing support for native two-sided sparse execution and memory storage. To tackle load imbalance, SparTen employs a software scheme, called greedy balancing, which groups filters by density via two variants, a software-only one which uses whole-filter density and a software-hardware hybrid which uses finer-grain density. Our simulations show that, on average, SparTen performs 4.7x, 1.8x, and 3x better than a dense architecture, one-sided sparse architecture, and SCNN, respectively. An FPGA implementation shows that SparTen performs 4.3x and 1.9x better than a dense architecture and a one-sided sparse architecture, respectively.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {151–165},
numpages = {15},
keywords = {Accelerators, Convolutional neural networks, Sparse tensors},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358280,
author = {Koppula, Skanda and Orosa, Lois and Ya\u{g}l\i{}k\c{c}\i{}, A. Giray and Azizi, Roknoddin and Shahroodi, Taha and Kanellopoulos, Konstantinos and Mutlu, Onur},
title = {EDEN: Enabling Energy-Efficient, High-Performance Deep Neural Network Inference Using Approximate DRAM},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358280},
doi = {10.1145/3352460.3358280},
abstract = {The effectiveness of deep neural networks (DNN) in vision, speech, and language processing has prompted a tremendous demand for energy-efficient high-performance DNN inference systems. Due to the increasing memory intensity of most DNN workloads, main memory can dominate the system's energy consumption and stall time. One effective way to reduce the energy consumption and increase the performance of DNN inference systems is by using approximate memory, which operates with reduced supply voltage and reduced access latency parameters that violate standard specifications. Using approximate memory reduces reliability, leading to higher bit error rates. Fortunately, neural networks have an intrinsic capacity to tolerate increased bit errors. This can enable energy-efficient and high-performance neural network inference using approximate DRAM devices.Based on this observation, we propose EDEN, the first general framework that reduces DNN energy consumption and DNN evaluation latency by using approximate DRAM devices, while strictly meeting a user-specified target DNN accuracy. EDEN relies on two key ideas: 1) retraining the DNN for a target approximate DRAM device to increase the DNN's error tolerance, and 2) efficient mapping of the error tolerance of each individual DNN data type to a corresponding approximate DRAM partition in a way that meets the user-specified DNN accuracy requirements.We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error models obtained from real approximate DRAM devices. We show that EDEN's DNN retraining technique reliably improves the error resiliency of the DNN by an order of magnitude. For a target accuracy within 1\% of the original DNN, our results show that EDEN enables 1) an average DRAM energy reduction of 21\%, 37\%, 31\%, and 32\% in CPU, GPU, and two different DNN accelerator architectures, respectively, across a variety of state-of-the-art networks, and 2) an average (maximum) speedup of 8\% (17\%) and 2.7\% (5.5\%) in CPU and GPU architectures, respectively, when evaluating latency-bound neural networks.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {166–181},
numpages = {16},
keywords = {DRAM, deep neural networks, energy efficiency, error tolerance, machine learning, memory systems},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358263,
author = {Huang, Chao-Tsung and Ding, Yu-Chun and Wang, Huan-Ching and Weng, Chi-Wen and Lin, Kai-Ping and Wang, Li-Wei and Chen, Li-De},
title = {eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358263},
doi = {10.1145/3352460.3358263},
abstract = {Convolutional neural networks (CNNs) have recently demonstrated superior quality for computational imaging applications. Therefore, they have great potential to revolutionize the image pipelines on cameras and displays. However, it is difficult for conventional CNN accelerators to support ultra-high-resolution videos at the edge due to their considerable DRAM bandwidth and power consumption. Therefore, finding a further memory- and computation-efficient microarchitecture is crucial to speed up this coming revolution.In this paper, we approach this goal by considering the inference flow, network model, instruction set, and processor design jointly to optimize hardware performance and image quality. We apply a block-based inference flow which can eliminate all the DRAM bandwidth for feature maps and accordingly propose a hardware-oriented network model, ERNet, to optimize image quality based on hardware constraints. Then we devise a coarse-grained instruction set architecture, FBISA, to support power-hungry convolution by massive parallelism. Finally, we implement an embedded processor---eCNN---which accommodates to ERNet and FBISA with a flexible processing architecture. Layout results show that it can support high-quality ERNets for super-resolution and denoising at up to 4K Ultra-HD 30 fps while using only DDR-400 and consuming 6.94W on average. By comparison, the state-of-the-art Diffy uses dualchannel DDR3-2133 and consumes 54.3W to support lower-quality VDSR at Full HD 30 fps. Lastly, we will also present application examples of high-performance style transfer and object recognition to demonstrate the flexibility of eCNN.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {182–195},
numpages = {14},
keywords = {computational imaging, convolutional neural network, edge inference, hardware accelerator, ultra-high-definition},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358282,
author = {Hu, Yu-Ching and Lokhandwala, Murtuza Taher and I., Te and Tseng, Hung-Wei},
title = {Dynamic Multi-Resolution Data Storage},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358282},
doi = {10.1145/3352460.3358282},
abstract = {Approximate computing that works on less precise data leads to significant performance gains and energy-cost reductions for compute kernels. However, without leveraging the full-stack design of computer systems, modern computer architectures undermine the potential of approximate computing.In this paper, we present Varifocal Storage, a dynamic multi-resolution storage system that tackles challenges in performance, quality, flexibility and cost for computer systems supporting diverse application demands. Varifocal Storage dynamically adjusts the dataset resolution within a storage device, thereby mitigating the performance bottleneck of exchanging/preparing data for approximate compute kernels. Varifocal Storage introduces Autofocus and iFilter mechanisms to provide quality control inside the storage device and make programs more adaptive to diverse datasets. Varifocal Storage also offers flexible, efficient support for approximate and exact computing without exceeding the costs of conventional storage systems by (1) saving the raw dataset in the storage device, and (2) targeting operators that complement the power of existing SSD controllers to dynamically generate lower-resolution datasets.We evaluate the performance of Varifocal Storage by running applications on a heterogeneous computer with our prototype SSD. The results show that Varifocal Storage can speed up data resolution adjustments by 2.02\texttimes{} or 1.74\texttimes{} without programmer input. Compared to conventional approximate-computing architectures, Varifocal Storage speeds up the overall execution time by 1.52\texttimes{}.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {196–210},
numpages = {15},
keywords = {Approximate Computing, Heterogeneous Computer Architectures/Systems, In-Storage Processing, Intelligent Storage Systems, Near-Data Processing},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358311,
author = {Shim, Youngseop and Kim, Myungsuk and Chun, Myoungjun and Park, Jisung and Kim, Yoona and Kim, Jihong},
title = {Exploiting Process Similarity of 3D Flash Memory for High Performance SSDs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358311},
doi = {10.1145/3352460.3358311},
abstract = {3D NAND flash memory exhibits two contrasting process characteristics from its manufacturing process. While process variability between different horizontal layers are well known, little has been systematically investigated about strong process similarity (PS) within the horizontal layer. In this paper, based on an extensive characterization study using real 3D flash chips, we show that 3D NAND flash memory possesses very strong process similarity within a 3D flash block: the word lines (WLs) on the same horizontal layer of the 3D flash block exhibit virtually equivalent reliability characteristics. This strong process similarity, which was not previously utilized, opens simple but effective new optimization opportunities for 3D flash memory. In this paper, we focus on exploiting the process similarity for improving the I/O latency. By carefully reusing various flash operating parameters monitored from accessing the leading WL, the remaining WLs on the same horizontal layer can be quickly accessed, avoiding unnecessary redundant steps for subsequent program and read operations. We also propose a new program sequence, called mixed order scheme (MOS), for 3D NAND flash memory which can further reduce the program latency. We have implemented a PS-aware FTL, called cubeFTL, which takes advantage of the proposed techniques. Our evaluation results show that cubeFTL can improve the IOPS by up to 48\% over an existing PS-unaware FTL.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {211–223},
numpages = {13},
keywords = {3D NAND Flash Memory, Process Similarity, Process Variability, SSD},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358320,
author = {Mailthody, Vikram Sharma and Qureshi, Zaid and Liang, Weixin and Feng, Ziyan and de Gonzalo, Simon Garcia and Li, Youjie and Franke, Hubertus and Xiong, Jinjun and Huang, Jian and Hwu, Wen-mei},
title = {DeepStore: In-Storage Acceleration for Intelligent Queries},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358320},
doi = {10.1145/3352460.3358320},
abstract = {Recent advancements in deep learning techniques facilitate intelligent-query support in diverse applications, such as content-based image retrieval and audio texturing. Unlike conventional key-based queries, these intelligent queries lack efficient indexing and require complex compute operations for feature matching. To achieve high-performance intelligent querying against massive datasets, modern computing systems employ GPUs in-conjunction with solid-state drives (SSDs) for fast data access and parallel data processing. However, our characterization with various intelligent-query workloads developed with deep neural networks (DNNs), shows that the storage I/O bandwidth is still the major bottleneck that contributes 56\%--90\% of the query execution time.To this end, we present DeepStore, an in-storage accelerator architecture for intelligent queries. It consists of (1) energy-efficient in-storage accelerators designed specifically for supporting DNN-based intelligent queries, under the resource constraints in modern SSD controllers; (2) a similarity-based in-storage query cache to exploit the temporal locality of user queries for further performance improvement; and (3) a lightweight in-storage runtime system working as the query engine, which provides a simple software abstraction to support different types of intelligent queries. DeepStore exploits SSD parallelisms with design space exploration for achieving the maximal energy efficiency for in-storage accelerators. We validate DeepStore design with an SSD simulator, and evaluate it with a variety of vision, text, and audio based intelligent queries. Compared with the state-of-the-art GPU+SSD approach, DeepStore improves the query performance by up to 17.7\texttimes{}, and energy-efficiency by up to 78.6\texttimes{}.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {224–238},
numpages = {15},
keywords = {Hardware Accelerators, In-Storage Computing, Information Retrieval, Intelligent Query, Solid-State Drive},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358303,
author = {Ajdari, Mohammadamin and Lee, Wonsik and Park, Pyeongsu and Kim, Joonsung and Kim, Jangwoo},
title = {FIDR: A Scalable Storage System for Fine-Grain Inline Data Reduction with Efficient Memory Handling},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358303},
doi = {10.1145/3352460.3358303},
abstract = {Storage systems play a critical role in modern servers which run highly data-intensive applications. To satisfy the high performance and capacity demands of such applications, storage systems now deploy an array of fast SSDs per server. To reduce the storage cost of employing many SSDs per server, storage systems actively perform inline data reduction (e.g., data deduplication, compression). Existing inline data reduction studies can achieve high performance and scalability by offloading computation-intensive data-reduction operations to dedicated hardware accelerators. However, such existing studies suffer from limited workload support and scalability. For example, they reduce only large data blocks, which incur many IO requests, leading to low data reduction rates, and their offloading overlooks memory-intensive operations, leading to the unoptimal scalability.In this paper, we propose FIDR, a highly scalable storage system to enable the inline data reduction of fine-grain data. We first identify key limitations of existing studies, and then set our scaling storage server design which effectively resolves the limitations by employing an optimal offloading mechanism. The key ideas of FIDR are to achieve high applicability by enabling fine-grain data reduction and high scalability by distributing data-reduction operations to optimal devices (e.g., host processor, accelerator, network interface card). The proposed offloading mechanism considers computation, memory capacity, and memory bandwidth requirements altogether. For evaluation, we implement an example FIDR system prototype using FPGAs. Our prototype system outperforms a current state-of-the-art data reduction system up to 3.3 times by significantly reducing both computation and memory resource requirements.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {239–252},
numpages = {14},
keywords = {FPGA, SSD array, compression, deduplication, memory management, small chunk, table management},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358257,
author = {Tannu, Swamit S. and Qureshi, Moinuddin},
title = {Ensemble of Diverse Mappings: Improving Reliability of Quantum Computers by Orchestrating Dissimilar Mistakes},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358257},
doi = {10.1145/3352460.3358257},
abstract = {Near-term quantum computers do not have the ability to perform error correction. Such Noisy Intermediate Scale Quantum (NISQ) computers can produce incorrect output as the computation is subjected to errors. The applications on a NISQ machine try to infer the correct output by running the same program thousands of times and logging the output. If the error rates are low and the errors are not correlated, then the correct answer can be inferred as the one appearing with the highest frequency. Unfortunately, quantum computers are subjected to correlated errors, which can cause an incorrect answer to appear more frequently than the correct answer.We observe that recent work on qubit mapping (including the recent work on variation-aware mapping) tries to obtain the best possible qubit allocation and uses it for all the trials. This approach significantly increases the vulnerability to correlated errors -- if the mapping becomes susceptible to a particular form of error, then all the trials will get subjected to the same error, which can cause the same wrong answer to appear as the output for a significant fraction of the trials. To mitigate the vulnerability to such correlated errors, this paper leverages the concept of diversity and proposes an Ensemble of Diverse Mappings (EDM). EDM uses diversity in qubit allocation to run copies of an input program with a diverse set of mappings, thus steering the trials towards making different mistakes. By combining the output probability distributions of the diverse ensemble, EDM amplifies the correct answer by suppressing the incorrect answers. Our experiments with ibmq-melbourne (14-qubit) machine shows that EDM improves the inference quality by 2.3x compared to the current state-of-the-art mapping algorithms.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {253–265},
numpages = {13},
keywords = {Correlated Errors, NISQ, Quantum Compilers},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358313,
author = {Gokhale, Pranav and Ding, Yongshan and Propson, Thomas and Winkler, Christopher and Leung, Nelson and Shi, Yunong and Schuster, David I. and Hoffmann, Henry and Chong, Frederic T.},
title = {Partial Compilation of Variational Algorithms for Noisy Intermediate-Scale Quantum Machines},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358313},
doi = {10.1145/3352460.3358313},
abstract = {Quantum computing is on the cusp of reality with Noisy Intermediate-Scale Quantum (NISQ) machines currently under development and testing. Some of the most promising algorithms for these machines are variational algorithms that employ classical optimization coupled with quantum hardware to evaluate the quality of each candidate solution. Recent work used GRadient Descent Pulse Engineering (GRAPE) to translate quantum programs into highly optimized machine control pulses, resulting in a significant reduction in the execution time of programs. This is critical, as quantum machines can barely support the execution of short programs before failing.However, GRAPE suffers from high compilation latency, which is untenable in variational algorithms since compilation is interleaved with computation. We propose two strategies for partial compilation, exploiting the structure of variational circuits to pre-compile optimal pulses for specific blocks of gates. Our results indicate significant pulse speedups ranging from 1.5x-3x in typical benchmarks, with only a small fraction of the compilation latency of GRAPE.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {266–278},
numpages = {13},
keywords = {optimal control, quantum computing, variational algorithms},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358265,
author = {Tannu, Swamit S. and Qureshi, Moinuddin K.},
title = {Mitigating Measurement Errors in Quantum Computers by Exploiting State-Dependent Bias},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358265},
doi = {10.1145/3352460.3358265},
abstract = {Quantum computers are susceptible to errors. While quantum computers can be guarded against errors using error correction codes, near-term quantum computers will not have sufficient number of qubits to implement error correction and must perform their computation in the presence of errors. Qubit measurement is typically the most error-prone operation on a quantum computer, with measurement errors ranging from 8\% to 30\% reported on current machines. This goal of this paper is to mitigate measurement errors by exploiting the state-dependent bias of measurement errors.Experiments on the IBM-Q5 and IBM-Q14 machines show variation in measurement errors depending on the state being measured. For example, measuring an all-zero state on IBM-Q5 has a fidelity of 84\%; however, the fidelity drops to 62\% while measuring the all-one state. To improve measurement fidelity, we propose Invert-and-Measure, which transforms the system from a vulnerable state to a stronger state and then performs the measurement in the stronger state. We propose two designs for Invert-and-Measure. First, Static Invert-and-Measure (SIM), which executes two instances of the program, one with standard measurements and the other with inverted measurements and combines the results. Second, Adaptive Invert and Measure (AIM), which learns the relative bias of different states using runtime profiling and produces specialized inversions to increase the likelihood of obtaining the correct answer. Our evaluations, using IBM-Q5 and IBM-Q14, show that SIM improves the application reliability by up to 2X, and AIM by up to 3X.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {279–290},
numpages = {12},
keywords = {Correlated Errors, NISQ, Quantum Compilers},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358287,
author = {Das, Poulami and Tannu, Swamit S. and Nair, Prashant J. and Qureshi, Moinuddin},
title = {A Case for Multi-Programming Quantum Computers},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358287},
doi = {10.1145/3352460.3358287},
abstract = {Existing and near-term quantum computers face significant reliability challenges because of high error rates caused by noise. Such machines are operated in the Noisy Intermediate Scale Quantum (NISQ) model of computing. As NISQ machines exhibit high error-rates, only programs that require a few qubits can be executed reliably. Therefore, NISQ machines tend to underutilize its resources. In this paper, we propose to improve the throughput and utilization of NISQ machines by using multi-programming and enabling the NISQ machine to concurrently execute multiple workloads.Multi-programming a NISQ machine is non-trivial. This is because, a multi-programmed NISQ machine can have an adverse impact on the reliability of the individual workloads. To enable multi-programming in a robust manner, we propose three solutions. First, we develop methods to partition the qubits into multiple reliable regions using error information from machine calibration so that each program can have a fair allocation of reliable qubits. Second, we observe that when two programs are of unequal lengths, measurement operations can impact the reliability of the co-running program. To reduce this interference, we propose a Delayed Instruction Scheduling (DIS) policy that delays the start of the shorter program so that all the measurement operations can be performed at the end. Third, we develop an Adaptive Multi-Programming (AMP) design that monitors the reliability at runtime and reverts to single program mode if the reliability impact of multi-programming is greater than a predefined threshold. Our evaluations with IBM-Q16 show that our proposals can improve resource utilization and throughput by up to 2x, while limiting the impact on reliability.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {291–303},
numpages = {13},
keywords = {Multi-programming, NISQ, Quantum Computer, Reliability},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358268,
author = {Baek, Eunjin and Lee, Hunjun and Kim, Youngsok and Kim, Jangwoo},
title = {FlexLearn: Fast and Highly Efficient Brain Simulations Using Flexible On-Chip Learning},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358268},
doi = {10.1145/3352460.3358268},
abstract = {To understand how the human brain works, neuroscientists heavily rely on brain simulations which incorporate the concept of time to their operating model. In the simulations, neurons transmit their signals through synapses whose weights change over time and by the activity of the associated neurons. Such changes in synaptic weights, known as learning, are thought to contribute to memory, and various learning rules exist to model different behaviors of the human brain. Due to the diverse neurons and learning rules, neuroscientists perform the simulations using highly programmable general-purpose processors. Unfortunately, the processors greatly suffer from the high computational overheads of the learning rules. As an alternative, brain simulation accelerators achieve orders of magnitude higher performance; however, they have limited flexibility and cannot support the diverse neurons and learning rules.In this paper, we present FlexLearn, a flexible on-chip learning engine to enable fast and highly efficient brain simulations. FlexLearn achieves high flexibility by supporting diverse biologically plausible sub-rules which can be combined to simulate various target learning rules. To design FlexLearn, we first identify 17 representative sub-rules which adjust the synaptic weights in different manners. Then, we design and compact the specialized datapaths for the sub-rules and identify dependencies between them to maximize parallelism. After that, we present an example flexible brain simulation processor by integrating the datapaths with the state-of-the-art flexible digital neuron and existing accelerator to support end-to-end simulations. Our evaluation using a 45-nm cell library shows that the 128-core brain simulation processor prototype with FlexLearn greatly improves the harmonic mean per-area performance and the energy efficiency by 30.07\texttimes{} and 126.87\texttimes{}, respectively, over the server-class CPU. The prototype also achieves the harmonic mean per-area speedup of 1.41\texttimes{} over the current state-of-the-art 128-core accelerator which supports programmable learning rules.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {304–318},
numpages = {15},
keywords = {brain simulations, neuromorphic accelerators, on-chip learning},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358275,
author = {Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W.},
title = {ExTensor: An Accelerator for Sparse Tensor Algebra},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358275},
doi = {10.1145/3352460.3358275},
abstract = {Generalized tensor algebra is a prime candidate for acceleration via customized ASICs. Modern tensors feature a wide range of data sparsity, with the density of non-zero elements ranging from 10-6\% to 50\%. This paper proposes a novel approach to accelerate tensor kernels based on the principle of hierarchical elimination of computation in the presence of sparsity. This approach relies on rapidly finding intersections---situations where both operands of a multiplication are non-zero---enabling new data fetching mechanisms and avoiding memory latency overheads associated with sparse kernels implemented in software.We propose the ExTensor accelerator, which builds these novel ideas on handling sparsity into hardware to enable better bandwidth utilization and compute throughput. We evaluate ExTensor on several kernels relative to industry libraries (Intel MKL) and state-of-the-art tensor algebra compilers (TACO). When bandwidth normalized, we demonstrate an average speedup of 3.4\texttimes{}, 1.3\texttimes{}, 2.8\texttimes{}, 24.9\texttimes{}, and 2.7\texttimes{} on SpMSpM, SpMM, TTV, TTM, and SDDMM kernels respectively over a server class CPU.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {319–333},
numpages = {15},
keywords = {Hardware Acceleration, Sparse Computation, Tensor Algebra},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358308,
author = {Nag, Anirban and Ramachandra, C. N. and Balasubramonian, Rajeev and Stutsman, Ryan and Giacomin, Edouard and Kambalasubramanyam, Hari and Gaillardon, Pierre-Emmanuel},
title = {GenCache: Leveraging In-Cache Operators for Efficient Sequence Alignment},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358308},
doi = {10.1145/3352460.3358308},
abstract = {Precision Medicine will rely on frequent genomic analysis, especially for patients undergoing cancer treatments or suffering from rare diseases. Sequence alignment is invoked in multiple stages of the genomic analysis pipeline. Recent projects have introduced accelerators, GenAx and Darwin, for 2nd and 3rd generation sequencers respectively. In this work, we improve upon the GenAx design by increasing its parallelism and reducing its memory bandwidth demands. This is achieved with a combination of hardware and software innovations. We first integrate in-cache operators from prior work into the GenAx memory hierarchy; we then augment the in-cache peripheral circuit to support additional new operators. We then re-structure the sequence alignment algorithm to (i) leverage the many in-cache operators, (ii) exploit the common case in genomic datasets, (iii) use Bloom Filters to reduce futile accesses, and (iv) maximize data reuse within a re-organized memory hierarchy. While the baseline GenAx accelerator processes a batch of reads in 194 seconds while nearly saturating the 153.6 GB/s memory bandwidth, the proposed GenCache architecture processes the same batch of reads in 37 seconds at an improved energy efficiency of 8.6\texttimes{}, while demanding 20 GB/s average memory bandwidth. Our hardware and software techniques thus interact synergistically to target both memory and compute bottlenecks, while not affecting the outputs of the application. We show that the basic principles in GenCache can also be exploited by 3rd generation sequence aligners.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {334–346},
numpages = {13},
keywords = {CISC Instructions, Cache Partitioning, Genomics, Hardware Acceleration, In-Cache Operators, Sequence Alignment},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358330,
author = {Sadi, Fazle and Sweeney, Joe and Low, Tze Meng and Hoe, James C. and Pileggi, Larry and Franchetti, Franz},
title = {Efficient SpMV Operation for Large and Highly Sparse Matrices using Scalable Multi-way Merge Parallelization},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358330},
doi = {10.1145/3352460.3358330},
abstract = {The importance of Sparse Matrix dense Vector multiplication (SpMV) operation in graph analytics and numerous scientific applications has led to development of custom accelerators that are intended to over-come the difficulties of sparse data operations on general purpose architectures. However, efficient SpMV operation on large problem (i.e. working set exceeds on-chip storage) is severely constrained due to strong dependence on limited amount of fast random access memory to scale. Additionally, unstructured matrix with high sparsity pose difficulties as most solutions rely on exploitation of data locality. This work presents an algorithm co-optimized scalable hardware architecture that can efficiently operate on very large (~billion nodes) and/or highly sparse (avg. degree &lt;10) graphs with significantly less on-chip fast memory than existing solutions. A novel parallelization methodology for implementing large and high throughput multi-way merge network is the key enabler of this high performance SpMV accelerator. Additionally, a data compression scheme to reduce off-chip traffic and special computation for nodes with exceptionally large number of edges, commonly found in power-law graphs, are presented. This accelerator is demonstrated with 16-nm fabricated ASIC and Stratix® 10 FPGA platforms. Experimental results show more than an order of magnitude improvement over current custom hardware solutions and more than two orders of magnitude improvement over commercial off-the-shelf (COTS) architectures for both performance and energy efficiency.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {347–358},
numpages = {12},
keywords = {SpMV, custom hardware, merge parallelization, sparse matrices},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358269,
author = {Zhu, Maohua and Zhang, Tao and Gu, Zhenyu and Xie, Yuan},
title = {Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358269},
doi = {10.1145/3352460.3358269},
abstract = {Deep neural networks have become the compelling solution for the applications such as image classification, object detection, speech recognition, and machine translation. However, the great success comes at the cost of excessive computation due to the over-provisioned parameter space. To improve the computation efficiency of neural networks, many pruning techniques have been proposed to reduce the amount of multiply-accumulate (MAC) operations, which results in high sparsity in the networks.Unfortunately, the sparse neural networks often run slower than their dense counterparts on modern GPUs due to their poor device utilization rate. In particular, as the sophisticated hardware primitives (e.g., Tensor Core) have been deployed to boost the performance of dense matrix multiplication by an order of magnitude, the performance of sparse neural networks lags behind significantly.In this work, we propose an algorithm and hardware co-design methodology to accelerate the sparse neural networks. A novel pruning algorithm is devised to improve the workload balance and reduce the decoding overhead of the sparse neural networks. Meanwhile, new instructions and micro-architecture optimization are proposed in Tensor Core to adapt to the structurally sparse neural networks. Our experimental results show that the pruning algorithm can achieve 63\% performance gain with model accuracy sustained. Furthermore, the hardware optimization gives an additional 58\% performance gain with negligible area overhead.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {359–371},
numpages = {13},
keywords = {graphics processing units, neural networks, pruning},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358307,
author = {Villa, Oreste and Stephenson, Mark and Nellans, David and Keckler, Stephen W.},
title = {NVBit: A Dynamic Binary Instrumentation Framework for NVIDIA GPUs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358307},
doi = {10.1145/3352460.3358307},
abstract = {Binary instrumentation frameworks are widely used to implement profilers, performance evaluation, error checking, and bug detection tools. While dynamic binary instrumentation tools such as PIN and DynamoRio are supported on CPUs, GPU architectures currently only have limited support for similar capabilities through static compile-time tools, which prohibits instrumentation of dynamically loaded libraries that are foundations for modern high-performance applications. This work presents NVBit, a fast, dynamic, and portable, binary instrumentation framework, that allows users to write instrumentation tools in CUDA/C/C++ and selectively apply that functionality to pre-compiled binaries and libraries executing on NVIDIA GPUs. Using dynamic recompilation at the SASS level, NVBit analyzes GPU kernel register requirements to generate efficient ABI compliant instrumented code without requiring the tool developer to have detailed knowledge of the underlying GPU architecture. NVBit allows basic-block instrumentation, multiple function injections to the same location, inspection of all ISA visible state, dynamic selection of instrumented or uninstrumented code, permanent modification of register state, source code correlation, and instruction removal. NVBit supports all recent NVIDIA GPU architecture families including Kepler, Maxwell, Pascal and Volta and works on any pre-compiled CUDA, OpenACC, OpenCL, or CUDA-Fortran application.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {372–383},
numpages = {12},
keywords = {CUDA, Dynamic binary instrumentation, GPGPU, GPU computing},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358285,
author = {Pothukuchi, Raghavendra Pradyumna and Greathouse, Joseph L. and Rao, Karthik and Erb, Christopher and Piga, Leonardo and Voulgaris, Petros G. and Torrellas, Josep},
title = {Tangram: Integrated Control of Heterogeneous Computers},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358285},
doi = {10.1145/3352460.3358285},
abstract = {Resource control in heterogeneous computers built with subsystems from different vendors is challenging. There is a tension between the need to quickly generate local decisions in each subsystem and the desire to coordinate the different subsystems for global optimization. In practice, global coordination among subsystems is considered hard, and current commercial systems use centralized controllers. The result is high response time and high design cost due to lack of modularity.To control emerging heterogeneous computers effectively, we propose a new control framework called Tangram that is fast, globally coordinated, and modular. Tangram introduces a new formal controller that combines multiple engines for optimization and safety, and has a standard interface. Building the controller for a subsystem requires knowing only about that subsystem. As a heterogeneous computer is assembled, the controllers in the different subsystems are connected hierarchically, exchanging standard coordination signals. To demonstrate Tangram, we prototype it in a heterogeneous server that we assemble using components from multiple vendors. Compared to state-of-the-art control, Tangram reduces, on average, the execution time of heterogeneous applications by 31\% and their energy-delay product by 39\%.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {384–398},
numpages = {15},
keywords = {Distributed resource management, formal control, heterogeneous computers, modular control},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358279,
author = {Choi, Jongouk and Liu, Qingrui and Jung, Changhee},
title = {CoSpec: Compiler Directed Speculative Intermittent Computation},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358279},
doi = {10.1145/3352460.3358279},
abstract = {Energy harvesting systems have emerged as an alternative to battery-operated embedded devices. Due to the intermittent nature of energy harvesting, researchers equip the systems with nonvolatile memory (NVM) and crash consistency mechanisms. However, prior works require non-trivial hardware modifications, e.g., a voltage monitor, nonvolatile flip-flops/scratchpad, dependence tracking modules, etc., thereby causing significant area/power/manufacturing costs.For low-cost yet performant intermittent computation, this paper presents CoSpec, a new architecture/compiler co-design scheme that works for commodity in-order processors used in energy-harvesting systems. To achieve crash consistency without requiring unconventional architectural support, CoSpec leverages speculation assuming that power failure is not going to occur and thus holds all committed stores in a store buffer (SB)---as if they were speculative---in case of mispeculation. CoSpec compiler first partitions a given program into a series of recoverable code regions with the SB size in mind, so that no region overflows the SB. When the program control reaches the end of each region, the speculation turns out to be successful, thus releasing all the buffered stores of the region to NVM. If power failure occurs during the execution of a region, all its speculative stores disappear in the volatile SB, i.e., they never affect program states in NVM. Consequently, the interrupted region can be restarted with consistent program states in the wake of power failure.To hide the latency of the SB release---i.e., essentially NVM writes---at each region boundary, CoSpec overlaps the NVM writes of the current region with the speculative execution of the next region. Such instruction level parallelism gives an illusion of out-of-order execution on top of the in-order processor, achieving a speedup of more than 1.2X when there is no power outage. Our experiments on a set of real energy harvesting traces with frequent outages demonstrate that CoSpec outperforms the state-of-the-art scheme by 1.8~3X on average.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {399–412},
numpages = {14},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358319,
author = {Shi, Zhan and Huang, Xiangru and Jain, Akanksha and Lin, Calvin},
title = {Applying Deep Learning to the Cache Replacement Problem},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358319},
doi = {10.1145/3352460.3358319},
abstract = {Despite its success in many areas, deep learning is a poor fit for use in hardware predictors because these models are impractically large and slow, but this paper shows how we can use deep learning to help design a new cache replacement policy. We first show that for cache replacement, a powerful LSTM learning model can in an offline setting provide better accuracy than current hardware predictors. We then perform analysis to interpret this LSTM model, deriving a key insight that allows us to design a simple online model that matches the offline model's accuracy with orders of magnitude lower cost.The result is the Glider cache replacement policy, which we evaluate on a set of 33 memory-intensive programs from the SPEC 2006, SPEC 2017, and GAP (graph-processing) benchmark suites. In a single-core setting, Glider outperforms top finishers from the 2nd Cache Replacement Championship, reducing the miss rate over LRU by 8.9\%, compared to reductions of 7.1\% for Hawkeye, 6.5\% for MPPPB, and 7.5\% for SHiP++. On a four-core system, Glider improves IPC over LRU by 14.7\%, compared with improvements of 13.6\% (Hawkeye), 13.2\% (MPPPB), and 11.4\% (SHiP++).},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {413–425},
numpages = {13},
keywords = {cache replacement, caches, deep learning},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358301,
author = {Huang, Ziqiang and Joao, Jos\'{e} A. and Rico, Alejandro and Hilton, Andrew D. and Lee, Benjamin C.},
title = {DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358301},
doi = {10.1145/3352460.3358301},
abstract = {Sprinting is a class of mechanisms that provides a short but significant performance boost while temporarily exceeding the thermal design point. We propose DynaSprint, a software runtime that manages sprints by dynamically predicting utility and modeling thermal headroom. Moreover, we propose a new sprint mechanism for caches, increasing capacity briefly for enhanced performance. For a system that extends last-level cache capacity from 2MB to 4MB per core and can absorb 10J of heat, DynaSprint-guided cache sprints improve performance by 17\% on average and by up to 40\% over a non-sprinting system. These performance outcomes, within 95\% of an oracular policy, are possible because DynaSprint accurately predicts phase behavior and sprint utility.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {426–439},
numpages = {14},
keywords = {caches, performance optimization, power/thermal management},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358272,
author = {Zhang, Guowei and Sanchez, Daniel},
title = {Leveraging Caches to Accelerate Hash Tables and Memoization},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358272},
doi = {10.1145/3352460.3358272},
abstract = {Hash tables are widely used, but they are inefficient in current systems: they use core resources poorly and suffer from limited spatial locality in caches. To address these issues we propose HTA, a technique that accelerates hash table operations via simple ISA extensions and hardware changes. HTA adopts an efficient hash table format that leverages the characteristics of caches. HTA accelerates most operations in hardware, and leaves rare cases to software.We present two implementations of HTA, Flat-HTA and Hierarchical-HTA. Flat-HTA adopts a simple, hierarchy-oblivious layout and reduces runtime overheads with simple changes to cores. Hierarchical-HTA is a more complex implementation that uses a hierarchy-aware layout to improve spatial locality at intermediate cache levels. It requires some changes to caches and provides modest benefits over Flat-HTA.We evaluate HTA on hash table-intensive benchmarks and use it to accelerate memoization, a technique that caches and reuses the outputs of repetitive computations. Flat-HTA improves the performance of the state-of-the-art hash table-intensive applications by up to 2\texttimes{}, while Hierarchical-HTA outperforms Flat-HTA by up to 35\%. Flat-HTA also outperforms software memoization by 2\texttimes{}.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {440–452},
numpages = {13},
keywords = {cache, hash table, memoization, microarchitecture, specialization},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358281,
author = {Hong, Seokin and Abali, Bulent and Buyuktosunoglu, Alper and Healy, Michael B. and Nair, Prashant J.},
title = {Touch\'{e}: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358281},
doi = {10.1145/3352460.3358281},
abstract = {Compression is seen as a simple technique to increase the effective cache capacity. Unfortunately, compression techniques either incur tag area overheads or restrict cache block placement to only include neighboring addresses. Ideally, we should be able to place compressed cache blocks without any restrictions or overheads.This paper proposes Touch\'{e}, a framework for storing multiple compressed blocks from arbitrary addresses within a cacheline without tag area overheads. The Touch\'{e} framework consists of three components. The first component, called the "Signature" (SIGN) engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). The second component, called the "Tag Appended Data" (TADA) mechanism, stores the full tag addresses with data. TADA enables Touch\'{e} to detect false positive signature matches by providing the full tag address. The third component, called the "Superblock Marker" (SMARK) mechanism, uses a unique marker in the tag entry to indicate compressed cache blocks from neighboring physical addresses in the same cacheline. Touch\'{e} is hardware-based and achieves an average speedup of 12\% (ideal 13\%) when compared to an uncompressed baseline.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {453–465},
numpages = {13},
keywords = {Caches, Compression, Data Array, Hashing, Tag Array},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358321,
author = {Gupta, Siddharth and Daglis, Alexandros and Falsafi, Babak},
title = {Distributed Logless Atomic Durability with Persistent Memory},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358321},
doi = {10.1145/3352460.3358321},
abstract = {Datacenter operators have started deploying Persistent Memory (PM), leveraging its combination of fast access and persistence for significant performance gains. A key challenge for PM-aware software is to maintain high performance while achieving atomic durability. The latter typically requires the use of logging, which introduces considerable overhead with additional CPU cycles, write traffic, and ordering requirements. In this paper, we exploit the data multiversioning inherent in the memory hierarchy to achieve atomic durability without logging. Our design, LAD, relies on persistent buffering space at the memory controllers (MCs)---already present in modern CPUs---to speculatively accumulate all of a transaction's updates before they are all atomically committed to PM. LAD employs an on-chip distributed commit protocol in hardware to manage the distributed speculative state each transaction accumulates across multiple MCs. We demonstrate that LAD is a practical design relying on modest hardware modifications to provide atomically durable transactions, while delivering up to 80\% of ideal---i.e., PM-oblivious software's---performance.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {466–478},
numpages = {13},
keywords = {Atomic Durability, Atomicity, Logging, Persistent Memory},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358290,
author = {Zuo, Pengfei and Hua, Yu and Xie, Yuan},
title = {SuperMem: Enabling Application-transparent Secure Persistent Memory with Low Overheads},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358290},
doi = {10.1145/3352460.3358290},
abstract = {Non-volatile memory (NVM) suffers from security vulnerability to physical access based attacks due to non-volatility. To ensure data security in NVM, counter mode encryption is often used by considering its high security level and low decryption latency. However, the counter mode encryption incurs new persistence problem for crash consistency guarantee due to the requirement for atomically persisting both data and its counter. To address this problem, existing work requires a large battery backup or complex modifications on both hardware and software layers due to employing a writeback counter cache. The large battery backup is expensive and software-layer modifications limit the portability of applications from the un-encrypted NVM to the encrypted one. Our paper proposes SuperMem, an application-transparent secure persistent memory by leveraging a write-through counter cache to guarantee the atomicity of data and counter writes without the needs of a large battery backup and software-layer modifications. To reduce the performance overhead of a baseline write-through counter cache, SuperMem leverages a locality-aware counter write coalescing scheme to reduce the number of write requests by exploiting the spatial locality of counter storage and data writes. Moreover, SuperMem leverages a cross-bank counter storage scheme to efficiently distribute data and counter writes to different banks, thus speeding up writes by exploiting bank parallelism. Experimental results demonstrate that SuperMem improves the performance by about 2\texttimes{} compared with an encrypted NVM with a baseline write-through counter cache, and achieves the performance comparable to an ideal secure NVM that exhibits the optimal performance of an encrypted NVM.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {479–492},
numpages = {14},
keywords = {Non-volatile memory, crash consistency, memory encryption},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358323,
author = {Gao, Congming and Ye, Min and Li, Qiao and Xue, Chun Jason and Zhang, Youtao and Shi, Liang and Yang, Jun},
title = {Constructing Large, Durable and Fast SSD System via Reprogramming 3D TLC Flash Memory},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358323},
doi = {10.1145/3352460.3358323},
abstract = {NAND flash memory based SSDs have been widely studied and adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. Compared with 2D SSD, 3D SSD stacks more layers into one block, constructing one block with more flash pages. For reliability and other reasons, technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. However, representing multiple bits per cell encounters additional challenges such as endurance and access latency. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can reduce the frequency of erases which determines the endurance of a cell, improve the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Further, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations, and when they should be applied. ReSSD is evaluated in a case study in 3D TLC SSD based RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 30.3\%, boost write performance by 16.7\%, and increase effective capacity by 7.71\%, with negligible overhead compared with conventional 3D SSD based RAID 5 system.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {493–505},
numpages = {13},
keywords = {3D TLC SSD, RAID 5, capacity, endurance, performance, reliability, reprogramming},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358293,
author = {Ando, Hideki},
title = {SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358293},
doi = {10.1145/3352460.3358293},
abstract = {The improvement of single-thread performance is much needed. Among the many structures that comprise a processor, the issue queue (IQ) is one of the most important structures that influences high single-thread performance. Correctly assigning the issue priority and providing high capacity efficiency are key features, but no conventional IQ organizations do not sufficiently have these.In this paper, we propose an IQ called the switching issue queue (SWQUE), which dynamically configures the IQ as a modified circular queue (CIRC-PC) or random queue with an age matrix (AGE) by responding to the degree of capacity demand. CIRC-PC corrects the issue priority when wrap-around occurs by exploiting the finding that instructions that are wrapped around are latency-tolerant. CIRC-PC is used for phases in which capacity efficiency is less important and the correct priority is more important; and AGE is used for phases in which capacity efficiency is more important. Our evaluation results using SPEC2017 benchmark programs show that SWQUE achieved higher performance by averages of 9.7\% and 2.9\% (up to 24.4\% or 10.6\%) for integer and floating-point programs, respectively, compared with AGE, which is widely used in current processors.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {506–518},
numpages = {13},
keywords = {instruction scheduling, issue queue},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358315,
author = {Soundararajan, Niranjan and Gupta, Saurabh and Natarajan, Ragavendra and Stark, Jared and Pal, Rahul and Sala, Franck and Rappoport, Lihu and Yoaz, Adi and Subramoney, Sreenivas},
title = {Towards the adoption of Local Branch Predictors in Modern Out-of-Order Superscalar Processors},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358315},
doi = {10.1145/3352460.3358315},
abstract = {Branch prediction accuracy plays a dominant role in the performance provided by modern Out-of-Order(OOO) superscalar processors. While global history-based branch predictors are more popular, local history-based predictors offer an additional dimension towards enhancing the overall branch prediction accuracy. Integrating the local predictors in modern cores, though, comes with non-trivial challenges associated with managing the local predictor's state and repairing this state on any branch misprediction is essential for the local predictor to operate effectively. Using a highly accurate, industry standard simulator modeling a Skylake-like OOO core and workloads spanning diverse categories including Server, High Performance Computing (HPC) and personal computing suites, besides SPEC, we methodically highlight the issues that need to be tackled, why local predictor repair is non-trivial and the performance opportunity that is lost when the local predictor repair is not handled efficiently. We discuss the issues with prior techniques and quantify their limitations when using them in current OOO cores. Further, we propose three practical, implementable and efficient repair techniques with minimal storage requirements that provide significant performance gains for local predictors. Unlike prior repair techniques that can only attain 50\% of the oracular gains, our realistic repair techniques retain about 80\% of the oracular gains resulting in significantly better application performance.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {519–530},
numpages = {12},
keywords = {Branch Prediction, Local predictors, Performance, Superscalar cores},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358325,
author = {Bera, Rahul and Nori, Anant V. and Mutlu, Onur and Subramoney, Sreenivas},
title = {DSPatch: Dual Spatial Pattern Prefetcher},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358325},
doi = {10.1145/3352460.3358325},
abstract = {High main memory latency continues to limit performance of modern high-performance out-of-order cores. While DRAM latency has remained nearly the same over many generations, DRAM bandwidth has grown significantly due to higher frequencies, newer architectures (DDR4, LPDDR4, GDDR5) and 3D-stacked memory packaging (HBM). Current state-of-the-art prefetchers do not do well in extracting higher performance when higher DRAM bandwidth is available. Prefetchers need the ability to dynamically adapt to available bandwidth, boosting prefetch count and prefetch coverage when headroom exists and throttling down to achieve high accuracy when the bandwidth utilization is close to peak.To this end, we present the Dual Spatial Pattern Prefetcher (DSPatch) that can be used as a standalone prefetcher or as a lightweight adjunct spatial prefetcher to the state-of-the-art delta-based Signature Pattern Prefetcher (SPP). DSPatch builds on a novel and intuitive use of modulated spatial bit-patterns. The key idea is to: (1) represent program accesses on a physical page as a bit-pattern anchored to the first "trigger" access, (2) learn two spatial access bit-patterns: one biased towards coverage and another biased towards accuracy, and (3) select one bit-pattern at run-time based on the DRAM bandwidth utilization to generate prefetches. Across a diverse set of workloads, using only 3.6KB of storage, DSPatch improves performance over an aggressive baseline with a PC-based stride prefetcher at the L1 cache and the SPP prefetcher at the L2 cache by 6\% (9\% in memory-intensive workloads and up to 26\%). Moreover, the performance of DSPatch+SPP scales with increasing DRAM bandwidth, growing from 6\% over SPP to 10\% when DRAM bandwidth is doubled.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {531–544},
numpages = {14},
keywords = {Data prefetching, memory latency, microarchitecture},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358288,
author = {Xia, Hongyan and Woodruff, Jonathan and Ainsworth, Sam and Filardo, Nathaniel W. and Roe, Michael and Richardson, Alexander and Rugg, Peter and Neumann, Peter G. and Moore, Simon W. and Watson, Robert N. M. and Jones, Timothy M.},
title = {CHERIvoke: Characterising Pointer Revocation using CHERI Capabilities for Temporal Memory Safety},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358288},
doi = {10.1145/3352460.3358288},
abstract = {A lack of temporal safety in low-level languages has led to an epidemic of use-after-free exploits. These have surpassed in number and severity even the infamous buffer-overflow exploits violating spatial safety. Capability addressing can directly enforce spatial safety for the C language by enforcing bounds on pointers and by rendering pointers unforgeable. Nevertheless, an efficient solution for strong temporal memory safety remains elusive.CHERI is an architectural extension to provide hardware capability addressing that is seeing significant commercial and open-source interest. We show that CHERI capabilities can be used as a foundation to enable low-cost heap temporal safety by facilitating out-of-date pointer revocation, as capabilities enable precise and efficient identification and invalidation of pointers, even when using unsafe languages such as C. We develop CHERIvoke, a technique for deterministic and fast sweeping revocation to enforce temporal safety on CHERI systems. CHERIvoke quarantines freed data before periodically using a small shadow map to revoke all dangling pointers in a single sweep of memory, and provides a tunable trade-off between performance and heap growth. We evaluate the performance of such a system using high-performance x86 processors, and further analytically examine its primary overheads. When configured with a heap-size overhead of 25\%, we find that CHERIvoke achieves an average execution-time overhead of under 5\%, far below the overheads associated with traditional garbage collection, revocation, or page-table systems.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {545–557},
numpages = {13},
keywords = {architecture, security, temporal safety, use-after-free},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358299,
author = {Sasaki, Hiroshi and Arroyo, Miguel A. and Ziad, M. Tarek Ibn and Bhat, Koustubha and Sinha, Kanad and Sethumadhavan, Simha},
title = {Practical Byte-Granular Memory Blacklisting using Califorms},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358299},
doi = {10.1145/3352460.3358299},
abstract = {Recent rapid strides in memory safety tools and hardware have improved software quality and security. While coarse-grained memory safety has improved, achieving memory safety at the granularity of individual objects remains a challenge due to high performance overheads usually between ~1.7x--2.2x. In this paper, we present a novel idea called Califorms, and associated program observations, to obtain a low overhead security solution for practical, byte-granular memory safety.The idea we build on is called memory blacklisting, which prohibits a program from accessing certain memory regions based on program semantics. State of the art hardware-supported memory blacklisting, while much faster than software blacklisting, creates memory fragmentation (on the order of few bytes) for each use of the blacklisted location. We observe that metadata used for blacklisting can be stored in dead spaces in a program's data memory and that this metadata can be integrated into the microarchitecture by changing the cache line format. Using these observations, a Califorms based system proposed in this paper reduces the performance overheads of memory safety to ~1.02x--1.16x while providing byte-granular protection and maintaining very low hardware overheads. Moreover, the fundamental idea of storing metadata in empty spaces and changing cache line formats can be used for other security and performance applications.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {558–571},
numpages = {14},
keywords = {caches, memory blacklisting, memory safety},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358306,
author = {Weisse, Ofir and Neal, Ian and Loughlin, Kevin and Wenisch, Thomas F. and Kasikci, Baris},
title = {NDA: Preventing Speculative Execution Attacks at Their Source},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358306},
doi = {10.1145/3352460.3358306},
abstract = {Speculative execution attacks like Meltdown and Spectre work by accessing secret data in wrong-path execution. Secrets are then transmitted and recovered by the attacker via a covert channel. Existing mitigations either require code modifications, address only specific exploit techniques, or block only the cache covert channel. Rather than battling exploit techniques and covert channels one by one, we seek to close off speculative execution attacks at their source. Our key observation is that these attacks require a chain of dependent wrong-path instructions to access and transmit secret data. We propose NDA, a technique to restrict speculative data propagation. NDA breaks the attacks' wrong-path dependence chains while still allowing speculation and dynamic scheduling. We describe a design space of NDA variants that differ in the constraints they place on dynamic scheduling and the classes of speculative execution attacks they prevent. NDA preserves much of the performance advantage of out-of-order execution: on SPEC CPU 2017, NDA variants close 68-96\% of the performance gap between in-order and unconstrained (insecure) out-of-order execution.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {572–586},
numpages = {15},
keywords = {meltdown, security, spectre, speculative execution},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358329,
author = {Huangfu, Wenqin and Li, Xueqi and Li, Shuangchen and Hu, Xing and Gu, Peng and Xie, Yuan},
title = {MEDAL: Scalable DIMM based Near Data Processing Accelerator for DNA Seeding Algorithm},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358329},
doi = {10.1145/3352460.3358329},
abstract = {Computational genomics has proven its great potential to support precise and customized health care. However, with the wide adoption of the Next Generation Sequencing (NGS) technology, 'DNA Alignment', as the crucial step in computational genomics, is becoming more and more challenging due to the booming bio-data. Consequently, various hardware approaches have been explored to accelerate DNA seeding - the core and most time consuming step in DNA alignment.Most previous hardware approaches leverage multi-core, GPU, and FPGA to accelerate DNA seeding. However, DNA seeding is bounded by memory and above hardware approaches focus on computation. For this reason, Near Data Processing (NDP) is a better solution for DNA seeding. Unfortunately, existing NDP accelerators for DNA seeding face two grand challenges, i.e., fine-grained random memory access and scalability demand for booming bio-data. To address those challenges, we propose a practical, energy efficient, Dual-Inline Memory Module (DIMM) based, NDP Accelerator for DNA Seeding Algorithm (MEDAL), which is based on off-the-shelf DRAM components. For small databases that can be fitted within a single DRAM rank, we propose the intra-rank design, together with an algorithm-specific address mapping, bandwidth-aware data mapping, and Individual Chip Select (ICS) to address the challenge of fine-grained random memory access, improving parallelism and bandwidth utilization. Furthermore, to tackle the challenge of scalability for large databases, we propose three inter-rank designs (polling-based communication, interrupt-based communication, and Non-Volatile DIMM (NVDIMM)-based solution). In addition, we propose an algorithm-specific data compression technique to reduce memory footprint, introduce more space for the data mapping, and reduce the communication overhead. Experimental results show that for three proposed designs, on average, MEDAL can achieve 30.50x/8.37x/3.43x speedup and 289.91x/6.47x/2.89x energy reduction when compared with a 16-thread CPU baseline and two state-of-the-art NDP accelerators, respectively.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {587–599},
numpages = {13},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358286,
author = {Kanellopoulos, Konstantinos and Vijaykumar, Nandita and Giannoula, Christina and Azizi, Roknoddin and Koppula, Skanda and Ghiasi, Nika Mansouri and Shahroodi, Taha and Luna, Juan Gomez and Mutlu, Onur},
title = {SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358286},
doi = {10.1145/3352460.3358286},
abstract = {Important workloads, such as machine learning and graph analytics applications, heavily involve sparse linear algebra operations. These operations use sparse matrix compression as an effective means to avoid storing zeros and performing unnecessary computation on zero elements. However, compression techniques like Compressed Sparse Row (CSR) that are widely used today introduce significant instruction overhead and expensive pointer-chasing operations to discover the positions of the non-zero elements. In this paper, we identify the discovery of the positions (i.e., indexing) of non-zero elements as a key bottleneck in sparse matrix-based workloads, which greatly reduces the benefits of compression.We propose SMASH, a hardware-software cooperative mechanism that enables highly-efficient indexing and storage of sparse matrices. The key idea of SMASH is to explicitly enable the hardware to recognize and exploit sparsity in data. To this end, we devise a novel software encoding based on a hierarchy of bitmaps. This encoding can be used to efficiently compress any sparse matrix, regardless of the extent and structure of sparsity. At the same time, the bitmap encoding can be directly interpreted by the hardware. We design a lightweight hardware unit, the Bitmap Management Unit (BMU), that buffers and scans the bitmap hierarchy to perform highly-efficient indexing of sparse matrices. SMASH exposes an expressive and rich ISA to communicate with the BMU, which enables its use in accelerating any sparse matrix computation.We demonstrate the benefits of SMASH on four use cases that include sparse matrix kernels and graph analytics applications. Our evaluations show that SMASH provides average performance improvements of 38\% for Sparse Matrix Vector Multiplication and 44\% for Sparse Matrix Matrix Multiplication, over a state-of-the-art CSR implementation, on a wide variety of matrices with different characteristics. SMASH incurs a very modest hardware area overhead of up to 0.076\% of an out-of-order CPU core.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {600–614},
numpages = {15},
keywords = {accelerators, compression, efficiency, graph processing, hardware-software cooperation, linear algebra, memory, sparse matrices, specialized architectures},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358318,
author = {Yan, Mingyu and Hu, Xing and Li, Shuangchen and Basak, Abanti and Li, Han and Ma, Xin and Akgun, Itir and Feng, Yujing and Gu, Peng and Deng, Lei and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
title = {Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358318},
doi = {10.1145/3352460.3358318},
abstract = {Graph analytics is an emerging application which extracts insights by processing large volumes of highly connected data, namely graphs. The parallel processing of graphs has been exploited at the algorithm level, which in turn incurs three irregularities onto computing and memory patterns that significantly hinder an efficient architecture design. Certain irregularities can be partially tackled by the prior domain-specific accelerator designs with well-designed scheduling of data access, while others remain unsolved.Unlike prior efforts, we fully alleviate these irregularities at their origin---the data-dependent program behavior. To achieve this goal, we propose GraphDynS, a hardware/software co-design with decoupled datapath and data-aware dynamic scheduling. Aware of data dependencies extracted from the decoupled datapath, GraphDynS can elaborately schedule the program on-the-fly to maximize parallelism. To extract data dependencies at runtime, we propose a new programming model in synergy with a microarchitecture design that supports datapath decoupling. Through data dependency information, we present several data-aware strategies to dynamically schedule workloads, data accesses, and computations. Overall, GraphDynS achieves 4.4\texttimes{} speedup and 11.6\texttimes{} less energy on average with half the memory bandwidth compared to a state-of-the-art GPGPU-based solution. Compared to a state-of-the-art graph analytics accelerator, GraphDynS also achieves 1.9\texttimes{} speedup and 1.8\texttimes{} less energy on average using the same memory bandwidth.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {615–628},
numpages = {14},
keywords = {accelerator, graph analytics, software and hardware co-design},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358259,
author = {Xu, Tiancheng and Tian, Boyuan and Zhu, Yuhao},
title = {Tigris: Architecture and Algorithms for 3D Perception in Point Clouds},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358259},
doi = {10.1145/3352460.3358259},
abstract = {Machine perception applications are increasingly moving toward manipulating and processing 3D point cloud. This paper focuses on point cloud registration, a key primitive of 3D data processing widely used in high-level tasks such as odometry, simultaneous localization and mapping, and 3D reconstruction. As these applications are routinely deployed in energy-constrained environments, real-time and energy-efficient point cloud registration is critical.We present Tigris, an algorithm-architecture co-designed system specialized for point cloud registration. Through an extensive exploration of the registration pipeline design space, we find that, while different design points make vastly different trade-offs between accuracy and performance, KD-tree search is a common performance bottleneck, and thus is an ideal candidate for architectural specialization. While KD-tree search is inherently sequential, we propose an acceleration-amenable data structure and search algorithm that exposes different forms of parallelism of KD-tree search in the context of point cloud registration. The co-designed accelerator systematically exploits the parallelism while incorporating a set of architectural techniques that further improve the accelerator efficiency. Overall, Tigris achieves 77.2\texttimes{} speedup and 7.4\texttimes{} power reduction in KD-tree search over an RTX 2080 Ti GPU, which translates to a 41.7\% registration performance improvements and 3.0\texttimes{} power reduction.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {629–642},
numpages = {14},
keywords = {Architecture-Algorithm Co-Design, KD-Tree, Nearest Neighbor Search, Perception, Point Cloud, Registration},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358253,
author = {Feng, Yu and Whatmough, Paul and Zhu, Yuhao},
title = {ASV: Accelerated Stereo Vision System},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358253},
doi = {10.1145/3352460.3358253},
abstract = {Estimating depth from stereo vision cameras, i.e., "depth from stereo", is critical to emerging intelligent applications deployed in energy- and performance-constrained devices, such as augmented reality headsets and mobile autonomous robots. While existing stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy.The key to ASV is to exploit unique characteristics inherent to stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly, we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high accuracy. The algorithm combines classic "hand-crafted" stereo algorithms with recent developments in Deep Neural Networks (DNNs), by leveraging the correspondence invariant unique to stereo vision systems. Secondly, we observe that the bottleneck of the ISM algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5\% hardware area overhead, these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall, ASV achieves 5\texttimes{} speedup and 85\% energy saving with 0.02\% accuracy loss compared to today's DNN-based stereo vision systems.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {643–656},
numpages = {14},
keywords = {DNN accelerator, Depth from stereo, Mobile computing, Stereo vision, constrained-optimization, data-flow, tiling},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358298,
author = {Zhang, Haibo and Zhao, Shulin and Pattnaik, Ashutosh and Kandemir, Mahmut T. and Sivasubramaniam, Anand and Das, Chita R.},
title = {Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358298},
doi = {10.1145/3352460.3358298},
abstract = {Video broadcast and streaming are among the most widely used applications for edge devices. Roughly 82\% of the mobile internet traffic is made up of video data. This is likely to worsen with the advent of 5G that will open up new opportunities for high resolution videos, virtual and augmented reality-based applications. The raw video data produced and consumed by edge devices is considerably higher than what is transmitted out of them. This leads to huge memory bandwidth and energy requirements from such edge devices. Therefore, optimizing the memory bandwidth and energy consumption needs is imperative for further improvements in energy efficiency of such edge devices. In this paper, we propose two mechanisms for on-the-fly compression and approximation of raw video data that is generated by the image sensors. The first mechanism, MidVB, performs lossless compression of the video frames coming out of the sensors and stores the compressed format into the memory. The second mechanism, Distill, builds on top of MidVB and further reduces memory consumption by approximating the video frame data. On an average, across 20 raw videos, MidVB and Distill are able to reduce the memory bandwidth by 43\% and 72\%, respectively, over the raw representation. They outperform a well known memory saving mechanism by 7\% and 36\%, respectively. Furthermore, MidVB and Distill reduce the energy consumption by 40\% and 67\%, respectively, over the baseline.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {657–669},
numpages = {13},
keywords = {Edge Computing, Memory, SoC, Video Streaming},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358277,
author = {Gobieski, Graham and Nagi, Amolak and Serafin, Nathan and Isgenc, Mehmet Meric and Beckmann, Nathan and Lucia, Brandon},
title = {MANIC: A Vector-Dataflow Architecture for Ultra-Low-Power Embedded Systems},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358277},
doi = {10.1145/3352460.3358277},
abstract = {Ultra-low-power sensor nodes enable many new applications and are becoming increasingly pervasive and important. Energy efficiency is the key determinant of the value of these devices: battery-powered nodes want their battery to last, and nodes that harvest energy should minimize their time spent recharging. Unfortunately, current devices are energy-inefficient.In this work, we present MANIC, a new, highly energy-efficient architecture targeting the ultra-low-power sensor domain. MANIC achieves high energy-efficiency while maintaining programmability and generality. MANIC introduces vector-dataflow execution, allowing it to exploit the dataflows in a sequence of vector instructions and amortize instruction fetch and decode over a whole vector of operations. By forwarding values from producers to consumers, MANIC avoids costly vector register file accesses. By carefully scheduling code and avoiding dead register writes, MANIC avoids costly vector register writes. Across seven benchmarks, MANIC is on average 2.8\texttimes{} more energy efficient than a scalar baseline, 38.1\% more energy-efficient than a vector baseline, and gets to within 26.4\% of an idealized design.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {670–684},
numpages = {15},
keywords = {Vector, dataflow, energy-harvesting, low-power, sensor},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358312,
author = {Donyanavard, Bryan and M\"{u}ck, Tiago and Rahmani, Amir M. and Dutt, Nikil and Sadighi, Armin and Maurer, Florian and Herkersdorf, Andreas},
title = {SOSA: Self-Optimizing Learning with Self-Adaptive Control for Hierarchical System-on-Chip Management},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358312},
doi = {10.1145/3352460.3358312},
abstract = {Resource management strategies for many-core systems dictate the sharing of resources among applications such as power, processing cores, and memory bandwidth in order to achieve system goals. System goals require consideration of both system constraints (e.g., power envelope) and user demands (e.g., response time, energy-efficiency). Existing approaches use heuristics, control theory, and machine learning for resource management. They all depend on static system models, requiring a priori knowledge of system dynamics, and are therefore too rigid to adapt to emerging workloads or changing system dynamics.We present SOSA, a cross-layer hardware/software hierarchical resource manager. Low-level controllers optimize knob configurations to meet potentially conflicting objectives (e.g., maximize throughput and minimize energy). SOSA accomplishes this for many-core systems and unpredictable dynamic workloads by using rule-based reinforcement learning to build subsystem models from scratch at runtime. SOSA employs a high-level supervisor to respond to changing system goals due to operating condition, e.g., switch from maximizing performance to minimizing power due to a thermal event. SOSA's supervisor translates the system goal into low-level objectives (e.g., core instructions-per-second (IPS)) in order to control subsystems by coordinating numerous knobs (e.g., core operating frequency, task distribution) towards achieving the goal. The software supervisor allows for flexibility, while the hardware learners allow quick and efficient optimization.We evaluate a simulation-based implementation of SOSA and demonstrate SOSA's ability to manage multiple interacting resources in the presence of conflicting objectives, its efficiency in configuring knobs, and adaptability in the face of unpredictable workloads. Executing a combination of machine-learning kernels and microbenchmarks on a multicore system-on-a-chip, SOSA achieves target performance with less than 1\% error starting with an untrained model, maintains the performance in the face of workload disturbance, and automatically adapts to changing constraints at runtime. We also demonstrate the resource manager with a hardware implementation on an FPGA.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {685–698},
numpages = {14},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358278,
author = {Alian, Mohammad and Kim, Nam Sung},
title = {NetDIMM: Low-Latency Near-Memory Network Interface Architecture},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358278},
doi = {10.1145/3352460.3358278},
abstract = {Optimizing bandwidth was the main focus of designing scale-out networks for several decades and this optimization trend has served well the traditional Internet applications. However, the emergence of datacenters as single computer entities has made latency as important as bandwidth in designing datacenter networks. PCIe interconnect is known to be latency bottleneck in communication networks as its latency overhead can contribute to up to ~90\% of the overall communication latency. Despite its overheads, PCIe is the de facto interconnect standard in servers as it has been well established and maintained for more than two decades. In addition to PCIe overhead, data movements in network software stack consume thousands of processor cycles and make ultra-low latency networking more challenging. Tackling PCIe and data movement overheads, we architect NetDIMM, a near-memory network interface card capable of in-memory buffer cloning. NetDIMM places a network interface card chip into the buffer device of a dual in-line memory module and leverages the asynchronous memory access capability of DDR5 to share the memory modules between the host processor and near-memory NIC. Our evaluation shows NetDIMM, on average, improves per packet latency by 49.9\% compared with a baseline network deploying PCIe NICs.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {699–711},
numpages = {13},
keywords = {near-memory computing, network architecture},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358256,
author = {Zhuo, Youwei and Wang, Chao and Zhang, Mingxing and Wang, Rui and Niu, Dimin and Wang, Yanzhi and Qian, Xuehai},
title = {GraphQ: Scalable PIM-Based Graph Processing},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358256},
doi = {10.1145/3352460.3358256},
abstract = {Processing-In-Memory (PIM) architectures based on recent technology advances (e.g., Hybrid Memory Cube) demonstrate great potential for graph processing. However, existing solutions did not address the key challenge of graph processing---irregular data movements.This paper proposes GraphQ, an improved PIM-based graph processing architecture over recent architecture Tesseract, that fundamentally eliminates irregular data movements. GraphQ is inspired by ideas from distributed graph processing and irregular applications to enable static and structured communication with runtime and architecture co-design. Specifically, GraphQ realizes: 1) batched and overlapped inter-cube communication by reordering vertex processing order; 2) streamlined inter-cube communication by using heterogeneous cores for different access types. Moreover, to tackle the discrepancy between inter-cube and inter-node bandwidth, we propose a hybrid execution model that performs additional local computation during the inter-node communication. This model is general enough and applicable to asynchronous iterative algorithms that can tolerate bounded stale values. Putting all together, GraphQ simultaneously maximizes intra-cube, inter-cube, and inter-node communication throughput. In a zSim-based simulator with five real-world graphs and four algorithms, GraphQ achieves on average 3.3\texttimes{} and maximum 13.9\texttimes{} speedup, 81\% energy saving compared with Tesseract. We show that increasing memory size in PIM also proportionally increases compute capability: a 4-node GraphQ achieves 98.34\texttimes{} speedup compared with a single node with the same memory size and conventional memory hierarchy.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {712–725},
numpages = {14},
keywords = {3D-stacked memory, data movement, graph analytics, memory systems, near-data processing, processing-in-memory},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358297,
author = {Jang, Jaeyoung and Heo, Jun and Lee, Yejin and Won, Jaeyeon and Kim, Seonghak and Jung, Sung Jun and Jang, Hakbeom and Ham, Tae Jun and Lee, Jae W.},
title = {Charon: Specialized Near-Memory Processing Architecture for Clearing Dead Objects in Memory},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358297},
doi = {10.1145/3352460.3358297},
abstract = {Garbage collection (GC) is a standard feature for high productivity programming, saving a programmer from many nasty memory-related bugs. However, these productivity benefits come with a cost in terms of application throughput, worst-case latency, and energy consumption. Since the first introduction of GC by the Lisp programming language in the 1950s, a myriad of hardware and software techniques have been proposed to reduce this cost. While the idea of accelerating GC in hardware is appealing, its impact has been very limited due to narrow coverage, lack of flexibility, intrusive system changes, and significant hardware cost. Even with specialized hardware GC performance is eventually limited by memory bandwidth bottleneck. Fortunately, emerging 3D stacked DRAM technologies shed new light on this decades-old problem by enabling efficient near-memory processing with ample memory bandwidth. Thus, we propose Charon1, the first 3D stacked memory-based GC accelerator. Through a detailed performance analysis of HotSpot JVM, we derive a set of key algorithmic primitives based on their GC time coverage and implementation complexity in hardware. Then we devise a specialized processing unit to substantially improve their memory-level parallelism and throughput with a low hardware cost. Our evaluation of Charon with the full-production HotSpot JVM running two big data analytics frameworks, Spark and GraphChi, demonstrates a 3.29\texttimes{} geomean speedup and 60.7\% energy savings for GC over the baseline 8-core out-of-order processor.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {726–739},
numpages = {14},
keywords = {Domain-specific architecture, Garbage collection, Java Virtual Machine, Memory management, Near-memory processing},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358284,
author = {Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
title = {TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358284},
doi = {10.1145/3352460.3358284},
abstract = {Recent studies from several hyperscalars pinpoint to embedding layers as the most memory-intensive deep learning (DL) algorithm being deployed in today's datacenters. This paper addresses the memory capacity and bandwidth challenges of embedding layers and the associated tensor operations. We present our vertically integrated hardware/software co-design, which includes a custom DIMM module enhanced with near-memory processing cores tailored for DL tensor operations. These custom DIMMs are populated inside a GPU-centric system interconnect as a remote memory pool, allowing GPUs to utilize for scalable memory bandwidth and capacity expansion. A prototype implementation of our proposal on real DL systems shows an average 6.2-17.6\texttimes{} performance improvement on state-of-the-art DNN-based recommender systems.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {740–753},
numpages = {14},
keywords = {DIMM, System architecture, graphics processing unit (GPU), machine learning, memory architecture, near-memory processing, neural network, neural processing unit (NPU)},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358252,
author = {Kwon, Hyoukjun and Chatarasi, Prasanth and Pellauer, Michael and Parashar, Angshuman and Sarkar, Vivek and Krishna, Tushar},
title = {Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358252},
doi = {10.1145/3352460.3358252},
abstract = {The data partitioning and scheduling strategies used by DNN accelerators to leverage reuse and perform staging are known as dataflow, which directly impacts the performance and energy efficiency of DNN accelerators. An accelerator micro architecture dictates the dataflow(s) that can be employed to execute layers in a DNN. Selecting a dataflow for a layer can have a large impact on utilization and energy efficiency, but there is a lack of understanding on the choices and consequences of dataflow, and of tools and methodologies to help architects explore the co-optimization design space.In this work, we first introduce a set of data-centric directives to concisely specify the DNN dataflow space in a compiler-friendly form. We then show how these directives can be analyzed to infer various forms of reuse and to exploit them using hardware capabilities. We codify this analysis into an analytical cost model, MAESTRO (Modeling Accelerator Efficiency via Patio-Temporal Reuse and Occupancy), that estimates various cost-benefit tradeoffs of a dataflow including execution time and energy efficiency for a DNN model and hardware configuration. We demonstrate the use of MAESTRO to drive a hardware design space exploration experiment, which searches across 480M designs to identify 2.5M valid designs at an average rate of 0.17M designs per second, including Pareto-optimal throughput- and energy-optimized design points.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {754–768},
numpages = {15},
keywords = {Cost modeling, Dataflow, Neural networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358258,
author = {Pentecost, Lillian and Donato, Marco and Reagen, Brandon and Gupta, Udit and Ma, Siming and Wei, Gu-Yeon and Brooks, David},
title = {MaxNVM: Maximizing DNN Storage Density and Inference Efficiency with Sparse Encoding and Error Mitigation},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358258},
doi = {10.1145/3352460.3358258},
abstract = {Deeply embedded applications require low-power, low-cost hardware that fits within stringent area constraints. Deep learning has many potential uses in these domains, but introduces significant inefficiencies stemming from off-chip DRAM accesses of model weights. Ideally, models would fit entirely on-chip. However, even with compression, memory requirements for state-of-the-art models make on-chip inference impractical. Due to increased density, emerging eNVMs are one promising solution.We present MaxNVM, a principled co-design of sparse encodings, protective logic, and fault-prone MLC eNVM technologies (i.e., RRAM and CTT) to enable highly-efficient DNN inference. We find bit reduction techniques (e.g., clustering and sparse compression) increase weight vulnerability to faults. This limits the capabilities of MLC eNVM. To circumvent this limitation, we improve storage density (i.e., bits-per-cell) with minimal overhead using protective logic. Tradeoffs between density and reliability result in a rich design space. We show that by balancing these techniques, the weights of large networks are able to reasonably fit on-chip. Compared to a naive, single-level-cell eNVM solution, our highly-optimized MLC memory systems reduce weight area by up to 29\texttimes{}. We compare our technique against NVDLA, a state-of-the-art industry-grade CNN accelerator, and demonstrate up to 3.2\texttimes{} reduced power and up to 3.5\texttimes{} reduced energy per ResNet50 inference.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {769–781},
numpages = {13},
keywords = {CTT, RRAM, eNVM, memory systems, neural networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358309,
author = {Silfa, Franyell and Dot, Gem and Arnau, Jose-Maria and Gonz\`{a}lez, Antonio},
title = {Neuron-Level Fuzzy Memoization in RNNs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358309},
doi = {10.1145/3352460.3358309},
abstract = {Recurrent Neural Networks (RNNs) are a key technology for applications such as automatic speech recognition or machine translation. Unlike conventional feed-forward DNNs, RNNs remember past information to improve the accuracy of future predictions and, therefore, they are very effective for sequence processing problems.For each application run, each recurrent layer is executed many times for processing a potentially large sequence of inputs (words, images, audio frames, etc.). In this paper, we make the observation that the output of a neuron exhibits small changes in consecutive invocations. We exploit this property to build a neuron-level fuzzy memoization scheme, which dynamically caches the output of each neuron and reuses it whenever it is predicted that the current output will be similar to a previously computed result, avoiding in this way the output computations.The main challenge in this scheme is determining whether the new neuron's output for the current input in the sequence will be similar to a recently computed result. To this end, we extend the recurrent layer with a much simpler Bitwise Neural Network (BNN), and show that the BNN and RNN outputs are highly correlated: if two BNN outputs are very similar, the corresponding outputs in the original RNN layer are likely to exhibit negligible changes. The BNN provides a low-cost and effective mechanism for deciding when fuzzy memoization can be applied with a small impact on accuracy.We evaluate our memoization scheme on top of a state-of-the-art accelerator for RNNs, for a variety of different neural networks from multiple application domains. We show that our technique avoids more than 24.2\% of computations, resulting in 18.5\% energy savings and 1.35x speedup on average.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {782–793},
numpages = {12},
keywords = {Binary Networks, Long Short Term Memory, Memoization, Recurrent Neural Networks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358304,
author = {Stevens, Jacob R. and Ranjan, Ashish and Das, Dipankar and Kaul, Bharat and Raghunathan, Anand},
title = {Manna: An Accelerator for Memory-Augmented Neural Networks},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358304},
doi = {10.1145/3352460.3358304},
abstract = {Memory-augmented neural networks (MANNs)-- which augment a traditional Deep Neural Network (DNN) with an external, differentiable memory-- are emerging as a promising direction in machine learning. MANNs have been shown to achieve one-shot learning and complex cognitive capabilities that are well beyond those of classical DNNs. We analyze the computational characteristics of MANNs and observe that they present a unique challenge due to soft reads and writes to the differentiable memory, each of which requires access to all the memory locations. This results in poor performance of MANNs on modern CPUs, GPUs, and other accelerators. To address this, we present Manna, a specialized hardware inference accelerator for MANNs. Manna is a memory-centric design that focuses on maximizing performance in an extremely low FLOPS/Byte context. The key architectural features from which Manna derives efficiency are: (i) investing most of the die area and power in highly banked on-chip memories that provide ample bandwidth rather than large matrix-multiply units that would be underutilized due to the low reuse (ii) a hardware-assisted transpose mechanism for accommodating the diverse memory access patterns observed in MANNs, (iii) a specialized processing tile that is equipped to handle the nearly-equal mix of MAC and non-MAC computations present in MANNs, and (iv) methods to map MANNs to Manna that minimize data movement while fully exploiting the little reuse present. We evaluate Manna by developing a detailed architectural simulator with timing and power models calibrated by synthesis to the 15 nm Nangate Open Cell library. Across a suite of 10 benchmarks, Manna demonstrates average speedups of 39x with average energy improvements of 122x over an NVIDIA 1080-Ti Pascal GPU and average speedups of 24x with average energy improvements of 86x over a state-of-the-art NVIDIA 2080-Ti Turing GPU.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {794–806},
numpages = {13},
keywords = {Hardware Accelerators, Memory Networks, Memory-Augmented Neural Networks, System Architecture},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358262,
author = {Liu, Xiao and Roberts, David and Ausavarungnirun, Rachata and Mutlu, Onur and Zhao, Jishen},
title = {Binary Star: Coordinated Reliability in Heterogeneous Memory Systems for High Performance and Scalability},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358262},
doi = {10.1145/3352460.3358262},
abstract = {As memory capacity scales, traditional cache and memory hierarchy designs are facing increasingly difficult challenges in ensuring high reliability with low storage and performance cost. Recent developments in 3D die-stacked DRAM caches and nonvolatile memories (NVRAMs) introduce promising opportunities in tackling the reliability, performance, and capacity challenges, due to the diverse reliability characteristics of the technologies. However, simply replacing DRAM with NVRAM does not solve the reliability issues of the memory system, as conventional memory system designs maintain separate reliability schemes across caches and main memory. Our goal in this paper is to enable a reliable and high-performance memory hierarchy design, as memory capacity scales. To this end, we propose Binary Star, which coordinates the reliability schemes and consistent cache writeback between 3D-stacked DRAM last-level cache and NVRAM main memory to maintain the reliability of the cache and the memory hierarchy. Binary Star significantly reduces the performance and storage overhead of consistent cache writeback by coordinating it with NVRAM wear leveling. As a result, Binary Star is much more reliable and offers better performance than state-of-the-art memory systems with error correction. On a set of memory-intensive workloads, we show that Binary Star reduces memory failures in time (FIT) by 92.9\% compared to state-of-the-art error correction schemes, while retaining 99\% of the performance of a conventional DRAM design that provides no error correction.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {807–820},
numpages = {14},
keywords = {hybrid memory, me, nonvolatile memory, reliability},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358267,
author = {Panwar, Gagandeep and Zhang, Da and Pang, Yihan and Dahshan, Mai and DeBardeleben, Nathan and Ravindran, Binoy and Jian, Xun},
title = {Quantifying Memory Underutilization in HPC Systems and Using it to Improve Performance via Architecture Support},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358267},
doi = {10.1145/3352460.3358267},
abstract = {A system's memory size is often dictated by worst-case workloads with highest memory requirements; this causes memory to be underutilized in the common case when the system is not running its worst-case workloads. Cognizant of this memory underutilization problem, many prior works have studied memory utilization and explored how to improve it in the context of cloud.In this paper, we perform the first large-scale study of system-level memory utilization in the context of HPC systems; through seven million machine-hours of measurements across four HPC systems, we find memory underutilization in HPC systems is much more severe than in cloud. Subsequently, we also perform the first exploration of architectural techniques to improve memory utilization specifically for HPC systems. We propose exposing each compute node's currently unused memory to its CPU(s) via novel architectural support for OS. This can enable many new microarchitecture techniques that use the abundant free memory to boost microarchitecture performance transparently without requiring any user code modification or recompilation; we refer to them as Free-memory-aware Microarchitecture Techniques (FMTs). We then present a detailed example of an FMT -- Free-memory-aware Memory Replication (FMR). On average across five HPC benchmark suites, FMR provides 13\% performance and 8\% system-level energy improvement compared to a highly optimized baseline representative of modern memory systems. To check the performance benefits our simulation reports, we emulated FMR in a real system and found close corroboration between simulation results and real-system emulation results. The paper ends by discussing other possible FMTs and applicability to other types of systems.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {821–835},
numpages = {15},
keywords = {DRAM, HPC Systems, Memory Architecture, Memory Management, Operating Systems, Supercomputing},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358326,
author = {Ni, Yuanjiang and Zhao, Jishen and Litz, Heiner and Bittman, Daniel and Miller, Ethan L.},
title = {SSP: Eliminating Redundant Writes in Failure-Atomic NVRAMs via Shadow Sub-Paging},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358326},
doi = {10.1145/3352460.3358326},
abstract = {Non-Volatile Random Access Memory (NVRAM) technologies are closing the performance gap between traditional storage and memory. However, the integrity of persistent data structures after an unclean shutdown remains a major concern. Logging is commonly used to ensure consistency of NVRAM systems, but it imposes significant performance overhead and causes additional wear out by writing extra data into NVRAM. Our goal is to eliminate the extra writes that are needed to achieve consistency. SSP (i) exploits a novel cache-line-level remapping mechanism to eliminate redundant data copies in NVRAM, (ii) minimizes the storage overheads using page consolidation and (iii) removes failure-atomicity overheads from the critical path, significantly improving the performance of NVRAM systems. Our evaluation results demonstrate that SSP reduces overall write traffic by up to 1.8\texttimes{}, reduces extra NVRAM writes in the critical path by up to 10\texttimes{} and improves transaction throughput by up to 1.6\texttimes{}, compared to a state-of-the-art logging design.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {836–848},
numpages = {13},
keywords = {NVRAM, failure-atomicity, shadow sub-paging},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358266,
author = {Chen, Renhai and Shao, Zili and Liu, Duo and Feng, Zhiyong and Li, Tao},
title = {Towards Efficient NVDIMM-based Heterogeneous Storage Hierarchy Management for Big Data Workloads},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358266},
doi = {10.1145/3352460.3358266},
abstract = {In this paper, we propose a holistic solution to address several important and challenging issues in storage data management in light of emerging NVDIMM-based architecture: namely, new performance modeling, NVDIMM-based migration, and architectural support for NVDIMMs on migration optimization. In particular, a novel NVDIMM-based heterogeneous storage performance model is proposed to effectively address bus contention issues caused by placing NVDIMMs on the memory bus. We also develop an NVDIMM-based lazy migration scheme to effectively minimize adverse effects caused by memory traffic interferences during storage data management processes. Finally, the NVDIMM-based architectural support for migration optimization is proposed to increase channel parallelism in the destination NVDIMMs and bypass buffer caches in the source NVDIMMs, so that the impact of memory traffic can be alleviated. We present detailed evaluation and analysis to quantify how well our techniques can enhance the I/O performances of big workloads via efficient heterogeneous storage hierarchy management. Our experimental results show that overall the proposed techniques yield up to 98\% performance improvement over the state-of-the-art techniques.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {849–860},
numpages = {12},
keywords = {NVDIMM, bus contention, heterogeneous storage, machine learning},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358271,
author = {Morais, Lucas and Silva, Vitor and Goldman, Alfredo and Alvarez, Carlos and Bosch, Jaume and Frank, Michael and Araujo, Guido},
title = {Adding Tightly-Integrated Task Scheduling Acceleration to a RISC-V Multi-core Processor},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358271},
doi = {10.1145/3352460.3358271},
abstract = {Task Parallelism is a parallel programming model that provides code annotation constructs to outline tasks and describe how their pointer parameters are accessed so that they might be executed in parallel, and asynchronously, by a runtime capable of inferring and honoring their data dependence relationships. It is supported by several parallelization frameworks, as OpenMP and StarSs.Overhead related to automatic dependence inference and to the scheduling of ready-to-run tasks is a major performance limiting factor of Task Parallel systems. To amortize this overhead, programmers usually trade the higher parallelism that could be leveraged from finer-grained work partitions for the higher runtime-efficiency of coarser-grained work partitions. Such problems are even more severe for systems with many cores, as the task spawning frequency required for preserving cores from starvation grows linearly with their number.To mitigate these problems, researchers have designed hardware accelerators to improve runtime performance. Nevertheless, the high CPU-accelerator communication overheads of these solutions hampered their gains.We thus propose a RISC-V based architecture that minimizes communication overhead between the HW Task Scheduler and the CPU by allowing Task Scheduling software to directly interact with the former through custom instructions. Empirical evaluation of the architecture is made possible by an FPGA prototype featuring an eight-core Linux-capable Rocket Chip implementing such instructions.To evaluate the prototype performance, we both (1) adapted Nanos, a mature Task Scheduling runtime, to benefit from the new task-scheduling-accelerating instructions; and (2) developed Phentos, a new HW-accelerated light weight Task Scheduling runtime. Our experiments show that task parallel programs using Nanos-RV --- the Nanos version ported to our system --- are on average 2.13 times faster than those being serviced by baseline Nanos, while programs running on Phentos are 13.19 times faster, considering geometric means. Using eight cores, Nanos-RV is able to deliver speedups with respect to serial execution of up to 5.62 times, while Phentos produces speedups of up to 5.72 times.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {861–872},
numpages = {12},
keywords = {Chisel, Picos, RISC-V, RoCC Interface, Rocket Chip, Task Scheduling},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358255,
author = {Parasar, Mayank and Jerger, Natalie Enright and Gratz, Paul V. and Miguel, Joshua San and Krishna, Tushar},
title = {SWAP: Synchronized Weaving of Adjacent Packets for Network Deadlock Resolution},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358255},
doi = {10.1145/3352460.3358255},
abstract = {An interconnection network forms the communication backbone in both on-chip and off-chip systems. In networks, congestion causes packets to be blocked. Indefinite blocking can occur if cyclic dependencies exist, leading to deadlock. All modern networks devote resources to either avoid deadlock by eliminating cyclic dependences or to detect and recover from it.Conventional buffered flow control does not allow a blocked packet to move forward unless the buffer at the next hop is guaranteed to be free. We introduce SWAP, a novel mechanism for enabling a blocked packet to perform an in-place swap with a buffered packet at the next hop. We prove that in-place swaps are sufficient to break any deadlock and are agnostic to the underlying topology or routing algorithm. This makes SWAP applicable across homogeneous or heterogeneous on-chip and off-chip topologies. We present a lightweight implementation of SWAP that reuses conventional router resources with minor additions to enable these swaps. The additional path diversity provided by SWAP provides 20-80\% higher throughput with synthetic traffic patterns across regular and irregular topologies compared to baseline escape VC based solutions, and consumes 2-8\texttimes{} lower network energy compared to deflection and global-synchronization based solutions.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {873–885},
numpages = {13},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358317,
author = {Zhou, Diyu and Tamir, Yuval},
title = {PUSh: Data Race Detection Based on Hardware-Supported Prevention of Unintended Sharing},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358317},
doi = {10.1145/3352460.3358317},
abstract = {Some of the most difficult to find bugs in multi-threaded programs are caused by unintended sharing, leading to data races. The detection of data races can be facilitated by requiring programmers to explicitly specify any intended sharing and then verifying compliance with these intentions. We present a novel dynamic checker based on this approach, called PUSh.PUSh prevents sharing of global objects, unless the program explicitly specifies sharing policies that permit it. The policies are enforced using off-the-shelf hardware mechanisms. Specifically, PUSh uses the conventional MMU and includes a key performance optimization that exploits memory protection keys (MPKs), recently added to the x86 ISA. Objects' sharing policies can be changed dynamically. If these changes are unordered, unordered accesses to shared objects may not be detected. PUSh uses happens-before tracking of the policy changes to verify that they are ordered.We have implemented PUSh for C programs and evaluated it using ten benchmarks with up to 32 threads. PUSh's memory overhead was under 2.4\% for eight of the benchmarks; 127\% and 260\% overhead for the remaining two. PUSh's performance overhead exceeded 18\% for only three of the benchmarks, reaching 99\% overhead in the worst case.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {886–898},
numpages = {13},
keywords = {concurrency, data race detection, memory protection keys},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358289,
author = {Gorman, Daphne I. and Possignolo, Rafael Trapani and Renau, Jose},
title = {EMI Architectural Model and Core Hopping},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358289},
doi = {10.1145/3352460.3358289},
abstract = {Processors radiate electromagnetic interference (EMI), which affects wireless communication technologies. However, despite the fact that the EMI generated by a processor is deterministic, architecturally modeling the EMI has proven to be a complex challenge. Moreover, EMI depends on the physical layout of the processor and on the binary being executed (both the application and its compilation options).This paper proposes Model for EMI on a SoC (MESC), the first architectural framework for modeling electromagnetic emissions from a core. MESC takes into account the layout and the switching activity of a process to model the expected EMI. We validate MESC on a real system to verify its accuracy. We then use MESC to demonstrate that two different core layouts can be leveraged to reduce EMI and propose EMI Core Hopper (EMI CHopper). EMI CHopper uses a multi-core system -- where each core has the same RTL but minimally different layouts -- and proposes hopping the application between cores to reduce in-band EMI when it interferes with wireless communication.Our evaluation shows that MESC is able to predict EMI within 95\% accuracy across time and across the frequency spectrum, even when using statistical sampling to obtain activity rates. Leveraging MESC, our proposed EMI CHopper reduces in-band EMI by up to 50\%, with low impact on performance. MESC will enable a new stream of micro-architectural research the same way architectural level power models have enabled exploration of performance and power simulation.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {899–910},
numpages = {12},
keywords = {Dynamic EMI reduction, EMI Model, Electromagnetic interference, System Simulation, Thread Migration},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358270,
author = {Li, Zhaoshi and Liu, Leibo and Deng, Yangdong and Wang, Jiawei and Liu, Zhiwei and Yin, Shouyi and Wei, Shaojun},
title = {FPGA-Accelerated Optimistic Concurrency Control for Transactional Memory},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358270},
doi = {10.1145/3352460.3358270},
abstract = {Transactional Memory (TM) has been considered as a promising alternative to existing synchronization operations, which are often the largest stumbling block to unleashing parallelism of applications. Efficient implementations of TM, however, are challenging due to the tension between lowering performance overhead and avoiding unnecessary aborts.In this paper, we present Reachability-based Optimistic Concurrency Control for Transactional Memory (ROCoCoTM), a novel scheme which offloads concurrency control (CC) algorithms, the central building blocks of TM systems, to reconfigurable hardware. To reduce the abort rate, an innovative formalization of mainstream CC algorithms is developed to reveal a common restriction that leads to unnecessary aborts. This restriction is resolved by the ROCoCo algorithm with a centralized validation phase, which can be efficiently pipelined in hardware. Thanks to a high-performance offloading engine implemented in reconfigurable hardware, ROCoCo algorithm results in decreased abort rates and reduced performance overhead. The whole system is implemented on Intel's HARP2 platform and evaluated with the STAMP benchmark suite. Experiments show 1.55x and 8.05x geomean speedup over TinySTM and an HTM based on Intel TSX, respectively. Given the fast-growing deployment of commodity CPU-FPGA platforms, ROCoCoTM paves the way for software programmers to exploit heterogeneous computing resources with a high-level transactional abstraction to effectively extract the parallelism in modern applications.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {911–923},
numpages = {13},
keywords = {FPGA, Hardware Accelerator, Transactional Memory},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358276,
author = {Dadu, Vidushi and Weng, Jian and Liu, Sihao and Nowatzki, Tony},
title = {Towards General Purpose Acceleration by Exploiting Common Data-Dependence Forms},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358276},
doi = {10.1145/3352460.3358276},
abstract = {With slowing technology scaling, specialized accelerators are increasingly attractive solutions to continue expected generational scaling of performance. However, in order to accelerate more advanced algorithms or those from challenging domains, supporting data-dependence becomes necessary. This manifests as either data-dependent control (eg. join two sparse lists), or data-dependent memory accesses (eg. hash-table access). These forms of data-dependence inherently couple compute with memory, and also preclude efficient vectorization -- defeating the traditional mechanisms of programmable accelerators (eg. GPUs).Our goal is to develop an accelerator which is broadly applicable across algorithms with and without data-dependence. To this end, we first identify forms of data-dependence which are both common and possible to exploit with specialized hardware: specifically stream-join and alias-free indirection. Then, we create an accelerator with an interface to support these, called the Sparse Processing Unit (SPU). SPU supports alias-free indirection with a compute-enabled scratchpad and aggressive stream reordering and stream-join with a novel dataflow control model for a reconfigurable systolic compute-fabric. Finally, we add robustness across datatypes by adding decomposability across the compute and memory pipelines. SPU achieves 16.5\texttimes{}, 10.3\texttimes{}, and 14.2\texttimes{} over a 24-core SKL CPU on ML, database, and graph algorithms respectively. SPU achieves similar performance to domain-specific accelerators. For ML, SPU achieves 1.8-7\texttimes{} speedup against a similarly provisioned GPGPU, with much less area and power.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {924–939},
numpages = {16},
keywords = {Irregularity, accelerators, data-dependence, dataflow, generality, indirection, join, reconfigurable, systolic},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358292,
author = {Sharifian, Amirali and Hojabr, Reza and Rahimi, Navid and Liu, Sihao and Guha, Apala and Nowatzki, Tony and Shriraman, Arrvindh},
title = {μIR -An intermediate representation for transforming and optimizing the microarchitecture of application accelerators},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358292},
doi = {10.1145/3352460.3358292},
abstract = {Creating high quality application-specific accelerators requires us to make iterative changes to both algorithm behavior and microarchitecture, and this is a tedious and error-prone process. High-Level Synthesis (HLS) tools [5, 10] generate RTL for application accelerators from annotated software. Unfortunately, the generated RTL is challenging to change and optimize. The primary limitation of HLS is that the functionality and microarchitecture are conflated together in a single language (such as C++). Making changes to the accelerator design may require code restructuring, and microarchitecture optimizations are tied with program correctness.We propose a generalized intermediate representation for describing accelerator microarchitecture, μIR, and an associated pass framework, μopt. μIR represents the accelerator as a concurrent structural graph in which the components roughly correspond to microarchitecture level hardware blocks (e.g., function units, network, memory banks). There are two important benefits i) it decouples microarchitecture optimizations from algorithm/program optimizations. ii) it decouples microarchitecture optimizations from the RTL generation. Computer architects express their ideas as a set of iterative transformations of the μIR graph that successively refine the accelerator architecture. The μIR graph is then translated to Chisel, while maintaining the execution model and cycle-level performance characteristics. In this paper, we study three broad classes of optimizations: Timing (e.g., Pipeline re-timing), Spatial (e.g., Compute tiling), and Higher-order Ops (e.g., Tensor function units) that deliver between 1.5 --- 8\texttimes{} improvement in performance; overall 5---20\texttimes{} speedup compared to an ARM A9 1Ghz. We evaluate the quality of the autogenerated accelerators on an Arria 10 FPGA and under ASIC UMC 28nm technology.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {940–953},
numpages = {14},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358274,
author = {Yu, Jiyong and Yan, Mengjia and Khyzha, Artem and Morrison, Adam and Torrellas, Josep and Fletcher, Christopher W.},
title = {Speculative Taint Tracking (STT): A Comprehensive Protection for Speculatively Accessed Data},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358274},
doi = {10.1145/3352460.3358274},
abstract = {Speculative execution attacks present an enormous security threat, capable of reading arbitrary program data under malicious speculation, and later exfiltrating that data over microarchitectural covert channels. Since these attacks first rely on being able to read arbitrary data (potential secrets), a conservative approach to defeat all attacks is to delay the execution of instructions that read those secrets, until those instructions become non-speculative.This paper's premise is that it is safe to execute and selectively forward the results of speculative instructions that read secrets, which improves performance, as long as we can prove that the forwarded results do not reach potential covert channels. We propose a comprehensive hardware protection based on this idea, called Speculative Taint Tracking (STT), capable of protecting all speculatively accessed data.Our work addresses two key challenges. First, to safely selectively forward secrets, we must understand what instruction(s) can form covert channels. We provide a comprehensive study of covert channels on speculative microarchitectures, and use this study to develop hardware mechanisms that block each class of channel. Along the way, we find new classes of covert channels related to implicit flow on speculative machines. Second, for performance, it is essential to disable protection on previously protected data, as soon as doing so is safe. We identify that the earliest time is when the instruction(s) producing the protected data become non-speculative, and design a novel microarchitecture for disabling protection at this moment.We provide an extensive formal analysis showing that STT enforces a novel form of non-interference, with respect to all speculatively accessed data. We further evaluate STT on 21 SPEC and 9 PARSEC workloads, and find it adds only 8.5\%/14.5\% overhead (depending on attack model) relative to an insecure machine, while reducing overhead by 4.7\texttimes{}/18.8\texttimes{} relative to a baseline secure scheme.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {954–968},
numpages = {15},
keywords = {Hardware, Information flow, Security, Speculative execution attacks},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358327,
author = {Townley, Daniel and Khasawneh, Khaled N. and Ponomarev, Dmitry and Abu-Ghazaleh, Nael and Yu, Lei},
title = {LATCH: A Locality-Aware Taint CHecker},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358327},
doi = {10.1145/3352460.3358327},
abstract = {We present LATCH (short for Locality-Aware Taint CHecker), a generalizable architecture for optimizing dynamic information flow tracking (DIFT). LATCH exploits the observation that information flows under DIFT exhibit strong temporal locality, with typical applications manipulating sensitive data during limited phases of computation. This property allows LATCH to monitor significant spans of execution using lightweight, coarse-grained checks, invoking precise, computationally intensive tracking logic only during periods of execution that involve sensitive data. LATCH implements this policy without sacrificing the accuracy of DIFT.We propose and evaluate three systems incorporating LATCH: S-LATCH to accelerate software-based DIFT on a single core; P-LATCH to accelerate multicore software-based DIFT, and H-LATCH to reduce the architectural complexity of hardware-based DIFT. We developed an FPGA prototype of the LATCH system, demonstrating that its advantages come with negligible impact on power and complexity and no effect on processor cycle time.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {969–982},
numpages = {14},
keywords = {dynamic information flow tracking, security in hardware},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358261,
author = {Sehatbakhsh, Nader and Nazari, Alireza and Khan, Haider and Zajic, Alenka and Prvulovic, Milos},
title = {EMMA: Hardware/Software Attestation Framework for Embedded Systems Using Electromagnetic Signals},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358261},
doi = {10.1145/3352460.3358261},
abstract = {Establishing trust for an execution environment is an important problem, and practical solutions for it rely on attestation, where an untrusted system (prover) computes a response to a challenge sent by the trusted system (verifier). The response typically is a checksum of the prover's program, which the verifier checks against expected values for a "clean" (trustworthy) system. The main challenge in attestation is that, in addition to checking the response, the verifier also needs to verify the integrity of the response computation. On higher-end processors, this integrity is verified cryptographically, using dedicated trusted hardware. On embedded systems, however, constraints prevent the use of such hardware support. Instead, a popular approach is to use the request-to-response time as a way to establish confidence. However, the overall request-to-response time provides only one coarse-grained measurement from which the integrity of the attestation is to be inferred, and even that is noisy because it includes the network latency and/or variations due to micro-architectural events. Thus, the attestation is vulnerable to attacks where the adversary has tampered with response computation, but the resulting additional computation time is small relative to the overall request-to-response time.In this paper, we make a key observation that execution-time measurement is only one example of using externally measurable side-channel information, and that other side-channels, some of which can provide much finer-grain information about the computation, can be used. As a proof of concept, we propose EMMA, a novel method for attestation that leverages electromagnetic side-channel signals that are emanated by the system during response computation, to confirm that the device has, upon receiving the challenge, actually computed the response using the valid program code for that computation. This new approach requires physical proximity, but imposes no overhead to the system, and provides accurate monitoring during the attestation. We implement EMMA on a popular embedded system, Arduino UNO, and evaluate our system with a wide range of attacks on attestation integrity. Our results show that EMMA can successfully detect these attacks with high accuracy. We compare our method with the existing methods and show how EMMA outperforms them in terms of security guarantees, scalability, and robustness.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {983–995},
numpages = {13},
keywords = {electromagnetic emanations, embedded systems, hardware security, side-channels, trusted execution environment},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358300,
author = {Wu, Hao and Nathella, Krishnendra and Pusdesris, Joseph and Sunwoo, Dam and Jain, Akanksha and Lin, Calvin},
title = {Temporal Prefetching Without the Off-Chip Metadata},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358300},
doi = {10.1145/3352460.3358300},
abstract = {Temporal prefetching offers great potential, but this potential is difficult to achieve because of the need to store large amounts of prefetcher metadata off chip. To reduce the latency and traffic of off-chip metadata accesses, recent advances in temporal prefetching have proposed increasingly complex mechanisms that cache and prefetch this off-chip metadata. This paper suggests a return to simplicity: We present a temporal prefetcher whose metadata resides entirely on chip. The key insights are (1) only a small portion of prefetcher metadata is important, and (2) for most workloads with irregular accesses, the benefits of an effective prefetcher outweigh the marginal benefits of a larger data cache. Thus, our solution, the Triage prefetcher, identifies important metadata and uses a portion of the LLC to store this metadata, and it dynamically partitions the LLC between data and metadata.Our empirical results show that when compared against spatial prefetchers that use only on-chip metadata, Triage performs well, achieving speedups on irregular subset of SPEC2006 of 23.5\% compared to 5.8\% for the previous state-of-the-art. When compared against state-of-the-art temporal prefetchers that use off-chip metadata, Triage sacrifices performance on single-core systems (23.5\% speedup vs. 34.7\% speedup), but its 62\% lower traffic overhead translates to better performance in bandwidth-constrained 16-core systems (6.2\% speedup vs. 4.3\% speedup).},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {996–1008},
numpages = {13},
keywords = {CPUs, Data prefetching, caches, irregular temporal prefetching},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358254,
author = {Mukkara, Anurag and Beckmann, Nathan and Sanchez, Daniel},
title = {PHI: Architectural Support for Synchronization- and Bandwidth-Efficient Commutative Scatter Updates},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358254},
doi = {10.1145/3352460.3358254},
abstract = {Many applications perform frequent scatter update operations to large data structures. For example, in push-style graph algorithms, processing each vertex requires updating the data of all its neighbors. Neighbors are often scattered over the whole graph, so these scatter updates have poor spatial and temporal locality. In current systems, scatter updates suffer high synchronization costs and high memory traffic. These drawbacks make push-style execution unattractive, and, when algorithms allow it, programmers gravitate towards pull-style implementations based on gather reads instead.We present PHI, a push cache hierarchy that makes scatter updates synchronization- and bandwidth-efficient. PHI adds support for pushing sparse, commutative updates from cores towards main memory. PHI adds simple compute logic at each cache level to buffer and coalesce these commutative updates throughout the hierarchy. This avoids synchronization, exploits temporal locality, and produces a load-balanced execution. Moreover, PHI exploits spatial locality by selectively deferring updates with poor spatial locality, batching them to achieve sequential main memory transfers.PHI is the first system to leverage both the temporal and spatial locality benefits of commutative scatter updates, some of which do not apply to gather reads. As a result, PHI not only makes push algorithms efficient, but makes them consistently faster than pull ones. We evaluate PHI on graph algorithms and other sparse applications processing large inputs. PHI improves performance by 4.7\texttimes{} on average (and by up to 11\texttimes{}), and reduces memory traffic by 2\texttimes{} (and by up to 5\texttimes{}).},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1009–1022},
numpages = {14},
keywords = {caches, graph analytics, locality, multicore, specialization},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358294,
author = {Margaritov, Artemiy and Ustiugov, Dmitrii and Bugnion, Edouard and Grot, Boris},
title = {Prefetched Address Translation},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358294},
doi = {10.1145/3352460.3358294},
abstract = {With explosive growth in dataset sizes and increasing machine memory capacities, per-application memory footprints are commonly reaching into hundreds of GBs. Such huge datasets pressure the TLB, resulting in frequent misses that must be resolved through a page walk -- a long-latency pointer chase through multiple levels of the in-memory radix tree-based page table.Anticipating further growth in dataset sizes and their adverse affect on TLB hit rates, this work seeks to accelerate page walks while fully preserving existing virtual memory abstractions and mechanisms -- a must for software compatibility and generality. Our idea is to enable direct indexing into a given level of the page table, thus eliding the need to first fetch pointers from the preceding levels. A key contribution of our work is in showing that this can be done by simply ordering the pages containing the page table in physical memory to match the order of the virtual memory pages they map to. Doing so enables direct indexing into the page table using a base-plus-offset arithmetic.We introduce Address Translation with Prefetching (ASAP), a new approach for reducing the latency of address translation to a single access to the memory hierarchy. Upon a TLB miss, ASAP launches prefetches to the deeper levels of the page table, bypassing the preceding levels. These prefetches happen concurrently with a conventional page walk, which observes a latency reduction due to prefetching while guaranteeing that only correctly-predicted entries are consumed. ASAP requires minimal extensions to the OS and trivial microarchitectural support. Moreover, ASAP is fully legacy-preserving, requiring no modifications to the existing radix tree-based page table, TLBs and other software and hardware mechanisms for address translation. Our evaluation on a range of memory-intensive workloads shows that under SMT colocation, ASAP is able to reduce page walk latency by an average of 25\% (42\% max) in native execution, and 45\% (55\% max) under virtualization.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1023–1036},
numpages = {14},
keywords = {microarchitecture, virtual memory, virtualization},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358264,
author = {Nikoleris, Nikos and Eeckhout, Lieven and Hagersten, Erik and Carlson, Trevor E.},
title = {Directed Statistical Warming through Time Traveling},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358264},
doi = {10.1145/3352460.3358264},
abstract = {Improving the speed of computer architecture evaluation is of paramount importance to shorten the time-to-market when developing new platforms. Sampling is a widely used methodology to speed up workload analysis and performance evaluation by extrapolating from a set of representative detailed regions. Installing an accurate cache state for each detailed region is critical to achieving high accuracy. Prior work requires either huge amounts of storage (checkpoint-based warming), an excessive number of memory accesses to warm up the cache (functional warming), or the collection of a large number of reuse distances (randomized statistical warming) to accurately predict cache warm-up effects.This work proposes DeLorean, a novel statistical warming and sampling methodology that builds upon two key contributions: directed statistical warming and time traveling. Instead of collecting a large number of randomly selected reuse distances as in randomized statistical warming, directed statistical warming collects a select number of key reuse distances, i.e., the most recent reuse distance for each unique memory location referenced in the detailed region. Time traveling leverages virtualized fast-forwarding to quickly 'look into the future' --- to determine the key cachelines --- and then 'go back in time' --- to collect the reuse distances for those key cachelines at near-native hardware speed through virtualized directed profiling.Directed statistical warming reduces the number of warm-up references by 30\texttimes{} compared to randomized statistical warming. Time traveling translates this reduction into a 5.7\texttimes{} simulation speedup. In addition to improving simulation speed, DeLorean reduces the prediction error from around 9\% to around 3\% on average. We further demonstrate how to amortize warm-up cost across multiple parallel simulations in design space exploration studies. Implementing DeLorean in gem5 enables detailed cycle-accurate simulation at a speed of 126 MIPS.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1037–1049},
numpages = {13},
keywords = {architectural simulation, cache warming, performance analysis, sampled simulation, statistical cache modeling},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358322,
author = {Kim, Donggyu and Zhao, Jerry and Bachrach, Jonathan and Asanovi\'{c}, Krste},
title = {Simmani: Runtime Power Modeling for Arbitrary RTL with Automatic Signal Selection},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358322},
doi = {10.1145/3352460.3358322},
abstract = {This paper presents a novel runtime power modeling methodology which automatically identifies key signals for power dissipation of any RTL design. The toggle-pattern matrix is constructed with the VCD dumps from a training set, where each signal is represented as a high-dimensional point. By clustering signals showing similar switching activities, a small number of signals are automatically selected, and then the design-specific but workload-independent activity-based power model is constructed using regression against cycle-accurate power traces obtained from industry-standard CAD tools. We can also automatically instrument an FPGA-accelerated RTL simulation with runtime activity counters to obtain power traces of realistic workloads at speed. Our methodology is demonstrated with a heterogeneous processor composed of an in-order core and a custom vector accelerator, running not only microbenchmarks but also real-world machine-learning applications.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1050–1062},
numpages = {13},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3358296,
author = {Shahrad, Mohammad and Balkind, Jonathan and Wentzlaff, David},
title = {Architectural Implications of Function-as-a-Service Computing},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358296},
doi = {10.1145/3352460.3358296},
abstract = {Serverless computing is a rapidly growing cloud application model, popularized by Amazon's Lambda platform. Serverless cloud services provide fine-grained provisioning of resources, which scale automatically with user demand. Function-as-a-Service (FaaS) applications follow this serverless model, with the developer providing their application as a set of functions which are executed in response to a user- or system-generated event. Functions are designed to be short-lived and execute inside containers or virtual machines, introducing a range of system-level overheads. This paper studies the architectural implications of this emerging paradigm. Using the commercial-grade Apache OpenWhisk FaaS platform on real servers, this work investigates and identifies the architectural implications of FaaS serverless computing. The workloads, along with the way that FaaS inherently interleaves short functions from many tenants frustrates many of the locality-preserving architectural structures common in modern processors. In particular, we find that: FaaS containerization brings up to 20x slowdown compared to native execution, cold-start can be over 10x a short function's execution time, branch mispredictions per kilo-instruction are 20x higher for short functions, memory bandwidth increases by 6x due to the invocation pattern, and IPC decreases by as much as 35\% due to inter-function interference. We open-source FaaSProfiler, the FaaS testing and profiling platform that we developed for this work.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1063–1075},
numpages = {13},
keywords = {OpenWhisk, architecture, cloud, faas, function-as-a-service, serverless},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

@inproceedings{10.1145/3352460.3361129,
author = {Yan, Mengjia and Choi, Jiho and Skarlatos, Dimitrios and Morrison, Adam and Fletcher, Christopher W. and Torrellas, Josep},
title = {InvisiSpec: Making Speculative Execution Invisible in the Cache Hierarchy (Corrigendum)},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3361129},
doi = {10.1145/3352460.3361129},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1076},
numpages = {1},
location = {Columbus, OH, USA},
series = {MICRO-52}
}

