@inproceedings{10.5555/3195638.3195640,
author = {Panda, Biswabandan and Seznec, Andr\'{e}},
title = {Dictionary sharing: an efficient cache compression scheme for compressed caches},
year = {2016},
publisher = {IEEE Press},
abstract = {The effectiveness of a compressed cache depends on three features: i) the compression scheme, ii) the compaction scheme, and iii) the cache layout of the compressed cache. Skewed compressed cache (SCC) and yet another compressed cache (YACC) are two recently proposed compressed cache layouts that feature minimal storage and latency overheads, while achieving comparable performance over more complex compressed cache layouts. Both SCC and YACC use compression techniques to compress individual cache blocks, and then a compaction technique to compact multiple contiguous compressed blocks into a single data entry. The primary attribute used by these techniques for compaction is the compression factor of the cache blocks, and in this process, they waste cache space.In this paper, we propose dictionary sharing (DISH), a dictionary based cache compression scheme that reduces this wastage. DISH compresses a cache block by keeping in mind that the block is a potential candidate for the compaction process. DISH encodes a cache block with a dictionary that stores the distinct 4-byte chunks of a cache block and the dictionary is shared among multiple neighboring cache blocks. The simple encoding scheme of DISH also provides a single cycle decompression latency and it does not change the cache layout of compressed caches.Compressed cache layouts that use DISH outperforms the compression schemes, such as BDI and CPACK+Z, in terms of compression ratio (from 1.7X to 2.3X), system performance (from 7.2\% to 14.3\%), and energy efficiency (from 6\% to 16\%).},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {1},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195641,
author = {Teran, Elvira and Wang, Zhe and Jim\'{e}nez, Daniel A.},
title = {Perceptron learning for reuse prediction},
year = {2016},
publisher = {IEEE Press},
abstract = {The disparity between last-level cache and memory latencies motivates the search for efficient cache management policies. Recent work in predicting reuse of cache blocks enables optimizations that significantly improve cache performance and efficiency. However, the accuracy of the prediction mechanisms limits the scope of optimization. This paper proposes perceptron learning for reuse prediction. The proposed predictor greatly improves accuracy over previous work. For multi- programmed workloads, the average false positive rate of the proposed predictor is 3.2\%, while sampling dead block prediction (SDBP) and signature-based hit prediction (SHiP) yield false positive rates above 7\%. The improvement in accuracy translates directly into performance. For single-thread workloads and a 4MB lastlevel cache, reuse prediction with perceptron learning enables a replacement and bypass optimization to achieve a geometric mean speedup of 6.1\%, compared with 3.8\% for SHiP and 3.5\% for SDBP on the SPEC CPU 2006 benchmarks. On a memory-intensive subset of SPEC, perceptron learning yields 18.3\% speedup, versus 10.5\% for SHiP and 7.7\% for SDBP. For multi- programmed workloads and a 16MB cache, the proposed technique doubles the efficiency of the cache over LRU and yields a geometric mean normalized weighted speedup of 7.4\%, compared with 4.4\% for SHiP and 4.2\% for SDBP.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {2},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195642,
author = {Kallurkar, Prathmesh and Sarangi, Smruti R.},
title = {pTask: a smart prefetching scheme for OS intensive applications},
year = {2016},
publisher = {IEEE Press},
abstract = {Instruction prefetching is a standard approach to improve the performance of operating system (OS) intensive workloads such as web servers, file servers and database servers. Sophisticated instruction prefetching techniques such as PIF [12] and RDIP [17] record the execution history of a program in dedicated hardware structures and use this information for prefetching if a known execution pattern is repeated. The storage overheads of the additional hardware structures are prohibitively high (64-200 KB per core). This makes it difficult for the deployment of such schemes in real systems. We propose a solution that uses minimal hardware modifications to tackle this problem. We notice that the execution of server applications keeps switching between tasks such as the application, system call handlers, and interrupt handlers. Each task has a distinct instruction footprint, and is separated by a special OS event. We propose a sophisticated technique to capture the instruction stream in the vicinity of such OS events; the captured information is then compressed significantly and is stored in a process's virtual address space. Special OS routines then use this information to prefetch instructions for the OS and the application codes. Using modest hardware support (4 registers per core), we report an increase in instruction throughput of 2--14\% (mean: 7\%) over state of the art instruction prefetching techniques for a suite of 8 popular OS intensive applications.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {3},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195643,
author = {Perais, Arthur and Endo, Fernando A. and Seznec, Andr\'{e}},
title = {Register sharing for equality prediction},
year = {2016},
publisher = {IEEE Press},
abstract = {Recently, Value Prediction (VP) has been gaining renewed traction in the research community. VP speculates on the result of instructions to increase Instruction Level Parallelism (ILP). In most embodiments, VP requires large tables to track predictions for many static instructions.However, in many cases, it is possible to detect that the result of an instruction is produced by an older inflight instruction, but not to predict the result itself. Consequently it is possible to rely on predicting register equality and handle speculation through the renamer.To do so, we propose to use Distance Prediction, a technique that was previously used to perform Speculative Memory Bypassing (short-circuiting def-store-load-use chains). Distance Prediction attempts to determine how many instructions separate the instruction of interest and the most recent older instruction that produced the same result. With this information, the physical register identifier of the older instruction can be retrieved from the ROB and provided to the renamer.In this paper, we first quantify the performance gains brought by renaming-based register equality prediction and regular VP on SPEC benchmarks. Second, we study the overlap between the two different schemes and show that those mechanisms often capture different behavior.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {4},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195644,
author = {Jeffrey, Mark C. and Subramanian, Suvinay and Abeydeera, Maleen and Emer, Joel and Sanchez, Daniel},
title = {Data-centric execution of speculative parallel programs},
year = {2016},
publisher = {IEEE Press},
abstract = {Multicore systems must exploit locality to scale, scheduling tasks to minimize data movement. While locality-aware parallelism is well studied in non-speculative systems, it has received little attention in speculative systems (e.g., HTM or TLS), which hinders their scalability.We present spatial hints, a technique that leverages program knowledge to reveal and exploit locality in speculative parallel programs. A hint is an abstract integer, given when a speculative task is created, that denotes the data that the task is likely to access. We show it is easy to modify programs to convey locality through hints. We design simple hardware techniques that allow a state-of-the-art, tiled speculative architecture to exploit hints by: (i) running tasks likely to access the same data on the same tile, (ii) serializing tasks likely to conflict, and (iii) balancing tasks across tiles in a locality-aware fashion. We also show that programs can often be restructured to make hints more effective.Together, these techniques make speculative parallelism practical on large-scale systems: at 256 cores, hints achieve near-linear scalability on nine challenging applications, improving performance over hint-oblivious scheduling by 3.3x gmean and by up to 16x. Hints also make speculation far more efficient, reducing wasted work by 6.4x and traffic by 3.5x on average.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {5},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195646,
author = {Daglis, Alexandres and Ustiugov, Dmitrii and Novakovi\'{c}, Stanko and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
title = {SABRes: atomic object reads for in-memory rack-scale computing},
year = {2016},
publisher = {IEEE Press},
abstract = {Modern in-memory services rely on large distributed object stores to achieve the high scalability essential to service thousands of requests concurrently. The independent and unpredictable nature of incoming requests results in random accesses to the object store, triggering frequent remote memory accesses. State-of-the-art distributed memory frameworks leverage the one-sided operations offered by RDMA technology to mitigate the traditionally high cost of remote memory access. Unfortunately, the limited semantics of RDMA onesided operations bound remote memory access atomicity to a single cache block; therefore, atomic remote object access relies on software mechanisms. Emerging highly integrated rack-scale systems that reduce the latency of one-sided operations to a small multiple of DRAM latency expose the overhead of these software mechanisms as a major latency contributor.This technology-triggered paradigm shift calls for new onesided operations with stronger semantics. We take a step in that direction by proposing SABRes, a new one-sided operation that provides atomic remote object reads in hardware. We then present LightSABRes, a lightweight hardware accelerator for SABRes that removes all atomicity-associated software overheads. Compared to a state-of-the-art software atomicity mechanism, LightSABRes improve the throughput of a microbenchmark atomically accessing 128B-8KB objects from remote memory by 15--97\%, and the throughput of a modern in-memory distributed object store by 30--60\%.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {6},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195647,
author = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
title = {A cloud-scale acceleration architecture},
year = {2016},
publisher = {IEEE Press},
abstract = {Hyperscale datacenter providers have struggled to balance the growing need for specialized hardware (efficiency) with the economic benefits of homogeneity (manageability). In this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate both network plane functions and applications. This Configurable Cloud architecture places a layer of reconfigurable logic (FPGAs) between the network switches and the servers, enabling network flows to be programmably transformed at line rate, enabling acceleration of local applications running on the server, and enabling the FPGAs to communicate directly, at datacenter scale, to harvest remote FPGAs unused by their local servers. We deployed this design over a production server bed, and show how it can be used for both service acceleration (Web search ranking) and network acceleration (encryption of data in transit at high-speeds). This architecture is much more scalable than prior work which used secondary rack-scale networks for inter-FPGA communication. By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at comparable latency to previous work, without the secondary network. Additionally, the scale of direct inter-FPGA messaging is much larger. The average round-trip latencies observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9, and 20 microseconds, respectively. The Configurable Cloud architecture has been deployed at hyperscale in Microsoft's production datacenters worldwide.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {7},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195648,
author = {Hu, Yang and Li, Tao},
title = {Towards efficient server architecture for virtualized network function deployment: implications and implementations},
year = {2016},
publisher = {IEEE Press},
abstract = {Recent years have seen a revolution in network infrastructure brought on by the ever-increasing demands for data volume. One promising proposal to emerge from this revolution is Network Functions Virtualization (NFV), which has been widely adopted by service and cloud providers. The essence of NFV is to run network functions as virtualized workloads on commodity Standard High Volume Servers (SHVS), which is the industry standard.However, our experience using NFV when deployed on modern NUMA-based SHVS paints a frustrating picture. Due to the complexity in the NFV data plane and its service function chain feature, modern NFV deployment on SHVS exhibits a unique processing pattern---heterogeneous software pipeline (HSP), in which the NFV traffic flows must be processed by heterogeneous software components sequentially from the NIC to the end receiver. Since the end-to-end performance of flows is cooperatively determined by the performance of each processing stage, the resource allocation/mapping scheme in NUMA-based SHVS must consider a thread-dependence scheduling to tradeoff the impact of co-located contention and remote packet transmission.In this paper, we develop a thread scheduling mechanism that collaboratively places threads of HSP to minimize the end-to-end performance slowdown for NFV traffic flow. It employs a dynamic programming-based method to search for the optimal thread mapping with negligible overhead. To serve this mechanism, we also develop a performance slowdown estimation model to accurately estimate the performance slowdown at each stage of HSP. We implement our collaborative thread scheduling mechanism on a real system and evaluate it using real workloads. On average, our algorithm outperforms state-of-the-art NUMA-aware and contention-aware scheduling policies by at least 7\% on CPU utilization and 23\% on traffic throughput with negligible computational overhead (less than 1 second).},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {8},
numpages = {12},
keywords = {NFV, NUMA, networking, thread scheduling},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195649,
author = {Chen, Renhai and Shao, Zili and Li, Tao},
title = {Bridging the I/O performance gap for big data workloads: a new NVDIMM-based approach},
year = {2016},
publisher = {IEEE Press},
abstract = {The long I/O latency posts significant challenges for many data-intensive applications, such as the emerging big data workloads. Recently, the NVDIMM (Non-Volatile Dual In-line Memory Module) technologies provide a promising solution to this problem. By employing non-volatile NAND flash memory as storage media and connecting them via DIMM (Dual Inline Memory Module) slots, the NVDIMM devices are exposed to memory bus so the access latencies due to going through I/O controllers can be significantly mitigated. However, placing NVDIMM on the memory bus introduces new challenges. For instance, by mixing I/O and memory traffic, NVDIMM can cause severe performance degradation on memory-intensive applications. Besides, there exists a speed mismatch between fast memory access and slow flash read/write operations. Moreover, garbage collection (GC) in NAND flash may cause up to several millisecond latency.This paper presents novel, enabling mechanisms that allow NVDIMM to more effectively bridge the I/O performance gap for big data workloads. To address the workload heterogeneity challenge, we develop a scheduling scheme in memory controller to minimize the interference between the native and the I/O-derived memory traffic by exploiting both data access criticality and resource utilization. For NVDIMM controller, several mechanisms are designed to better orchestrate traffic between the memory controller and NAND flash to alleviate the speed discrepancy issue. To mitigate the lengthy GC period, we propose a proactive GC scheme for the NVDIMM controller and flash controller to intelligently synchronize and transfer data involving in forthcoming GC operations. We present detailed evaluation and analysis to quantify how well our techniques fit with the NVDIMM design. Our experimental results show that overall the proposed techniques yield 10\%~35\% performance improvements over the state-of-the-art baseline schemes.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {9},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195650,
author = {Gottesman, Yonatan and Etsion, Yoav},
title = {NeSC: self-virtualizing nested storage controller},
year = {2016},
publisher = {IEEE Press},
abstract = {The emergence of high-speed, multi GB/s storage devices has shifted the performance bottleneck of storage virtualization to the software layers of the hypervisor. The hypervisor overheads can be avoided by allowing the virtual machine (VM) to directly access the storage device (a method known as direct device assignment), but this method voids all protection guarantees provided by filesystem permissions, since the device has no notion of client isolation. Recently, following the introduction of 10Gbs and higher networking interfaces, the PCIe specification was extended to include the SR-IOV specification for self-virtualizing devices, which allows a single physical device to present multiple virtual interfaces on the PCIe interconnect. Using SR-IOV, a hypervisor can directly assign a virtual PCIe device interface to each of its VMs. However, as networking interfaces simply multiplex packets sent from/to different clients, the specification does not dictate the semantics of a virtual storage device and how to maintain data isolation in a self-virtualizing device.In this paper we present the self-virtualizing, nested storage controller (NeSC) architecture, which includes a filesystem-agnostic protection mechanism that enables the physical device to export files as virtual PCIe storage devices. The protection mechanism maps file offsets to physical blocks and thereby offloads the hypervisor's storage layer functionality to hardware. Using NeSC, a hypervisor can securely expose its files as virtual PCIe devices and directly assign them to VMs. We have prototyped a 1GB/s NeSC controller using a Virtex-7 FPGA development board connected to the PCIe interconnect. Our evaluation of NeSC on a real system shows that NeSC virtual devices enable VMs to access their data with near-native performance (in terms of both throughput and latency).},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {10},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195652,
author = {ElTantawy, Ahmed and Aamodt, Tor M.},
title = {MIMD synchronization on SIMT architectures},
year = {2016},
publisher = {IEEE Press},
abstract = {In the single-instruction multiple-threads (SIMT) execution model, small groups of scalar threads operate in lockstep. Within each group, current SIMT hardware implementations serialize the execution of threads that follow different paths, and to ensure efficiency, revert to lockstep execution as soon as possible. These constraints must be considered when adapting algorithms that employ synchronization. A deadlock-free program on a multiple-instruction multiple-data (MIMD) architecture may deadlock on a SIMT machine. To avoid this, programmers need to restructure control flow with SIMT scheduling constraints in mind. This requires programmers to be familiar with the underlying SIMT hardware.In this paper, we propose a static analysis technique that detects SIMT deadlocks by inspecting the application control flow graph (CFG). We further propose a CFG transformation that avoids SIMT deadlocks when synchronization is local to a function. Both the analysis and the transformation algorithms are implemented as LLVM compiler passes. Finally, we propose an adaptive hardware reconvergence mechanism that supports MIMD synchronization without changing the application CFG, but which can leverage our compiler analysis to gain efficiency. The static detection has a false detection rate of only 4\%--5\%. The automated transformation has an average performance overhead of 8.2\%--10.9\% compared to manual transformation. Our hardware approach performs on par with the compiler transformation, however, it avoids synchronization scope limitations, static instruction and register overheads, and debuggability challenges that are present in the compiler only solution.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {11},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195653,
author = {Chang, Li-Wen and Hajj, Izzat El and Rodrigues, Christopher and G\'{o}mez-Luna, Juan and Hwu, Wen-mei},
title = {Efficient kernel synthesis for performance portable programming},
year = {2016},
publisher = {IEEE Press},
abstract = {The diversity of microarchitecture designs in heterogeneous computing systems allows programs to achieve high performance and energy efficiency, but results in substantial software re-development cost for each type or generation of hardware. To mitigate this cost, a performance portable programming system is required.One fundamental difference between architectures that makes performance portability challenging is the hierarchical organization of their computing elements. To address this challenge, we introduce TANGRAM, a kernel synthesis framework that composes architecture-neutral computations and composition rules into high-performance kernels customized for different architectural hierarchies.TANGRAM is based on an extensible architectural model that can be used to specify a variety of architectures. This model is coupled with a generic design space exploration and composition algorithm that can generate multiple composition plans for any specified architecture. A custom code generator then compiles these plans for the target architecture while performing various optimizations such as data placement and tuning.We show that code synthesized by TANGRAM for different types and generations of devices achieves no less than 70\% of the performance of highly optimized vendor libraries such as Intel MKL and NVIDIA CUBLAS/CUSPARSE.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {12},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195654,
author = {Hajj, Izzat El and G\'{o}mez-Luna, Juan and Li, Cheng and Chang, Li-Wen and Milojicic, Dejan and Hwu, Wen-mei},
title = {KLAP: kernel launch aggregation and promotion for optimizing dynamic parallelism},
year = {2016},
publisher = {IEEE Press},
abstract = {Dynamic parallelism on GPUs simplifies the programming of many classes of applications that generate parallelizable work not known prior to execution. However, modern GPUs architectures do not support dynamic parallelism efficiently due to the high kernel launch overhead, limited number of simultaneous kernels, and limited depth of dynamic calls a device can support.In this paper, we propose Kernel Launch Aggregation and Promotion (KLAP), a set of compiler techniques that improve the performance of kernels which use dynamic parallelism. Kernel launch aggregation fuses kernels launched by threads in the same warp, block, or kernel into a single aggregated kernel, thereby reducing the total number of kernels spawned and increasing the amount of work per kernel to improve occupancy. Kernel launch promotion enables early launch of child kernels to extract more parallelism between parents and children, and to aggregate kernel launches across generations mitigating the problem of limited depth.We implement our techniques in a real compiler and show that kernel launch aggregation obtains a geometric mean speedup of 6.58x over regular dynamic parallelism. We also show that kernel launch promotion enables cases that were not originally possible, improving throughput by a geometric mean of 30.44x.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {13},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195655,
author = {Jing, Naifeng and Wang, Jianfei and Fan, Fengfeng and Yu, Wenkang and Jiang, Li and Li, Chao and Liang, Xiaoyao},
title = {Cache-emulated register file: an integrated on-chip memory architecture for high performance GPGPUs},
year = {2016},
publisher = {IEEE Press},
abstract = {The on-chip memory design is critical to the GPGPU performance because it serves between the massive threads and the huge external memory as a low-latency and high-throughput data communication point. However, the existing on-chip memory hierarchy is inherited from the conventional CPU architecture and is oftentimes sub-optimal to the SIMT (single instruction, multiple threads) execution. In this study, we surpass the traditional memory hierarchy design and reform the on-chip memory into an integrated architecture with the cache-emulated register file (RF) capability tailored for high performance GPGPU computing. With the lightweight support from ISA, compiler and the modified microarchitecture, this integrated architecture can dynamically emulate a variable-sized RF and a cache in a uniform way. Evaluation results demonstrate that this novel architecture can deliver better performance and energy efficiency with smaller on-chip memory size. For example, it can gain an average of 50\% performance improvement for the cache-sensitive applications.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {14},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195656,
author = {Vijaykumar, Nandita and Hsieh, Kevin and Pekhimenko, Gennady and Khan, Samira and Shrestha, Ashish and Ghose, Saugata and Jog, Adwait and Gibbons, Phillip B. and Mutlu, Onur},
title = {Zorua: a holistic approach to resource virtualization in GPUs},
year = {2016},
publisher = {IEEE Press},
abstract = {This paper introduces a new resource virtualization framework, Zorua, that decouples the programmer-specified resource usage of a GPU application from the actual allocation in the on-chip hardware resources. Zorua enables this decoupling by virtualizing each resource transparently to the programmer. The virtualization provided by Zorua builds on two key concepts---dynamic allocation of the on-chip resources and their oversubscription using a swap space in memory.Zorua provides a holistic GPU resource virtualization strategy, designed to (i) adaptively control the extent of oversubscription, and (ii) coordinate the dynamic management of multiple on-chip resources (i.e., registers, scratchpad memory, and thread slots), to maximize the effectiveness of virtualization. Zorua employs a hardware-software code-sign, comprising the compiler, a runtime system and hardware-based virtualization support. The runtime system leverages information from the compiler regarding resource requirements of each program phase to (i) dynamically allocate/deallocate the different resources in the physically available on-chip resources or their swap space, and (ii) manage the tradeoff between higher thread-level parallelism due to virtualization versus the latency and capacity overheads of swap space usage.We demonstrate that by providing the illusion of more resources than physically available via controlled and coordinated virtualization, Zorua offers several important benefits: (i) Programming Ease. Zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available on-chip resources. (ii) Portability. Zorua alleviates the necessity of re-tuning an application's resource usage when porting the application across GPU generations. (iii) Performance. By dynamically allocating resources and carefully oversubscribing them when necessary, Zorua improves or retains the performance of applications that are already highly tuned to best utilize the hardware resources. The holistic virtualization provided by Zorua can also enable other uses, including fine-grained resource sharing among multiple kernels and low-latency preemption of GPU programs.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {15},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195657,
author = {Santriaji, Muhammad Husni and Hoffmann, Henry},
title = {GRAPE: minimizing energy for GPU applications with performance requirements},
year = {2016},
publisher = {IEEE Press},
abstract = {Many applications have performance requirements (e.g., real-time deadlines or quality-of-service goals) and we can save tremendous energy by tailoring resource usage so the application just meets its performance using the minimal resources. This problem is a classic constrained optimization: the performance goal is the constraint and energy consumption is the objective to be optimized. While several existing hardware approaches solve unconstrained optimizations (i.e., maximizing performance or minimizing energy), we are not aware of a hardware approach that minimizes GPU energy under an externally defined performance constraint. Therefore, we propose GRAPE, a hardware control system for GPUs that coordinates core usage, wavefront/warp action, core speed, and memory speed to deliver user-specified performance while minimizing energy. We implement GRAPE in VHDL (to demonstrate feasibility) and as an extension to GPGPU-Sim (for performance and power measurement). We find that GRAPE can be implemented with very low hardware overhead; however, compared to the no-overhead approach of race-to-idle, GRAPE reduces energy by 9--26\% (depending on the performance goal), while meeting performance goals with an average error of 0.75\%.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {16},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195659,
author = {Sharma, Hardik and Park, Jongse and Mahajan, Divya and Amaro, Emmanuel and Kim, Joon Kyung and Shao, Chenkai and Mishra, Asit and Esmaeilzadeh, Hadi},
title = {From high-level deep neural models to FPGAs},
year = {2016},
publisher = {IEEE Press},
abstract = {Deep Neural Networks (DNNs) are compute-intensive learning models with growing applicability in a wide range of domains. FPGAs are an attractive choice for DNNs since they offer a programmable substrate for acceleration and are becoming available across different market segments. However, obtaining both performance and energy efficiency with FPGAs is a laborious task even for expert hardware designers. Furthermore, the large memory footprint of DNNs, coupled with the FPGAs' limited on-chip storage makes DNN acceleration using FPGAs more challenging. This work tackles these challenges by devising DnnWeaver, a framework that automatically generates a synthesizable accelerator for a given (DNN, FPGA) pair from a high-level specification in Caffe [1]. To achieve large benefits while preserving automation, DnnWeaver generates accelerators using hand-optimized design templates. First, DnnWeaver translates a given high-level DNN specification to its novel ISA that represents a macro dataflow graph of the DNN. The DnnWeaver compiler is equipped with our optimization algorithm that tiles, schedules, and batches DNN operations to maximize data reuse and best utilize target FPGA's memory and other resources. The final result is a custom synthesizable accelerator that best matches the needs of the DNN while providing high performance and efficiency gains for the target FPGA.We use DnnWeaver to generate accelerators for a set of eight different DNN models and three different FPGAs, Xilinx Zynq, Altera Stratix V, and Altera Arria 10. We use hardware measurements to compare the generated accelerators to both multicore CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650Ti, and Tesla K40). In comparison, the generated accelerators deliver superior performance and efficiency without requiring the programmers to participate in the arduous task of hardware design.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {17},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195660,
author = {Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W.},
title = {vDNN: virtualized deep neural networks for scalable, memory-efficient neural network design},
year = {2016},
publisher = {IEEE Press},
abstract = {The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89\%, OverFeat by 91\%, and GoogLeNet by 95\%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18\% performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {18},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195661,
author = {Judd, Patrick and Albericio, Jorge and Hetherington, Tayler and Aamodt, Tor M. and Moshovos, Andreas},
title = {Stripes: bit-serial deep neural network computing},
year = {2016},
publisher = {IEEE Press},
abstract = {Motivated by the variance in the numerical precision requirements of Deep Neural Networks (DNNs) [1], [2], Stripes (STR), a hardware accelerator is presented whose execution time scales almost proportionally with the length of the numerical representation used. STR relies on bit-serial compute units and on the parallelism that is naturally present within DNNs to improve performance and energy with no accuracy loss. In addition, STR provides a new degree of adaptivity enabling on-the-fly trade-offs among accuracy, performance, and energy. Experimental measurements over a set of DNNs for image classification show that STR improves performance over a state-of-the-art accelerator [3] from 1.30x to 4.51x and by 1.92x on average with no accuracy loss. STR is 57\% more energy efficient than the baseline at a cost of 32\% additional area. Additionally, by enabling configurable, per-layer and per-bit precision control, STR allows the user to trade accuracy for further speedup and energy efficiency.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {19},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195662,
author = {Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
title = {Cambricon-x: an accelerator for sparse neural networks},
year = {2016},
publisher = {IEEE Press},
abstract = {Neural networks (NNs) have been demonstrated to be useful in a broad range of applications such as image recognition, automatic translation and advertisement recommendation. State-of-the-art NNs are known to be both computationally and memory intensive, due to the ever-increasing deep structure, i.e., multiple layers with massive neurons and connections (i.e., synapses). Sparse neural networks have emerged as an effective solution to reduce the amount of computation and memory required. Though existing NN accelerators are able to efficiently process dense and regular networks, they cannot benefit from the reduction of synaptic weights.In this paper, we propose a novel accelerator, Cambricon-X, to exploit the sparsity and irregularity of NN models for increased efficiency. The proposed accelerator features a PE-based architecture consisting of multiple Processing Elements (PE). An Indexing Module (IM) efficiently selects and transfers needed neurons to connected PEs with reduced bandwidth requirement, while each PE stores irregular and compressed synapses for local computation in an asynchronous fashion. With 16 PEs, our accelerator is able to achieve at most 544 GOP/s in a small form factor (6.38 mm2 and 954 mW at 65 nm). Experimental results over a number of representative sparse networks show that our accelerator achieves, on average, 7.23x speedup and 6.43x energy saving against the state-of-the-art NN accelerator.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {20},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195663,
author = {Ji, Yu and Zhang, YouHui and Li, ShuangChen and Chi, Ping and Jiang, CiHang and Qu, Peng and Xie, Yuan and Chen, WenGuang},
title = {NEUTRAMS: neural network transformation and co-design under neuromorphic hardware constraints},
year = {2016},
publisher = {IEEE Press},
abstract = {With the recent reincarnations of neuromorphic computing comes the promise of a new computing paradigm, with a focus on the design and fabrication of neuromorphic chips. A key challenge in design, however, is that programming such chips is difficult. This paper proposes a systematic methodology with a set of tools to address this challenge. The proposed toolset is called NEUTRAMS (&lt;u&gt;Neu&lt;/u&gt;ral network &lt;u&gt;Tra&lt;/u&gt;nsformation, &lt;u&gt;M&lt;/u&gt;apping and &lt;u&gt;S&lt;/u&gt;imulation), and includes three key components: a neural network (NN) transformation algorithm, a configurable clock-driven simulator of neuromorphic chips and an optimized runtime tool that maps NNs onto the target hardware for better resource utilization. To address the challenges of hardware constraints on implementing NN models (such as the maximum fan-in/fan-out of a single neuron, limited precision, and various neuron models), the transformation algorithm divides an existing NN into a set of simple network units and retrains each unit iteratively, to transform the original one into its counterpart under such constraints. It can support both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs), including convolutional neural networks (CNNs) and multilayer perceptrons (MLPs) and recurrent neural networks (RNNs). With the combination of these tools, we have explored the hardware/software co-design space of the correlation between network error-rates and hardware constraints and consumptions. Doing so provides insights which can support the design of future neuromorphic architectures. The usefulness of such a toolset has been demonstrated with two different designs: a real Complementary Metal-Oxide-Semiconductor (CMOS) neuromorphic chip for both SNNs and ANNs and a processing-in-memory architecture design for ANNs.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {21},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195664,
author = {Alwani, Manoj and Chen, Han and Ferdman, Michael and Milder, Peter},
title = {Fused-layer CNN accelerators},
year = {2016},
publisher = {IEEE Press},
abstract = {Deep convolutional neural networks (CNNs) are rapidly becoming the dominant approach to computer vision and a major component of many other pervasive machine learning tasks, such as speech recognition, natural language processing, and fraud detection. As a result, accelerators for efficiently evaluating CNNs are rapidly growing in popularity. The conventional approaches to designing such CNN accelerators is to focus on creating accelerators to iteratively process the CNN layers. However, by processing each layer to completion, the accelerator designs must use off-chip memory to store intermediate data between layers, because the intermediate data are too large to fit on chip.In this work, we observe that a previously unexplored dimension exists in the design space of CNN accelerators that focuses on the dataflow across convolutional layers. We find that we are able to fuse the processing of multiple CNN layers by modifying the order in which the input data are brought on chip, enabling caching of intermediate data between the evaluation of adjacent CNN layers. We demonstrate the effectiveness of our approach by constructing a fused-layer CNN accelerator for the first five convolutional layers of the VGGNet-E network and comparing it to the state-of-the-art accelerator implemented on a Xilinx Virtex-7 FPGA. We find that, by using 362KB of on-chip storage, our fused-layer accelerator minimizes off-chip feature map data transfer, reducing the total transfer by 95\%, from 77MB down to 3.6MB per image.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {22},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195666,
author = {Jain, Animesh and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason},
title = {Continuous shape shifting: enabling loop co-optimization via near-free dynamic code rewriting},
year = {2016},
publisher = {IEEE Press},
abstract = {The class of optimizations characterized by manipulating a loop's interaction space for improved cache locality and reuse (i.e, cache tiling / blocking / strip mine and interchange) are static optimizations requiring a priori information about the microarchitectural and runtime environment of an application binary. However, particularly in datacenter environments, deployed applications face numerous dynamic environments over their lifetimes. As a result, this class of optimizations can result in sub-optimal performance due to the inability to flexibly adapt iteration spaces as cache conditions change at runtime.This paper introduces continuous shape shifiting, a compilation approach that removes the risks of cache tiling optimizations by dynamically rewriting (and reshaping) deployed, running application code. To realize continuous shape shifting, we present ShapeShifter, a framework for continuous monitoring of co-running applications and their runtime environments to reshape loop iteration spaces and pinpoint near-optimal loop tile configurations. Upon identifying a need for reshaping, a new tiling approach is quickly constructed for the application, new code is dynamically generated and is then seamlessly stitched into the running application with near-zero overhead. Our evaluation on a wide spectrum of runtime scenarios demonstrates that ShapeShifter achieves an average of 10--40\% performance improvement (up to 2.4X) on real systems depending on the runtime environment compared to an oracle static loop tiling baseline.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {23},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195667,
author = {Zekany, Stephen and Rings, Daniel and Harada, Nathan and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason},
title = {Crystalball: statically analyzing runtime behavior via deep sequence learning},
year = {2016},
publisher = {IEEE Press},
abstract = {Understanding dynamic program behavior is critical in many stages of the software development lifecycle, for purposes as diverse as optimization, debugging, testing, and security. This paper focuses on the problem of predicting dynamic program behavior statically. We introduce a novel technique to statically identify hot paths that leverages emerging deep learning techniques to take advantage of their ability to learn subtle, complex relationships between sequences of inputs. This approach maps well to the problem of identifying the behavior of sequences of basic blocks in program execution. Our technique is also designed to operate on the compiler's intermediate representation (IR), as opposed to the approaches taken by prior techniques that have focused primarily on source code, giving our approach language-independence. We describe the pitfalls of conventional metrics used for hot path prediction such as accuracy, and motivate the use of Area Under the Receiver Operating Characteristic curve (AUROC). Through a thorough evaluation of our technique on complex applications that include the SPEC CPU2006 benchmarks, we show that our approach achieves an AUROC of 0.85.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {24},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195668,
author = {Liu, Qingrui and Jung, Changhee and Lee, Dongyoon and Tiwari, Devesh},
title = {Low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection},
year = {2016},
publisher = {IEEE Press},
abstract = {This paper presents Turnstile, a hardware/software cooperative technique for low-cost soft error resilience. Leveraging the recent advance of acoustic sensor based soft error detection, Turnstile achieves guaranteed recovery by taking into account the bounded detection latency. The compiler forms verifiable regions and selectively inserts store instructions to checkpoint their register inputs so that Turnstile can verify the register/memory states with regard to a region boundary in a unified way without expensive register file protection.At runtime, for each region, Turnstile regards any stores (to both memory and register checkpoints) as unverified, and thus holds them in a store queue until the region ends and spends the time of the error detection latency. If no error is detected during the time, the verified stores are merged into memory systems, and registers are checkpointed. When all the stores including checkpointing stores prior to a region boundary are verified, the architectural and memory states with regard to the boundary are verified, thus it can serve as a recovery point. In this way, Turnstile contains the errors within the core without extra memory buffering.When an error is detected, Turnstile invalidates unverified entries in the store queue and restores the checkpointed register values to get the architectural and memory states back to what they were at the most recently verified region boundary. Then, Turnstile simply redirects program control to the verified region boundary and continues execution. The experimental results demonstrate that Turnstile can offer guaranteed soft error recovery with low performance overhead (&lt;8\% on average).},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {25},
numpages = {12},
keywords = {acoustic sensor, compiler, fine-grained recovery, region boundary buffer, soft error resilience},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195669,
author = {Alsop, Johnathan and Orr, Marc S. and Beckmann, Bradford M. and Wood, David A.},
title = {Lazy release consistency for GPUs},
year = {2016},
publisher = {IEEE Press},
abstract = {The heterogeneous-race-free (HRF) memory model has been embraced by the Heterogeneous System Architecture (HSA) Foundation and OpenCL™ because it clearly and precisely defines the behavior of current GPUs. However, compared to the simpler SC for DRF memory model, HRF has two shortcomings. The first is that HRF requires programmers to label atomic memory operations with the correct scope of synchronization. This explicit labeling can save significant coherence overhead when synchronization is local, but it is tedious and error-prone. The second shortcoming is that HRF restricts important dynamic data sharing patterns like work stealing. Prior work on remote-scope promotion (RSP) attempted to resolve the second shortcoming. However, RSP further complicates the memory model and no scalable implementation of RSP has been proposed. For example, we found that the previously proposed RSP implementation actually results in slowdowns of up to 30\% on large GPUs, compared to a na\"{\i}ve baseline system that forgoes work stealing and scopes. Meanwhile, DeNovo has been shown to offer efficient synchronization with an SC for DRF memory model, performing on average 21\% better than our baseline system, but it introduces additional overheads to maintain ownership of all modified data.To resolve these deficiencies, we propose to adapt lazy release consistency---previously only proposed for homogeneous CPU systems---to a heterogeneous system. Our approach, called hLRC, uses a DeNovo-like mechanism to track ownership of synchronization variables, lazily performing coherence actions only when a synchronization variable changes locations. hLRC allows GPU programmers to use the simpler SC for DRF memory model without tracking ownership for all modified data. Our evaluation shows that lazy release consistency provides robust performance improvement across a set of work-stealing graph analysis applications---29\% on average versus the baseline system.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {26},
numpages = {13},
keywords = {graphics processing unit (GPU), lazy release consistency, memory model, scope promotion, scoped synchronization, work stealing},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195670,
author = {Ha, Heonjae and Pedram, Ardavan and Richardson, Stephen and Kvatinsky, Shahar and Horowitz, Mark},
title = {Improving energy efficiency of DRAM by exploiting half page row access},
year = {2016},
publisher = {IEEE Press},
abstract = {DRAM energy is an important component to optimize in modern computing systems. One outstanding source of DRAM energy is the energy to fetch data stored on cells to the row buffer, which occurs during two DRAM operations, row activate and refresh. This work exploits previously proposed half page row access, modifying the wordline connections within a bank to halve the number of cells fetched to the row buffer, to save energy in both cases. To accomplish this, we first change the data wire connections in the sub-array to reduce the cost of row buffer overfetch in multi-core systems which yields a 12\% energy savings on average and a slight performance improvement in quad-core systems. We also propose charge recycling refresh, which reuses charge left over from a prior half page refresh to refresh another half page. Our charge recycling scheme is capable of reducing both auto- and self-refresh energy, saving more than 15\% of refresh energy at 85°C, and provides even shorter refresh cycle time. Finally, we propose a refresh scheduling scheme that can dynamically adjust the number of charge recycled half pages, which can save up to 30\% of refresh energy at 85°C.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {27},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195672,
author = {Zhan, Jia and Kayiran, Onur and Loh, Gabriel H. and Das, Chita R. and Xie, Yuan},
title = {OSCAR: orchestrating STT-RAM cache traffic for heterogeneous CPU-GPU architectures},
year = {2016},
publisher = {IEEE Press},
abstract = {As we integrate data-parallel GPUs with general-purpose CPUs on a single chip, the enormous cache traffic generated by GPUs will not only exhaust the limited cache capacity, but also severely interfere with CPU requests. Such heterogeneous multicores pose significant challenges to the design of shared last-level cache (LLC). This problem can be mitigated by replacing SRAM LLC with emerging non-volatile memories like Spin-Transfer Torque RAM (STT-RAM), which provides larger cache capacity and near-zero leakage power. However, without careful design, the slow write operations of STT-RAM may offset the capacity benefit, and the system may still suffer from contention in the shared LLC and on-chip interconnects.While there are cache optimization techniques to alleviate such problems, we reveal that the true potential of STT-RAM LLC may still be limited because now that the cache hit rate has been improved by the increased capacity, the on-chip network can become a performance bottleneck. CPU and GPU packets contend with each other for the shared network bandwidth. Moreover, the mixed-criticality read/write packets to STT-RAM add another layer of complexity to the network resource allocation. Therefore, being aware of the disparate latency tolerance of CPU/GPU applications and the asymmetric read/write latency of STT-RAM, we propose OSCAR to Orchestrate STT-RAM Caches traffic for heterogeneous ARchitectures. Specifically, an integration of asynchronous batch scheduling and priority based allocation for on-chip interconnect is proposed to maximize the potential of STT-RAM based LLC. Simulation results on a 28-GPU and 14-CPU system demonstrate an average of 17.4\% performance improvement for CPUs, 10.8\% performance improvement for GPUs, and 28.9\% LLC energy saving compared to SRAM based LLC design.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {28},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195673,
author = {Zhan, Jia and Akgun, Itir and Zhao, Jishen and Davis, Al and Faraboschi, Paolo and Wang, Yuangang and Xie, Yuan},
title = {A unified memory network architecture for in-memory computing in commodity servers},
year = {2016},
publisher = {IEEE Press},
abstract = {In-memory computing is emerging as a promising paradigm in commodity servers to accelerate data-intensive processing by striving to keep the entire dataset in DRAM. To address the tremendous pressure on the main memory system, discrete memory modules can be networked together to form a memory pool, enabled by recent trends towards richer memory interfaces (e.g. Hybrid Memory Cubes, or HMCs). Such an inter-memory network provides a scalable fabric to expand memory capacity, but still suffers from long multi-hop latency, limited bandwidth, and high power consumption---problems that will continue to exacerbate as the gap between interconnect and transistor performance grows. Moreover, inside each memory module, an intra-memory network (NoC) is typically employed to connect different memory partitions. Without careful design, the back-pressure inside the memory modules can further propagate to the inter-memory network to cause a performance bottleneck.To address these problems, we propose co-optimization of intra- and inter-memory network. First, we re-organize the intra-memory network structure, and provide a smart I/O interface to reuse the intra-memory NoC as the network switches for inter-memory communication, thus forming a unified memory network. Based on this architecture, we further optimize the inter-memory network for both high performance and lower energy, including a distance-aware selective compression scheme to drastically reduce communication burden, and a light-weight power-gating algorithm to turn off under-utilized links while guaranteeing a connected graph and deadlock-free routing. We develop an event-driven simulator to model our proposed architectures. Experiment results based on both synthetic traffic and real big-data workloads show that our unified memory network architecture can achieve 75.1\% average memory access latency reduction and 22.1\% total memory energy saving.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {29},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195674,
author = {Kim, Gwangsun and Kim, Changhyun and Jeong, Jiyun and Parker, Mike and Kim, John},
title = {Contention-based congestion management in large-scale networks},
year = {2016},
publisher = {IEEE Press},
abstract = {Global adaptive routing exploits non-minimal paths to improve performance on adversarial traffic patterns and load-balance network channels in large-scale networks. However, most prior work on global adaptive routing have assumed admissible traffic pattern where no endpoint node is oversubscribed. In the presence of a greedy flow or hotspot traffic, we show how exploiting path diversity with global adaptive routing can spread network congestion and degrade performance. When global adaptive routing is combined with congestion management, the two types of congestion - network congestion that occurs within the interconnection network channels and endpoint congestion that occurs from oversubscribed endpoint nodes - are not properly differentiated. As a result, previously proposed congestion management mechanisms that are effective in addressing endpoint congestion are not necessarily effective when global adaptive routing is also used in the network. Thus, we propose a novel, low-cost contention-based congestion management (CBCM) to identify endpoint congestion based on the contention within the intermediate routers and at the endpoint nodes. While contention also occurs for network congestion, the endpoint nodes or the destination determines whether the congestion is endpoint congestion or network congestion. If it is only network congestion, CBCM ignores the network congestion and adaptive routing is allowed to minimize network congestion. However, if endpoint congestion occurs, CBCM throttles the hotspot senders and minimally route the traffic through a separate VC. Our evaluation across different traffic patterns and network sizes demonstrates that our approach is more robust in identifying endpoint congestion in the network while complementing global adaptive routing to avoid network congestion.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {30},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195675,
author = {DiTomaso, Dominic and Boraten, Travis and Kodi, Avinash and Louri, Ahmed},
title = {Dynamic error mitigation in NoCs using intelligent prediction techniques},
year = {2016},
publisher = {IEEE Press},
abstract = {Network-on-chips (NoCs) are quickly becoming the standard communication fabric for multi-core systems. As technology continues to scale down into the nanometer regime, device behavior will become increasingly unreliable due to a combination of aging, soft errors, aggressive transistor design, and process-voltage-temperature variations. Further, stringent timing constraints in NoCs are designed so that data can be pushed faster. The net result is an increase in errors which must be mitigated by the NoC. Typical techniques for handling faults are often reactive as they respond to faults after the error has occurred, making the recovery process inefficient in energy and time. In this paper, we take a different approach wherein we propose to use proactive, fault-tolerant schemes to be employed before the fault affects the system. We propose to utilize machine learning techniques to train a decision tree which can be used to predict faults efficiently in the network. Based on the prediction model, we dynamically mitigate these predicted faults through error correction codes (ECC) and relaxed timing transmission. Our results indicate that, on average, we can accurately predict timing errors 60.6\% better than a static single error correction and double error detection (SECDED) technique resulting in an average 26.8\% reduction in retransmitted packets, a average net speedup of 3.31X, and an average energy savings of 60.0\% over other designs for real traffic patterns.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {31},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195676,
author = {Wang, Shibo and Ipek, Engin},
title = {Reducing data movement energy via online data clustering and encoding},
year = {2016},
publisher = {IEEE Press},
abstract = {Modern computer systems expend significant amounts of energy on transmitting data over long and highly capacitive interconnects. A promising way of reducing the data movement energy is to design the interconnect such that the transmission of 0s is considerably cheaper than that of 1s. Given such an interconnect with asymmetric transmission costs, data movement energy can be reduced by encoding the transmitted data such that the number of 1s in each transmitted codeword is minimized. This paper presents a new data encoding technique based on online data clustering that exploits this opportunity.The transmitted data blocks are dynamically clustered based on the similarities between their binary representations. Each data block is expressed as the bitwise XOR between one of multiple cluster centers and a residual with a small number of 1s. The data movement energy is minimized by sending the residual along with an identifier that specifies which cluster center to use in decoding the transmitted data. At runtime, the proposed approach continually updates the cluster centers based on the observed data to adapt to phase changes.The proposed technique is compared to three previously proposed energy-efficient data encoding techniques on a set of 14 applications. The results indicate respective energy savings of 5\%, 9\%, and 12\% in DDR4, LPDDR3, and last level cache subsystems as compared to the best existing baseline encoding technique.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {32},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195678,
author = {Ros, Alberto and Kaxiras, Stefanos},
title = {Racer: TSO consistency via race detection},
year = {2016},
publisher = {IEEE Press},
abstract = {Several recent efforts aim to simplify coherence and its associate costs (e.g., directory size, complexity) in multicores. The bulk of these efforts rely on program data-race-free (DRF) semantics to eliminate explicit invalidations and use self-invalidation instead. While such protocols are simple, they require software cooperation. This is acceptable only for (correct) software that abides by the SC-for-DRF semantics defined in many modern programming language standards (e.g., C++11, Java, the latest C standards) but many are unwilling to trust coherence that relies solely on program semantics for its correctness.To address this important issue, this work proposes Racer, an efficient self-invalidation/write-through approach that guarantees the memory consistency model of the most common family of processors (TSO-x86), and at the same time maintains the relaxed-ordering advantages of SC-for-DRF protocols.Lacking a directory and explicit invalidations, Racer achieves this by detecting read-after-write races and causing self-invalidation on the racing reader's cache. Racer also uses a coalescing store buffer (at the L1 level) that allows coalescing and reordering of stores but upon detecting a race, delays the racing read until all its stores appear in order to the read. Race detection is performed using an efficient signature-based mechanism at the level of the shared cache.Racer performs significantly better than a traditional non-scalable directory-based protocol that does not allow reordering at the protocol level (14.2\% in time and 26.4\% in energy), a directory protocol for TSO (1.9\% in time and 15.5\% in energy), and state-of-the-art SC-for-DRF protocol that relies on acquire-release annotations in the programs (6.7\% in time and 9.5\% in energy). Racer self-invalidates less than program-level annotations as it only enforces ordering on dynamically detected races and provides significant reductions in network traffic and memory-system energy consumption.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {33},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195679,
author = {Zhang, Guowei and Chiu, Virginia and Sanchez, Daniel},
title = {Exploiting semantic commutativity in hardware speculation},
year = {2016},
publisher = {IEEE Press},
abstract = {Hardware speculative execution schemes such as hardware transactional memory (HTM) enjoy low run-time overheads but suffer from limited concurrency because they rely on reads and writes to detect conflicts. By contrast, software speculation schemes can exploit semantic knowledge of concurrent operations to reduce conflicts. In particular, they often exploit that many operations on shared data, like insertions into sets, are semantically commutative: they produce semantically equivalent results when reordered. However, software techniques often incur unacceptable run-time overheads.To solve this dichotomy, we present CommTM, an HTM that exploits semantic commutativity. CommTM extends the coherence protocol and conflict detection scheme to support user-defined commutative operations. Multiple cores can perform commutative operations to the same data concurrently and without conflicts. CommTM preserves transactional guarantees and can be applied to arbitrary HTMs.CommTM scales on many operations that serialize in conventional HTMs, like set insertions, reference counting, and top-K insertions, and retains the low overhead of HTMs. As a result, at 128 cores, CommTM outperforms a conventional eager-lazy HTM by up to 3.4X and reduces or eliminates aborts.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {34},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195680,
author = {Chou, Chiachen and Jaleel, Aamer and Qureshi, Moinuddin K.},
title = {CANDY: enabling coherent DRAM caches for multi-node systems},
year = {2016},
publisher = {IEEE Press},
abstract = {This paper investigates the use of DRAM caches for multi-node systems. Current systems architect the DRAM cache as Memory-Side Cache (MSC), restricting the DRAM cache to cache only the local data, and relying on only the small on-die caches for the remote data. As MSC keeps only the local data, it is implicitly coherent and obviates the need of any coherence support. Unfortunately, as accessing the data in the remote node incurs a significant inter-node network latency, MSC suffers from such latency overhead on every on-die cache miss to the remote data. A desirable alternative is to allow the DRAM cache to cache both the local and the remote data. However, as data blocks can be cached in multiple DRAM caches, this design requires coherence support for DRAM caches to ensure correctness, and is termed Coherent DRAM Cache (CDC).We identify two key challenges in architecting giga-scale CDC. First, the coherence directory can be as large as few tens of MB. Second, cache misses to the read-write shared data in CDC cause longer delay due to the need to access the DRAM cache. To address both problems, this paper proposes CANDY, a low-cost and scalable solution that consists of two techniques for these challenges. First, CANDY places the coherence directory in 3D DRAM to avoid SRAM storage overhead, and re-purposes the existing on-die coherence directory as a DRAM-cache Coherence Buffer to cache recently accessed directory entries. Second, we propose Sharing-Aware Bypass, which dynamically detects the read-write shared data at run-time and enforces such data to bypass the DRAM cache. Our experiment on a 4-node system with 1GB DRAM cache per node shows that CANDY outperforms MSC by 25\%, while incurring a negligible overhead of 8KB per node. CANDY is within 5\% of an impractical system that has a 64MB SRAM directory per node, and zero cache latency to access the read-write shared data.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {35},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195681,
author = {Huang, Cheng-Chieh and Kumar, Rakesh and Elver, Marco and Grot, Boris and Nagarajan, Vijay},
title = {C3D: mitigating the NUMA bottleneck via coherent DRAM caches},
year = {2016},
publisher = {IEEE Press},
abstract = {Massive datasets prevalent in scale-out, enterprise, and high-performance computing are driving a trend toward ever-larger memory capacities per node. To satisfy the memory demands and maximize performance per unit cost, today's commodity HPC and server nodes tend to feature multi-socket shared memory NUMA organizations. An important problem in these designs is the high latency of accessing memory on a remote socket that results in degraded performance in workloads with large shared data working sets.This work shows that emerging DRAM caches can help mitigate the NUMA bottleneck by filtering up to 98\% of remote memory accesses. To be effective, these DRAM caches must be private to each socket to allow caching of remote memory, which comes with the challenge of ensuring coherence across multiple sockets and GBs of DRAM cache capacity. Moreover, the high access latency of DRAM caches, combined with high inter-socket communication latencies, can make hits to remote DRAM caches slower than main memory accesses. These features challenge existing coherence protocols optimized for on-chip caches with fast hits and modest storage capacity. Our solution to these challenges relies on two insights. First, keeping DRAM caches clean avoids the need to ever access a remote DRAM cache on a read. Second, a non-inclusive on-chip directory that avoids tracking blocks in the DRAM cache enables a light-weight protocol for guaranteeing coherence without the staggering directory costs. Our design, called Clean Coherent DRAM Caches (C3D), leverages these insights to improve performance by 6.4--50.7\% in a quad-socket system versus a baseline without DRAM caches.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {36},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195683,
author = {Kazdagli, Mikhail and Reddi, Vijay Janapa and Tiwari, Mohit},
title = {Quantifying and improving the efficiency of hardware-based mobile malware detectors},
year = {2016},
publisher = {IEEE Press},
abstract = {Hardware-based malware detectors (HMDs) are a key emerging technology to build trustworthy systems, especially mobile platforms. Quantifying the efficacy of HMDs against malicious adversaries is thus an important problem. The challenge lies in that real-world malware adapts to defenses, evades being run in experimental settings, and hides behind benign applications. Thus, realizing the potential of HMDs as a small and battery-efficient line of defense requires a rigorous foundation for evaluating HMDs.We introduce Sherlock---a white-box methodology that quantifies an HMD's ability to detect malware and identify the reason why. Sherlock first deconstructs malware into atomic, orthogonal actions to synthesize a diverse malware suite. Sherlock then drives both malware and benign programs with real user-inputs, and compares their executions to determine an HMD's operating range, i.e., the smallest malware actions an HMD can detect.We show three case studies using Sherlock to not only quantify HMDs' operating ranges but design better detectors. First, using information about concrete malware actions, we build a discrete-wavelet transform based unsupervised HMD that outperforms prior work based on power transforms by 24.7\% (AUC metric). Second, training a supervised HMD using Sherlock's diverse malware dataset yields 12.5\% better HMDs than past approaches that train on ad-hoc subsets of malware. Finally, Sherlock shows why a malware instance is detectable. This yields a surprising new result---obfuscation techniques used by malware to evade static analyses makes them more detectable using HMDs.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {37},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195684,
author = {Lehman, Tamara Silbergleit and Hilton, Andrew D. and Lee, Benjamin C.},
title = {Poisonivy: safe speculation for secure memory},
year = {2016},
publisher = {IEEE Press},
abstract = {Encryption and integrity trees guard against physical attacks, but harm performance. Prior academic work has speculated around the latency of integrity verification, but has done so in an insecure manner. No industrial implementations of secure processors have included speculation. This work presents PoisonIvy, a mechanism which speculatively uses data before its integrity has been verified while preserving security and closing address-based side-channels. PoisonIvy reduces performance overheads from 40\% to 20\% for memory intensive workloads and down to 1.8\%, on average.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {38},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195685,
author = {Yan, Mengjia and Shalabi, Yasser and Torrellas, Josep},
title = {Replayconfusion: detecting cache-based covert channel attacks using record and replay},
year = {2016},
publisher = {IEEE Press},
abstract = {Cache-based covert channel attacks use highly-tuned shared-cache conflict misses to pass information from a trojan to a spy process. Detecting such attacks is very challenging. State of the art detection mechanisms do not consider the general characteristics of such attacks and, instead, focus on specific communication protocols. As a result, they fail to detect attacks using different protocols and, hence, have limited coverage.In this paper, we make the following observation about these attacks: not only are the malicious accesses highly tuned to the mapping of addresses to the caches; they also follow a distinctive cadence as bits are being received. Changing the mapping of addresses to the caches substantially disrupts the conflict miss patterns, but retains the cadence. This is in contrast to benign programs.Based on this observation, we propose a novel, high-coverage approach to detect cache-based covert channel attacks. It is called ReplayConfusion, and is based on Record and deterministic Replay (RnR). After a program's execution is recorded, it is deterministically replayed using a different mapping of addresses to the caches. We then analyze the difference between the cache miss rate timelines of the two runs. If the difference function is both sizable and exhibits a periodic pattern, it indicates that there is an attack. This paper also introduces a new taxonomy of cache-based covert channel attacks, and shows that ReplayConfusion uncovers examples from all the categories. Finally, ReplayConfusion only needs simple hardware.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {39},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195686,
author = {Evtyushkin, Dmitry and Ponomarev, Dmitry and Abu-Ghazaleh, Nael},
title = {Jump over ASLR: attacking branch predictors to bypass ASLR},
year = {2016},
publisher = {IEEE Press},
abstract = {Address Space Layout Randomization (ASLR) is a widely-used technique that protects systems against a range of attacks. ASLR works by randomizing the offset of key program segments in virtual memory, making it difficult for an attacker to derive the addresses of specific code objects and consequently redirect the control flow to this code. In this paper, we develop an attack to derive kernel and user-level ASLR offset using a side-channel attack on the branch target buffer (BTB). Our attack exploits the observation that an adversary can create BTB collisions between the branch instructions of the attacker process and either the user-level victim process or on the kernel executing on its behalf. These collisions, in turn, can impact the timing of the attacker's code, allowing the attacker to identify the locations of known branch instructions in the address space of the victim process or the kernel. We demonstrate that our attack can reliably recover kernel ASLR in about 60 milliseconds when performed on a real Haswell processor running a recent version of Linux. Finally, we describe several possible protection mechanisms, both in software and in hardware.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {40},
numpages = {13},
keywords = {address space layout randomization, bypass, exploit mitigation, kernel vulnerabilities, side channel, timing attacks, timing channel},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195688,
author = {Jain, Animesh and Hill, Parker and Lin, Shih-Chieh and Khan, Muneeb and Haque, Md E. and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason},
title = {Concise loads and stores: the case for an asymmetric compute-memory architecture for approximation},
year = {2016},
publisher = {IEEE Press},
abstract = {Cache capacity and memory bandwidth play critical roles in application performance, particularly for data-intensive applications from domains that include machine learning, numerical analysis, and data mining. Many of these applications are also tolerant to imprecise inputs and have loose constraints on the quality of output, making them ideal candidates for approximate computing. This paper introduces a novel approximate computing technique that decouples the format of data in the memory hierarchy from the format of data in the compute subsystem to significantly reduce the cost of storing and moving bits throughout the memory hierarchy and improve application performance. This asymmetric compute-memory extension to conventional architectures, ACME, adds two new instruction classes to the ISA - load-concise and store-concise - along with three small functional units to the micro-architecture to support these instructions. ACME does not affect exact execution of applications and comes into play only when concise memory operations are used. Through detailed experimentation we find that ACME is very effective at trading result accuracy for improved application performance. Our results show that ACME achieves a 1.3X speedup (up to 1.8X) while maintaining 99\% accuracy, or a 1.1X speedup while maintaining 99.999\% accuracy. Moreover, our approach incurs negligible area and power overheads, adding just 0.005\% area and 0.1\% power to a conventional modern architecture.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {41},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195689,
author = {Venkatagiri, Radha and Mahmoud, Abdulrahman and Hari, Siva Kumar Sastry and Adve, Sarita V.},
title = {Approxilyzer: towards a systematic framework for instruction-level approximate computing and its application to hardware resiliency},
year = {2016},
publisher = {IEEE Press},
abstract = {Approximate computing environments trade off computational accuracy for improvements in performance, energy, and resiliency cost. For widespread adoption of approximate computing, a fundamental requirement is to understand how perturbations to a computation affect the outcome of the execution in terms of its output quality.This paper presents a framework for approximate computing, called Approxilyzer, that quantifies the quality impact of a single-bit error in all dynamic instructions of an execution with high accuracy (95\% on average). We demonstrate two uses of Approxilyzer. First, we show how Approxilyzer can be used to quantitatively tune output quality vs. resiliency vs. overhead to enable ultra-low cost resiliency solutions (with a single bit error model). For example, we show that Approxilyzer determines that a very small loss in output quality (1\%) can yield large resiliency overhead reduction (up to 55\%) for 99\% resiliency coverage. Second, we show how Approxilyzer can be used to provide a first-order estimate of the approximation potential of general-purpose programs. It does so in an automated way while requiring minimal user input and no program modifications. This enables programmers or other tools to focus on the promising subset of approximable instructions for further analysis.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {42},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195690,
author = {Miguel, Joshua San and Albericio, Jorge and Jerger, Natalie Enright and Jaleel, Aamer},
title = {The bunker cache for spatio-value approximation},
year = {2016},
publisher = {IEEE Press},
abstract = {The cost of moving and storing data is still a fundamental concern for computer architects. Inefficient handling of data can be attributed to conventional architectures being oblivious to the nature of the values that these data bits carry. We observe the phenomenon of spatio-value similarity, where data elements that are approximately similar in value exhibit spatial regularity in memory. This is inherent to 1) the data values of real-world applications, and 2) the way we store data structures in memory. We propose the Bunker Cache, a design that maps similar data to the same cache storage location based solely on their memory address, sacrificing some application quality loss for greater efficiency. The Bunker Cache enables performance gains (ranging from 1.08X to 1.19X) via reduced cache misses and energy savings (ranging from 1.18X to 1.39X) via reduced off-chip memory accesses and lower cache storage requirements. The Bunker Cache requires only modest changes to cache indexing hardware, integrating easily into commodity systems.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {43},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195692,
author = {Gogte, Vaibhav and Kolli, Aasheesh and Cafarella, Michael J. and D'Antoni, Loris and Wenisch, Thomas F.},
title = {HARE: hardware accelerator for regular expressions},
year = {2016},
publisher = {IEEE Press},
abstract = {Rapidly processing text data is critical for many technical and business applications. Traditional software-based tools for processing large text corpora use memory bandwidth inefficiently due to software overheads and thus fall far short of peak scan rates possible on modern memory systems. Prior hardware designs generally target I/O rather than memory bandwidth. In this paper, we present HARE, a hardware accelerator for matching regular expressions against large in-memory logs. HARE comprises a stall-free hardware pipeline that scans input data at a fixed rate, examining multiple characters from a single input stream in parallel in a single accelerator clock cycle.We describe a 1GHz 32-character-wide HARE design targeting ASIC implementation that processes data at 32 GB/s---matching modern memory bandwidths. This ASIC design outperforms software solutions by as much as two orders of magnitude. We further demonstrate a scaled-down FPGA proof-of-concept that operates at 100MHz with 4-wide parallelism (400 MB/s). Even at this reduced rate, the prototype outperforms grep by 1.5--20x on commonly used regular expressions.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {44},
numpages = {12},
keywords = {finite automata, regular expression matching, text processing},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195693,
author = {Murray, Sean and Floyd-Jones, William and Qi, Ying and Konidaris, George and Sorin, Daniel J.},
title = {The microarchitecture of a real-time robot motion planning accelerator},
year = {2016},
publisher = {IEEE Press},
abstract = {We have developed a hardware accelerator for motion planning, a critical operation in robotics. In this paper, we present the microarchitecture of our accelerator and describe a prototype implementation on an FPGA. We experimentally show that the accelerator improves performance by three orders of magnitude and improves power consumption by more than one order of magnitude. These gains are achieved through careful hardware/software co-design. We modify conventional motion planning algorithms to aggressively precompute collision data, as well as implement a microarchitecture that leverages the parallelism present in the problem.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {45},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195694,
author = {Chen, Tao and Suh, G. Edward},
title = {Efficient data supply for hardware accelerators with prefetching and access/execute decoupling},
year = {2016},
publisher = {IEEE Press},
abstract = {This paper presents an architecture framework to easily design hardware accelerators that can effectively tolerate long and variable memory latency using prefetching and access/execute decoupling. Hardware accelerators are becoming increasingly popular in modern computing systems as a promising approach to achieve higher performance and energy efficiency when technology scaling is slowing down. However, today's high-performance accelerators require significant manual efforts to design, in large part due to the need to carefully orchestrate data transfers between external memory and an accelerator. Instead, the proposed framework utilizes automated program analysis along with High-Level Synthesis (HLS) tools to enable prefetching and access/execute decoupling with minimal manual efforts. The framework adds tags to accelerator memory accesses so that hardware prefetching can effectively preload data for accesses with regular patterns. To handle irregular memory accesses, the framework generates an accelerator with decoupled access/execute architecture using program slicing. Experimental results show that the proposed optimizations can significantly improve performance of HLS-generated accelerators (average speedup of 2.28x across eight accelerators) and often reduce energy consumption (average of 15\%).},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {46},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195696,
author = {Yazdani, Reza and Segura, Albert and Arnau, Jose-Maria and Gonzalez, Antonio},
title = {An ultra low-power hardware accelerator for automatic speech recognition},
year = {2016},
publisher = {IEEE Press},
abstract = {Automatic Speech Recognition (ASR) is becoming increasingly ubiquitous, especially in the mobile segment. Fast and accurate ASR comes at a high energy cost which is not affordable for the tiny power budget of mobile devices. Hardware acceleration can reduce power consumption of ASR systems, while delivering high-performance.In this paper, we present an accelerator for large-vocabulary, speaker-independent, continuous speech recognition. It focuses on the Viterbi search algorithm, that represents the main bottleneck in an ASR system. The proposed design includes innovative techniques to improve the memory subsystem, since memory is identified as the main bottleneck for performance and power in the design of these accelerators. We propose a prefetching scheme tailored to the needs of an ASR system that hides main memory latency for a large fraction of the memory accesses with a negligible impact on area. In addition, we introduce a novel bandwidth saving technique that removes 20\% of the off-chip memory accesses issued during the Viterbi search.The proposed design outperforms software implementations running on the CPU by orders of magnitude and achieves 1.7x speedup over a highly optimized CUDA implementation running on a high-end Geforce GTX 980 GPU, while reducing by two orders of magnitude (287x) the energy required to convert the speech into text.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {47},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195697,
author = {Shao, Yakun Sophia and Xi, Sam (Likun) and Srinivasan, Vijayalakshmi and Wei, Gu-Yeon and Brooks, David},
title = {Co-designing accelerators and SoC interfaces using gem5-aladdin},
year = {2016},
publisher = {IEEE Press},
abstract = {Increasing demand for power-efficient, high-performance computing has spurred a growing number and diversity of hardware accelerators in mobile and server Systems on Chip (SoCs). This paper makes the case that the co-design of the accelerator microarchitecture with the system in which it belongs is critical to balanced, efficient accelerator microarchitectures. We find that data movement and coherence management for accelerators are significant yet often unaccounted components of total accelerator runtime, resulting in misleading performance predictions and inefficient accelerator designs. To explore the design space of accelerator-system co-design, we develop gem5-Aladdin, an SoC simulator that captures dynamic interactions between accelerators and the SoC platform, and validate it to within 6\% against real hardware. Our co-design studies show that the optimal energy-delay-product (EDP) of an accelerator microarchitecture can improve by up to 7.4X when system-level effects are considered compared to optimizing accelerators in isolation.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {48},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195698,
author = {Sharifian, Amirali and Kumar, Snehasish and Guha, Apala and Shriraman, Arrvindh},
title = {CHAINSAW: von-neumann accelerators to leverage fused instruction chains},
year = {2016},
publisher = {IEEE Press},
abstract = {A central tenet behind accelerators is to partition a program execution into regions with different behavior (e.g., SIMD, Irregular, Compute-Intensive) and then use behavior-specialized architectures [1] for each region. It is unclear whether the gains in efficiency arise from recognizing that a simpler microarchitecture is sufficient for the acceleratable code region or the actual microarchitecture, or a combination of both. Many proposals [2], [3] seem to choose dataflow-based accelerators which encounters challenges with fabric utilization and static power when the available instruction parallelism is below the peak operation parallelism available [4].In this paper, we develop, Chainsaw, a Von-Neumann based accelerator and demonstrate that many of the fundamental overheads (e.g., fetch-decode) can be amortized by adopting the appropriate instruction abstraction. The key insight is the notion of chains, which are compiler fused sequences of instructions. chains adapt to different acceleration behaviors by varying the length of the chains and the types of instructions that are fused into a chain. Chains convey the producer-consumer locality between dependent instructions, which the Chainsaw architecture then captures by temporally scheduling such operations on the same execution unit and uses pipeline registers to forward the values between dependent operations. Chainsaw is a generic multi-lane architecture (4-stage pipeline per lane) and does not require any specialized compound function units; it can be reloaded enabling it to accelerate multiple program paths. We have developed a complete LLVM-based compiler prototype and simulation infrastructure and demonstrated that a 8-lane Chainsaw is within 73\% of the performance of an ideal dataflow architecture, while reducing the energy consumption by 45\% compared to a 4-way OOO processor.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {49},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195699,
author = {Asghari-Moghaddam, Hadi and Son, Young Hoon and Ahn, Jung Ho and Kim, Nam Sung},
title = {Chameleon: versatile and practical near-DRAM acceleration architecture for large memory systems},
year = {2016},
publisher = {IEEE Press},
abstract = {The performance of computer systems is often limited by the bandwidth of their memory channels, but further increasing the bandwidth is challenging under the stringent pin and power constraints of packages. To further increase performance under these constraints, various near-DRAM acceleration (NDA) architectures, which tightly integrate accelerators with DRAM devices using 3D/2.5D-stacking technology, have been proposed. However, they have not prevailed yet because they often rely on expensive HBM/HMC-like DRAM devices which also suffer from limited capacity, whereas the scalability of memory capacity is critical for some computing segments such as servers. In this paper, we first demonstrate that data buffers in a load-reduced DIMM (LRDIMM), which is originally developed to support large memory systems for servers, are supreme places to integrate near-DRAM accelerators. Second, we propose Chameleon, an NDA architecture that can be realized without relying on 3D/2.5D-stacking technology and seamlessly integrated with large memory systems for servers. Third, we explore three microarchitectures that abate constraints imposed by taking LRDIMM architecture for NDA. Our experiment demonstrates that a Chameleon-based system can offer 2.13X higher geo-mean performance while consuming 34\% lower geo-mean data transfer energy than a system that integrates the same accelerator logic within the processor.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {50},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195701,
author = {Clemons, Jason and Cheng, Chih-Chi and Frosio, Iuri and Johnson, Daniel and Keckler, Stephen W.},
title = {A patch memory system for image processing and computer vision},
year = {2016},
publisher = {IEEE Press},
abstract = {From self-driving cars to high dynamic range (HDR) imaging, the demand for image-based applications is growing quickly. In mobile systems, these applications place particular strain on performance and energy efficiency. As traditional memory systems are optimized for 1D memory access, they are unable to efficiently exploit the multi-dimensional locality characteristics of image-based applications which often operate on sub-regions of 2D and 3D image data. We have developed a new Patch Memory System (PMEM) tailored to application domains that process 2D and 3D data streams. PMEM supports efficient multidimensional addressing, automatic handling of image boundaries, and efficient caching and prefetching of image data. In addition to an optimized cache, PMEM includes hardware for offloading structured address calculations from processing units. We improve average energy-delay by 26\% compared to EVA, a memory system for computer vision applications. Compared to a traditional cache, our results show that PMEM can reduce processor energy by 34\% for a selection of CV and IP applications, leading to system performance improvement of up to 32\% and energy-delay product improvement of 48--86\% on the applications in this study.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {51},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195702,
author = {Vasilyev, Artem and Bhagdikar, Nikhil and Pedram, Ardavan and Richardson, Stephen and Kvatinsky, Shahar and Horowitz, Mark},
title = {Evaluating programmable architectures for imaging and vision applications},
year = {2016},
publisher = {IEEE Press},
abstract = {Algorithms for computational imaging and computer vision are rapidly evolving, and hardware must follow suit: the next generation of image signal processors (ISPs) must be "programmable" to support new algorithms created with high-level frameworks. In this work, we compare flexible ISP architectures, using applications written in the Darkroom image processing language. We target two fundamental architecture classes: programmable in time, as represented by SIMD, and programmable in space, as typified by coarse grain reconfigurable array architectures (CGRA).We consider several optimizations on these two base architectures, such as register file partitioning for SIMD, bus based routing and pipelined wires for CGRA, and line buffer variations. After these optimizations on average, CGRA provides 1.6x better energy efficiency and 1.4x better compute density versus a SIMD solution, and 1.4x the energy efficiency and 3.1x the compute density of an FPGA. However the cost of providing general programmability is still high: compared to an ASIC, CGRA has 6x worse energy and area efficiency, and this ratio would be roughly 10x if memory dominated applications were excluded.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {52},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195703,
author = {Yan, Kaige and Zhang, Xingyao and Tan, Jingweijia and Fu, Xin},
title = {Redefining QoS and customizing the power management policy to satisfy individual mobile users},
year = {2016},
publisher = {IEEE Press},
abstract = {Delivering an excellent use experience to the customers is the top challenge faced by today's mobile device designers and producers. There have been multiple studies on achieving the good trade-offs between QoS and energy to enhance the user experience, however, they generally lack a comprehensive and accurate understanding of QoS, and ignore the fact that each individual user has his/her own preference between QoS and energy. In this study, we overcome these two drawbacks and propose a customized power management policy that dynamically configures the mobile platform to achieve the user-specific optimal QoS and energy trade-offs and hence, satisfy each individual mobile user. We first introduce a novel and comprehensive definition of QoS, and propose the accurate QoS measurement and management methodologies. We then observe that user's personality greatly determines his/her preferences between QoS and energy, and propose an online personality-guided user satisfaction prediction model based on the QoS and energy, guided by the user personality inferred from his/her device usage history. Our validation proves our model can achieve very high prediction accuracy. Finally, we propose our customized power management policy based on the prediction model for individual users. The experiment results show that our technique can improve the user experience by around 36\% compared with the state-of-the-art power management policies.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {53},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195704,
author = {Skarlatos, Dimitrios and Thomas, Renji and Agrawal, Aditya and Qin, Shibin and Pilawa-Podgurski, Robert and Karpuzcu, Ulya R. and Teodorescu, Radu and Kim, Nam Sung and Torrellas, Josep},
title = {Snatch: opportunistically reassigning power allocation between processor and memory in 3D stacks},
year = {2016},
publisher = {IEEE Press},
abstract = {The pin count largely determines the cost of a chip package, which is often comparable to the cost of a die. In 3D processor-memory designs, power and ground (P/G) pins can account for the majority of the pins. This is because packages include separate pins for the disjoint processor and memory power delivery networks (PDNs). Supporting separate PDNs and P/G pins for processor and memory is inefficient, as each set has to be provisioned for the worst-case power delivery requirements.In this paper, we propose to reduce the number of P/G pins of both processor and memory in a 3D design, and dynamically and opportunistically divert some power between the two PDNs on demand. To perform the power transfer, we use a small bidirectional on-chip voltage regulator that connects the two PDNs. Our concept, called Snatch, is effective. It allows the computer to execute code sections with high processor or memory power requirements without having to throttle performance. We evaluate Snatch with simulations of an 8-core multicore stacked with two memory dies. In a set of compute-intensive codes, the processor snatches memory power for 30\% of the time on average, speeding-up the codes by up to 23\% over advanced turbo-boosting; in memory-intensive codes, the memory snatches processor power. Alternatively, Snatch can reduce the package cost by about 30\%.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {54},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195705,
author = {Zu, Yazhou and Huang, Wei and Paul, Indrani and Reddi, Vijay Janapa},
title = {Ti-states: processor power management in the temperature inversion region},
year = {2016},
publisher = {IEEE Press},
abstract = {Temperature inversion is a transistor-level effect that can improve performance when temperature increases. It has largely been ignored in the past because it does not occur in the typical operating region of a processor, but temperature inversion is becoming increasing important in current and future technologies. In this paper, we study temperature inversion's implications on architecture design, and power and performance management. We present the first public comprehensive measurement-based analysis on the effects of temperature inversion on a real processor, using the AMD A10-8700P processor as our system under test. We show that the extra timing margin introduced by temperature inversion can provide more than 5\% Vdd reduction benefit, and this improvement increases to more than 8\% when operating in the near-threshold, low-voltage region. To harness this opportunity, we present Ti-states, a power management technique that sets the processor's voltage based on real-time silicon temperature to improve power efficiency. Ti-states lead to 6\% to 12\% measured power saving across a range of different temperatures compared to a fixed margin. As technology scales to FD-SOI and FinFET, we show there is an ideal operating temperature for various workloads to maximize the benefits of temperature inversion. The key is to counterbalance leakage power increase at higher temperatures with dynamic power reduction by the Ti-states. The projected optimal temperature is typically around 60°C and yields 8\% to 9\% chip power saving. The optimal high-temperature can be exploited to reduce design cost and runtime operating power for overall cooling. Our findings are important for power and thermal management in future chips and process technologies.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {55},
numpages = {13},
keywords = {power management, reliability, technology scaling, temperature inversion, timing margin},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195707,
author = {Ham, Tae Jun and Wu, Lisa and Sundaram, Narayanan and Satish, Nadathur and Martonosi, Margaret},
title = {Graphicionado: a high-performance and energy-efficient accelerator for graph analytics},
year = {2016},
publisher = {IEEE Press},
abstract = {Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76 − 6.54x speedup while consuming 50 − 100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {56},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195708,
author = {Tang, Xulong and Kandemir, Mahmut and Yedlapalli, Praveen and Kotra, Jagadish},
title = {Improving bank-level parallelism for irregular applications},
year = {2016},
publisher = {IEEE Press},
abstract = {Observing that large multithreaded applications with irregular data access patterns exhibit very low memory bank-level parallelism (BLP) during their execution, we propose a novel loop iteration scheduling strategy built upon the inspector-executor paradigm. A unique characteristic of this strategy is that it considers both bank-level parallelism (from an inter-core perspective) and bank reuse (from an intra-core perspective) in a unified framework. Its primary goal is to improve bank-level parallelism, and bank reuse is taken into account only if doing so does not hurt bank-level parallelism. Our experiments with this strategy using eight application programs on both a simulator and a real multicore system show an average BLP improvement of 46.8\% and an average execution time reduction of 18.3\%.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {57},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195709,
author = {Kolli, Aasheesh and Rosen, Jeff and Diestelhorst, Stephan and Saidi, Ali and Pelley, Steven and Liu, Sihang and Chen, Peter M. and Wenisch, Thomas F.},
title = {Delegated persist ordering},
year = {2016},
publisher = {IEEE Press},
abstract = {Systems featuring a load-store interface to persistent memory (PM) are expected soon, making in-memory persistent data structures feasible. Ensuring persistent data structure recoverability requires constraints on the order PM writes become persistent. But, current memory systems reorder writes, providing no such guarantees. To complement their upcoming 3D XPoint memory, Intel has announced new instructions to enable programmer control of data persistence. We describe the semantics implied by these instructions, an ordering model we call synchronous ordering.Synchronous ordering (SO) enforces order by stalling execution when PM write ordering is required, exposing PM write latency on the execution critical path. It incurs an average slowdown of 7.21x over volatile execution without ordering in PM-write-intensive benchmarks. SO tightly couples enforcing order and flushing writes to PM, but this tight coupling is unneeded in many recoverable software systems. Instead, we propose delegated ordering, wherein ordering requirements are communicated explicitly to the PM controller, fully decoupling PM write ordering from volatile execution and cache management. We demonstrate that delegated ordering can bring performance within 1.93x of volatile execution, improving over SO by 3.73x.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {58},
numpages = {13},
keywords = {delegated ordering, memory persistency, persistent memory, relaxed consistency},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195710,
author = {Sehatbakhsh, Nader and Nazari, Alireza and Zajic, Alenka and Prvulovic, Milos},
title = {Spectral profiling: observer-effect-free profiling by monitoring EM emanations},
year = {2016},
publisher = {IEEE Press},
abstract = {This paper presents Spectral Profiling, a new method for profiling program execution without instrumenting or otherwise affecting the profiled system. Spectral Profiling monitors EM emanations unintentionally produced by the profiled system, looking for spectral "spikes" produced by periodic program activity (e.g. loops). This allows Spectral Profiling to determine which parts of the program have executed at what time. By analyzing the frequency and shape of the spectral "spike", Spectral Profiling can obtain additional information such as the per-iteration execution time of a loop. The key advantage of Spectral Profiling is that it can monitor a system as-is, without program instrumentation, system activity, etc. associated with the profiling itself, i.e. it completely eliminates the "Observer's Effect" and allows profiling of programs whose execution is performance-dependent and/or programs that run on even the simplest embedded systems that have no resources or support for profiling. We evaluate the effectiveness of Spectral Profiling by applying it to several benchmarks from MiBench suite on a real system, and also on a cycle-accurate simulator. Our results confirm that Spectral Profiling yields useful information about the runtime behavior of a program, allowing Spectral Profiling to be used for profiling in systems where profiling infrastructure is not available, or where profiling overheads may perturb the results too much ("Observer's Effect").},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {59},
numpages = {11},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195711,
author = {Kim, Jinchun and Pugsley, Seth H. and Gratz, Paul V. and Reddy, A. L. Narasimha and Wilkerson, Chris and Chishti, Zeshan},
title = {Path confidence based lookahead prefetching},
year = {2016},
publisher = {IEEE Press},
abstract = {Designing prefetchers to maximize system performance often requires a delicate balance between coverage and accuracy. Achieving both high coverage and accuracy is particularly challenging in workloads with complex address patterns, which may require large amounts of history to accurately predict future addresses. This paper describes the Signature Path Prefetcher (SPP), which offers effective solutions for three classic challenges in prefetcher design. First, SPP uses a compressed history based scheme that accurately predicts complex address patterns. Second, unlike other history based algorithms, which miss out on many prefetching opportunities when address patterns make a transition between physical pages, SPP tracks complex patterns across physical page boundaries and continues prefetching as soon as they move to new pages. Finally, SPP uses the confidence it has in its predictions to adaptively throttle itself on a per-prefetch stream basis. In our analysis, we find that SPP improves performance by 27.2\% over a no-prefetching baseline, and outperforms the state-of-the-art Best Offset prefetcher by 6.4\%. SPP does this with minimal overhead, operating strictly in the physical address space, and without requiring any additional processor core state, such as the PC.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {60},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.5555/3195638.3195712,
author = {Hashemi, Milad and Mutlu, Onur and Patt, Yale N.},
title = {Continuous runahead: transparent hardware acceleration for memory intensive workloads},
year = {2016},
publisher = {IEEE Press},
abstract = {Runahead execution pre-executes the application's own code to generate new cache misses. This pre-execution results in prefetch requests that are overwhelmingly accurate (95\% in a realistic system configuration for the memory intensive SPEC CPU2006 benchmarks), much more so than a global history buffer (GHB) or stream prefetcher (by 13\%/19\%). However, we also find that current runahead techniques are very limited in coverage: they prefetch only a small fraction (13\%) of all runahead-reachable cache misses. This is because runahead intervals are short and limited by the duration of each full-window stall. In this work, we explore removing the constraints that lead to these short intervals. We dynamically filter the instruction stream to identify the chains of operations that cause the pipeline to stall. These operations are renamed to execute speculatively in a loop and are then migrated to a Continuous Runahead Engine (CRE), a shared multi-core accelerator located at the memory controller. The CRE runs ahead with the chain continuously, increasing prefetch coverage to 70\% of runahead-reachable cache misses. The result is a 43.3\% weighted speedup gain on a set of memory intensive quad-core workloads and a significant reduction in system energy consumption. This is a 21.9\% performance gain over the Runahead Buffer, a state-of-the-art runahead proposal and a 13.2\%/13.5\% gain over GHB/stream prefetching. When the CRE is combined with GHB prefetching, we observe a 23.5\% gain over a baseline with GHB prefetching alone.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {61},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

