@inproceedings{10.1109/MICRO61859.2024.00005,
author = {Johnson, Daniel},
title = {Message from the MICRO 2024 General Chairs: “Hi, How Are you?” - “Jeremiah The Innocent” Mural},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00005},
doi = {10.1109/MICRO61859.2024.00005},
abstract = {It is our pleasure and honor to welcome y'all to Austin for the 57th edition of the IEEE/ACM International Symposium on Microarchitecture 2024 (MICRO-57), the premier forum for presenting, discussing, and debating innovative microarchitecture ideas and techniques for advanced computing and communication systems. MICRO brings together researchers from around the world for technical exchange on traditional microarchitecture topics and emerging research areas in fields related to computer microarchitecture, compilers, computer chips, and computer systems.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {xxiii–xxv},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00006,
author = {Jim\'{e}nez, Daniel A. and Alameldeen, Alaa R.},
title = {Message from the MICRO 2024 Program Chairs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00006},
doi = {10.1109/MICRO61859.2024.00006},
abstract = {It is our pleasure to present the technical program for MICRO 2024, the 57th ACM/IEEE International Symposium on Microarchitecture. This year's program features a wide variety of exciting topics representing the state of the art in computer architecture research. We had a record 497 submissions, an increase of 15&amp; over last year. We accepted a record 113 papers. The reviewing process included 152 program committee (PC) members, 176 external review committee (ERC) members, and few other expert reviewers.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {xxvi–xxvii},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00010,
title = {Keynotes},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00010},
doi = {10.1109/MICRO61859.2024.00010},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {xxxvi–xxxvii},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00001,
title = {Title Page},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00001},
doi = {10.1109/MICRO61859.2024.00001},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1},
numpages = {1},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00002,
title = {Title Page},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00002},
doi = {10.1109/MICRO61859.2024.00002},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1},
numpages = {1},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00011,
author = {Xue, Yuqi and Liu, Yiqi and Nai, Lifeng and Huang, Jian},
title = {Hardware-Assisted Virtualization of Neural Processing Units for Cloud Platforms},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00011},
doi = {10.1109/MICRO61859.2024.00011},
abstract = {Cloud platforms today have been deploying hardware accelerators like neural processing units (NPUs) for powering machine learning (ML) inference services. To maximize the resource utilization while ensuring reasonable quality of service, a natural approach is to virtualize NPUs for efficient resource sharing for multi-tenant ML services. However, virtualizing NPUs for modern cloud platforms is not easy. This is not only due to the lack of system abstraction support for NPU hardware, but also due to the lack of architectural and ISA support for enabling fine-grained dynamic operator scheduling for virtualized NPUs. We present Neu10, a holistic NPU virtualization framework. We investigate virtualization techniques for NPUs across the entire software and hardware stack. Neul0 consists of (1) a flexible NPU abstraction called vNPU, which enables fine-grained virtualization of the heterogeneous compute units in a physical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go computing model and flexible vNPU-to-pNPU mappings for improved resource utilization and cost-effectiveness; (3) an ISA extension of modern NPU architecture for facilitating fine-grained tensor operator scheduling for multiple vNPUs. We implement Neu10 based on a production-level NPU simulator. Our experiments show that Neul0 improves the throughput of ML inference services by up to 1.4 × and reduces the tail latency by up to 4.6 ×, while improving the NPU utilization by 1.2 × on average, compared to state-of-the-art NPU sharing approaches.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1–16},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00012,
author = {Psomadakis, Stratos and Alverti, Chloe and Karakostas, Vasileios and Katsakioris, Christos and Siakavaras, Dimitrios and Nikas, Konstantinos and Goumas, Georgios and Koziris, Nectarios},
title = {Elastic Translations: Fast Virtual Memory with Multiple Translation Sizes},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00012},
doi = {10.1109/MICRO61859.2024.00012},
abstract = {Large pages have been the de facto mitigation technique to address the translation overheads of virtual memory, with prior work mostly focusing on the large page sizes supported by the x86 architecture, i.e., 2MiB and IGiB. ARMv8-A and RISC- V support additional intermediate translation sizes, i.e., 64KiB and 32MiB, via OS-assisted TLB coalescing, but their performance potential has largely fallen under the radar due to the limited system software support. In this paper, we propose Elastic Translations (ET), a holistic memory management solution, to fully explore and exploit the aforementioned translation sizes for both native and virtualized execution. ET implements mechanisms that make the OS memory manager coalescing- aware, enabling the transparent and efficient use of intermediate- sized translations. ET also employs policies to guide translation size selection at runtime using lightweight HW -assisted TLB miss sampling. We design and implement ET for ARMv8-A in Linux and KVM. Our real-system evaluation of ET shows that ET improves the performance of memory intensive workloads by up to 39 % in native execution and by 30 % on average in virtualized execution.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {17–35},
numpages = {19},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00013,
author = {Kwon, Osang and Lee, Yongho and Park, Junhyeok and Jang, Sungbin and Tak, Byungchul and Hong, Seokin},
title = {Distributed Page Table: Harnessing Physical Memory as an Unbounded Hashed Page Table},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00013},
doi = {10.1109/MICRO61859.2024.00013},
abstract = {Virtual memory systems rely on the page table, a crucial component that maps virtual addresses to physical addresses (i.e., address translation). While the Radix Page Table (RPT) has traditionally been used for this task, its limitations have become more apparent with the rise of memory-intensive applications. Recently, Hashed Page Tables (HPTs) have been explored as an alternative page table structure to offer faster address translation. However, the HPT introduces its own set of challenges particularly in resizing the page table and allocating contiguous physical memory space for storing the table. To tackle the fundamental problem of the existing HPT designs, this paper introduces Distributed Page Table (DPT), a novel approach that utilizes the physical memory as a huge hashed page table. DPT distributes Page Table Entries (PTEs) across the entire physical memory space, significantly reducing the hash collisions while avoiding the table resizing overheads. When distributing the PTEs across the physical memory, they can be mapped to memory locations already allocated to data pages. This new type of collision, referred to as address collision, may reduce the effectiveness of the DPT. This paper showcases that the DPT can effectively resolve the address collision with three simple yet efficient techniques: Strided Open Addressing (SOA), Collision-Aware Virtual Address Allocation (CVA) and Collided Page Displacement (CPD). Our experimental results demonstrate that DPT achieves average performance improvements of 12.6%, 11.6%, and 8.7% compared to traditional RPT, the latest large-coverage TLB design, and state-of-the-art HPTs, respectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {36–49},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00014,
author = {Im, Dongseok and Yoo, Hoi-Jun},
title = {CamPU: A Multi-Camera Processing Unit for Deep Learning-based 3D Spatial Computing Systems},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00014},
doi = {10.1109/MICRO61859.2024.00014},
abstract = {A 3D spatial computing system that understands a surrounding environment and interacts with real-world objects has emerged with the development of deep learning technologies. A multi-camera system captures a surrounding view of a scene using multiple cameras, and a deep neural network (DNN) system extracts semantic features from multi-camera images and provides useful information to users. However, processing a multi-camera system requires massive memory accesses as the number of cameras increases while processing a DNN system can improve throughput by exploiting batch processing. This performance gap limits the overall performance of 3D spatial computing systems. To solve this problem, a multi-camera processing unit (CamPU) is proposed. CamPU exploits the inter- and intra-data reuse methods on multi-camera images, minimizing memory accesses for image projection. Moreover, the out-of-order image projection unit with cache memory is designed to increase multi-image projection throughput by avoiding redundant cache accesses and hiding the latency of high-level memory accesses. Lastly, the overlap-aware blending unit speeds up image blending by efficiently handling overlapping regions between adjacent images. The CamPU architecture is evaluated through RTL-level simulation, and the CamPU-integrated DNN platform provides a comprehensive analysis of end-to-end multi-camera deep learning-based 3D spatial systems. Finally, CamPU speedups the overall system performance 2.9 x faster than an NVIDIA RTX2080Ti GPU platform.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {50–63},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00015,
author = {Yoo, Seungjae and Kim, Hangyeol and Kim, Joo-Young},
title = {AdapTiV: Sign-Similarity Based Image-Adaptive Token Merging for Vision Transformer Acceleration},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00015},
doi = {10.1109/MICRO61859.2024.00015},
abstract = {The advent of Vision Transformers (ViT) has set a new performance leap in computer vision by leveraging self-attention mechanisms. However, the computational efficiency of ViTs is limited by the quadratic complexity of self-attention and redundancy among image tokens. To address these issues, token merging strategies have been explored to reduce input size by merging similar tokens. Nonetheless, implementing token merging presents a degradation of latency performance due to its two factors: inefficient computations and fixed merge rate nature. This paper introduces AdapTiV, a novel hardware-software co-designed accelerator that accelerates ViTs through image-adaptive token merging, effectively addressing the afore-mentioned challenges. Under the design philosophy of reducing the overhead of token merging and concealing its latency within the Layer Normalization (LN) process, AdapTiV incorporates algorithmic innovations such as Local Matching, which restricts the search space for token merging, thereby reducing the computational complexity; Sign Similarity, which simplifies the calculation of similarity between tokens; and Dynamic Merge Rate, which enables image-adaptive token merging. Additionally, the hardware component that supports AdapTiV's algorithms, named the Adaptive Token Merging Engine, employs Sign-Driven Scheduling to conceal the overhead of token merging effectively. This engine integrates submodules such as a Sign Similarity Computing Unit, which calculates the similarity between tokens using a newly introduced similarity metric; a Sign Scratchpad, which is a lightweight, image-width-sized memory that stores previous tokens; a Sign Scratchpad Managing Unit, which controls the Sign Scratchpad; and a Token Integration Map to facilitate efficient, image-adaptive token merging. Our evaluations demonstrate that AdapTiV achieves, on average, 309.4 ×, 18.4×, 89.8×, 6.3× speedups and 262.1×, 21.5×, 496.6×, 11.2× improvements in energy efficiency over edge CPUs, edge GPUs, server CPUs, and server GPUs, while maintaining an accuracy loss below 1 % without additional training.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {64–77},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00016,
author = {Li, Sixu and Zhao, Yang and Li, Chaojian and Guo, Bowei and Zhang, Jingqun and Zhu, Wenbo and Ye, Zhifan and Wan, Cheng and Lin, Yingyan Celine},
title = {Fusion-3D: Integrated Acceleration for Instant 3D Reconstruction and Real-Time Rendering},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00016},
doi = {10.1109/MICRO61859.2024.00016},
abstract = {Recent breakthroughs in Neural Radiance Field (NeRF) based 3D reconstruction and rendering have spurred the possibility of immersive experiences in augmented and virtual reality (AR/VR). However, current NeRF acceleration techniques are still inadequate for real-world AR/VR applications due to: 1) the lack of end-to-end pipeline acceleration support, which causes impractical off-chip bandwidth demands for edge devices, and 2) limited scalability in handling large-scale scenes. To tackle these limitations, we have developed an end-to-end, scalable 3D acceleration framework called Fusion-3D, capable of instant scene reconstruction and real-time rendering. Fusion-3D achieves these goals through two key innovations: 1) an optimized end-to-end processor for all three stages of the NeRF pipeline, featuring dynamic scheduling and hardware-aware sampling in the first stage, and a shared, reconfigurable pipeline with mixed-precision arithmetic in the second and third stages; 2) a multi-chip architecture for handling large-scale scenes, integrating a three-level hierarchical tiling scheme that minimizes inter-chip communication and balances workloads across chips. Extensive experiments validate the effectiveness of Fusion-3D in facilitating real-time, energy-efficient 3D reconstruction and rendering. Specifically, we tape out a prototype chip in 28nm CMOS to evaluate the effectiveness of the proposed end-to-end processor. Extensive simulation based on the on-silicon measurements demonstrates a &lt;tex&gt;$mathbf{2.5}times$&lt;/tex&gt; and &lt;tex&gt;$mathbf{6}times$&lt;/tex&gt; throughput improvement in training and inference, respectively, compared to state-of-the-art accelerators. Furthermore, to assess the multi-chip architecture, we integrate four chips into a single PCB as a prototype. Further simulation results show that the multi-chip system achieves a &lt;tex&gt;$mathbf{7.3}times$&lt;/tex&gt; and &lt;tex&gt;$mathbf{6.5}times$&lt;/tex&gt; throughput improvement in training and inference, respectively, over the Nvidia 2080Ti GPU. To the best of our knowledge, Fusion-3D is the first to achieve both instant (≤ 2 seconds) 3D reconstruction and real-time (≥ 30 FPS) rendering, while only requiring the bandwidth of the most commonly used USB port (0.625 GB/s, 5 Gbps) in edge devices for off-chip communication.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {78–91},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00017,
author = {Nath, Sumon and Navarro-Torres, Agustin and Ros, Alberto and Panda, Biswabandan},
title = {Secure Prefetching for Secure Cache Systems},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00017},
doi = {10.1109/MICRO61859.2024.00017},
abstract = {Transient execution attacks like Spectre and its vari-ants can cause information leakage through a cache hierarchy. There are two classes of techniques that mitigate speculative execution attacks: delay-based and invisible speculation. Invisible speculation-based techniques like GhostMinion are the high-performing yet secure techniques that mitigate all kinds of spec-ulative execution attacks. Similar to a cache system, hardware prefetchers can also cause speculative information leakage. To mitigate it, GhostMinion advocates on-commit prefetching on top of strictness ordering in the cache system. Our experiments show that the GhostMinion cache system interacts negatively with the hardware prefetchers leading to redundant traffic between different levels of cache. This traffic causes contention and increases the miss latency leading to performance loss. Next, we observe that on-commit prefetching enforced by GhostMinion leads to nerformance loss as it affects the prefetcher timeliness. We perform the first thorough analysis of the interaction between state-of-the-art prefetching techniques and the secure cache system. Based on this, we propose two microarchitectural solutions that ensure high performance while designing secure prefetchers on top of secure cache system. The first solution detects and filters redundant traffic when updating the cache hierarchy non-speculatively. The second solution ensures the timeliness of the prefetcher to compensate for the delayed triggering of prefetch requests at commit, resulting in a secure yet high-performing prefetcher. Overall, our enhancements are secure and provide synergistic interactions between hardware prefetchers and a secure cache system. Our experiments show that our filter consistently improves the performance of secure cache systems like GhostMinion in the presence of state-of-the-art prefetchers (by 1.9% for single-core and 19.0% for multi-core for the top-performing prefetcher). We see a synergistic behavior of the filter with our proposed secure prefetcher, which leads to a further increase in performance by 6.3% and 23.0% (over the top-performing prefetcher), for single-core and multi-core systems, respectively. Our enhancements are extremely lightweight incurring a storage overhead of 0.59 KB per core.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {92–104},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00018,
author = {Bai, Yunkai and Li, Peinan and Huang, Yubiao and Huang, Michael C. and Zhao, Shijun and Zhao, Lutan and Zhang, Fengwei and Meng, Dan and Hou, Rui},
title = {HyperTEE: A Decoupled TEE Architecture with Secure Enclave Management},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00018},
doi = {10.1109/MICRO61859.2024.00018},
abstract = {Trusted Execution Environment (TEE) architectures have been deployed in various commercial processors to provide secure environments for confidential programs and data. However, as a relatively new feature against security threats, existing designs still face a number of problems. Exploiting the management vulnerabilities, attackers can disclose secrets via controlled-channel or micro-architecture side-channel attacks. To address these problems, this paper proposes a novel TEE architecture, named HyperTEE. In our architecture, enclave management tasks are decoupled from the original computing subsystem to a dedicated, physically isolated Enclave Manage-ment Subsystem (EMS). A properly architected EMS prevents current management vulnerabilities and offers more secure enclave communication. We implemented the HyperTEE prototype on the FPGA platform. Experiments show that HyperTEE only introduces less than 1% area overhead, and 2.0 % and 1.9 % performance overhead on average for enclaves and non-enclave workloads, respectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {105–120},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00019,
author = {Choi, Jaeseok and Joe, Hyunwoo and Jung, Changhee and Choi, Jongouk},
title = {Defending Against EMI Attacks on Just-In-Time Checkpoint for Resilient Intermittent Systems},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00019},
doi = {10.1109/MICRO61859.2024.00019},
abstract = {Energy harvesting systems have emerged as an alternative to battery-powered IoT devices. The systems utilize a just-in-time checkpoint protocol that stores volatile states when a power outage occurs, ensuring crash consistency. However, this paper uncovers a new security vulnerability in the checkpoint protocol, revealing its susceptibility to electromagnetic interference (EMI). If exploited, adversaries could cause denial of service or data corruption in victim devices. To defeat EMI attacks, this paper introduces GECKO, a compiler-directed countermeasure that operates on commodity platforms used in energy harvesting systems without requiring hardware support. Our experiments on real boards demonstrate that GECKO defeats the EMI attack with a trivial performance overhead by 6% on average.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {121–135},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00020,
author = {Esmaili-Dokht, Pouya and Sgherzi, Francesco and Girelli, Val\'{e}ria Soldera and Boixaderas, Isaac and Carmin, Mariana and Monemi, Alireza and Armejach, Adri\`{a} and Mercadal, Estanislao and Llort, Germ\'{a}n and Radojkovi\'{c}, Petar and Moreto, Miquel and Gim\'{e}nez, Judit and Martorell, Xavier and Ayguad\'{e}, Eduard and Labarta, Jesus and Confalonieri, Emanuele and Dubey, Rishabh and Adlard, Jason},
title = {A Mess of Memory System Benchmarking, Simulation and Application Profiling},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00020},
doi = {10.1109/MICRO61859.2024.00020},
abstract = {The Memory stress (Mess) framework provides a unified view of the memory system benchmarking, simulation and application profiling. The Mess benchmark provides a holistic and detailed memory system characterization. It is based on hundreds of measurements that are represented as a family of bandwidth-latency curves. The benchmark increases the coverage of all the previous tools and leads to new findings in the behavior of the actual and simulated memory systems. We deploy the Mess benchmark to characterize Intel, AMD, IBM, Fujitsu, Amazon and NVIDIA servers with DDR4, DDR5, HBM2 and HBM2E memory. The Mess memory simulator uses bandwidth-latency concept for the memory performance simulation. We integrate Mess with widely-used CPUs simulators enabling modeling of all high-end memory technologies. The Mess simulator is fast, easy to integrate and it closely matches the actual system performance. By design, it enables a quick adoption of new memory technologies in hardware simulators. Finally, the Mess application profiling positions the application in the bandwidth-latency space of the target memory system. This information can be correlated with other application runtime activities and the source code, leading to a better overall understanding of the application's behavior. The current Mess benchmark release covers all major CPU and GPU ISAs, x86, ARM, Power, RISC-V, and NVIDIA's PTX. We also release as open source the ZSim, gem5 and OpenPiton Metro-MPI integrated with the Mess simulator for DDR4, DDR5, Optane, HBM2, HBM2E and CXL memory expanders. The Mess application profiling is already integrated into a suite of production HPC performance analysis tools.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {136–152},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00021,
author = {Bang, Jehyeon and Choi, Yujeong and Kim, Myeongwoo and Kim, Yongdeok and Rhu, Minsoo},
title = {vTrain: A Simulation Framework for Evaluating Cost-Effective and Compute-Optimal Large Language Model Training},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00021},
doi = {10.1109/MICRO61859.2024.00021},
abstract = {As large language models (LLMs) become widespread in various application domains, a critical challenge the AI community is facing is how to train these large AI models in a cost-effective manner. Existing LLM training plans typically employ a heuristic based parallel training strategy which is based on empirical observations rather than grounded upon a thorough examination of the search space of LLM parallelization. Such limitation renders existing systems to leave significant performance left on the table, wasting millions of dollars worth of training cost. This paper presents our profiling-driven simulator called vTrain, providing AI practitioners a fast yet accurate software framework to determine an efficient and cost-effective LLM training system configuration. We demonstrate vTrain's practicality through several case studies, e.g., effectively evaluating optimal training parallelization strategies that balances training time and its associated training cost, efficient multi-tenant GPU cluster schedulers targeting multiple LLM training jobs, and determining a compute-optimal LLM model architecture given a fixed compute budget.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {153–167},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00022,
author = {Yang, Jianchao and Wen, Mei and Chen, Dong and Chen, Zhaoyun and Xue, Zeyu and Li, Yuhang and Shen, Junzhong and Shi, Yang},
title = {HyFiSS: A Hybrid Fidelity Stall-Aware Simulator for GPGPUs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00022},
doi = {10.1109/MICRO61859.2024.00022},
abstract = {The widespread adoption of GPUs has driven the development of GPU simulators, which, in turn, lead advancements in both GPU architectures and software optimization. Trace-driven cycle-accurate Cycle-accurate simulators, which provide detailed microarchitectural models and clock-level precision, come at the cost of extended simulation times and require high computational resources. Their scalability has become a bottleneck. A growing trend is the adoption of cycle-approximate simulators, which introduce mathematical modeling of partial hardware units and utilize sampling to accelerate simulation. However, this approach faces challenges regarding the accuracy of performance predictions. To address these limitations, we introduce HyFiSS, a hybrid fidelity stall-aware GPU simulator. HyFiSS features fine-grained stall events tracking and attribution by constructing a detailed execution pipeline model for various stall events on Streaming Multiprocessors (SMs). It accurately emulates the thread block scheduler behavior using real-time scheduling logs and utilizes sampling based on thread block sets to minimize the precision loss due to fine-grained sampling points on the microarchitectural state. We achieve a balance between reliability, speed, and the level of simulation detail, especially regarding bottlenecks. By evaluating a diverse set of benchmarks, HyFiSS achieves a mean absolute percentage error in predicting active cycles that is comparable to the state-of-the-art cycle-accurate simulator Accel-Sim. Moreover, HyFiSS achieves a substantial 12.8 × speedup in the simulation efficiency compared to Accel-Sim. HyFiSS also requires at least 3.2 × less disk storage than both Accel-Sim and another state-of-the-art cycle-approximate simulator PPT-GPU due to its efficient SASS (Streaming Assembler) traces compression. With precise, per-cycle stall events statistics, HyFiSS can provide accurate GPU performance metrics and stall cause reporting. This significantly simplifies performance analysis, bottleneck identification, and performance optimization tasks for researchers, making it easier to enhance GPU performance effectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {168–185},
numpages = {18},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00023,
author = {Han, Ruobing and Zhao, Jisheng and Kim, Hyesoon},
title = {Unleashing CPU Potential for Executing GPU Programs Through Compiler/Runtime Optimizations},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00023},
doi = {10.1109/MICRO61859.2024.00023},
abstract = {Although modern CPUs deliver high performance, they are often under-utilized in CPU-GPU heterogeneous systems. Enabling CPUs to execute GPU programs facilitates workload sharing between CPUs and GPUs, which can increase CPU device utilization and overall benefit heterogeneous systems. The flat collapsing transformation represents a state-of-the-art solution for GPU-to-CPU migration and is widely accepted in both academic and commercial projects. However, in this paper, we identify that CPU programs transformed by flat collapsing transformation are not compatible with standard CPU compiler optimizations and runtime environments, which leads to suboptimal performance. Based on the observation, we propose four compiler/runtime optimizations. These optimizations complement flat collapsing transformation and help generate high-performance programs. Our evaluations demonstrate an average performance improvement of 20.84% over the state-of-the-art framework on an x86 CPU and 16.10% on an ARM CPU.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {186–200},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00024,
author = {Chen, Yishen and Amarasinghe, Saman},
title = {A Framework for Fine-Grained Program Versioning},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00024},
doi = {10.1109/MICRO61859.2024.00024},
abstract = {Static dependence analysis is critical for optimizations such as vectorization and loop-invariant code motion. However, traditional static dependence analysis is often imprecise, making these optimizations less effective. To address this issue, production compilers use loop versioning to rule out some categories of memory dependencies at run time. However, loop versioning is loop-centric and usually tied to specific optimizations (e.g., loop vectorization), making it less effective for non-loop optimizations such as superword-level parallelism (SLP) vectorization. In this paper, we propose a fine-grained versioning framework to rule out program dependencies at run time. Our framework is general and not tailored to any specific optimizations. To use our system, a client optimization specifies groups of instructions (or loops) whose independence is desired but unprovable statically. In response, our system duplicates the appropriate instructions and guards the original ones with run-time checks to guarantee their independence; if the checks fail, the duplicated instructions execute instead. In a case study, we extended an existing SLP vectorizer with minimal modifications using our framework, resulting in a 1.17× speedup over Clang's vectorizers on TSVC and a 1.51× speedup on PolyBench. In both benchmarks, we encountered programs that could not be vectorized with loop versioning alone. In a second case study, we used our framework to implement a more aggressive variant of redundant load elimination than the one implemented by Clang. Our redundant load elimination results in a 1.012× speedup on the SPEC 2017 Floating Point benchmarks, with the maximum speedup being 1.064×.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {201–214},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00025,
author = {Zhou, Yuchen and Zeng, Jianping and Jung, Changhee},
title = {LightWSP: Whole-System Persistence on the Cheap},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00025},
doi = {10.1109/MICRO61859.2024.00025},
abstract = {Whole-system persistence (WSP) has recently attracted more interest thanks to its transparency and performance benefits over partial-system persistence where users are not only burdened by complex persistent programming but also incapable of using DRAM as LLC. Nevertheless, existing WSP work either introduces high hardware cost or causes non-trivial performance overhead. To this end, this paper presents LightWSP, a compiler/architecture co-design scheme that can achieve WSP in a lightweight yet performant manner. LightWSP compiler partitions program into a series of recoverable regions (epochs) with their live-out registers checkpointed, while LightWSP hardware persists the stores of the regions-whose boundary serves as a power failure recovery point-enforcing crash consistency; LightWSP leverages the battery-backed write pending queue (WPQ) of a memory controller as a redo buffer, i.e., all stores are first buffered in WPQ and then persisted together in non-volatile memory (NVM) at each region end. In this way, no matter when power failure happens, NVM is never corrupted by the stores of the power-interrupted region, facilitating correct recovery. In particular, LightWSP supports multiple memory controllers on the cheap without costly speculation/misspeculation handling mechanisms used by prior work. The experimental results with 38 applications show that LightWSP incurs only an average of 9.0% run-time overhead. This is on par with the state-of-the-art work, that complicates the core microarchitecture significantly with its intrusive design for memory controller speculation, yet the hardware cost of LightWSP is near zero (0.5B per core).},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {215–230},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00026,
author = {Deutsch, Peter W. and Ulitzsch, Vincent Quentin and Gurumurthi, Sudhanva and Sridharan, Vilas and Emer, Joel S. and Yan, Mengjia},
title = {DelayAVF: Calculating Architectural Vulnerability Factors for Delay Faults},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00026},
doi = {10.1109/MICRO61859.2024.00026},
abstract = {Reliability is a key design consideration for modern microprocessors. A surge of reports from major cloud vendors describing new silent data corruption (SDC) behaviours at scale suggest a recent change in the nature of faults in the wild. Recent publications have suggested that one root cause of these SDCs may be small delay faults (SDFs) induced by marginal defects that increase a circuit's propagation time by a small (sub-cycle) delay. Reasoning about the effects of these faults early in the design of a processor is thus of increasing importance for reliability at-scale. Computer architects currently reason about the resilience of microarchitectures against particle strike induced faults using Architectural Vulnerability Factor (AVF) which describes the probability that a particle strike impacting a particular microar-chitectural structure results in a program-visible failure. In this paper, we develop an AVF-like metric to quantify a processor's vulnerability to SDFs. We conduct a systematic analysis of the potential impacts of SDFs and determine that particle strike AVF is insufficient to reason about SDFs. Considering SDFs requires additional reasoning about the timing characteristics of a circuit, the state element(s) that experience an error due to a fault, and whether the resulting state element errors cause a program-visible failure. In this paper we present DelayAVF, a metric that quantifies microarchitectural vulnerability to small delay faults. We develop a two-step methodology to analyze the DelayAVF of a hardware design. We then analyze the DelayAVF of an open-source RISC-V core, finding new architectural reliability insights that do not present themselves through traditional AVF analysis. Finally, we provide approximations for DelayAVF that allow for the reuse of particle strike AVF data (for instance, from existing fault injection studies).},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {231–245},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00027,
author = {Manzhosov, Evgeny and Sethumadhavan, Simha},
title = {Polymorphic Error Correction},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00027},
doi = {10.1109/MICRO61859.2024.00027},
abstract = {In this paper, we propose a new memory error correction scheme, Polymorphic ECC, based on a novel idea of redundancy polymorphism for error correction. With redundancy polymorphism, we can use the check bits, i.e., parity bits in traditional ECC, to correct errors from different fault models. For example, the error correction procedure will use the same redundancy value for single-bit errors, double-bit errors, ChipKill, and others. As a result, Polymorphic ECC corrects more errors than traditional codes, which typically target a single fault model or require multiple redundancies for multi-fault model support, leading to higher storage overheads. Our construction is very compact, allowing us to embed an inlined cryptographic message authentication code (MAC) with each cacheline, ensuring data integrity and near 100% error detection without needing any extra storage. The MAC, further permits iterative correction among the many supported fault models. In the paper, we show that the novel combination of redundancy polymorphism with iterative correction, corrects errors due to fault models not covered by traditional codes and guarantees data integrity with up to 60-bit MACs while using 64-byte cachelines and standard 40-bit DDR5 memory channels.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {246–262},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00028,
author = {Zhou, Heng and Wu, Bing and Cheng, Huan and Liu, Jinpeng and Lei, Taoming and Feng, Dan and Tong, Wei},
title = {DRCTL: A Disorder-Resistant Computation Translation Layer Enhancing the Lifetime and Performance of Memristive CIM Architecture},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00028},
doi = {10.1109/MICRO61859.2024.00028},
abstract = {The memristive Computing-in-Memory (CIM) sys-tem can efficiently accelerate matrix-vector multiplication (MVM) operations through in-situ computing. The data layout has a significant impact on the communication performance of CIM systems. Existing software-level communication opti-mizations aim to reduce communication distance by carefully designing static data layouts, while wear-leveling (WL) and error mitigation methods use dynamic scheduling to enhance system reliability, resulting in randomized data layouts and increased communication overhead. Besides, existing CIM compilers di-rectly map data to physical crossbars and generate instructions, which causes inconvenience for dynamic scheduling. To address these challenges of balancing communication performance and reliability while coordinating existing CIM compilers and dy-namic scheduling, we propose a disorder-resistant computation translation layer (DRCTL), which improves system lifetime and communication performance through co-optimization of data layout and dynamic scheduling. It consists of three parts: (1) We propose an address conversion method for dynamic scheduling, which updates the addresses in the instruction stream after dynamic scheduling, thereby avoiding recompilation. (2) Dynamic scheduling strategy for reliability improvement. We propose a hierarchical wear-leveling (HWL) strategy, which reduces communication by increasing scheduling granularity. (3) Communication optimization for dynamic scheduling. We propose data layout-aware selective remapping (LASR), which helps dynamic scheduling methods improve communication lo-cality and reduce latency by exploiting data dependencies. The experiments demonstrate that HWL extends lifetime by 100.3-205.9 x compared to not using WL. Even with a slight lifetime decrease compared to the state-of-the-art WL (TIWL), it still supports continuous neural network training for 7 years. After applying LASR to HWL, the number of execution cycles, energy consumption, on-chip and off-chip NoC accesses decrease by an average of 26.91 %, 26.88%, 36.41 %, and 80.62%, respectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {263–277},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00029,
author = {Park, Junhyeok and Kwon, Osang and Lee, Yongho and Kim, Seongwook and Byeon, Gwangeun and Yoon, Jihun and Nair, Prashant J. and Hong, Seokin},
title = {A Case for Speculative Address Translation with Rapid Validation for GPUs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00029},
doi = {10.1109/MICRO61859.2024.00029},
abstract = {A unified address space is vital for heterogeneous systems as it enables efficient data sharing between CPUs and GPUs. However, GPU address translation faces challenges due to high TLB pressure, particularly with irregular and memory-intensive applications. Compared to an ideal scenario, we observe that address translation overheads cause a slowdown of up to 34.5% in modern heterogeneous systems. This paper introduces Avatar, a novel framework to accelerate address translation in GPUs. Avatar comprises two key components: Contiguity-Aware Speculative Translation (CAST) and In-Cache Validation (CAVA) mechanisms. Avatar identifies the potential for predicting virtual-to-physical address mapping by monitoring contiguous pages that lie in both virtual and physical address spaces. Leveraging this insight, CAST speculatively translates virtual addresses into physical addresses. This speculative address translation enables immediate data fetching into GPUs while addressing translation occurs in the background, reducing TLB-miss overhead. Unfortunately, modern GPUs lack support for speculative execution, which limits CAST's performance gain. Data fetched from speculated physical addresses is unusable until validation. CAVA addresses this limitation by quickly validating speculated physical addresses. To this end, CAVA embeds page mapping information into each 32B sector of 128B cache lines. Thus, CAVA enables fetching a sector block from memory for a speculated address and rapidly validating the speculative translation using the embedded mapping information. Our experiments show that Avatar achieves a 90.3% (high) speculation accuracy and improves GPU performance by 37.2% (on average).},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {278–292},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00030,
author = {B, Pratheek and Cox, Guilherme and Vesely, Jan and Basu, Arkaprava},
title = {SUV: Static Analysis Guided Unified Virtual Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00030},
doi = {10.1109/MICRO61859.2024.00030},
abstract = {Unified Virtual Memory (UVM) eases GPU programming and enables oversubscription of the limited GPU memory capacity. Unfortunately, UVM may cause significant slowdowns due to thrashing of the GPU memory and overheads of page faults, especially under memory oversubscription. We propose to leverage high-level memory access patterns of CUDA applications to reduce the overheads of UVM. We create SUV, a hybrid framework that leverages compiler-inferred (static analysis) memory access semantics to make proactive memory management decisions where possible and selectively leverages runtime page migrations where needed. It can pin data structures entirely or partially on the GPU memory or CPU's DRAM based on their inferred usefulness and automatically issue software prefetches at kernel boundaries. It also selectively lets parts of data structures migrate on demand onto reserved HBM capacity at runtime. SUV reduces execution times of a variety of applications by 74% over UVM under memory oversubscription.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {293–308},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00031,
author = {Li, Bingyao and Wang, Yueqi and Wang, Tianyu and Eeckhout, Lieven and Yang, Jun and Jaleel, Aamer and Tang, Xulong},
title = {STAR: Sub-Entry Sharing-Aware TLB for Multi-Instance GPU},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00031},
doi = {10.1109/MICRO61859.2024.00031},
abstract = {NVIDIA's Multi-Instance GPU (MIG) technology enables partitioning GPU computing power and memory into sep-arate hardware instances, providing complete isolation including compute resources, caches, and memory. However, prior work identifies that MIG does not partition the last-level TLB (i.e., L3 TLB), which remains shared among all instances. To enhance TLB reach, NVIDIA GPUs reorganized the TLB structure with 16 sub-entries in each L3 TLB entry that have a one-to-one mapping to the address translations for 16 pages of size 64 KB located within the same 1 MB aligned range. Our comprehensive investigation of address translation efficiency in MIG identifies two main issues caused by L3 TLB sharing interference: (i) it results in performance degradation for co-running applications, and (ii) TLB sub-entries are not fully utilized before eviction. Based on this observation, we propose STAR to improve the utilization of TLB sub-entries through dynamic sharing of TLB entries across multiple base addresses. STAR evaluates TLB entries based on their sub-entry utilization to optimize address translation storage, dynamically adjusting between a shared and non-shared state to cater to current demand. We show that STAR improves overall performance by an average of 28.7% across various multi-tenant workloads.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {309–323},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00032,
author = {Park, Soyoung and Namkoong, Hojung and Choi, Boyeol and Sullivan, Michael B. and Kim, Jungrae},
title = {CacheCraft: Enhancing GPU Performance under Memory Protection through Reconstructed Caching},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00032},
doi = {10.1109/MICRO61859.2024.00032},
abstract = {Contemporary GPUs use Error Correcting Codes (ECC) to protect against memory errors. GPUs with Graphics DDR (GDDR) utilize in-band ECC (a.k.a. inline ECC), which sequentially accesses data and redundancy to enable ECC functionality using non-ECC memory chips. However, the additional access reduces data throughput and can incur significant performance penalties for bandwidth-intensive applications. This paper introduces CacheCraft, a novel GPU micro-architecture engineered to address the inefficiencies of current in-band ECC protection. It reconfigures the traditional 128B cache line from four 32B sectors into four 30B sectors and one 8B sector. This adjustment creates a 2B space in each 32B memory chunk, designated for storing the redundancy of the sector data, thereby enabling a single memory access to deliver reliable data. Our evaluation shows that this single-access in-band ECC can significantly mitigate the bandwidth penalty of memory protection. While traditional in-band ECC increases memory access by 41.9% (peaking at 96.9%), CacheCraft reduces this extra bandwidth requirement to 21.9 % (peaking at 28.2 %). This significant reduction (47.8 % on average and up to 89.4 %) can substantially enhance the performance of memory-intensive applications by as much as 23.5 %.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {324–337},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00033,
author = {Deng, Xianglong and Fan, Shengyu and Hu, Zhicheng and Tian, Zhuoyu and Yang, Zihao and Yu, Jiangrui and Cao, Dingyuan and Meng, Dan and Hou, Rui and Li, Meng and Lou, Qian and Zhang, Mingzhe},
title = {Trinity: A General Purpose FHE Accelerator},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00033},
doi = {10.1109/MICRO61859.2024.00033},
abstract = {Fully Homomorphic Encryption (FHE) is crucial for privacy-preserving computing, which allows direct computation on encrypted data. While various FHE schemes have been proposed, none of them efficiently support both arithmetic FHE and logic FHE simultaneously. To address this issue, researchers explore the combination of different FHE schemes within a single application and propose algorithms for the conversion between them. Unfortunately, all prior ASIC-based FHE accelerators are designed to support a single FHE scheme, and none of them supports the acceleration for FHE scheme conversion. This necessitates FHE acceleration systems to integrate multiple accelerators for different schemes, leading to increased system complexity and hindering performance enhancement. In this paper, we present the first multi-modal FHE accelerator based on a unified architecture, which efficiently supports CKKS, TFHE, and their conversion scheme within a single accelerator. To achieve this goal, we first analyze the theoretical foundations of the aforementioned schemes and highlight their composition from a finite number of arithmetic kernels. Then, we investigate the challenges for efficiently supporting these kernels within a unified architecture, which include 1) concurrent support for NTT and FFT, 2) maintaining high hardware utilization across various polynomial lengths, and 3) ensuring consistent performance across diverse arithmetic kernels. To tackle these challenges, we propose a novel FHE accelerator named Trinity, which in-corporates algorithm optimizations, hardware component reuse, and dynamic workload scheduling to enhance the acceleration of CKKS, TFHE, and their conversion scheme. By adaptive select the proper allocation of components for NTT and MAC, Trinity maintains high utilization across NTTs with various polynomial lengths and imbalanced arithmetic workloads. The experiment results show that, for the pure CKKS and TFHE workloads, the performance of our Trinity outperforms the state-of-the- art accelerator for CKKS (SHARP) and TFHE (Morphling) by 1.49 x and 4.23 x, respectively. Moreover, Trinity achieves 919.3 x performance improvement for the FHE-conversion scheme over the CPU-based implementation. Notably, despite the performance improvement, the hardware overhead of Trinity is only 85 % of the summed circuit areas of SHARP and Morphling.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {338–351},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00034,
author = {Zhou, Minxuan and Nam, Yujin and Wang, Xuan and Lee, Youhak and Wilkerson, Chris and Kumar, Raghavan and Taneja, Sachin and Mathew, Sanu and Cammarota, Rosario and Rosing, Tajana},
title = {UFC: A Unified Accelerator for Fully Homomorphic Encryption},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00034},
doi = {10.1109/MICRO61859.2024.00034},
abstract = {Fully homomorphic encryption (FHE) is crucial for post-quantum privacy-preserving computing. Researchers have proposed various FHE schemes that excel at different encrypted computations, such as single-instruction multiple-data (SIMD) arithmetic or arbitrary single-data functions. Hybrid-scheme FHE, which exploits appropriate schemes for specific tasks, is essential for real-world applications requiring optimal performance and accuracy. However, existing FHE accelerators only adopt scheme-specific custom designs, leading to inefficiency or lack of capability to support applications in hybrid FHE settings. In this work, we propose a Unified FHE aCcelerator (UFC) that provides better performance and cost-efficiency than prior scheme-specific accelerators on hybrid FHE applications. Our design process involves a comprehensive analysis of processing flows to abstract the primitives covering all operations in hybrid FHE applications. The UFC architecture primarily comprises hardware function units for these primitives, diverging from the deeply pipelined units in previous designs. This approach enables high hardware utilization across different FHE schemes. Further-more, we propose several algorithm-hardware co-optimizations to minimize the hardware cost of supporting various data shuffling patterns in FHE. This enables high-throughput implementation of function units that provide good cost efficiency. We also propose several compiler-level optimizations to achieve high hardware utilization of the unified architecture for computing FHE data in various algorithmic parameter settings. We evaluate the performance of UFC on different FHE programs, including scheme-specific and hybrid-scheme workloads. Our experiments show that UFC provides up to 6.0 × speedup and 1.6 × delay-energy-area efficiency improvement over state-of-the-art FHE accelerators.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {352–365},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00035,
author = {Samardzic, Nikola and Langowski, Simon and Devadas, Srinivas and Sanchez, Daniel},
title = {Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00035},
doi = {10.1109/MICRO61859.2024.00035},
abstract = {Zero-Knowledge Proofs (ZKPs) are a cryptographic tool that enables one party (a prover) to prove to another (a verifier) that a statement is true, without requiring the prover to disclose any data to the verifier. ZKPs have many use cases, such as letting clients delegate computation to servers with cryptographic correctness guarantees, while enabling the server to use secret data in these computations. ZKP applications span verifiable machine learning (ML) and databases, online auctions, electronic voting, and blockchains. While ZKPs are already widely used in blockchains, the prohibitive costs of proof generation limit them to proving very simple computations. We present a novel accelerator, NoCap, that leverages hardware-algorithm co-design to achieve transformative speedups. NoCap generates proofs 586× faster than a 32-core CPU, and 41× faster than PipeZK, a state-of-the-art ZKP accelerator. We leverage recent algorithmic developments to achieve these speedups: we identify and combine two recent hash-based ZKP algorithms, Orion and Spartan, which have similar performance on CPUs to the ZKPs targeted by prior accelerators, but are much more amenable to hardware acceleration. Though these algorithms result in larger proofs, we show that the end-to-end speedups (including prover time, proof transmission, and verification time) more than justify this size increase. We contribute a novel hardware organization to exploit these acceleration opportunities: NoCap is a programmable vector processor with functional units tailored to the needs of hash-based ZKPs. We also contribute a co-designed implementation of the Spartan+Orion ZKP tailored to accelerators, with optimizations that improve parallelism and reduce memory traffic. As a result, NoCap achieves speedups that enable new use cases for ZKP.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {366–379},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00036,
author = {Ji, Zhuoran and Zhao, Jianyu and Zhang, Zhaorui and Xu, Jiming and Yan, Shoumeng and Ju, Lei},
title = {A Compiler-Like Framework for Optimizing Cryptographic Big Integer Multiplication on GPUs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00036},
doi = {10.1109/MICRO61859.2024.00036},
abstract = {With the growth of digital data and rising security concerns, techniques for privacy-preserving computation have become increasingly essential. Big integer multiplication, pivotal for these applications, is compute-intensive but poses challenges for GPU acceleration due to its complexity and the need for application-specific tailored implementations. This paper presents IMCompiler, a compiler-like framework that automatically gen-erates optimized GPU kernels for integer multiplications used in cryptosystems. It features a frontend-IR-backend structure, where the Intermediate Representation (IR) employs a segmented integer multiplication algorithm to decouple architecture-specific optimizations from high-level parameters. The frontend can then easily translate integer multiplication with various high-level parameters into the IR, while the backend focuses on fine-tuning a single GPU kernel for each device, enabling automatic code generation. Moreover, we introduce a computation diagram to facilitate the analysis of parallelization strategies, inspiring many optimizations, including two-dimensional parallelization, tailored caching strategy, index transposing, and lazy carrying. Experiments show that IMCompiler achieves a 4.47× speedup compared to the widely used baseline and 1.42 × over Nvidia's official library. The speedup will be even higher for larger integers and higher-capacity GPUs.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {380–392},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00037,
author = {Lim, Katie and Giordano, Matthew and Stavrinos, Theano and Zhang, Irene and Nelson, Jacob and Kasikci, Baris and Anderson, Thomas},
title = {Beehive: A Flexible Network Stack for Direct-Attached Accelerators},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00037},
doi = {10.1109/MICRO61859.2024.00037},
abstract = {Direct-attached accelerators, where application accelerators are directly connected to the datacenter network via a hardware network stack, offer substantial benefits in terms of reduced latency, CPU overhead, and energy use. However, a key challenge is that modern datacenter network stacks are complex, with interleaved protocol layers, network management functions, and virtualization support. To operators, network feature agility, diagnostics, and manageability are often considered just as important as raw performance. By contrast, existing hardware network stacks only support basic protocols and are often difficult to extend since they use fixed processing pipelines. We propose Beehive, a new, open-source FPGA network stack for direct-attached accelerators designed to enable flexible and adaptive construction of complex network functionality in hardware. Application and network protocol elements are modularized as tiles over a network-on-chip substrate. Elements can be added or scaled up/down to match workload characteristics with minimal effort or changes to other elements. Flexible diagnostics and control are integral, with tooling to ensure deadlock safety. Our implementation interoperates with standard Linux TCP and UDP clients, with a 4x improvement in end-to-end RPC tail latency for Linux UDP clients versus a CPU-attached accelerator. Beehive is available at https://github:com/beehive-fpga/beehive},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {393–408},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00038,
author = {Genc, Hasan Nazim and Kim, Hansung and Ganesh, Prashanth and Shao, Yakun Sophia},
title = {Stellar: An Automated Design Framework for Dense and Sparse Spatial Accelerators},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00038},
doi = {10.1109/MICRO61859.2024.00038},
abstract = {Domain-specific hardware, coupled with co-designed algorithmic optimizations, plays a pivotal role in accelerating both dense and sparse workloads, surpassing the capability of general-purpose platforms. However, the diverse nature of these specialized hardware platforms makes it challenging to systematically implement, evaluate, and compare different solutions. To address these shortcomings, we introduce Stellar, a novel accelerator design framework tailored for dense and sparse spatial accelerators. Stellar introduces abstractions that systematically decouple different dimensions of accelerator design, addressing the need for a clear separation of concerns for automated design solutions. This modular approach enhances the clarity and flexibility of the design process, while enabling automated hardware generation for a range of dense and sparse accelerator designs. Stellar outputs synthesizable Verilog implementations of these accelerators, paired with RISC-V programming interfaces. We demonstrate that Stellar-generated accelerators are comparable to hand-written, high-quality hardware designs, enabling effective evaluation, comparison, and design-space exploration for both dense and sparse accelerators.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {409–422},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00039,
author = {Li, Zixi and Wentzlaff, David},
title = {LUCIE: A Universal Chiplet-Interposer Design Framework for Plug-and-Play Integration},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00039},
doi = {10.1109/MICRO61859.2024.00039},
abstract = {Multi-chip modules, recently also known as chiplets, are a promising approach to building large-scale silicon in the post-Moore's Law era. While chiplet-based designs offer advantages like averting manufacturing yield issues and enabling heterogeneous integration, there has not been a standard for seamless communication and integration of chiplets. This paper proposes LUCIE, a Lightweight Universal Chiplet-Interposer Ecosystem that provides a universal, modular, and reconfigurable framework for plug-and-play integration of chiplets. The key ideas behind LUCIE include a flexible 2D grid-based interposer design and a placement tool to aid chiplet integration. Its lightweight design and direct connection between chiplets allowed designs to fit into the LUCIE framework with minimal effort while achieving great performance. Performance analysis showed that LUCIE's performance is comparable to custom interposers, up to twice as fast as NoC-on-NoP designs in certain scenarios, and around 20% more power efficient than NoC-on-NoP designs. Compared to using custom interposers, cost analysis showed that LUCIE saves up to 18.9% of manufacturing cost, $60,000 of Non-Recurring Engineering cost, and 30 weeks less time-to-market. The LUCIE framework is more flexible and ideal for more topologies, such as 2D mesh and star topology. With the development of a novel, graph-based placement algorithm, LUCIE significantly reduces design complexity compared to monolithic and custom interposer-based chiplet systems while providing a high degree of modularity and reconfigurability.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {423–436},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00040,
author = {Wang, Qinggang and Zheng, Long and An, Zhaozeng and Xiong, Shuyi and Wang, Runze and Huang, Yu and Yao, Pengcheng and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {A Scalable, Efficient, and Robust Dynamic Memory Management Library for HLS-based FPGAs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00040},
doi = {10.1109/MICRO61859.2024.00040},
abstract = {Nowadays, high-level synthesis (HLS) has gained prominence for FPGA-based architecture prototyping, enhancing productivity significantly. Despite this advancement, HLS tools are impeded by a critical drawback: they lack support for dynamic memory management (DMM), leading to static mem-ory allocation and suboptimal use of memory resources. In response, numerous efforts have been made to develop DMM solutions compatible with HLS. However, our analysis indicates that existing solutions fail to concurrently meet the desired trifecta of scalability (efficient management of memory of any size), efficiency (minimal latency in memory (de-)allocation), and robustness (low allocation failure rates). This limitation hampers their applicability in real-world scenarios. In this paper, we introduce GraDMM, a “three-birds-one- stone” solution that comprehensively enhances the scalability, efficiency, and robustness of DMM. The key insight is to formulate memory (de-)allocation as graph analytics and lever-age sophisticated FPGA-based graph processing techniques. To achieve scalability, GraDMM specializes a simplified pipeline that significantly suppresses resource utilization expansion caused by managed memory scaling. This is crucial for managing arbitrarily sized memory on resource-limited FPGA platforms. For efficiency, GraDMM implements a data-centric concurrent traversal scheme and a shortcut-assisted fast traversal policy to accelerate (de-)allocation-guided graph traversal, reducing mem-ory (de-)allocation latency. To enhance robustness, GraDMM incorporates an adaptive memory defragmenter that defragments managed memory to minimize fragmentation-induced allocation failures. GraDMM is encapsulated as a library, providing high- level interfaces for users and ensuring synthesizability with Vi- vado HLS. Experimental results demonstrate that GraDMM out-performs three state-of-the-art HLS-compatible DMM solutions by significant margins: 56.71 %-85.59% in resource consumption savings, 78.94 % -99.99 % in (de-)allocation latency improvement, and 10.71 %-65.75% in allocation failure reduction.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {437–450},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00041,
author = {Weston, Kevin and Johnson, Avery and Janfaza, Vahid and Mahmud, Farabi and Muzahid, Abdullah},
title = {Customizing Cache Indexing Through Entropy Estimation},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00041},
doi = {10.1109/MICRO61859.2024.00041},
abstract = {Modern computers heavily rely on caches as one of the means to achieve higher performance. As a result, cache management has been the topic of extensive research. Compared to cache replacement and prefetching, cache indexing has re-ceived far less interest over the years. Being in the critical path, a good cache index function must exhibit a high performance while having a minimal computational delay. Previous indexing schemes fall short of these requirements, having either moderate performance or a prohibitively expensive delay. We propose ENTROPyINDEX, an entropy-based cache indexing scheme that can deliver superior performance while maintaining a minimal computational cost. ENTROPyINDEX is based on the idea of constructing the index function dynamically at runtime using the address bits with the highest entropy (randomness) to maximize the balance of the cache access distribution. The entropy of the address bits is measured by determining which bits change between two subsequent cache misses. ENTROPyINDEX periodically compares the entropy of different bits and selects the ones that change the most. This dynamic selection scheme allows ENTROPyINDEX to adapt to different types of applications. Our experimental results show that ENTROPyINDEX outper-forms previous indexing schemes both with and without hardware prefetching. For SPEC 2006, SPEC 2017, PARSEC 3.0 and GAP benchmarks without prefetching, ENTROPyINDEX delivers a geometric mean IPC improvement of 3.39% (with the highest being 52.2%), compared to a 1.74% improvement of the state-of-the-art index function (PRIME) and a 1.76% improvement of a commercialized indexing scheme (XORHASH) over the baseline power-of-two modulo scheme. With prefetching, ENTROPyINDEX is the only indexing scheme with a substantial performance gain of 1.42% (with the highest being 30.1 %), compared to a 0.41 % improvement of Prime and a 0.49% improvement of Xorhash over the same baseline. For non-uniform applications and no-prefetching, ENTROPyINDEX gives an IPC speed up of 5.58%, compared to a 2.26% speed up of Prime and a 2.23% speed up of Xorhash. For non-uniform applications with prefetching, the IPC speed up of ENTROPyINDEX is 2.08%, compared to a 0.35% speed up of Prime and a 0.53% speed up of Xorhash. For CVP workloads without prefetching, ENTROPyINDEX delivers a speed up of 3.04% over the baseline compared to a 1.52% of Prime and a 2.04% of Xorhash. For CVP workloads with prefetching, ENTROPyINDEX improves the IPC by 1.60%, compared to 0.63% of Prime and 1.07% of Xorhash.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {451–463},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00042,
author = {Schall, David and Sandberg, Andreas and Grot, Boris},
title = {The Last-Level Branch Predictor},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00042},
doi = {10.1109/MICRO61859.2024.00042},
abstract = {Branch prediction is crucial for modern high-performance processors, ensuring efficient execution by anticipating branch outcomes. Despite decades of research, achieving high prediction accuracy remains challenging, particularly in server workloads, where large branch working sets and hard-to-predict branches are prevalent. State-of-the-art predictors, such as the 64KB TAGE-SC-L design, experience high misprediction rates on server workloads, with 3.6-20% (9.2% on average) of execution cycles wasted due to mispredictions on a modern server CPU. While more predictor capacity can reduce mispredictions by up to 36% in the limit (with infinite storage), realizing meaningful gains in practice requires hundreds of KBs of storage, which is infeasible for a latency- and area-sensitive in-core predictor. This work introduces the Last-Level Branch Predictor (LLBP), a microarchitectural approach that improves branch prediction accuracy through additional high-capacity storage backing the baseline TAGE predictor. LLBP leverages the insight that branches requiring longer histories tend to span multiple program contexts - notionally, function calls. A given program context, which can be thought of as a call chain, localizes the branch prediction state, affording a small number of patterns per context even for hard-to-predict branches. LLBP predicts upcoming contexts and prefetches the associated branch metadata into a small in-core buffer, which is accessed in parallel with the unmodified TAGE predictor. Our results show that a 512KB LLBP backing 64KB TAGE-SC-L reduces MPKI by 0.5-25.9% (avg. 8.9 %) over the baseline without LLBP.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {464–479},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00043,
author = {Deshmukh, Aniket and Cai, LingzheChester and Patt, Yale N.},
title = {Timely, Efficient, and Accurate Branch Precomputation},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00043},
doi = {10.1109/MICRO61859.2024.00043},
abstract = {Out-of-order cores rely on high-accuracy branch predictors to supply useful instructions to the processor backend. However, there remains a large fraction of mispredictions caused by hard-to-predict (H2P) branches that modern predictors have been unable to improve. Precomputation is an alternative to prediction that speculatively executes the dependence chain of a branch earlier to override the branch predictor at Fetch time. However, prior work sacrifices H2P branch coverage and precomputation accuracy to produce timely results. Our work relaxes this timeliness constraint by using precomputation results to issue early misprediction flushes instead of overriding the branch predictor. This allows us to construct a highly accurate precomputation thread with good misprediction coverage, without sacrificing timeliness. The thread is efficient as it utilizes on-core execution resources and re-uses existing hardware for issuing early flushes. Using our Timely, Efficient, and Accurate thread for precomputation yields a 10.1% improvement in performance over an aggressive baseline OoO core.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {480–492},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00044,
author = {Mori, Kenichiro and Kosugi, Sota and Yoshida, Hiroto and Shimada, Hajime and Ando, Hideki},
title = {Localizing the Tag Comparisons in the Wakeup Logic to Reduce Energy Consumption of the Issue Queue},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00044},
doi = {10.1109/MICRO61859.2024.00044},
abstract = {There is a high demand to reduce the energy consumption of microprocessors. Among resources in a processor, the issue queue is one of the largest energy consumers, with much of the energy being consumed by the wakeup logic. The wakeup logic comprises the content-addressable memory, where tag comparisons are performed for all entries. Such global tag comparisons consume an appreciable amount of energy. This paper proposes a scheme called segmenting wakeup logic (SegWU), where the wakeup logic is segmented logically. An instruction is dispatched to an available entry in the segment selected based on the partial bits of the source tag values. At the wakeup time, a broadcast destination tag is compared only in the segment that corresponds to the partial bits of the destination tag value. Tag comparisons are not performed in the other segments. This localization of tag comparisons reduces the energy consumption. Our evaluation results for SPEC2017 benchmark programs show that SegWU reduces the number of tag comparisons by 90.3%. This lowers the IQ energy consumption, and the net reduction excluding the secondary effect (activity lowering of the front-end) and overhead is 3.0% in the base core energy consumption with no performance degradation. The overall energy reduction including the secondary effect is 6.9%.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {493–506},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00045,
author = {Hsiao, Yao and Nikoleris, Nikos and Khyzha, Artem and Mulligan, Dominic P. and Petri, Gustavo and Fletcher, Christopher W. and Trippel, Caroline},
title = {RTL2MμPATH: Multi-μPATH Synthesis with Applications to Hardware Security Verification},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00045},
doi = {10.1109/MICRO61859.2024.00045},
abstract = {The Check tools automate formal memory consistency model and security verification of processors by analyzing abstract models of microarchitectures, called μSPEC models. Despite the efficacy of this approach, a verification gap between μSPEC models, which must be manually written, and RTL limits the Check tools' broad adoption. Our prior work, called RTL2μSPEC, narrows this gap by automatically synthesizing formally verified μSPEC models from System Verilog implementations of simple processors. But, RTL2μSPEC assumes input designs where an instruction (e.g., a load) cannot exhibit more than one microarchitectural execution path (μPATH, e.g., a cache hit or miss path)-its single-execution-path assumption. In this paper, we first propose an automated approach and tool, called RTL2MμPATH, that resolves RTL2μSPEC's single-execution-path assumption. Given a System Verilog processor design, instruction encodings, and modest design metadata, RTL2MμPATH finds a complete set of formally verified μPATHS for each instruction. Next, we make an important observation: an instruction that can exhibit more than one μPATH strongly indicates the presence of a microarchitectural side channel in the input design. Based on this observation, we then propose an automated approach and tool, called Synthlc, that extends RTL2MμPATH with a symbolic information flow analysis to support synthesizing a variety of formally verified leakage contracts from System Verilog processor designs. Leakage contracts are foundational to state-of-the-art defenses against hardware side-channel attacks. SYnthlcis the first automated methodology for formally verifying hardware adherence to them.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {507–524},
numpages = {18},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00046,
author = {Song, Zhuoran and He, Houshu and Liu, Fangxin and Hao, Yifan and Song, Xinkai and Jiang, Li and Liang, Xiaoyao},
title = {SRender: Boosting Neural Radiance Field Efficiency via Sensitivity-Aware Dynamic Precision Rendering},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00046},
doi = {10.1109/MICRO61859.2024.00046},
abstract = {Neural Radiance Field (NeRF) holds immense promise for generating photo-realistic images and videos. How-ever, the computation and memory demands significantly impede its applicability in real-time tasks such as virtual reality and augmented reality. While accelerators dedicatedly designed for NeRF have emerged, they may not be the optimal solution as they only consider the optimization of full-precision NeRF models, leading to unsatisfactory memory and computation saving. This paper proposes Sensitivity-Aware Dynamic Precision Rendering, abbreviated as SRender, an algorithm-hardware co-design framework that borrows the idea of Adaptive Rendering commonly used in conventional graphics rendering to accelerate the rendering process of NeRF exploiting the fine-grained per-ray and per-point levels sensitivity. The fundamental idea behind SRender is that rays and points with high sensitivity require high-precision data types, while the insensitive rays and points do not, allowing them to be sacrificed for greater acceleration. Accordingly, the algorithm part of SRender dynamically configures the precision of the hash table and the multilayer perceptron (MLP), and the hardware part features a specialized accelerator that exploits such mixed-precision memory access and computation patterns. Specifically, we predict the sensitive rays and points based on low-precision hash encoding and feature computation. Then, we trigger a high-precision rendering again on the sensitive ones to guarantee the rendering quality. The hardware design of SRender includes a sensitivity prediction engine, coarse-then-fine-grained encoding engines, and a recovery engine, which translates the computational saving in the algorithm to real speedup. Using the coarse-then-fine-grained encoding engine, we maximize the data reuse on-chip, thereby reducing the stress of off-chip memory bandwidth. Experiments show that the edge-level SRender can achieve 3.94x and 34.06x speedup over state-of-the-art NeRF accelerator NeuRex and Jetson AGX Xavier GPU with negligible rendering quality loss.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {525–537},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00047,
author = {Chen, Yi and Zhao, Yongwei and Hao, Yifan and Wen, Yuanbo and Dai, Yuntao and Li, Xiaqing and Liu, Yang and Zhang, Rui and Zou, Mo and Song, Xinkai and Hu, Xing and Du, Zidong and Chen, Huaping and Guo, Qi and Chen, Tianshi},
title = {Cambricon-C: Efficient 4-Bit Matrix Unit via Primitivization},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00047},
doi = {10.1109/MICRO61859.2024.00047},
abstract = {Deep learning trends to use low precision numeral formats to cope with the ever-growing model sizes. For example, the large language model LLaMA2 has been widely deployed in 4-bit precision. With larger models and fewer unique values caused by low precision, an increasing proportion of arithmetic in matrix multiplication is repeating. Although discussed in prior works, such value redundancy has not been fully exploited, and the cost to leverage the value redundancy often offsets any advantages. In this paper, we propose to primitivize the matrix multiplication, that is decomposing it down to the 1-ary successor function (a.k.a. counting) to merge repeating arithmetic. We revisited various techniques to propose Cambricon-C SA, a 4-bit primitive matrix multiplication unit that doubles the energy efficiency over conventional systolic arrays. Experimental results show that Cambricon-C SA can achieve &lt;tex&gt;$mathbf{1}.mathbf{95}times$&lt;/tex&gt; energy efficiency improvement compared with MAC-based systolic array.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {538–550},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00048,
author = {Chen, Yuzong and Meng, Jian and Seo, Jae-sun and Abdelfattah, Mohamed S.},
title = {BBS: Bi-Directional Bit-Level Sparsity for Deep Learning Acceleration},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00048},
doi = {10.1109/MICRO61859.2024.00048},
abstract = {Bit-level sparsity methods skip ineffectual zero-bit operations and are typically applicable within bit-serial deep learning accelerators. This type of sparsity at the bit-level is especially interesting because it is both orthogonal and compatible with other deep neural network (DNN) efficiency methods such as quantization and pruning. Furthermore, it comes at little or no accuracy degradation and can be performed completely post-training. However, current bit-sparsity approaches lack practicality because of (1) load imbalance from the random distribution of zero bits, (2) unoptimized external memory access because all bits are fetched from off-chip memory, and (3) high hardware implementation overhead, including large multiplexers and shifters to support sparsity at the bit level. In this work, we improve the practicality and efficiency of bit-level sparsity through a novel algorithmic bit-pruning, averaging, and compression method, and a co-designed efficient bit-serial hardware accelerator. On the algorithmic side, we introduce bi-directional bit sparsity (BBS). The key insight of BBS is that we can leverage bit sparsity in a symmetrical way to prune either zero-bits or one-bits. This significantly improves the load balance of bit-serial computing and guarantees the level of sparsity to be more than 50%. On top of BBS, we further propose two bit-level binary pruning methods that require no retraining, and can be seamlessly applied to quantized DNNs. Combining binary pruning with a new tensor encoding scheme, BBS can both skip computation and reduce the memory footprint associated with bi-directional sparse bit columns. On the hardware side, we demonstrate the potential of BBS through BitVert, a bit-serial architecture with an efficient PE design to accelerate DNNs with low overhead, exploiting our proposed binary pruning. Evaluation on seven representative DNN models shows that our approach achieves: (1) on average 1.66× reduction in model size with negligible accuracy loss of &lt; 0.5%; (2) up to 3.03× speedup and 2.44× energy saving compared to prior DNN accelerators.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {551–564},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00049,
author = {Odema, Mohanad and Chen, Luke and Kwon, Hyoukjun and Al Faruque, Mohammad Abdullah},
title = {SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00049},
doi = {10.1109/MICRO61859.2024.00049},
abstract = {Emerging multi-model workloads with heavy models like recent large language models significantly increased the compute and memory demands on hardware. To address such increasing demands, designing a scalable hardware architecture became a key problem. Among recent solutions, the 2.5D silicon interposer multi-chip module (MCM)-based AI accelerator has been actively explored as a promising scalable solution due to their significant benefits in the low engineering cost and composability. However, previous MCM accelerators are based on homogeneous architectures with fixed dataflow, which encounter major challenges from highly heterogeneous multi-model work-loads due to their limited workload adaptivity. Therefore, in this work, we explore the opportunity in the heterogeneous dataflow MCM AI accelerators. We identify the scheduling of multi-model workload on heterogeneous dataflow MCM AI accelerator is an important and challenging problem due to its significance and scale, which reaches &lt;tex&gt;$mathbf{O}(10^{56})$&lt;/tex&gt; even for a two-model workload on 6×6 chiplets. We develop a set of heuristics to navigate the huge scheduling space and codify them into a scheduler, SCAR, with advanced techniques such as inter-chiplet pipelining. Our evaluation on ten multi-model workload scenarios for datacenter multitenancy and AR/VR use-cases has shown the efficacy of our approach, achieving on average 27.6% and 29.6% less energy-delay product (EDP) for the respective applications settings compared to homogeneous baselines.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {565–579},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00050,
author = {Yin, Lingxiang and Gandham, Sanjay and Lin, Mingjie and Zheng, Hao},
title = {SCALE: A Structure-Centric Accelerator for Message Passing Graph Neural Networks},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00050},
doi = {10.1109/MICRO61859.2024.00050},
abstract = {Message passing paradigm has been widely used in developing complex Graph Neural Network (GNN) models, allowing for concise representations of edge and vertex-wise operations. Despite its pivotal role in theoretical advancement, the respective expression of edge and vertex operations, along with evolving GNN variants and datasets, has inevitably led to enormous computational complexity due to heterogeneous computation kernels. In particular, such inconsistent computation characteristics present new challenges in leveraging intermediate data reuse, ensuring both edge and vertex-wise workload balance, and sustaining system scalability. In this paper, we propose a structurecentric accelerator, SCALE, that can support a variety of message passing GNN models with improved parallelism, data reuse, and scalability. The central idea is to find latent similarities among GNN primitives such as shared dataflow structure, rather than strictly adhering to heterogeneous model structure. This serves as a hinge to homogenize inconsistencies in various GNN computation kernels. To accomplish this concept, SCALE consists of three unique designs, a novel systolic array-like architecture, a degree and vertex-aware scheduling, and a coherent dataflow tailored for fused graph and neural operations. The proposed systolic array-like architecture can support varying dataflows such as all-reduce, of distinct GNN operations improving parallelism, data reuse, and throughput. The degree and vertex-aware scheduling can remedy the workload imbalance encountered in vertex and edge-wise operations. Moreover, the proposed dataflow can unify the data movement of both graph and neural operators without extra communication and storage overheads. Our simulation results show that SCALE achieves 1.82× speedup and 38.9% energy reduction on average over the state-of-the-art GNN accelerators [1]–[4].},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {580–593},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00051,
author = {Ham, Hyungkyu and Hong, Jeongmin and Park, Geonwoo and Shin, Yunseon and Woo, Okkyun and Yang, Wonhyuk and Bae, Jinhoon and Park, Eunhyeok and Sung, Hyojin and Lim, Euicheol and Kim, Gwangsun},
title = {Low-Overhead General-Purpose Near-Data Processing in CXL Memory Expanders},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00051},
doi = {10.1109/MICRO61859.2024.00051},
abstract = {Emerging Compute Express Link (CXL) enables cost-efficient memory expansion beyond the local DRAM of processors. While its CXL.mem protocol provides minimal latency overhead through an optimized protocol stack, frequent CXL memory accesses can result in significant slowdowns for memory-bound applications whether they are latency-sensitive or bandwidth-intensive. The near-data processing (NDP) in the CXL controller promises to overcome such limitations of passive CXL memory. However, prior work on NDP in CXL memory proposes application-specific units that are not suitable for practical CXL memory-based systems that should support various applications. On the other hand, existing CPU or GPU cores are not cost-effective for NDP because they are not optimized for memory-bound applications. In addition, the communication between the host processor and CXL controller for NDP offloading should achieve low latency, but existing CXL.io/PCIe-based mechanisms incur &lt;tex&gt;$mumathbf{s}-mathbf{scale}$&lt;/tex&gt; latency and are not suitable for fine-grained NDP.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {594–611},
numpages = {18},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00052,
author = {Huo, Pingyi and Devulapally, Anusha and Maruf, Hasan Al and Park, Minseo and Nair, Krishnakumar and Arunachalam, Meena and Akbulut, Gulsum Gudukbay and Kandemir, Mahmut Taylan and Narayanan, Vijaykrishnan},
title = {PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00052},
doi = {10.1109/MICRO61859.2024.00052},
abstract = {Deep Learning Recommendation Models (DLRMs) have become increasingly popular and prevalent in today's datacenters, consuming most of the AI inference cycles. The performance of DLRMs is heavily influenced by available band-width due to their large vector sizes in embedding tables and concurrent accesses. To achieve substantial improvements over existing solutions, novel approaches towards DLRM optimization are needed, especially, in the context of emerging interconnect technologies like CXL. This study delves into exploring CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to accelerate DLRMs while optimizing their memory and bandwidth scalability. We present an in-depth characterization of industry-scale DLRM workloads running on CXL-ready systems, identifying the predominant bottlenecks in existing CXL systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements near-data processing through downstream ports of the fabric switch. PIFS-Rec achieves a latency that is 3.89 x lower than Pond, an industry-standard CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by 2.03x.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {612–626},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00053,
author = {Lee, Dongjae and Hyun, Bongjoon and Kim, Taehun and Rhu, Minsoo},
title = {PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00053},
doi = {10.1109/MICRO61859.2024.00053},
abstract = {Processing-in-memory (PIM) has emerged as a promising solution for accelerating memory-intensive workloads as they provide high memory bandwidth to the processing units. This approach has drawn attention not only from the academic community but also from the industry, leading to the development of real-world commercial PIM devices. In this work, we first conduct an in-depth characterization on UPMEM's general-purpose PIM system and analyze the bottlenecks caused by the data transfers across the DRAM and PIM address space. Our characterization study reveals several critical challenges associated with DRAM↔PIM data transfers in memory bus integrated PIM systems, for instance, its high CPU core utilization, high power consumption, and low read/write throughput for both DRAM and PIM. Driven by our key findings, we introduce the PIM-MMU architecture which is a hardware/software co-design that enables energy-efficient DRAM↔PIM transfers for PIM systems. PIM-MMU synergistically combines a hardware-based data copy engine, a PIM-optimized memory scheduler, and a heterogeneity-aware memory mapping function, the utilization of which is supported by our PIM-MMU software stack, significantly improving the efficiency of DRAM↔PIM data transfers. Experimental results show that PIM-MMU improves the DRAM↔PIM data transfer throughput by an average &lt;tex&gt;$4.1times$&lt;/tex&gt; and enhances its energy-efficiency by &lt;tex&gt;$4.1times$&lt;/tex&gt;, leading to a &lt;tex&gt;$2.2times$&lt;/tex&gt; end-to-end speedup for real-world PIM workloads.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {627–642},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00054,
author = {Feldmann, Axel and Golden, Courtney and Yang, Yifan and Emer, Joel S. and Sanchez, Daniel},
title = {Azul: An Accelerator for Sparse Iterative Solvers Leveraging Distributed On-Chip Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00054},
doi = {10.1109/MICRO61859.2024.00054},
abstract = {Solving sparse systems of linear equations is a fundamental primitive in many numeric algorithms. Iterative solvers provide an efficient way of solving large, highly sparse systems. However, iterative solvers are inefficient on existing architectures because they perform computations with (1) poor short-term reuse, which causes frequent off-chip memory traffic; and (2) challenaing data dependences, which limit parallelism. We present Azul, a hardware accelerator that achieves high arithmetic intensity by keeping data in distributed on-chip SRAM. Azul is organized as a grid of tiles, each with a small memory and a simple processing element (PE). This enables keeping solver data on-chip across iterations, achieving high reuse. We present a novel scheduling algorithm that maps data and computation across PEs to avoid communication bottlenecks while achieving high parallelism, and a specialized PE that achieves high utilization of arithmetic units. When tested on a representative set of matrices for sparse iterative solvers, Azul is gmean 217 × faster than state-of-the art GPU implementations, 159× faster than a previously proposed accelerator for sparse iterative solvers, and 90 × faster than a previously proposed distributed-SRAM accelerator.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {643–656},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00055,
author = {Yang, Kailin and Mart\'{\i}nez, Jos\'{e} F.},
title = {FloatAP: Supporting High-Performance Floating-Point Arithmetic in Associative Processors},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00055},
doi = {10.1109/MICRO61859.2024.00055},
abstract = {Associative Processors (AP) enable in-situ, data-parallel computation in content-addressable memories (CAM). In particular, arithmetic operations are accomplished via bit-serial sequences of bulk search and update primitives. The data-parallel nature of AP-based computation, whereby thousands of operations can take place concurrently, more than makes up for the slowness of its bit-serial arithmetic; as a result, APs can deliver very high performance. Indeed, recent AP proposals have shown promising results across various application domains, primarily for integer codes. However, floating-point (FP) support is crucial for many critical AI/ML and scientific workloads. This paper introduces FloatAP, the first highly programmable AP architecture tailored for high-performance FP arithmetic. We show that the straightforward application of AP-based bit-serial arithmetic to FP leads to insufficient computational through-put. With FloatAP, we propose a bit- and component-parallel vector mantissa alignment method that significantly improves the throughput of vector FP addition and reduction. We also repurpose the existing adder trees in modern APs to efficiently implement multiplication and dot product operations, markedly outperforming AP's traditional bit-serial multiplication. FloatAP is flexible enough to accommodate different FP formats while maintaining high utilization and integer compat-ibility. It is programmable using RISC-V ‘V’ vector extension and easily accommodates domain-specific languages and ML frameworks. We evaluate FloatAP using contemporary ML workloads, including CNN-, RNN-, and LLM-based parallel codes. The results show that, on average, FloatAP achieves 2.7 × higher inference throughput compared to an area-equivalent NVIDIA A100 Tensor Core GPU for single-precision FP (1.5 × for bfloat16) and 2.4 × higher energy efficiency (1.6 ×for bfloat16).},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {657–670},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00056,
author = {Zhang, Yicong and Wang, Mingyu and Wang, Wangguang and Mai, Yangzhan and Huang, Haiqiu and Yu, Zhiyi},
title = {Atomic Cache: Enabling Efficient Fine-Grained Synchronization with Relaxed Memory Consistency on GPGPUs Through In-Cache Atomic Operations},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00056},
doi = {10.1109/MICRO61859.2024.00056},
abstract = {General-purpose graphics processing unit (GPGPU), widely recognized as an exceptional computing platform for de-ploying emerging parallel applications, requires strict adherence to atomicity and memory consistency models for shared variable synchronization. This is crucial to ensure deterministic execution and leverage the performance advantages of the GPGPU single-instruction -multiple-threads architecture. However, the escalating demand for shared variable updates across thread blocks, notably in applications like deep neural networks and graph analysis, significantly exacerbates the serialization overhead of atomic operations due to the von Neumann bottleneck. Additionally, the overhead introduced by memory fences supporting the memory consistency model further complicates this fine-grained synchronization requirement. To address these challenges, this paper proposes Atomic Cache, facilitating an In-Cache computing hardware-software co-design for GPGPUs. At the software level, we propose relaxed memory consistency based on non-ordering commutativity to alleviate the execution of in-cache atomic operations, thereby mitigating the performance overhead of memory fences. At the hardware level, we present the In-Situ Store Atomic Cache Macro, which empowers the Atomic Cache to efficiently execute atomic logic and arithmetic operations within the cache array. This innovation alleviates the von Neumann bottleneck associated with serialized execution of atomic operations. The experimental evaluation results demonstrate that the Atomic Cache can save more than 60% of memory access energy while incurring only 9.42% chip area overhead. Furthermore, it not only delivers an average speedup ratio of 2.59 × and an IPC performance improvement of 1.48× for RISC-V GPGPUs, but also achieves an average speedup ratio of 1.31 × and an IPC performance improvement of 39.92% when compared to state-of-the-art designs employing local atomic buffers.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {671–685},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00057,
author = {Kang, Ni and Alawneh, Ahmad and Zhang, Mengchi and Rogers, Timothy G.},
title = {Concurrency-Aware Register Stacks for Efficient GPU Function Calls},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00057},
doi = {10.1109/MICRO61859.2024.00057},
abstract = {Since the early days of computers, dividing a program into functions or subroutines has been a common way to manage complexity. Functions make programs easier to read, facilitate code reuse, and provide clean interfaces for separate compilation. However, function calls incur runtime overhead. We quantify the impact of this runtime overhead on GPUs and demonstrate that the register spills/fills required to maintain the function call application binary interface place significant bandwidth and capacity pressure on shared resources. To alleviate this overhead, we introduce Concurrency-Aware Register Stacks (CARS), a hardware mechanism that re-purposes segments of the GPU register file as a software-controlled hardware stack. CARS exploits the regularity in function prologue/epilogues to rename registers pushed to the stack with linear base + offset addressing, similar to the baseline GPU. Informed by lightweight call graph analysis and dynamic function behavior, CARS balances the space devoted to register stacks with the concurrency required to hide latency in GPUs. Without harming function-free programs, CARS improves the performance and energy efficiency of 22 function-calling applications by 26% and 28%, respectively, outperforming idealized GPUs with impractical resources.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {686–699},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00058,
author = {Dalmia, Preyesh and Kumar, Rajesh Shashi and Sinclair, Matthew D.},
title = {CPElide: Efficient Multi-Chiplet GPU Implicit Synchronization},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00058},
doi = {10.1109/MICRO61859.2024.00058},
abstract = {Chiplets are transforming computer system designs, allowing system designers to combine heterogeneous computing resources at unprecedented scales. Breaking larger, mono-lithic chips into smaller, connected chip lets helps performance continue scaling, avoids die size limitations, improves yield, and reduces design and integration costs. However, chip let-based designs introduce an additional level of hierarchy, which causes indirection and non-uniformity. This clashes with typ-ical heterogeneous systems: unlike CPU-based multi-chiplet systems, heterogeneous systems do not have significant OS support or complex coherence protocols to mitigate the impact of this indirection. Thus, exploiting locality across application phases is harder in multi-chiplet heterogeneous systems. We propose CPElide, which utilizes information already avail-able in heterogeneous systems' embedded microprocessor (the command processor) to track inter-chiplet data dependencies and aggressively perform implicit synchronization only when necessary, instead of conservatively like the state-of-the-art HMG. Across 24 workloads CPElide improves average performance (13%, 19%), energy (14%, 11 %), and network traffic (14%,17%), respectively, over current approaches and HMG.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {700–717},
numpages = {18},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00059,
author = {Vittal, Suhas and Javadi-Abhari, Ali and Cross, Andrew W. and Bishop, Lev S. and Qureshi, Moinuddin},
title = {Flag-Proxy Networks: Overcoming the Architectural, Scheduling and Decoding Obstacles of Quantum LDPC Codes},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00059},
doi = {10.1109/MICRO61859.2024.00059},
abstract = {Quantum error correction is necessary for achieving exponential speedups on important applications. The planar surface code has remained the most studied error-correcting code for the last two decades because of its relative simplicity. However, encoding a singular logical qubit with the planar surface code requires physical qubits quadratic in the code distance &lt;tex&gt;$(d)$&lt;/tex&gt;, making it space-inefficient for the large-distance codes necessary for promising applications. Thus, Quantum Low-Density Parity-Check (QLDPC) have emerged as an alternative to the planar surface code but require a higher degree of connectivity. Furthermore, the problems of fault-tolerant syndrome extraction and decoding are understudied for these codes and also remain obstacles to their usage. In this paper, we consider two under-studied families of QLDPC codes: hyperbolic surface codes and hyperbolic color codes. We tackle the three challenges mentioned above as follows. First, we propose Flag-Proxy Networks (FPNs), a generalizable architecture for quantum codes that achieves low connectivity through flag and proxy qubits. Second, we propose a greedy syndrome extraction scheduling algorithm for general quantum codes and further use this algorithm for fault-tolerant syndrome extraction on FPNs. Third, we present two decoders that leverage flag measurements to decode the hyperbolic codes accurately. Our work finds that degree-4 FPNs of the hyperbolic surface and color codes are respectively &lt;tex&gt;$2.9times$&lt;/tex&gt; and &lt;tex&gt;$5.5times$&lt;/tex&gt; more space-efficient than the &lt;tex&gt;$d=5$&lt;/tex&gt; planar surface code, and become even more space-efficient when considering higher distances. The hyperbolic codes also have error rates comparable to their planar counterparts.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {718–734},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00060,
author = {Wang, Meng and Das, Poulami and Nair, Prashant J.},
title = {Qoncord: A Multi-Device Job Scheduling Framework for Variational Quantum Algorithms},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00060},
doi = {10.1109/MICRO61859.2024.00060},
abstract = {Quantum computers face challenges due to limited resources, particularly in cloud environments. Despite these ob-stacles, Variational Quantum Algorithms (VQAs) are considered promising applications for present-day Noisy Intermediate-Scale Quantum (NISQ) systems. VQAs require multiple optimization iterations to converge on a globally optimal solution. Moreover, these optimizations, known as restarts, need to be repeated from different points to mitigate the impact of noise. Unfortunately, the job scheduling policies for each VQA task in the cloud are heavily unoptimized. Notably, each VQA execution instance is typically scheduled on a single NISQ device. Given the variety of devices in the cloud, users often prefer higher-fidelity devices to ensure higher-quality solutions. However, this preference leads to increased queueing delays and unbalanced resource utilization. We propose Qoncord, an automated job scheduling framework to address these cloud-centric challenges for VQAs. Qoncord leverages the insight that not all training iterations and restarts are equal, Qoncord strategically divides the training process into exploratory and fine-tuning phases. Early exploratory iterations, more resilient to noise, are executed on less busy machines, while fine-tuning occurs on high-fidelity machines. This adaptive approach mitigates the impact of noise, optimizes resource usage, and reduces queuing delays in cloud environments. Qoncord also significantly reduces execution time and minimizes restart overheads by eliminating low-performance iterations. Thus, Qoncord offers similar solutions 17.4 × faster. It also provides 13.3% better solutions for the same time budget as the baseline.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {735–749},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00061,
author = {Yin, Keyi and Fang, Xiang and Humble, Travis S. and Li, Ang and Shi, Yunong and Ding, Yufei},
title = {Surf-Deformer: Mitigating Dynamic Defects on Surface Code via Adaptive Deformation},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00061},
doi = {10.1109/MICRO61859.2024.00061},
abstract = {In this paper, we introduce Surf-Deformer, a code deformation framework that seamlessly integrates adaptive defect mitigation functionality into the current surface code workflow. It crafts several basic deformation instructions based on fundamental gauge transformations, which can be combined to explore a larger design space than previous methods. This enables more optimized deformation processes tailored to specific defect situations, restoring the QEC capability of deformed codes more efficiently with minimal qubit resources. Additionally, we design an adaptive code layout that accommodates our defect mitigation strategy while ensuring efficient execution of logical operations. Our evaluation shows that Surf-Deformer outperforms previous methods by significantly reducing the end-to-end failure rate of various quantum programs by 35× to 70×, while requiring only about 50% of the qubit resources compared to the previous method to achieve the same level of failure rate. Ablation studies show that Surf-Deformer surpasses previous defect removal methods in preserving QEC capability and facilitates surface code communication by achieving nearly optimal throughnut.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {750–764},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00062,
author = {Xu, Ruifan and Luo, Jin and Zhang, Yawen and Lin, Yibo and Wang, Runsheng and Huang, Ru and Liang, Yun},
title = {Hestia: An Efficient Cross-Level Debugger for High-Level Synthesis},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00062},
doi = {10.1109/MICRO61859.2024.00062},
abstract = {High-level synthesis (HLS) offers an opportunity to design hardware at the software level, which automatically trans-forms high-level specifications into RTL designs. However, HLS compilers are often considered complex black-box procedures, lacking transparency for designers and hindering the debugging process. Programmers often rely on simulating the HLS design to comprehend the behavior of the generated hardware. RTL simulation, the prevalent hardware debugging method, is time-consuming and inundates designers with excessive details when applied to HLS designs. Conversely, software-level simulation is fast but does not model hardware-specific details. The debug-ging challenge primarily stems from the semantic gap between software descriptions and RTL implementations. In this paper, we present Hestia, an efficient cross-level debugger enabling debugging HLS designs at different abstraction levels. Hestia provides a multi-level interpreter, aiding in debugging various issues in the HLS procedure with less hardware details and lower time costs. With an equivalent mapping across different levels, Hestia facilitates bug identifi-cation and localization, providing breakpoints and stepping at multiple granularities. We demonstrate the effectiveness of Hestia from three aspects: simulation efficiency, debugging capability, and scalability. Experimental results show that Hestia achieves significant simulation speedup compared to RTL simulators and prior work. The experiment of a case study also illustrates how Hestia helps find and localize bugs easily.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {765–779},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00063,
author = {Mosallaei, Ali and Isaacs, Katherine E. and Sun, Yifan},
title = {Looking into the Black Box: Monitoring Computer Architecture Simulations in Real-Time with AkitaRTM},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00063},
doi = {10.1109/MICRO61859.2024.00063},
abstract = {Computer architecture simulators are essential for validating novel chip designs. However, they often provide little transparency during execution. This opaqueness limits the ability of users to identify issues during the simulation, leading to both wasted computational and human time. We address these issues by providing an intuitive user experience during simulation execution. Particularly, we reveal the status of executions and allow users to control the execution of a computer architecture simulator through AkitaRTM, an interactive web-based tool for real-time monitoring of computer architecture simulations. We based its design on the design workflow inefficiencies experienced by computer architects when using simulations. We demonstrate AkitaRTM's utility through two case studies, the second leading to a patch in the simulator. Additionally, we conducted a user study with computer architects, aiming to validate AkitaRTM. We found that, in addition to solving the observed problems, AkitaRTM also provided an educational benefit by making simulators more transparent to users. Based on these findings, we reflect upon the design of AkitaRTM and provide guidance for future human-centered tools in this space.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {780–794},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00064,
author = {Nayak, Ajay and Basu, Arkaprava},
title = {Over-Synchronization in GPU Programs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00064},
doi = {10.1109/MICRO61859.2024.00064},
abstract = {The performance of GPU (Graphics Processing Unit)-accelerated functions affects a large spectrum of modern software. Efficiently synchronizing across thousands of concurrent threads is critical to the performance of GPU programs. GPU vendors have introduced advanced programming constructs, e.g., scopes, for efficiently synchronizing within a chosen subset of threads. However, programmers must explicitly employ them, where applicable, to benefit from such features. We demonstrate how GPU programs can leave performance on the table by failing to fully harness advanced synchronization features in modern GPUs - leading to over-synchronization. We discover three different variants of over-synchronization observed in real-world applications. We then build a tool, ScopeAdvice, to find cases of over-synchronization in CUDA programs. Avoiding reported over-synchronization improves the performance of several GPU applications by up to 55%.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {795–809},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00065,
author = {Cebrian, Juan M. and Jahre, Magnus and Ros, Alberto},
title = {Temporarily Unauthorized Stores: Write First, Ask for Permission Later},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00065},
doi = {10.1109/MICRO61859.2024.00065},
abstract = {x86 processors implement a total store order (x86-TSO) consistency model, which requires stores to update memory in a sequenced manner. The latency of stores is then hidden by the store buffer (SB), which holds stores until the write is performed. On a long latency cache miss, however, stores block the SB, eventually stalling the processor and degrading performance. Contemporary industrial high-performance processors deal with this situation by overprovisioning the size of the SB, but this comes at the cost of energy and latency overheads. In this work, we remove the stalls caused by stores blocked at the head of the SB while reusing existing processor resources, either improving performance when SB size is kept constant or maintaining performance while reducing SB size. Our proposal, Temporarily Unauthorized Stores (TUS), achieves this by extending the functionality of 1) the write combining buffers, to allow them to coalesce stores while maintaining x86- TSO consistency, and 2) immediately write data to the first-level cache upon a miss (i.e., providing an always-hit illusion) but temporarily keeping the written data invisible to the cache coherence protocol, i.e., these stores are temporarily unauthorized. TUS makes temporarily unauthorized stores visible in x86- TSO order without speculation or rollbacks once write permission is obtained. In essence, TUS logically transforms the write combining buffers and the first-level cache into an “extension” of the SB. TUS improves performance by up to 26 % (3.2 % on average) while reducing the total energy-delay-product (EDP) by up to 35.9% (6.4% on average) for SB-bound benchmarks with a 114-entry SB compared to our baseline architecture with an SB of the same size. When configured with a 32-entry SB, TUS yields a performance improvement of 2 % over a 114-entry SB baseline while reducing SB energy per search by a factor of 2 x, SB area by 21 %, and store-to-Ioad forwarding latency from 5 to 3 cycles.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {810–822},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00066,
author = {Patel, Vipin and Biswas, Swarnendu and Chaudhuri, Mainak},
title = {Leveraging Cache Coherence to Detect and Repair False Sharing On-the-fly},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00066},
doi = {10.1109/MICRO61859.2024.00066},
abstract = {Performance bugs due to false sharing do not man-ifest as observable correctness errors, and hence are challenging to detect and repair. Prior approaches aim to both detect and repair false sharing instances automatically but most of them suffer from one or more of the following drawbacks: (i) high performance overhead due to expensive tracking of shadow memory, (ii) reliance on imprecise hardware events, and (iii) limited applicability and portability. We present extensions to the MESI cache coherence protocol for efficiently identifying and mitigating false sharing instances. The FSDetect protocol tracks the frequency of coherence misses per cache block to identify harmful instances of falsely shared lines while incurring negligible performance overhead. The FSLite protocol extends FSDetect to transparently privatize the falsely shared lines on accesses after detection, thereby eliminating the performance problem arising from false sharing. FSLite maintains coherence by performing precise byte-level updates of privatized blocks at the LLC on termination of privatization. Our simulation results on a variety of multithreaded workloads show that FSDetect can precisely identify all known harmful instances of false sharing. FSLite, on average, improves the performance of applications suffering from false sharing by 1.39X over the unmodified baseline, at the cost of a minimal increase in the chip area. Furthermore, applications running with FSLite stress the network less and show improved energy behavior.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {823–839},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00067,
author = {Nicol\'{a}s-Conesa, V\'{\i}ctor and Titos-Gil, Rub\'{e}n and Fern\'{a}ndez-Pascual, Ricardo and Acacio, Manuel E. and Ros, Alberto},
title = {Chaining Transactions for Effective Concurrency Management in Hardware Transactional Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00067},
doi = {10.1109/MICRO61859.2024.00067},
abstract = {Hardware Transactional Memory (HTM) offers the opportunity to ease parallel programming. However, driven by hardware limitations, commercial implementations eschew the complexity involved in early sophisticated proposals from academia, and, among other things, opt for simple conflict resolution policies that inevitably increase transaction aborts. To increase thread level parallelism, previous works propose conflict resolution schemes that, instead of aborting, add a second level of speculation consisting in using not-yet-committed data from another transaction. This policy, which we refer to as requester-speculates, has not yet been considered in the context of the kind of best-effort HTM support provided by commercial processors. This work proposes CHAining TransactionS (CHATS), a simple yet effective realization of the requester-speculates con-flict resolution policy in which cyclic dependencies between transactions are avoided and the commit ordering respects the dependencies that transactions make once speculative values are communicated. The ultimate result is a best-effort HTM implementation that forces a partial order between transactions in a way that ensures effective utilization of forwarded data and that gets away from the complexity of previous proposals. Simulations using gem5 demonstrate the effectiveness of CHATS in both commercial-like setups and academic state-of-the-art best-effort systems (22% and 16% reduction in execution time, on average, respectively). These improvements are achieved by requiring less than 280 bytes of extra storage.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {840–855},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00068,
author = {Won, William and Elavazhagan, Midhilesh and Srinivasan, Sudarshan and Gupta, Swati and Krishna, Tushar},
title = {TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00068},
doi = {10.1109/MICRO61859.2024.00068},
abstract = {The surge of artificial intelligence, particularly large language models, has driven the rapid development of large-scale machine learning clusters. Executing distributed models on these clusters is often constrained by communication overhead, making efficient utilization of available network resources crucial. As a result, the routing algorithm employed for collective communications (i.e., collective algorithms) plays a pivotal role in determining overall performance. Unfortunately, existing collective communication libraries for distributed machine learning are limited by a fixed set of basic collective algorithms. This limitation hinders communication optimization, especially in modern clusters with heterogeneous and asymmetric topologies. Furthermore, manually designing collective algorithms for all possible combinations of network topologies and collective patterns requires heavy engineering and validation efforts. To address these challenges, this paper presents Tacos, an autonomous synthesizer capable of automatically generating topology-aware collective algorithms tailored to specific collective patterns and network topologies. Tacos is highly flexible, synthesizing an All-Reduce algorithm for a heterogeneous 128-NPU system in just 1.08 seconds, while achieving up to a 4.27× performance improvement over state-of-the-art synthesizers. Additionally, Tacos demonstrates better scalability with polynomial synthesis times, in contrast to NP-hard approaches which only scale to systems with tens of NPUs. Tacos can synthesize for 40K NPUs in just 2.52 hours.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {856–870},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00069,
author = {Feng, Yinxiao and Li, Wei and Ma, Kaisheng},
title = {Ring Road: A Scalable Polar-Coordinate-based 2D Network-on-Chip Architecture},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00069},
doi = {10.1109/MICRO61859.2024.00069},
abstract = {Networks-on-chip (NoCs) are scaled out to build large-scale multi-chip networks to meet the growing demand for computing. However, the traditional router-based NoC architecture has significant limitations: 1) The overhead of routers is high, especially for long-distance traffic that traverses numerous chips and hops; 2) On/off-chip traffic is mixed up, so the local NoC performance is dragged down by the cross-chip traffic; 3) The routing design of intra/inter-chip networks is also entangled. The alternative router-less solution based on isolated multi-ring (IMR) can address some of these issues; however, the complete lack of routers leads to wiring overhead and limited scalability thus not suitable for large-scale multi-chip networks. Therefore, we are motivated to design a new network architecture that can reduce router/wiring overhead, isolate on/off-chip traffic, and decouple inter/intra-chip routing design by combining the advantages of both routers and router-less rings. In this paper, we propose Ring Road, in which high-speed isolated multi-rings are integrated with compact routers, thus achieving flexible traffic delivery with less router usage and overhead. A polar-coordinate-based description is presented to describe the topology, and corresponding deadlock-free routing algorithms are discussed. As a standalone NoC, Ring Road is low-cost, low-latency, and high-performance. It can also be scaled out into multi-chip networks without redesigning the on-chip routing regardless of inter-chip topology. With low-cost isolated ring channels, no matter how heavy the cross-chip traffic is, the local NoC performance remains consistent. Circuit implementation and post-synthesis analysis show that Ring Road achieves &lt;tex&gt;$1.7sim 2times$&lt;/tex&gt; bandwidth-per-area and &lt;tex&gt;$1.6sim 1.9times$&lt;/tex&gt; bandwidth-per-power compared with the traditional mesh router. Cycle-accurate simulation shows that Ring Road achieves better performance and energy efficiency at various workloads and configurations.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {871–884},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00070,
author = {Jin, Zhixian and Rocca, Christopher and Kim, Jiho and Kasan, Hans and Rhu, Minsoo and Bakhoda, Ali and Aamodt, Tor M. and Kim, John},
title = {Uncovering Real GPU NoC Characteristics: Implications on Interconnect Architecture},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00070},
doi = {10.1109/MICRO61859.2024.00070},
abstract = {A critical component of high-throughput processors such as GPUs is the network-on-chip (NoC) that interconnects the large number of cores and the memory partitions together. In this work, we provide a detailed analysis, in terms of latency and bandwidth, of real GPU NoC across several generations of modern NVIDIA GPUs. Our analysis identifies how non-uniform latency exists between the cores and the memory partitions based on their physical location in the GPU. The non-uniformity can result in up to approximately 70 % difference in on-chip latency. In comparison, the bandwidth provided from the cores to the memory partitions is approximately uniform. However, recent GPUs that consist of multiple GPU “partitions” present different on-chip latency and bandwidth characteristics when communicating between the partitions. Based on our analysis of real GPU interconnect, we discuss potential implications including its impact on timing used in side-channel attacks as well as NoC microarchitectures. We show how the non-uniform latency can be exploited in a timing side-channel attack within a GPU as the core location impacts performance (or timing). In addition, proper understanding (and proper assumptions) of GPU NoC is critical to ensure a network that does not bottleneck the overall system performance.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {885–898},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00071,
author = {Qureshi, Moinuddin and Qazi, Salman and Jaleel, Aamer},
title = {MINT: Securely Mitigating Rowhammer with a Minimalist in-DRAM Tracker},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00071},
doi = {10.1109/MICRO61859.2024.00071},
abstract = {This paper investigates secure low-cost in-DRAM trackers for mitigating Rowhammer (RH). In-DRAM solutions have the potential to solve the RH problem within the DRAM chip without relying on other parts of the system. However, in-DRAM mitigation suffers from two key challenges: First, the mitigations are synchronized with refresh, which means that we cannot mitigate at arbitrary times. Second, the SRAM area available for aggressor tracking is limited to only a few bytes. Existing low-cost in-DRAM trackers (such as TRR) have been broken by well-crafted access patterns, whereas, secure counter-based schemes require impractical overheads of hundreds or thousands of entries per bank. The goal of our paper is to develop an ultra-low-cost secure in-DRAM tracker. Our solution is based on a simple observation: If only one row can be mitigated at refresh, we should ideally need to track only one row. We propose a Minimalist In-DRAM Tracker (MINT), which provides secure mitigation with just a single entry. Unlike prior trackers that decide the row to be mitigated based on the past behavior (select based on activation counts) or solely based on the current activation (select with some probability), MINT decides which row in the future will get mitigated. At each refresh, MINT probabilistically decides which activation in the upcoming interval will be selected for mitigation at the next refresh. MINT provides guaranteed protection against classic single and double-sided attacks. We also derive the minimum RH threshold (TRH*) tolerated by MINT across all patterns. MINT has a TRH* of 1482, which can be lowered to 356 with RFM. The TRH* of MINT is lower than a prior counter-based design with 677 entries per bank, and is within 2x of the TRH* of an idealized design that stores one-counter-per-row. We also analyze the impact of refresh postponement on the TRH* of low-cost in-DRAM trackers, and propose an efficient solution to make such trackers compatible with refresh postponement.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {899–914},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00072,
author = {Canpolat, O\u{g}uzhan and Ya\u{g}l\i{}k\c{c}\i{}, A. Giray and Olgun, Ataberk and Yuksel, Ismail Emir and Tu\u{g}rul, Yahya Can and Kanellopoulos, Konstantinos and Ergin, O\u{g}uz and Mutlu, Onur},
title = {BreakHammer: Enhancing RowHammer Mitigations by Carefully Throttling Suspect Threads},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00072},
doi = {10.1109/MICRO61859.2024.00072},
abstract = {RowHammer is a major read disturbance mechanism in DRAM where repeatedly accessing (hammering) a row of DRAM cells (DRAM row) induces bitflips in other physically nearby DRAM rows. RowHammer solutions perform preventive actions (e.g., refresh neighbor rows of the hammered row) that mitigate such bitflips to preserve memory isolation, a fundamental building block of security and privacy in modern computing systems. However, preventive actions induce non-negligible memory request latency and system performance overheads as they interfere with memory requests. As shrinking technology node size over DRAM chip generations exacerbates RowHammer, the overheads of RowHammer solutions become prohibitively expensive. As a result, a malicious program can effectively hog the memory system and deny service to benign applications by causing many RowHammer-preventive actions. In this work, we tackle the performance overheads of RowHammer solutions by tracking and throttling the generators of memory accesses that trigger RowHammer solutions. To this end, we propose BreakHammer. BreakHammer 1) observes the time-consuming RowHammer-preventive actions of existing RowHammer mitigation mechanisms, 2) identifies hardware threads that trigger many of these actions, and 3) reduces the memory bandwidth usage of each identified thread. As such, BreakHammer significantly reduces the number of RowHammer-preventive actions performed, thereby improving 1) system performance and DRAM energy, and 2) reducing the maximum slowdown induced on a benign application, with near-zero area overhead. Our extensive evaluations demonstrate that BreakHammer effectively reduces the negative performance, energy, and fairness effects of eight RowHammer mitigation mechanisms. To foster further research we open-source our BreakHammer implementation and scripts at https://github.com/CMU-SAFARI/BreakHammer.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {915–934},
numpages = {20},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00073,
author = {Saxena, Anish and Jaleel, Aamer and Qureshi, Moinuddin},
title = {ImPress: Securing DRAM Against Data-Disturbance Errors via Implicit Row-Press Mitigation},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00073},
doi = {10.1109/MICRO61859.2024.00073},
abstract = {DRAM cells are susceptible to Data-Disturbance Errors (DDE), which can be exploited by an attacker to compromise system security. Rowhammer is a well-known DDE vulnerability that occurs when a row is repeatedly activated. Rowhammer can be mitigated by tracking aggressor rows inside DRAM (in-DRAM) or at the Memory Controller (MC). Row-Press (RP) is a new DDE vulnerability that occurs when a row is kept open for a long time. RP significantly reduces the number of activations required to induce an error, thus breaking existing RH solutions. Prior work on Explicit Row-Press mitigation, ExPress, requires the memory controller to limit the maximum row-open-time, and redesign existing Rowhammer solutions with reduced Rowhammer threshold. Unfortunately, ExPress incurs significant performance and storage overheads, and being a memory controller-based solution, it is incompatible with in-DRAM trackers. In this paper, we propose Implicit Row-Press mitigation (in-Press), which does not restrict row-open-time, is compatible with memory controller-based and in-DRAM solutions and does not reduce the tolerated Rowhammer threshold. In-Press treats a row open for a specified time as equivalent to an activation. We design in-Press by developing a Unified Charge-Loss Model, which combines the net effect of both Rowhammer and Row-Press for arbitrary patterns. We analyze both controller-based (Graphene and PARA) and in-DRAM trackers (Mithril and MINT). We show that in-Press makes Rowhammer solutions resilient to Row-Press transparently without affecting the Rowhammer threshold.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {935–948},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00074,
author = {Hassan, Hasan and Olgun, Ataberk and Ya\u{g}l\i{}k\c{c}\i{}, A. Giray and Luo, Haocong and Mutlu, Onur and Zurich, ETH},
title = {Self-Managing DRAM: A Low-Cost Framework for Enabling Autonomous and Efficient DRAM Maintenance Operations},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00074},
doi = {10.1109/MICRO61859.2024.00074},
abstract = {The memory controller is in charge of managing DRAM maintenance operations (e.g., refresh, RowHammer protection, memory scrubbing) to reliably operate modern DRAM chips. Implementing new maintenance operations often necessitates modifications in the DRAM interface, memory controller, and potentially other system components. Such modifications are only possible with a new DRAM standard, which takes a long time to develop, likely leading to slow progress in the adoption of new architectural techniques in DRAM chips. We propose a new low-cost DRAM architecture, Self-Managing DRAM (SMD), that enables autonomous in-DRAM maintenance operations by transferring the responsibility for controlling main-tenance operations from the memory controller to the SMD chip. To enable autonomous maintenance operations, we make a single, simple modification to the DRAM interface, such that an SMD chip rejects memory controller accesses to DRAM regions (e.g., a subarray or a bank) under maintenance, while allowing memory accesses to other DRAM regions. Thus, SMD enables 1) implementing new in-DRAM maintenance mechanisms (or modifying existing ones) with no further changes in the DRAM interface, memory controller, or other system components, and 2) overlapping the latency of a maintenance operation in one DRAM region with the latency of accessing data in another. We evaluate SMD and show that it 1) can be implemented without adding new pins to the DDRx interface with low latency (0.4% of row activation latency) and area (1.1% of a 45.5 mm2 DRAM chip) overhead, 2) achieves 4.1 % average speedup across 20 four-core memory-intensive workloads over a DDR4-based system/-DRAM co-design technique that intelligently parallelizes main-tenance operations with memory accesses, and 3) guarantees for-ward progress for rejected memory accesses. We believe and hope SMD can enable innovations in DRAM architecture to rapidly come to fruition. We open source all SMD source code and data at https://github.com/CMU-SAFARI/SelfManagingDRAM.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {949–965},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00075,
author = {Laghari, Muhammad and Liu, Yuqing and Panwar, Gagandeep and Bears, David and Jearls, Chandler and Srinivas, Raghavendra and Choukse, Esha and Cameron, Kirk W. and Butt, Ali R. and Jian, Xun},
title = {Memory Allocation Under Hardware Compression},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00075},
doi = {10.1109/MICRO61859.2024.00075},
abstract = {As the scaling of memory density slows physically, a promising solution is to scale memory logically by enhancing the CPU's memory controller to encode and store data more densely in memory. This is known as hardware memory compression. Hardware memory compression decouples OS-managed physical memory from actual memory (i.e., DRAM); the memory controller spends a dynamically varying amount of DRAM on each physical page, depending on the compressibility of the page's content. The newly-decoupled actual memory effectively forms a new layer of memory beyond the traditional layers of virtual, pseudo-physical, and physical memory. We note unlike these traditional memory layers, each with its own specialized allocation interface (e.g., malloc/mmap for virtual memory, page tables+MMU for physical memory), this new layer of memory introduced by hardware memory compression still awaits its own unique memory allocation interface; its absence makes the allocation of actual memory imprecise and, sometimes, even impossible. Imprecisely allocating less actual memory, and/or unable to allocate more, can harm performance. Even imprecisely allocating more actual memory to some jobs can be harmful as it can result in allocating less actual memory to other jobs in highly-occupied memory systems, where compression is useful. To restore precise memory allocation, we design a new memory allocation specialized for this new layer of memory and, subsequently, architect a new MMU-like component in the memory controller and tackle the corresponding design challenges. We create a full-system FPGA prototype of a hardware-compressed memory system with precise memory allocation. Our evaluations using the prototype show that jobs perform stably under colocation. The performance variation is only 1%-2%; in comparison, it is 19%-89% under the prior art.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {966–982},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00076,
author = {Kim, Youngin and Song, William J.},
title = {Genie Cache: Non-Blocking Miss Handling and Replacement in Page-Table-Based DRAM Cache},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00076},
doi = {10.1109/MICRO61859.2024.00076},
abstract = {This paper presents Genie Cache that enables non-blocking miss-handling and replacement in a page-table-based DRAM cache (DC). Various DC designs have been proposed to meet the growing bandwidth demand of emerging memory-bound applications. The related literature can be categorized into hardware-based (HW-based) and page-table-based (PT-based) schemes based on their tag storage methods. HW-based designs store DC metadata (e.g., tags) in on-package DRAM for scalability but use extra bandwidth and energy for metadata access. PT-based schemes store DC tags in page table entries (PTEs), enabling virtual-to-cache address translations using the existing memory management units (MMUs) without the DC bandwidth overhead. However, their miss-handling and eviction mechanisms relying on operating systems (OS) incur nontrivial latency overhead. To minimize the OS intervention, Genie Cache implements non-blocking miss handling and replacement using a hardware unit called DRAM cache management unit (DCMU) and a novel pre-write back mechanism. In Genie Cache, DC misses detected by MMUs are forwarded to the DCMU, which handles the misses by allocating page frames and updating PTEs without calling OS routines. When the PT-based DRAM cache runs low on free pages, an eviction routine is called to flush TLBs and evict a batch of cached pages to avoid frequent TLB shootdowns. Since writing back many dirty pages in a blocking manner causes substantial application stall cycles, Genie Cache proactively writes dirty pages back to off-package memory, allowing the eviction routine to simply evict cleaned pages. Experimental results show that Genie Cache achieves 51.3% speedup over the state-of-the-art PT-based design via non-blocking miss handling and replacement.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {983–996},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00077,
author = {Cho, Albert and Daglis, Alexandros},
title = {StarNUMA: Mitigating NUMA Challenges with Memory Pooling},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00077},
doi = {10.1109/MICRO61859.2024.00077},
abstract = {Large multi-socket machines are mission-critical high-performance systems for workloads requiring massive memory shared by hundreds of processors. Beyond eight sockets, such systems typically feature multi-hop inter-socket networks, exacerbating the Non-Uniform Memory Access (NUMA) challenge. NUMA effects stem from major disparity in latency and bandwidth characteristics of local and remote memory, often in the 4–10× range. While judicious data placement across the distributed memory's fragments can ameliorate NUMA effects, we observe that in challenging workloads with irregular access patterns, a large fraction of accessed pages are “vagabond”: being actively shared by multiple sockets, they lack a fitting home socket location. On 16-socket systems, such pages incur up to 75% remote memory accesses, which encounter significant latency overheads and bandwidth bottlenecks. StarNUMA introduces a new architectural block for multi-socket architectures to ameliorate the challenge posed by vagabond pages. By leveraging the capabilities of the emerging CXL interconnect, StarNUMA augments a typical NUMA architecture with a memory pool that is directly accessible by every socket in a single high-bandwidth interconnect hop. We show that placement of vagabond pages in StarNUMA's memory pool effectively curbs the latency overheads and queuing delays of the bandwidth-constrained multi-hop inter-socket network, reducing the average memory access time of 16-socket systems by 48%. In turn, faster memory access yields performance improvements of 1.54× on average, and up to 2.17×.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {997–1012},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00078,
author = {Alawneh, Ahmad and Kang, Ni and Khairy, Mahmoud and Rogers, Timothy G.},
title = {ThreadFuser: A SIMT Analysis Framework for MIMD Programs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00078},
doi = {10.1109/MICRO61859.2024.00078},
abstract = {The broad usage of accelerators, such as GPUs, faces two important challenges. Developing code for a new accelerator is expensive and unpredictable. Porting large parallel programs from Multiple Instruction Multiple Data (MIMD) CPUs to Single Instruction Multiple Thread (SIMT) GPUs involves significant effort that may or may not result in improved performance versus the CPU. This high activation energy to create new workloads introduces the second challenge: architects and systems researchers lack a diverse SIMT codebase to study new designs. To tackle these challenges, we introduce ThreadFuser, an analysis framework that efficiently and accurately predicts the performance of any pre-written MIMD program on SIMT hardware. ThreadFuser conducts thorough control and data flow analysis on dynamic CPU program traces, determining the impact of lock-step execution on CPU binaries. Thread-Fuser efficiently delivers accurate reports on a MIMD program's divergence and synchronization characteristics. Moreover, ThreadFuser seamlessly integrates with state-of-the-art GPU simulators to conduct detailed analyses and produce fine-grained performance measurements. We evaluate ThreadFuser on a diverse set of 36 CPU workloads, demonstrating the potential and challenges of executing MIMD code on a SIMT machine. We demonstrate ThreadFuser's potential to inform software development decisions and open new areas to explore in data-parallel hardware design.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1013–1026},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00079,
author = {Barnes, Aaron and Shen, Fangjia and Rogers, Timothy G.},
title = {Extending GPU Ray-Tracing Units for Hierarchical Search Acceleration},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00079},
doi = {10.1109/MICRO61859.2024.00079},
abstract = {Specialized ray-tracing acceleration units have become a common feature in GPU hardware, enabling real-time ray-tracing of complex scenes for the first time. The ray-tracing unit accelerates the traversal of a hierarchical tree data structure called a bounding volume hierarchy to determine whether rays have intersected triangle primitives. Hierarchical search algorithms are a fundamental software pattern common in many important domains, such as recommendation systems and point cloud registration, but are difficult for GPUs to accelerate because they are characterized by extensive branching and recursion. The ray-tracing unit overcomes these limitations with specialized hardware to traverse hierarchical data structures efficiently, but is mired by a highly specialized graphics API, which is not readily adaptable to general-purpose computation. We present the Hierarchical Search Unit (HSU), a flexible datapath to accelerate a more general class of hierarchical search algorithms, of which ray-tracing is one. We synthesize a baseline ray-intersection datapath and maximize functional unit reuse while extending the ray-tracing unit to support additional computations and a more general set of instructions. We demonstrate that the unit can improve the performance of three hierarchical search data structures in approximate nearest neighbors search algorithms and a B-tree key-value store index. For a minimal extension to the existing unit, our HSU improves the state-of-the-art GPU approximate nearest neighbor implementation by an average of 24.8% using the GPU's general computing interface.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1027–1040},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00080,
author = {Ha, Dongho and Liu, Lufei and Chou, Yuan Hsi and Go, Seokjin and Ro, Won Woo and Tseng, Hung-Wei and Aamodt, Tor M.},
title = {Generalizing Ray Tracing Accelerators for Tree Traversals on GPUs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00080},
doi = {10.1109/MICRO61859.2024.00080},
abstract = {Tree traversal is a fundamental operation in many applications, such as database indexing and physics simulations. Although tree traversals feature high parallelism, they are inherently divergent and irregular, leading to inefficient performance on GPUs. Tree traversals are also prevalent in ray tracing, which is executed on dedicated Ray-Tracing Accelerators (RTAs) in modern GPUs to mitigate inefficiencies such as control flow divergence and underutilization of memory bandwidth by irregular memory accesses. In this paper, we propose the Tree Traversal Accelerator (TTA) to replicate the success of RTAs in ray tracing for general tree traversal applications. TTAs extend RTAs to support tree structures and operations beyond those in ray tracing, such as B- Tree search and radius search algorithms, by modifying existing computing units. Despite TTAs' effectiveness, they still rely on fixed-function computations, making it challenging to support other tree-based applications such as N-Body simulation fully. Thus, we introduce TTA + as an alternative design, which modularizes the RTA computing units and makes them programmable, trading some efficiency for flexibility. With less than 1 % increase in RTA area, our proposals can achieve up to S.4x speedup for B-Tree search, 1.7x for N-Body simulation, and 1.2x for select ray-tracing applications.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1041–1057},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00081,
author = {Tom\'{a}s, Aurora and Arag\'{o}n, Juan L. and Parcerisa, Joan-Manuel and Gonz\'{a}lez, Antonio},
title = {LIBRA: Memory Bandwidth- and Locality-Aware Parallel Tile Rendering},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00081},
doi = {10.1109/MICRO61859.2024.00081},
abstract = {The increasing demand for high-quality graphics requires a significant increase in computational power of modern GPUs. The common approach to follow is augmenting the number of compute units (i.e., shader cores). However, this can result in underutilized resources if the workload is not properly balanced. This is particularly challenging in Tile-Based Rendering (TBR) GPUs, the predominant architecture in mobile GPUs, running graphics applications due to limited per-tile workload. This work proposes parallel tile rendering to efficiently in-crease the computational capabilities of TBR GPUs. This solves the problem of not having enough work to utilize the additional compute units but causes memory-intensive applications to underperform due to the increased memory pressure. To this end, we introduce LIBRA, a parallel tile rendering architecture that includes a novel locality-aware approach to schedule tiles to Raster Units to evenly distribute memory requests during the rendering of each frame. This alleviates memory congestion, therefore, reducing memory access time. LIBRA leverages frame-to-frame coherence to predict the memory pressure of each tile of a frame without penalizing the hit ratio of the cache memories. Evaluations over a wide range of commercial gaming applications show that LIBRA reduces the average memory latency by 13.5% and achieves an average speedup of 20.9%. It also provides an 11.4% improvement in throughput (frames per second) and a total GPU energy reduction of 9.2%, while adding negligible overhead.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1058–1072},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00082,
author = {Lee, Hunjun and Jang, Yeongwoo and Jung, Daye and Song, Seunghyun and Kim, Jangwoo},
title = {Rearchitecting a Neuromorphic Processor for Spike-Driven Brain-Computer Interfacing},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00082},
doi = {10.1109/MICRO61859.2024.00082},
abstract = {Brain-computer interfaces (BCIs) are electrophysiological devices (e.g., electrode arrays) that connect the brain to a computer. They offer neuroscientific and neurological innovations by utilizing a dedicated processor for continuous BCI signal processing. Recent studies propose a scaled-up BCI that adopts an order of magnitude larger number of electrodes to more precisely interface with the brain. As the BCI scales, utilizing a spike-driven processor emerges as an alternative processing method, where the BCI offloads computations to the processor upon detecting spikes. However, the processor design for spike-driven processing has been relatively unexplored compared to that of the continuous processor. In this work, we propose NeuroLobe, a flexible and efficient processor design for spike-driven processing. The key idea is to utilize a neuromorphic processor to take advantage of its event-driven computing nature. We carefully rearchitect the existing neuromorphic system for the purpose of flexibly and efficiently deploying the BCI algorithms. First, we extend the instruction set architecture of the existing neuromorphic processor to flexibly deploy representative spike-driven BCI algorithms. Second, we redesign the connection controller and execution path to improve the performance. Third, we design a custom synchronization unit for scalable processing. Fourth, we implement a custom software stack to minimize load imbalance among the cores. Lastly, we design a multitask controller to simultaneously process multiple algorithms. We evaluate NeuroLobe on four representative BCI algorithms with 11 configurations. Evaluation results show that NeuroLobe surpasses CPU and GPU in terms of speed and energy efficiency.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1073–1089},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00083,
author = {Wang, Zongwu and Liu, Fangxin and Yang, Ning and Huang, Shiyuan and Li, Haomin and Jiang, Li},
title = {COMPASS: SRAM-Based Computing-in-Memory SNN Accelerator with Adaptive Spike Speculation},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00083},
doi = {10.1109/MICRO61859.2024.00083},
abstract = {Brain-inspired spiking neural networks (SNNs) are considered energy-efficient alternatives to conventional deep neural networks (DNNs). By adopting event-driven information processing, SNNs can significantly reduce the computational demands associated with DNNs, while still achieving comparable performance. However, current SNNs primarily prioritize high accuracy by constructing complex neuron models that generate sparse spikes. Unfortunately, this approach results in low energy efficiency and high latency, posing a significant challenge for deploying SNNs at the edge. Furthermore, the dominant computation in SNNs, which involves spike-wise Accumulate-Compare operations, is well-suited for Computing-in-Memory (CIM) architectures. However, exploiting high parallel processing and spike sparsity in CIM-based SNN accelerators is challenging due to the irregularity and time dependency of spikes. To address these limitations, the paper proposes COMPASS, a SRAM-based CIM architecture for efficient SNNs. We first introduce an efficient method to exploit irregular sparsity for both input spikes (explicit) and output spikes (implicit). This is achieved through a speculation mechanism that exploit dynamic spike patterns, enabling lean hardware for sparsity utilization. Additionally, the CIM architecture is carefully modified to facilitate dynamic spike pattern generation and exploitation with minimal overhead. Moreover, we design an adaptive dataflow with temporal spike representation tailored for input/output spikes, reducing memory footprint and enabling parallel execution. Comprehensive evaluation results demonstrate that COMPASS can achieve 26.7x end-to-end speedup over recent SNN accelerators hardware implementation with up to 386.7x less energy per inference.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1090–1106},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00084,
author = {Yin, Ruokai and Kim, Youngeun and Wu, Di and Panda, Priyadarshini},
title = {LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural Networks},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00084},
doi = {10.1109/MICRO61859.2024.00084},
abstract = {Spiking Neural Networks (SNNs) have gained significant research attention over the past decade due to their potential for enabling resource-constrained edge devices. While existing SNN accelerators efficiently process sparse spikes with dense weights, the opportunities for accelerating SNNs with sparse weights, referred to as dual-sparsity, remain underexplored. In this work, we focus on accelerating dual-sparse SNNs, particularly on their core operation: sparse-matrix-sparse-matrix multiplication (spMspM). Our observations reveal that executing a dual-sparse SNN on existing spMspM accelerators designed for dual-sparse Artificial Neural Networks (ANNs) results in sub-optimal efficiency. The main challenge is that SNNs, which naturally processes multiple timesteps, introducing an additional loop in ANN spMspM, leading to longer latency and more memory traffic. To address this issue, we propose a fully temporal-parallel (FTP) dataflow that minimizes data movement across timesteps and reduces the end-to-end latency of dual-sparse SNNs. To enhance the efficiency of the FTP dataflow, we introduce an FTP-friendly spike compression mechanism that efficiently compresses single-bit spikes and ensures contiguous memory access. Additionally, we propose an FTP-friendly inner-join circuit that reduces the cost of expensive prefix-sum circuits with negligible throughput penalties. These innovations are encapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs. Running dual-sparse SNN workloads on LoAS demonstrates significant speedup (up to &lt;tex&gt;$8.51 times$&lt;/tex&gt;) and energy reduction (up to &lt;tex&gt;$3.68times$&lt;/tex&gt;) compared to prior dual-sparse accelerators.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1107–1121},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00085,
author = {Liu, Xiaoyi and Pu, Zhongzhu and Qu, Peng and Zheng, Weimin and Zhang, Youhui},
title = {ActiveN: A Scalable and Flexibly-Programmable Event-Driven Neuromorphic Processor},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00085},
doi = {10.1109/MICRO61859.2024.00085},
abstract = {At present, most neuromorphic chips utilize custom circuits and/or on-chip memory to achieve neural computations and parameter storage. However, with the diversified development of brain-inspired applications, this approach faces significant challenges regarding programming flexibility and scalability. To address these issues, we propose a RISC-V-based many-core neuromorphic architecture, ActiveN. Each neuro-core is equipped with an active-message-enabled micro-architecture to support the event-driven programming model of Spiking Neural Networks (SNNs), and the memory subsystem is enhanced to identify sparse data and forward them directly. These mechanisms significantly mitigate the impact of memory latency on performance to enable the storage of synapse data in off-chip (or off-die) bulk storages (e.g., DRAMs, HBMs), which not only improve storage scalability but also increase computing density. Furthermore, the core's instruction set is customized to include a compact and complete set of fixed-and floating-point computing instructions, supporting various neural models and SNN computation algorithms flexibly. End-to-end prototype testing demonstrates that, compared to a state-of-the-art chip based on custom circuitry and on-chip SRAM that also supports event-driven operations, ActiveN can integrate over 10x more processing units (512) with strong scalability to fully utilize memory bandwidth. Additionally, it achieves 7.9 times the performance of this counterpart and 96.6 times the performance of an NVIDIA A100 GPU, while maintaining flexible programmability.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1122–1137},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00086,
author = {Jin, Zhixian and Ahn, Jaeguk and Kim, Jiho and Kasan, Hans and Song, Jina and Song, Wonjun and Kim, John},
title = {Ghost Arbitration: Mitigating Interconnect Side-Channel Timing Attacks in GPU},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00086},
doi = {10.1109/MICRO61859.2024.00086},
abstract = {Network-on-chip (NoC) is a critical shared resource in scalable multicore processors; however, it is well-known that shared resources can lead to side-channel attacks. In this work, we demonstrate how contention for on-chip bandwidth in GPUs can lead to fine-grain information leakage and enable side-channel attacks. As a case study, we demonstrate how RSA key bit information can be leaked on a real GPU. We also describe how interconnect characteristics from the side-channel or an interconnect-gram can be used to fingerprint kernels executing on the GPU. To defend against such fine-grain side-channel attack, we propose secure arbitration that prevents information leakage while minimizing performance impact during normal execution. In particular, we present a novel ghost arbitration that prevents interconnect contention from being leveraged to leak information by keeping track of “ghost” requests or requests when other nodes receive free arbitration to enable least-recently-used priority. However, if the attacker reverse engineers the arbitration, a naive implementation of ghost arbitration can still lead to information leakage. Thus, we propose a weighted ghost arbitration that exploits “malicious” communication patterns to prevent information leakage with minimal loss in performance. Compared to previously proposed arbitration that is secure (e.g., strict time-division multiplexing), ghost arbitration is able to improve performance by up to &lt;tex&gt;$4times$&lt;/tex&gt; •},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1138–1152},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00087,
author = {Chowdhuryy, Md Hafizul Islam and Yao, Fan},
title = {IvLeague: Side Channel-Resistant Secure Architectures Using Isolated Domains of Dynamic Integrity Trees},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00087},
doi = {10.1109/MICRO61859.2024.00087},
abstract = {Modern secure processors rely on hardware-assisted encryption and tree-based integrity verification to protect off-chip data. However, despite extensive research on performance optimization’ there is a significant lack of emphasis on side channel vulnerabilities in secure architectures. Given the strong focus on data security, it is critical to ensure that the integration of new design elements into secure architectures does not inadvertently introduce additional vulnerabilities. Existing integrity verification mechanisms use a global in-tegrity tree shared across security domains, which can introduce side channel leakage through integrity tree metadata sharing. In this work, we present IvLeague framework - a novel integrity ver-ification mechanism for side channel-resistant isolated integrity trees among dynamic domains in secure processors. Specifically, IvLeague splits the global tree into multiple fixed-size subtrees, dynamically allocating these subtrees to domains during runtime. IvLeague enables efficient runtime scaling of memory coverage for individual domains. Additionally, we IvLeague-Invert, an optimization which shortens the integrity verification path by mapping data pages to high-level tree nodes. Finally, IvLeague-Pro further improves the integrity verification of hotpages by enabling efficient hotpage tracking and migrating hotpages closer to the root. We extensively evaluate all three IvLeague schemes using 16 real-world workloads with varying memory footprints. IvLeague scheme, along with its optimizations, demonstrates a 5%-19% speedup over the insecure baseline, while providing effective side channel protection for the integrity tree. Moreover, IvLeague ensures high utilization of TreeLings (over 99.5%) and supports workloads with highly skewed memory footprints.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1153–1168},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00088,
author = {Miao, Yuanqing and Zhang, Yingtian and Wu, Dinghao and Zhang, Danfeng and Tan, Gang and Zhang, Rui and Kandemir, Mahmut Taylan},
title = {Veiled Pathways: Investigating Covert and Side Channels Within GPU Uncore},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00088},
doi = {10.1109/MICRO61859.2024.00088},
abstract = {With the emergence of GPUs as first-class compute engines, more concentrated focus has been put into covert and side channel discovery in these architectures. However, most of the covert and side channels uncovered on GPUs to date are rooted in “GPU cores”, which include computational cores, cache and core interconnects, but they do not consider “GPU uncore”, which include non-computational engines, GPU DRAM, host-G PU links and inter-GPulinks. In this paper, we delve into the less-explored domains of GPU uncore, unveiling four novel leakage sources for covert and side channel exploitation: (1) GPU DRAM frequency scaling; (2) NVENC utilization; (3) NVDEC utilization; (4) NVJPEG utilization. What makes these covert and side channels interesting is that they all take effect under the GPU MPS mode - which fractionalizes GPU cores and GPU memory on both desktop-scale and server-scale GPUs. Furthermore, our study reevaluates PCI-e bandwidth allocation on GPUs. Notably, we have engineered covert and side channel capable of bypassing GPU MIG isolation - a mechanism implemented by NVIDIA to physically segregate hardware resources on server-scale GPUs. Our research showcases concrete examples of these covert and side channels, highlighting their potency in breaching system security, all achieved without necessitating root privileges. This underscores the practical implications and urgency of addressing these vulnerabilities in GPU architectures.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1169–1183},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00089,
author = {Agarwal, Nikhil and Fream, Mitchell and Ghosh, Souradip and Schwedock, Brian C. and Beckmann, Nathan},
title = {The TYR Dataflow Architecture: Improving Locality by Taming Parallelism},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00089},
doi = {10.1109/MICRO61859.2024.00089},
abstract = {Architectures should aim to maximize parallelism within a machine's finite memories, but prior designs tend to extremes, either maximizing parallelism or minimizing state. In particular, prior unordered dataflow architectures suffer from a parallelism explosion that creates unbounded state, requires prohibitively large associative memories, and risks deadlock. The few architectures that successfully navigate the parallelism-state tradeoff are limited to embarrassingly parallel programs. Tyr is a new, general-purpose unordered dataflow architecture that achieves high parallelism with bounded state. The key insight is that prior unordered dataflow architectures are overly conservative, unnecessarily allocating tags from a single, global tag space. Tyr exploits program structure to break up tags into local tag spaces that operate independently. Local tag spaces eliminate tag competition between co-dependent parts of the program, provably guaranteeing forward progress with only two tags per local tag space. Tyr thus opens the door to an efficient, scalable implementation of unordered dataflow. Simulation of parallel programs demonstrates that Tyr achieves parallelism nearly identical to a naïve unordered dataflow architecture with orders-of-magnitude less state.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1184–1200},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00090,
author = {Zhang, Yunan and Tsai, Po-An and Tseng, Hung-Wei},
title = {Sparsepipe: Sparse Inter-operator Dataflow Architecture with Cross-Iteration Reuse},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00090},
doi = {10.1109/MICRO61859.2024.00090},
abstract = {Sparse Tensor Algebra (STA) applications are limited by data movement and can benefit from better data reuse. Prior research has focused on intra-operator data reuse, such as better dataflow or caching technique for a single STA operation, missing other reuse opportunities at the application level. By expressing STA applications as sparse tensor dataflow graphs, we first identify two unexplored inter-operator data reuse opportunities: 1) producer-consumer reuse and 2) cross-iteration reuse. Producer-consumer reuse combines multiple operations to reduce data movement for intermediate results. Cross-iteration data reuse, a new opportunity identified in this paper, reduces the data movement for the shared sparse data (e.g., graph) across iterations. We then propose Output-stationary-_Element-wise-Input-stationary (OEI) dataflow, a novel dataflow to capture both reuse opportunities in STA applications, and Sparsepipe, a sparse dataflow architecture to support the OEI dataflow and maximize data reuse. Evaluation results show that Sparsepipe with OEI dataflow is 19.82×/4.65× faster than CPU/GPU and 1.77× faster than an ideal sparse accelerator that cannot exploit inter-operator reuse.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1201–1216},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00091,
author = {Jain, Rishabh and Bhasi, Vivek M. and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut T. and Das, Chita R.},
title = {Pushing the Performance Envelope of DNN-based Recommendation Systems Inference on GPUs},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00091},
doi = {10.1109/MICRO61859.2024.00091},
abstract = {Personalized recommendation is a ubiquitous appli-cation on the internet, with many industries and hyperscalers extensively leveraging Deep Learning Recommendation Models (DLRMs) for their personalization needs (like ad serving or movie suggestions). With growing model and dataset sizes pushing computation and memory requirements, GPUs are being increasingly preferred for executing DLRM inference. However, serving newer DLRMs, while meeting acceptable latencies, continues to remain challenging, making traditional deployments increasingly more GPU-hungry, resulting in higher inference serving costs. In this paper, we show that the embedding stage continues to be the primary bottleneck in the GPU inference pipeline, leading up to a 3.2 x embedding-only performance slowdown. To thoroughly grasp the problem, we conduct a detailed microarchitecture characterization and highlight the presence of low occupancy in the standard embedding kernels. By leveraging direct compiler optimizations, we achieve optimal occupancy, pushing the performance by up to 53 %. Yet, long memory latency stalls continue to exist. To tackle this challenge, we propose spe-cialized plug-and-play-based software prefetching and L2 pinning techniques, which help in hiding and decreasing the latencies. Further, we propose combining them, as they complement each other. Experimental evaluations using AI00 GPUs with large models and datasets show that our proposed techniques improve performance by up to 103% for the embedding stage, and up to 77 % for the overall D LRM inference pipeline.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1217–1232},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00092,
author = {Lee, Hyun Ryong and Sanchez, Daniel},
title = {Terminus: A Programmable Accelerator for Read and Update Operations on Sparse Data Structures},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00092},
doi = {10.1109/MICRO61859.2024.00092},
abstract = {Sparse data structures like hash tables, trees, or compressed tensors are ubiquitous, but operations on these structures are expensive and inefficient on current systems. Prior work has proposed hardware acceleration for these operations, but these techniques have two key shortcomings: they limit the types of data structures they support, and they focus on reads but do not support fine-grained updates to these structures. We present Terminus, a programmable accelerator for read and update operations on sparse data structures. Terminus extends each general-purpose core with a programmable dataflow engine capable of accelerating a wide range of structures and operations. Terminus engines are flexible yet simple, as they focus on common operations and defer rare, complex ones to cores. Terminus features a simple concurrency control mechanism based on address ranges that enables safe updates while preserving parallelism. We evaluate Terminus on serial and parallel benchmarks on a wide range of sparse data structures. Terminus improves performance by gmean &lt;tex&gt;$7.4times$&lt;/tex&gt; over a CPU baseline, showing that Terminus can accelerate fine-grained reads and writes that were previously not possible in prior accelerators for sparse structures.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1233–1246},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00093,
author = {Wang, Huizheng and Fang, Jiahao and Tang, Xinru and Yue, Zhiheng and Li, Jinxi and Qin, Yubin and Guan, Sihan and Yang, Qinze and Wang, Yang and Li, Chao and Hu, Yang and Yin, Shouyi},
title = {SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage Coordinated Tiling},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00093},
doi = {10.1109/MICRO61859.2024.00093},
abstract = {Benefiting from the self-attention mechanism, Transformer models have attained impressive contextual comprehension capabilities for lengthy texts. The requirements of high-throughput inference arise as the large language models (LLMs) become increasingly prevalent, which calls for large-scale token parallel processing (LTPP). However, existing dynamic sparse accelerators struggle to effectively handle LTPP, as they solely focus on separate stage optimization, and with most efforts confined to computational enhancements. By re-examining the end-to-end flow of dynamic sparse acceleration, we pinpoint an ever-overlooked opportunity that the LTPP can exploit the intrinsic coordination among stages to avoid excessive memory access and redundant computation. Motivated by our observation, we present SOFA, a cross-stage compute-memory efficient algorithm-hardware co-design, which is tailored to tackle the challenges posed by LTPP of Transformer inference effectively. We first propose a novel leading zero computing paradigm, which predicts attention sparsity by using log-based add-only operations to avoid the significant overhead of prediction. Then, a distributed sorting and a sorted updating FlashAttention mechanism are proposed with cross-stage coordinated tiling principle, which enables fine-grained and lightweight coordination among stages, helping optimize memory access and latency. Further, we propose a SOFA accelerator to support these optimizations efficiently. Extensive experiments on 20 benchmarks show that SOFA achieves &lt;tex&gt;$9.5times$&lt;/tex&gt; speed up and &lt;tex&gt;$71.5times$&lt;/tex&gt; higher energy efficiency than Nvidia A100 GPU. Compared to eight SOTA accelerators, SOFA achieves an average &lt;tex&gt;$15.8times$&lt;/tex&gt; energy efficiency, &lt;tex&gt;$10.3times$&lt;/tex&gt; area efficiency and &lt;tex&gt;$9.3times$&lt;/tex&gt; speed up, respectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1247–1263},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00094,
author = {Yu, Hui and Zhang, Yu and He, Ligang and Zhao, Yingqi and Li, Xintao and Xin, Ruida and Zhao, Jin and Liao, Xiaofei and Liu, Haikun and He, Bingsheng and Jin, Hai},
title = {RAHP: A Redundancy-aware Accelerator for High-performance Hypergraph Neural Network},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00094},
doi = {10.1109/MICRO61859.2024.00094},
abstract = {Hypergraph Neural Network (HyperGNN) has emerged as a potent methodology for dissecting intricate multilateral connections among various entities. Current software/hardware solutions leverage a sequential execution model that relies on hyperedge and vertex indices for conducting standard matrix operations for HyperGNN inference. Yet, they are impeded by the dual challenges of redundant computation and irregular memory access overheads. This is primarily due to the frequent and repetitive access and updating of a number of feature vectors corresponding to the same hyperedges and vertices. To address these challenges, we propose the first redundancy-aware accelerator, RAHP, which enables high performance execution of HyperGNN inference. Specifically, we present a redundancy-aware asynchronous execution approach into the accelerator design for HyperGNN to reduce redundant computations and off-chip memory accesses. To unveil opportunities for data reuse and unlock the parallelism that existing HyperGNN solutions fail to capture, it prioritizes vertices with the highest degree as roots, prefetching other vertices along the hypergraph structure to capture the common vertices among multiple hyperedges, and synchronizing the computations of hyperedges and vertices in real-time. By such means, this facilitates the concurrent processing of relevant hyperedge and vertex computations of the common vertices along the hypergraph topology, resulting in smaller redundant computations overhead. Furthermore, by efficiently caching intermediate results of the common vertices, it curtails memory traffic and off-chip communications. To fully harness the performance potential of our proposed approach in the accelerator, RAHP incorporates a topology-driven data loading mechanism to minimize off-chip memory accesses on the fly. It is also endowed with an adaptive data synchronization scheme to mitigate the effects of conflicting updates of both hyperedges and vertices. Moreover, RAHP employs the similarity-based data caching strategy to further mitigate the overhead of redundant data transfers. We have implemented and assessed RAHP on a Xilinx Alveo U280 FPGA card. Experimental evaluations demonstrate that RAHP achieves average speedups of 439.2x and 64.7x for HyperGNN inference, alongside average energy savings of 542.8x and 84.2x, compared to the cutting-edge software-based HyperGNN implementations on Intel Xeon CPUs and NVIDIA A100 GPUs, respectively. Additionally, in the realm of HyperGNN inference, RAHP secures average speedups of 7.8x, 5.4x, and 3.8x, and average energy savings of 10.2x, 8.9x, and 6.5x over the foremost GNN accelerators, i.e., FlowGNN, LL-GNN, and ReGNN, respectively.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1264–1277},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00095,
author = {Schwedock, Brian C. and Beckmann, Nathan},
title = {Leviathan: A Unified System for General-Purpose Near-Data Computing},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00095},
doi = {10.1109/MICRO61859.2024.00095},
abstract = {The rising cost of data movement poses a significant challenge to future computing systems. The call to arms for novel data-centric systems has spawned a wave of near-data computing (NDC) architectures that move compute closer to data. Despite large benefits promised by NDC, prior designs suffer from limited applicability and difficult programming. This paper identifies the commonalities and differences across NDC designs to develop Leviathan, a unified architecture and programming interface for near-cache NDC. We build a taxonomy of NDC and identify the key dimensions as what, where, and when to compute. Leviathan provides a simple reactive-programming interface and automatically executes actions near data at the right time and place. The ability to integrate multiple NDC paradigms makes Leviathan the only general-purpose system to support a variety of specialized NDC designs. Across a range of NDC-specialized applications, Leviathan improves performance by 1.5×-3.7× and reduces energy by 22%-77% vs. a baseline multicore, while adding only ≈6% area compared to the last-level cache.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1278–1294},
numpages = {17},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00096,
author = {Li, Zerun and Chen, Xiaoming and Han, Yinhe},
title = {TMiner: A Vertex-Based Task Scheduling Architecture for Graph Pattern Mining},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00096},
doi = {10.1109/MICRO61859.2024.00096},
abstract = {Graph pattern mining discovers important patterns in graphs. It is both computation-and memory-intensive, characterized by numerous set operations and irregular memory access. Graph pattern mining inherently involves a large number of independent tasks, helping to alleviate its computational bottleneck through parallel processing. However, after exploiting parallelism, memory access will become the primary bottleneck. Existing parallelism strategies severely result in redundant and inefficient memory access, making the performance memory bounded. This paper proposes TMiner, a graph pattern mining architecture with optimized memory performance through a systematically designed software-hardware stack. TMiner fundamentally reduces redundant memory access of parallel graph pattern mining in three aspects. (1) TMiner leverages a task partitioning approach based on disjoint neighbor vertex set access, reducing redundant memory access between PEs. (2) TMiner utilizes the global neighbor vertex information to coalesce the access from different neighbor vertex subsets at compilation time, which not only reduces redundant memory access within a task but also improves the data locality. (3) TMiner adopts a data reuse-oriented task scheduling mechanism, which dynamically migrates and merges tasks with similar memory access patterns, reducing redundant memory access within a PE at runtime. A DIMM-based near-memory architecture that exploits the DRAM's internal bandwidth is elaborated for high-performance graph pattern mining, which incorporates the proposed memory access optimization techniques and an extended ISA. Compared with the state-of-the-art software and hardware baselines, TMiner significantly improves the performance.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1295–1308},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00097,
author = {Chen, Xuan-Jun and Chen, Han-Ping and Yang, Chia-Lin},
title = {PointCIM: A Computing-in-Memory Architecture for Accelerating Deep Point Cloud Analytics},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00097},
doi = {10.1109/MICRO61859.2024.00097},
abstract = {Efficient deep point cloud (PC) analytics is crucial for numerous emerging applications such as autonomous vehicles and augmented and virtual reality. Our roofline model analysis reveals that the “memory wall” bottleneck primarily constrains the execution efficiency of deep PC analytics, providing valuable insight into optimization opportunities. In contrast to previous works, which greatly rely on approximating the original algorithm to fit hardware limitations, the approach presented in this paper is analytical; that is, our approach does not require any modification to the original algorithm, thus preserving its integrity and accuracy. In this paper, we introduce PointCIM, the first deep PC analytics accelerator that leverages computing-in-memory (CIM) optimization opportunities to address memory inefficiency. We identify that existing in-memory methods cannot fully support the distance function required by PC network inference. To address the challenge, we propose computation optimizations, including the Base+Offset mapping and early stopping for bit-serial computation, not only to enable full support for PC network inference in memory, but also to significantly improve hardware efficiency. We design the CIM architecture support for the proposed computation optimizations, including the memristor crossbar architecture, custom peripheral logic, data layout, and pipelined execution. Evaluation results show that the designed accelerator provides an average speedup of 17.1× and an energy reduction of 9.6× compared to the baseline of a typical edge SoC. We also compare PointCIM with several state-of-the-art PC accelerators, yielding up to 10.7× speedup and 4.9× energy savings.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1309–1322},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00098,
author = {Bakhshalipour, Mohammad and Zare, Hamidreza and Samandi, Farid and Golshan, Fatemeh and Lotfi-Kamran, Pejman and Sarbazi-Azad, Hamid},
title = {Blenda: Dynamically-Reconfigurable Stacked DRAM},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00098},
doi = {10.1109/MICRO61859.2024.00098},
abstract = {This paper proposes Blenda, a dynamically-partitioned memory-cache blend architecture for giga-scale die-stacked DRAMs. Blenda architects the stacked DRAM partly as memory and partly as cache, and dynamically adjusts each part's size to workloads' demands. The memory part hosts hot data objects and serves requests to them efficiently (i.e., without metadata overheads). The cache part captures transient data and filters requests to bandwidth-limited off-chip DRAM. Blenda provides three key contributions: (i) Blenda partitions stacked DRAM's capacity in a workload-aware manner: different workloads enjoy different memory-cache configurations. (ii) Blenda is reactive: the configuration is adjusted to workloads' phases dynamically and application-transparently: no reboot or user involvement are needed. (iii) Blenda gracefully transitions among configurations: no data invalidation is required upon most reconfigurations. We simulate 15 diverse big-data workloads running on a state-of-the-art processor and show that Blenda outperforms the best-performing prior architecture by 34%. Blenda's total storage overhead is less than 100 bytes per core.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1323–1337},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00099,
author = {Tan, Cheng and Jiang, Miaomiao and Patil, Deepak and Ou, Yanghui and Li, Zhaoying and Ju, Lei and Mitra, Tulika and Park, Hyunchul and Tumeo, Antonino and Zhang, Jeff},
title = {ICED: An Integrated CGRA Framework Enabling DVFS-Aware Acceleration},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00099},
doi = {10.1109/MICRO61859.2024.00099},
abstract = {Coarse-grained reconfigurable arrays (CGRAs) are a promising solution to enable energy-efficient acceleration of applications from different domains. By leveraging reconfiguration at the functional level, they can adapt to significantly different computational patterns. However, the relationships of voltage and frequency with the utilization of CGRA resources and the dynamic management of them are not well explored, leading to inefficient designs. CGRAs have also been successful in accelerating data-dependent streaming applications. However, in these applications, the execution time of each kernel in the pipeline might dynamically vary depending on the characteristics of the input. This also leads to under-utilization of resources for the dynamically changing kernels that do not limit the application throughput. DVFS can also improve energy efficiency for these applications by dynamically changing the voltage and frequency levels of tiles that host non-performance-constraining kernels. This paper proposes ICED - an integrated DVFS-aware framework to map applications on CGRAs that support power islands. ICED proposes a CGRA architecture supporting DVFS islands at varying granularity (from a single tile to a group of tiles) and the related DVFS-aware compilation and mapping toolchain. ICED is the first work that introduces DVFS support for spatio-temporal CGRAs at power-island levels. The experimental evaluation shows that ICED improves average utilization by &lt;tex&gt;$mathbf{2}.mathbf{3}times$&lt;/tex&gt; and energy-efficiency by &lt;tex&gt;$mathbf{1}.mathbf{32}times$&lt;/tex&gt; over a conventional CGRA. With streaming applications, ICED can achieve up to &lt;tex&gt;$mathbf{1}.mathbf{26}times$&lt;/tex&gt; energy-efficiency compared with a state-of-the-art CGRA that introduces partial dynamic reconfiguration to adapt to variations in kernels' throughput.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1338–1352},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00100,
author = {Prabhakar, Raghu and Sivaramakrishnan, Ram and Gandhi, Darshan and Du, Yun and Wang, Mingran and Song, Xiangyu and Zhang, Kejie and Gao, Tianren and Wang, Angela and Li, Xiaoyan and Sheng, Yongning and Brot, Joshua and Sokolov, Denis and Vivek, Apurv and Leung, Calvin and Sabnis, Arjun and Bai, Jiayu and Zhao, Tuowen and Gottscho, Mark and Jackson, David and Luttrell, Mark and Shah, Manish K. and Chen, Zhengyu and Liang, Kaizhao and Jain, Swayambhoo and Thakker, Urmish and Huang, Dawei and Jairath, Sumti and Brown, Kevin J. and Olukotun, Kunle},
title = {SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00100},
doi = {10.1109/MICRO61859.2024.00100},
abstract = {Monolithic large language models (LLMs) like GPT-4 have paved the way for modern generative AI applications. Training, serving, and maintaining monolithic LLMs at scale, however, remains prohibitively expensive and challenging. The disproportionate increase in compute-to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI. Recent research has shown that a composition of many smaller expert models, each with several orders of magnitude fewer parameters, can match or exceed the capabilities of monolithic LLMs. Composition of Experts (CoE) is a modular approach that lowers the cost and complexity of training and serving. However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them. In this paper, we describe how combining CoE, streaming dataflow, and a three-tier memory system scales the AI memory wall. We describe Samba-CoE, a CoE system with 150 experts and a trillion total parameters. We deploy Samba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) -a commercial dataflow accelerator architecture that has been codesigned for enterprise inference and training applications. The chip introduces a new three-tier memory system with on-chip distributed SRAM, on-package HBM, and off-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out over multiple sockets. We demonstrate speedups ranging from 2× to 13× on various benchmarks running on eight RDU sockets compared with an unfused baseline. We show that for CoE inference deployments, the 8-socket RDU Node reduces machine footprint by up to 19 ×, speeds up model switching time by 15× to 31×, and achieves an overall speedup of 3.7× over a DGX H100 and 6.6× over a DGX A100.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1353–1366},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00101,
author = {Roelandts, Jaime and Naithani, Ajeya and Ainsworth, Sam and Jones, Timothy M. and Eeckhout, Lieven},
title = {Scalar Vector Runahead},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00101},
doi = {10.1109/MICRO61859.2024.00101},
abstract = {Modern graph and database processing typically takes place on high-end servers in data centers. However, with growing concerns of data privacy, trustworthiness, and all-time connectivity, there has been a shift toward increased analytics processing on edge devices such as mobile phones. In an ideal scenario, we would run these applications on the energy-efficient in-order cores available in these systems rather than the power-hungry out-of-order cores. However, these applications typically feature an extremely low computation-to-communication ratio and irregular memory accesses, meaning their performance is memory-bound, and out-of-order cores provide significant performance advantages over their in-order counterparts. Although prior work on Vector Runahead has substantially improved the performance of graph applications on very large out-of-order cores, it incurs high complexity and power consumption, so is unsuitable for energy-efficient in-order processors. Scalar Vector Runahead (SVR) extracts high memory-level parallelism on simple in-order cores by piggybacking on existing instructions executed on the processor leading to future irregular memory accesses. SVR executes multiple transient, independent, parallel instances of memory accesses and their chains initiated from different values of a predicted induction variable to move mutually independent memory accesses next to each other to hide dependent stalls. With a hardware overhead of only 2 KiB, SVR delivers &lt;tex&gt;$mathbf{3.2}times$&lt;/tex&gt; higher performance than a baseline 3-wide in-order core, and &lt;tex&gt;$mathbf{1.3}times$&lt;/tex&gt; higher performance than a full out-of-order core, while halving energy consumption. Increasing the overhead to 9 KiB to account for a larger register file, SVR can extend the speedup relative to an out-of-order core to &lt;tex&gt;$mathbf{1.7}times$&lt;/tex&gt;.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1367–1381},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00102,
author = {Brunner, Roman and Kumar, Rakesh},
title = {Weeding out Front-End Stalls with Uneven Block Size Instruction Cache},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00102},
doi = {10.1109/MICRO61859.2024.00102},
abstract = {The core front-end remains a critical bottleneck in modern server workloads owing to their multi-MB instruction footprints stemming from deep software stacks. Prior work has mainly investigated instruction prefetching and cache replacement policies to mitigate this bottleneck. In this work, we take an orthogonal approach and analyze instruction cache storage efficiency. Our analysis shows that, on average, about 60% of the bytes in a cache block are never accessed before the block is evicted from the instruction cache. This represents a huge storage inefficiency that more than halves the effective cache capacity. We observe that this inefficiency is caused by the fixed cache block sizes which are unable to accommodate the varying spatial locality inherent in the instruction stream. To mitigate this inefficiency, we propose Uneven Block Size (UBS) instruction cache, which supports different cache block sizes in a cache set. Our evaluation shows that UBS cache improves the storage efficiency by 32 percentage points over the baseline instruction cache. Further, by supporting uneven block sizes, UBS cache accommodates more than twice the number of blocks than a conventional cache within a given storage budget. Overall, the additional blocks combined with the better storage efficiency result in UBS cache approaching the performance of a 64KB conventional cache on a set of server workloads while requiring a storage budget similar to a 32KB conventional cache.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1382–1396},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00103,
author = {Stojkovic, Jovan and Choukse, Esha and Saurez, Enrique and Goiri, \'{I}\~{n}igo and Torrellas, Josep},
title = {Mosaic: Harnessing the Micro-Architectural Resources of Servers in Serverless Environments},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00103},
doi = {10.1109/MICRO61859.2024.00103},
abstract = {With serverless computing, users develop scalable applications using lightweight functions as building blocks, while cloud providers own most of the computing stack, allowing for better resource optimizations. In this paper, we observe that modern server-class processors are inefficiently utilized in serverless environments. Cores perform frequent context switches within function invocations and have a high degree of oversubscription. In such an environment, functions frequently lose their micro-architectural state in stateful hardware structures like caches, TLBs, and branch predictors, causing performance degradation. At the same time, modern processors are dimensioned for the needs of a broad set of applications, rendering them suboptimal for serverless workloads. Based on these insights, we propose Mosaic, an architecture optimized for serverless environments that maintains generality to efficiently support other workloads. Mosaic has two components: (1) MosaicCPU, a processor architecture that efficiently runs both serverless workloads and traditional monolithic applications, and (2) MosaicScheduler, a software stack for serverless systems that maximizes the benefits of MosaicCPU. MosaicCPU slices micro-architectural structures into small chunks and assigns tiles of such chunks to functions. The processor retains the state of functions in their tiles across context switches, thereby improving performance. Furthermore, currently-inactive tiles are set to a low power mode, thereby reducing energy consumption. In addition, MosaicScheduler maximizes efficiency by introducing predictive right-sizing of the per-function tiles, alongside with smart scheduling based on the state of the tiles. Overall, compared to conventional server-class processors, Mosaic improves the throughput of serverless workloads by 225% while using 22% less power.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1397–1412},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00104,
author = {Gao, Peng and Liu, Yang and Wang, Jun and Cai, Wanlin and Shen, Guangchong and Hong, Zonghui and Qu, Jiali and Wang, Ning},
title = {SOPHGO BM1684X: A Commercial High Performance Terminal AI Processor with Large Model Support},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00104},
doi = {10.1109/MICRO61859.2024.00104},
abstract = {This paper presents BM1684X, a cutting-edge AI processor from SOPHGO designed to meet the demanding requirements of broad AI applications. Firstly, we employ SIMD architecture with very large data width to design our TPU to reduce the area ratio of the instruction unit and greatly improves the computing power density. Secondly, the customization of special acceleration instructions within the EU enables the dynamic pipeline execution, leading to a reduction in the total number of instructions and execution time. This customization enhances the performance of TPU in processing RQ and DQ operations, crucial for AI computations. Thirdly, the CUBE array within the TPU implements the multiplication and addition operations of 64 pairs of INT8 operands in the channel dimension of the feature map. By utilizing an addition tree instead of a conventional adder, the implementation significantly reduces both area and power consumption, optimizing the efficiency of TPU. Additionally, the BM1684X processor incorporates a 64-input, 64-output, 8-bit crossbar within the lane, facilitating high-performance data gathering. This crossbar design enhances data gathering capabilities, enabling efficient data processing and manipulation within the TPU architecture. Furthermore, BM1684X offers three distinct memory access modes, showing the processor's versatility in addressing a wide range of AI processing needs and optimizing DRAM utilization for various tasks and workloads. Finally, we design a TPU-MLIR toolchain, highlighting its rich features such as unified processing of multiple frameworks, hierarchical design of model abstractions, correctness guarantees, and traceability of each transformation step. BM1684X excels in providing high-performance computing for a variety of AI models including large models, demonstrating its capabilities through comprehensive evaluations with industry-leading peers.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1413–1428},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00105,
author = {Yun, Sungmin and Kyung, Kwanhee and Cho, Juhwan and Choi, Jaewan and Kim, Jongmin and Kim, Byeongho and Lee, Sukhan and Sohn, Kyomin and Ahn, Jung Ho},
title = {Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00105},
doi = {10.1109/MICRO61859.2024.00105},
abstract = {Large language models (LLMs) have emerged due to their capability to generate high-quality content across diverse contexts. To reduce their explosively increasing demands for computing resources, a mixture of experts (MoE) has emerged. The MoE layer enables exploiting a huge number of parameters with less computation. Applying state-of-the-art continuous batching increases throughput; however, it leads to frequent DRAM access in the MoE and attention layers. We observe that conventional computing devices have limitations when processing the MoE and attention layers, which dominate the total execution time and exhibit low arithmetic intensity (Op/B). Processing MoE layers only with devices targeting low-Op/B such as processing-in-memory (PIM) architectures is challenging due to the fluctuating Op/B in the MoE layer caused by continuous batching, To address these challenges, we propose Duplex, which comprises xPU tailored for high-Op/B and Logic-PIM to effectively perform low-Op/B operation within a single device. Duplex selects the most suitable processor based on the Op/B of each layer within LLMs. As the Op/B of the MoE layer is at least 1 and that of the attention layer has a value of 4–8 for grouped query attention, prior PIM architectures are not efficient, which place processing units inside DRAM dies and only target extremely low-Op/B (under one) operations. Based on recent trends, Logic-Pimadds more through-silicon vias (TSVs) to enable high-bandwidth communication between the DRAM die and the logic die and place powerful processing units on the logic die, which is best suited for handling low-Op/B operations ranging from few to a few dozens. To maximally utilize the xPU and Logic-Pim,we propose expert and attention co-processing. By exploiting proper processing units for MoE and attention layers, Duplex shows up to 2.67 × higher throughput and consumes 42.0% less energy compared to GPU systems for LLM inference.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1429–1443},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00106,
author = {Lee, Seung Yul and Lee, Hyunseung and Hong, Jihoon and Cho, SangLyul and Lee, Jae W.},
title = {VGA: Hardware Accelerator for Scalable Long Sequence Model Inference},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00106},
doi = {10.1109/MICRO61859.2024.00106},
abstract = {Effectively modeling relationships between distant elements in a long input sequence is an important task that remains challenging to this day. The state-of-the-art models for processing sequential data are self-attention-based transformer models. However, the computational complexity of self-attention is quadratic to the input sequence length, which often becomes the limiting factor in scaling the sequence length. Recently, state space model (SSM)-based global convolution models, which replace attention with convolution, have been found to be effective for modeling long sequences, with a sub-quadratic complexity using Fast Fourier Transform (FFT). However, they show sub-optimal performance on data-parallel accelerators like GPU, due to the regions of extremely low compute utilization with memory bandwidth-bound operations. To address this inefficiency, this paper proposes the Vandermonde matrix Generating Accelerator (VGA), a custom accelerator that performs FFT-based convolution in an area/power-efficient manner. VGA introduces Complex number Compute Units (CCUs) to fully utilize the high on-chip SRAM bandwidth, and parameters are generated on the fly to drastically reduce the required SRAM capacity. VGA achieves 76×(48×) higher area (power) efficiency than NVIDIA A100 GPU when executing the global convolution operator of H3, a state-of-the-art SSM-based model.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1444–1457},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00107,
author = {Nayak, Nandeeka and Wu, Xinrui and Odemuyiwa, Toluwanimi O. and Pellauer, Michael and Emer, Joel S. and Fletcher, Christopher W.},
title = {FuseMax: Leveraging Extended Einsums to Optimize Attention Accelerator Design},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00107},
doi = {10.1109/MICRO61859.2024.00107},
abstract = {Attention for transformers is a critical workload that has recently received significant ‘attention’ as a target for custom acceleration. Yet, while prior work succeeds in reducing attention's memory-bandwidth requirements, it creates load imbalance between operators that comprise the attention computation (resulting in severe compute under-utilization) and requires on-chip memory that scales with sequence length (which is expected to grow over time). This paper ameliorates these issues, enabling attention with nearly 100% compute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer size requirements that are independent of sequence length. The main conceptual contribution is to use a recently proposed abstraction―the cascade of Einsums― to describe, formalize, and taxonomize the space of attention algorithms that appear in the literature. In particular, we show how Einsum cascades can be used to infer non-trivial lower bounds on the number of passes a kernel must take through its input data, which has implications for either required on-chip buffer capacity or memory traffic. We show how this notion can be used to meaningfully divide the space of attention algorithms into several categories and use these categories to inform our design process. Based on the above characterization, we propose FuseMax―a novel mapping and binding of attention onto a spatial array-style architecture. On attention, in an iso-area comparison, FuseMax achieves an average 6.7 x speedup over the prior state-of-the-art, FLAT, while using 79% of the energy. Similarly, on full end-to-end transformer inference, FuseMax achieves an average 5.3 x speedup over FLAT using 83% of the energy.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1458–1473},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00108,
author = {Yu, Zhongkai and Liang, Shengwen and Ma, Tianyun and Cai, Yunke and Nan, Ziyuan and Huang, Di and Song, Xinkai and Hao, Yifan and Zhang, Jie and Zhi, Tian and Zhao, Yongwei and Du, Zidong and Hu, Xing and Guo, Qi and Chen, Tianshi},
title = {Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00108},
doi = {10.1109/MICRO61859.2024.00108},
abstract = {Deploying advanced large language models on edge devices, such as smartphones and robotics, is a growing trend that enhances user data privacy and network connectivity resilience while preserving intelligent capabilities. However, such a task exhibits single-batch computing with incredibly low arithmetic intensity, which poses the significant challenges of huge memory footprint and bandwidth demands on limited edge resources. To address these issues, we introduce Cambricon-LLM, a chiplet-based hybrid architecture with NPU and a dedicated NAND flash chip to enable efficient on-device inference of 70B LLMs. Such a hybrid architecture utilizes both the high computing capability of NPU and the data capacity of the NAND flash chip, with the proposed hardware-tiling strategy that minimizes the data movement overhead between NPU and NAND flash chip. Specifically, the NAND flash chip, enhanced by our innovative in-flash computing and on-die ECC techniques, excels at performing precise lightweight on-die processing. Simultaneously, the NPU collaborates with the flash chip for matrix operations and handles special function computations beyond the flash's on-die processing capabilities. Overall, Cambricon-LLM enables the on-device inference of 70B LLMs at a speed of 3.44 token/s, and 7B LLMs at a speed of 36.34 token/s, which is over 22× to 45× faster than existing flash-offloading technologies, showing the potentiality of deploying powerful LLMs in edge devices.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1474–1488},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00109,
author = {Chen, Jian and Gao, Congming and Lu, Youyou and Zhang, Yuhao and Shu, Jiwu},
title = {Ares-Flash: Efficient Parallel Integer Arithmetic Operations Using NAND Flash Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00109},
doi = {10.1109/MICRO61859.2024.00109},
abstract = {In-Flash Processing (IFP) has been proposed in recent years to realize computation ability inside NAND flash memory. Distinguished from processing-in-memory (PIM) and in-storage-processing (ISP), IFP reduces the data movement starting from the most bottom flash memory medium. It is especially beneficial to those applications that require high processing parallelism (e.g., huge databases, large-scale image processing, etc.). However, IFP works often require expensive extra hardware modification, which brings lots of energy dissipation and area overhead. Recent works, Parabit and Flash-Cosmos, try to implement calculations using flash memory with minimum hardware modification. But their accomplishments still remain at the level of simple bitwise operations, and thus it prevents their work from being widely adopted. In this work, we propose Ares-Flash, a new technology using flash memory to support more complex integer arithmetic operations (e.g., addition, accumulation, and multiplication). Ares leverages the designed page buffer to perform basic mechanisms: full-adder logic and bit-shift. Moreover, we construct computational operations using dedicated control sequences in the page buffer upon two basic mechanisms. Our experimental results indicate that Ares is highly efficient and significantly mitigates data movement from storage to memory or computing units (e.g., CPUs, GPUs). Quantitatively, Ares averagely improves performance and energy efficiency by 8.58×/9.89× and 97.9×/14× compared to the out-storage-processing(OSP)/instorage-processing(ISP) under accumulation tasks with real workloads. It also improves 8.2×/4.5× and 89×/13.2× when performing vector-vector multiplication on real-world workloads.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1489–1503},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00110,
author = {Ji, Houxiang and Vanavasam, Srikar and Zhou, Yang and Xia, Qirong and Huang, Jinghan and Yuan, Yifan and Wang, Ren and Gupta, Pekon and Chitlur, Bhushan and Jeong, Ipoom and Kim, Nam Sung},
title = {Demystifying a CXL Type-2 Device: A Heterogeneous Cooperative Computing Perspective},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00110},
doi = {10.1109/MICRO61859.2024.00110},
abstract = {CXL is the latest interconnect technology built on PCIe, providing three protocols to facilitate three distinct types of devices, each with unique capabilities. Among these devices, a CXL Type-2 device has become commercially available, followed by CXL Type-3 devices. Therefore, it is timely to understand capabilities and characteristics of the CXL Type-2 device, as well as explore suitable applications. In this work, first, we delve into three key features of a CXL Type-2 device: cache-coherent device accelerator to host memory, device accelerator to device memory, and host CPU to device memory accesses. Second, using microbenchmarks, we comprehensively characterize the latency and bandwidth of these memory accesses with a CXL Type-2 device, and then compare them with those of equivalent memory accesses with comparable devices, such as emulated CXL Type-2, CXL Type-3, and PCIe devices. Lastly, as applications that exploit the unique capabilities of a CXL Type-2 device, we propose two CXL-based Linux memory optimization features: compressed RAM cache for swap (zswap) and memory deduplication (ksm). Our evaluation shows that Redis, when running with traditional CPU-based zswap and ksm, suffers from a tail latency increase of 4.5-10.3× compared to Redis running alone. While PCIe-based zswap and ksm still experience a tail latency increase of up to 8.1×, CXL-based zswap and ksm practically eliminate the tail latency increase with faster and more efficient host-device communication than PCIe-based zswap and ksm.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1504–1517},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00111,
author = {Zhou, Zhe and Chen, Yiqi and Zhang, Tao and Wang, Yang and Shu, Ran and Xu, Shuotao and Cheng, Peng and Qu, Lei and Xiong, Yongqiang and Zhang, Jie and Sun, Guangyu},
title = {NeoMem: Hardware/Software Co-Design for CXL-Native Memory Tiering},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00111},
doi = {10.1109/MICRO61859.2024.00111},
abstract = {The Compute Express Link (CXL) interconnect makes it feasible to integrate diverse types of memory into servers via its byte-addressable SerDes links. Considering the various access latency, harnessing the full potential of CXL-based heterogeneous memory systems requires efficient memory tiering. However, prior work can hardly make a fundamental progress owing to low-resolution and high-overhead memory access profiling techniques. To address this critical challenge, we propose a novel memory tiering solution called NeoMem, which features a hardware/software co-design. NeoMem offloads memory profiling functions to CXL device-side controllers, integrating a dedicated hardware unit called NeoProf. NeoProf readily monitors memory accesses and provides the OS with crucial page hotness statistics and other useful system state information. On the OS kernel side, we design a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics. We implement NeoMem on a real FPGA-based CXL memory platform and Linux kernel v6.3. Comprehensive evaluations demonstrate that NeoMem achieves 32% ~ 67% geomean speedup over several existing memory tiering solutions.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1518–1531},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00112,
author = {Choi, Junhyuk and Byun, Ilkwon and Hong, Juwon and Min, Dongmoon and Kim, Junpyo and Cho, Jungmin and Jeong, Hyeonseong and Tanaka, Masamitsu and Inoue, Koji and Kim, Jangwoo},
title = {SuperCore: An Ultra-Fast Superconducting Processor for Cryogenic Applications},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00112},
doi = {10.1109/MICRO61859.2024.00112},
abstract = {Superconductor single-flux-quantum (SFQ) logic family has been recognized as a promising technology for cryogenic applications (e.g., quantum computing, astronomy, metrology) thanks to its ultra-fast and low-energy characteristics. Therefore, recent efforts in SFQ-based computing have focused on developing fast and low-power SFQ processors for cryogenic applications. However, there still has been little progress toward a convincing SFQ processor design due to the critical performance challenges originating from its extremely deep pipeline. In this paper, we propose a super-fast and low-power in-order SFQ processor by tackling the challenges from the deep pipeline. First, we develop a minimal-depth SFQ processor pipeline with novel architecture-level ideas. Next, we conduct in-depth performance analyses and identify three real performance bottlenecks in the deeply pipelined SFQ processors (i.e., stall/flush logic, RAW stall, fetch unit). Finally, we propose SuperCore, our super-fast SFQ-based processor architecture, with three SFQ-friendly solutions that effectively resolve the identified bottlenecks. With our solutions applied, SuperCore achieves 11 times speed-up over the SFQ processor baseline. In addition, SuperCore achieves six times speed-up and consumes up to 193 times less power compared to in-order CMOS processors running at 4K.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1532–1547},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00113,
author = {Yang, Guowei and Karimi, Sina and Ocampo, Carlos A. R\'{\i}os and Coskun, Ayse K. and Joshi, Ajay},
title = {SOPHIE: A Scalable Recurrent Ising Machine Using Optically Addressed Phase Change Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00113},
doi = {10.1109/MICRO61859.2024.00113},
abstract = {Ising problems are nondeterministic-polynomial-hard (NP-hard) problems prevalent in various domains, such as statistical physics, circuit design, and machine learning. They pose significant challenges for traditional algorithms and architectures. Researchers have recently developed nature-inspired Ising machines to tackle these optimization problems efficiently. Many optimization problems can be mapped to the Ising model, and physical laws will drive the Ising machine towards the solution. However, existing Ising machines suffer from scalability issues, i.e., performance drops when problem sizes exceed their physical capacity. In this paper, we propose SOPHIE, a Scalable Optical PHase-change memory (OPCM) based Ising Engine. SOPHIE integrates architectural, algorithmic, and device optimizations to address scalability challenges in Ising machines. We architect SOPHIE using 2.5D integration, where we integrate a controller chiplet, a DRAM chiplet, laser sources, and multiple OPCM chiplets. SOPHIE utilizes OPCMs to perform matrix-vector multiplications efficiently. Our symmetric tile mapping at the architecture level reduces approximately half of the OPCM array area, enhancing the scalability of SOPHIE. We use algorithmic optimizations to efficiently handle large problems that cannot fit within hardware constraints. Specifically, we adopt a symmetric local update technique and a stochastic global synchronization strategy. These two algorithmic approaches decompose large problems into isolated tiles, reduce computation requirements, and minimize communication in SOPHIE. We apply device-level optimizations to adopt the modified algorithm. These device-level optimizations include employing bi-directional OPCM arrays and dual-precision analog-to-digital converters. SOPHIE is 3 x faster than the state-of-the-art photonic Ising machines on small graphs and 125x faster than the FPGA-based designs on large problems. SOPHIE alleviates the hardware capacity constraints, offering a scalable and efficient alternative for solving Ising problems.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1548–1561},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00114,
author = {Wu, Lizhou and Zhu, Haozhe and He, Siqi and Zheng, Jiapei and Chen, Chixiao and Zeng, Xiaoyang},
title = {GauSPU: 3D Gaussian Splatting Processor for Real-Time SLAM Systems},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00114},
doi = {10.1109/MICRO61859.2024.00114},
abstract = {3D Gaussian Splatting (3DGS) has recently emerged as a promising technique in the realms of 3D vision and robotics. Its capacity for rapid rendering and high-fidelity reconstruction makes it an attractive candidate for integration into Simultaneous Localization and Mapping (SLAM) systems. However, existing 3DGS-based SLAM systems still suffer from inadequate tracking throughput due to tremendous recursion in volume rendering and irregular memory access for gradient backpropagation. To address these challenges, this paper proposes GauSPU, an algorithm-hardware co-designed accelerator for supporting real-time 3DGS-based SLAM. On the algorithm side, we present a sparse-tile-sampling (STS) method for efficient pose tracking. The STS focuses on informative image regions, discarding the rest to alleviate computational workload while maintaining accuracy. At the hardware level, we make twofold efforts. Firstly, we design a sparsity-adaptive ray recursion unit (SA-RRU) to accelerate volume rendering by leveraging irregular spatial sparsity. The SA-RRU introduces a sub-tile-wise execution pattern and a Morton-based thread allocation scheme to optimize sparsity utilization. Additionally, a sparsity-aware task dispatcher ensures efficient fine-grained task scheduling. Secondly, we propose a memory-access-relaxed backpropagation engine (MAR-BE) for efficient gradient aggregation. It comprises a gradient buffer unit (GBU) for coalescing partial gradients and a pose backward unit (PBU) for pipeline-fused backpropagation, collaboratively eliminating the costly atomic operations. Sufficient experiments demonstrate that, through the integration of GauSPU and GPU, the system achieves a throughput of 33.6 FPS for real-time pose tracking in 3DGS-SLAM, presenting a significant &lt;tex&gt;$63.9times$&lt;/tex&gt; improvement in energy efficiency compared to the RTX3090 baseline.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1562–1573},
numpages = {12},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00115,
author = {Wang, Maolin and McInerney, Ian and Stellato, Bartolomeo and Tu, Fengbin and Boyd, Stephen and So, Hayden Kwok-Hay and Cheng, Kwang-Ting},
title = {Multi-Issue Butterfly Architecture for Sparse Convex Quadratic Programming},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00115},
doi = {10.1109/MICRO61859.2024.00115},
abstract = {Convex quadratic optimization solvers are extensively utilized in various domains; however, achieving optimal performance in diverse situations remains a significant challenge due to the sparse nature of objective and constraint matrices. General-purpose architectures struggle with hardware utilization when performing critical sparse matrix operations, such as factorization and multiplication. To address this issue, we introduce a pipelined spatial architecture, Multi-Issue Butterfly (MIB), which supports all primitive scalar, vector, and matrix operations required by the Alternating Direction Method of Multipliers (ADMM) based solver algorithm. The proposed architecture features a butterfly computational network with innovative working modes for each node, controlled by runtime instructions. We developed a companion scheduling method for matrix operations based on their sparsity patterns. For factorization, an elimination tree guides the network instructions reordering to avoid data hazards caused by computation dependencies. For matrix-vector multiplication, data prefetching resolves structural hazards caused by read and write conflicts to register files. Instructions without hazards are issued simultaneously to increase pipeline throughput and function unit utilization. We evaluate the proposed architecture using FPGA prototypes, representing the first fully FPGA-based generic QP solver. Our assessment includes extensive performance and efficiency bench-marks across 100 QP problems from five application domains. Compared to the same algorithm variation running on CPU backends, our prototype achieves a geometric mean of &lt;tex&gt;$30.5times$&lt;/tex&gt; end-to-end speedup, &lt;tex&gt;$127.0 times$&lt;/tex&gt; greater energy efficiency, and &lt;tex&gt;$16.5times$&lt;/tex&gt; less runtime jitter. In comparison to GPU backends, the prototype attains a geometric mean of &lt;tex&gt;$4.3times$&lt;/tex&gt; faster end-to-end speedup, &lt;tex&gt;$21.7times$&lt;/tex&gt; higher energy efficiency, and &lt;tex&gt;$33.4times$&lt;/tex&gt; less runtime jitter.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1574–1587},
numpages = {14},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00116,
author = {Gao, Yiming and Jiang, Chao and Piard, Wesley and Chen, Xiangru and Patel, Bhavesh and Lam, Herman},
title = {HgPCN: A Heterogeneous Architecture for E2E Embedded Point Cloud Inference},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00116},
doi = {10.1109/MICRO61859.2024.00116},
abstract = {Point cloud is an important type of geometric data structure for many embedded applications such as autonomous driving and augmented reality. Current Point Cloud Networks (PCNs) have proven to achieve great success in using inference to perform point cloud analysis, including object part segmentation, shape classification, and so on. However, point cloud applications on the computing edge require more than just the inference step. They require an end-to-end (E2E) processing of the point cloud workloads: pre-processing of raw data, input preparation, and inference to perform point cloud analysis. Current PCN approaches to support end-to-end processing of point cloud workload cannot meet the real-time latency requirement on the edge, i.e., the ability of the AI service to keep up with the speed of raw data generation by 3D sensors. Latency for end-to-end processing of the point cloud workloads stems from two reasons: memory-intensive down-sampling in the pre-processing phase and the data structuring step for input preparation in the inference phase. In this paper, we present HgPCN, an end-to-end heterogeneous architecture for real-time embedded point cloud applications. In HgPCN, we introduce two novel methodologies based on spatial indexing to address the two identified bottlenecks. In the Pre-processing Engine of HgPCN, an Octree-Indexed-Sampling method is used to optimize the memory-intensive down-sampling bottleneck of the pre-processing phase. In the Inference Engine, HgPCN extends a commercial DLA with a customized Data Structuring Unit which is based on a Voxel-Expanded Gathering method to fundamentally reduce the workload of the data structuring step in the inference phase. The initial prototype of HgPCN has been implemented on an Intel PAC (Xeon+FPGA) platform. Four commonly available point cloud datasets were used for comparison, running on three baseline devices: Intel Xeon W-2255, Nvidia Xavier NX Jetson GPU, and Nvidia 4060ti GPU. These point cloud datasets were also run on two existing PCN accelerators for comparison: PointACC and Mesorasi. Our results show that for the inference phase, depending on the dataset size, HgPCN achieves speedup from 1.3× To 10.2× vs. PointACC, 2.2× To 16.5× vs. Mesorasi, and 6.4× To 21× vs. Jetson NX GPU. Along with optimization of the memory-intensive down-sampling bottleneck in pre-processing phase, the overall latency shows that HgPCN can reach the real-time requirement by providing end-to-end service with keeping up with the raw data generation rate.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1588–1600},
numpages = {13},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00117,
author = {Bakhtiar, Ubaid and Hosseini, Helya and Asgari, Bahar},
title = {Acamar: A Dynamically Reconfigurable Scientific Computing Accelerator for Robust Convergence and Minimal Resource Underutilization},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00117},
doi = {10.1109/MICRO61859.2024.00117},
abstract = {Although modern supercomputers are capable of delivering Exaflops now, they do not always achieve their peak performance. For instance, even today's high-end supercom-puters achieve only less than 5% of their peak FLOPS when running HPCG, a benchmark designed to represent real-world scientific computing programs. To improve the efficiency of the key kernels in scientific computing, such as those used in solving partial differential equations, computer architects have begun to expand the applications of domain-specific architectures (DSAs) to scientific computing. However, DSAs that often have a fixed design are not likely to be practical solutions, as one specialized solution cannot fit all the diverse scientific computing workloads, making them less effective. The challenges of hardware inef-ficiency in today's supercomputers and the ineffectiveness of DSAs are further exacerbated by sparsity, a key characteristic of scientific computing workloads. While prior studies have proposed DSA solutions for sparse computations, they too are static and not adaptable to variations in the patterns and levels of sparsity across different scientific workloads. To address these challenges and target not only the diversity of computations in such workloads but also variations in sparsity, we propose Acamar&lt;sup&gt;1&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt;Acamar /’ rckomarr/ is a binary star system in the constellation of Eridanus., a dynamically reconfigurable accelerator. Acamar is adaptable to various solvers across different workloads and dynamically optimizes the trade-off between resource utilization and latency for sparse computations. The adaptable design also enables selecting a solver that guarantees convergence. We evaluate Acamar based on its Vitis HLS implementation on Xilinx Alveo u55c. Our experiments show a resource utilization and latency improvement up to 3.5 x and 6 x as well as improved performance efficiency and achieved throughput over a static design and Nvidia GTX 1650 Super.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1601–1616},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00118,
author = {Haghi, Pouya and Wu, Chunshu and Azad, Zahra and Li, Yanfei and Gui, Andrew and Hao, Yuchen and Li, Ang and Geng, Tony Tong},
title = {Bridging the Gap Between LLMs and LNS with Dynamic Data Format and Architecture Codesign},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00118},
doi = {10.1109/MICRO61859.2024.00118},
abstract = {Deep Neural Networks (DNNs) have achieved tremendous success in the past few years. However, their training and inference demand exceptional computational and memory resources. Quantization has been shown as an effective approach to mitigate the cost, with the mainstream data types reduced from FP32 to FP16/BF16 and recently FP4 in the latest NVIDIA B100 GPUs. With increasingly aggressive quantization, however, the conventional floating-point formats suffer from limited precision in representing numbers around zero. Recently, NVIDIA demonstrated the potential of using a Logarithmic Number System (LNS) for the next generation of tensor cores. While LNS mitigates the hurdles in representing small numbers, in this work we observed a mismatch between LNS and the emerging Large Language Models (LLM), where LLM exhibits significant outliers when directly adopting the LNS format. In this paper, we present a data-format/architecture codesign to bright this gap. On the format side, we propose a dynamic LNS format to flexibly represent outliers at a higher precision, by exploiting asymmetry in the LNS representation and identifying outliers through a per-block basis. On the architecture side, for demonstration, we realize the dynamic LNS format in a systolic array, which can handle the irregularity of the outliers at runtime. We implement our approach on an Alveo U280 FPGA as a prototype. Experimental results show that our design can effectively handle the outliers and resolve the mismatch between LNS and LLM, contributing to an accuracy improvement of 15.4% and 16% over the floating-point and the original LNS baselines, with up to 15.3% over the state-of-the-art quantization methods using four LLM models. Our observation and design lay a solid foundation for the large-scale adoption of the LNS format in the next-generation deep learning hardware.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1617–1631},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00119,
author = {Leitersdorf, Orian and Ronen, Ronny and Kvatinsky, Shahar},
title = {PyPIM: Integrating Digital Processing-in-Memory from Microarchitectural Design to Python Tensors},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00119},
doi = {10.1109/MICRO61859.2024.00119},
abstract = {Digital processing-in-memory (PIM) architectures mitigate the memory wall problem by facilitating parallel bitwise operations directly within the memory. Recent works have demonstrated their algorithmic potential for accelerating data-intensive applications; however, there remains a significant gap in the programming model and microarchitectural design. This is further exacerbated by aspects unique to memristive PIM such as partitions and operations across both directions of the memory array. To address this gap, this paper provides an end-to-end architectural integration of digital memristive PIM from a high-level Python library for tensor operations (similar to NumPy and PyTorch) to the low-level microarchitectural design. We begin by proposing an efficient microarchitecture and instruction set architecture (ISA) that bridge the gap between the low-level control periphery and an abstraction of PIM parallelism. We subsequently propose a PIM development library that converts high-level Python to ISA instructions and a PIM driver that translates ISA instructions into PIM micro-operations. We evaluate PyPIM via a cycle-accurate simulator on a wide variety of benchmarks that both demonstrate the versatility of the Python library and the performance compared to theoretical PIM bounds. Overall, PyPIM drastically simplifies the development of PIM applications and enables the conversion of existing tensor-oriented Python programs to PIM with ease.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1632–1647},
numpages = {16},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00120,
author = {Li, Yiwei and Tian, Boyu and Ren, Yi and Gao, Mingyu},
title = {Stream-Based Data Placement for Near-Data Processing with Extended Memory},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00120},
doi = {10.1109/MICRO61859.2024.00120},
abstract = {The data access bottleneck in memory-intensive applications has motivated various architectural innovations in the main memory system, with Near-Data Processing (NDP) and Compute Express Link (CXL) as two recent prominent representations. In this work, we focus on addressing the memory capacity limitation of 3D-stacked NDP systems using CXL-based extended memory, where the DRAM space of the 3D NDP stacks is used as the cache of the CXL-based memory. Nevertheless, this architecture exhibits unique challenges to address the significant interconnect latency and expensive metadata management problems. We propose NDPExt, a hardware-software co-design approach to achieve efficient NDP with extended memory. On the hardware side, NDPExt uses coarse-grained data streams rather than conventional fine-grained cachelines to manage the NDP stacks as a distributed DRAM cache, in order to reduce metadata cost and apply custom optimizations to different data. On the software side, NDPExt periodically derives the optimized cache configuration to allocate the DRAM cache space to each stream based on profiled miss behaviors. The configuration cooptimizes capacity sizing, spatial placement, and data replication. Combining the two techniques allows NDPExt to achieve 1.41× on average and up to 2.43× performance improvements over state-of-the-art cache management solutions.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1648–1662},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00121,
author = {Guo, Hongrui and Zou, Mo and Hao, Yifan and Du, Zidong and Ren, Erxiang and Liu, Yang and Zhao, Yongwei and Ma, Tianrui and Zhang, Rui and Hu, Xing and Qiao, Fei and Xu, Zhiwei and Guo, Qi and Chen, Tianshi},
title = {Cambricon-M: A Fibonacci-Coded Charge-Domain SRAM-Based CIM Accelerator for DNN Inference},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00121},
doi = {10.1109/MICRO61859.2024.00121},
abstract = {Charge-domain SRAM-based Computing-in-memory (CIM) proves to be a promising method for DNN inference, and benefits from avoiding data movement between computing units and memory. However, the high resolution Analog-to-Digital Converters (ADCs) dominates the energy consumption (up to 64%), limiting the energy efficiency of SRAM-CIM architectures. The main reason is the wide range of input analog values, requiring high resolution ADCs to convert the high precision averaged analog voltages into high bitwidth digital data. In this paper, to reduce the ADC overhead, we propose Cambricon-M, a novel Fibonacci-coded SRAM-based charge-domain CIM accelerator for DNN inference. Cambricon-M features the Fibonacci coding, which guarantees low density of ‘1’ in operands (i.e., the adjacent two bits of each ‘1’ are both ‘0’), narrowing the output voltage range and enabling low resolution ADCs. Further, Cambricon-M exploits the high bit-level sparsity to address the extra energy and area overhead caused by the larger bitwidth in Fibonacci coding. Specifically, Cambricon-M proposes zero-skipping methods to reduce ineffectual input/output, and the bit-slice based compression method to reduce memory capacity/bandwidth pressure. Experimental results show that Cambricon-M reduces ADC energy by 68.7%, and improves the energy efficiency 3.48× and 1.62× compared to TPUv4 and an ISAAC-based charge-domain SRAM-CIM accelerator.},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1663–1677},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

@inproceedings{10.1109/MICRO61859.2024.00122,
author = {Zhu, Yihang and Cai, Lei and Yu, Lianfeng and Fan, Anjunyi and Yan, Longhao and Jing, Zhaokun and Yan, Bonan and Tiw, Pek Jun and Li, Yuqi and Tao, Yaoyu and Yang, Yuchao},
title = {MeMCISA: Memristor-Enabled Memory-Centric Instruction-Set Architecture for Database Workloads},
year = {2024},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO61859.2024.00122},
doi = {10.1109/MICRO61859.2024.00122},
abstract = {The exponential growth of data exerts great pressure on hardware design for database systems. Memory-centric computing (MCC) architecture, which enable compute capabilities near or inside memory storage, demonstrate great potential in enhancing the efficiency of database operations with higher compute parallelism and reduced data movements. However, existing MCC architecture mainly focus on artificial intelligence (AI) computations and those designed for database applications can only run a limited number of standalone queries such as SORT or JOIN, lacking efficient support for increasingly diverse and complex database workloads. For example, realizing a commercial recommendation engine on database requires supporting workloads including but not limited to vector aggregation, convolution or &lt;tex&gt;$N$&lt;/tex&gt; -hop neighborhoods computing, etc. In this work, we develop a memristor-enabled memory-centric instruction-set architecture (MeMCISA) aiming to efficiently accelerate versatile workloads in modern database systems. MeMCISA features scalable multi-bank memristor-based storage organization with near-memory circuitries and caches in banks. An out-of-order (O0O) scheduling scheme is designed for MeMCISA based on a vector instruction set with four types of instructions (bit-level, element-level, vector-level, and control-level), combining memristor-enabled in-memory computing and near-memory computing to efficiently run workloads with varying computational kernels and data sizes. MeMCISA can support parallel instruction executions across different memristor banks as well as different hardware modules within a memristor bank. Furthermore, we develop data dependency handling mechanisms to support vector dependency scenarios in MeMCISA that do not exist in conventional scalar-based instruction sets. A prototype MeMCISA is implemented based on a 40nm CMOS technology with necessary peripheral hardware including instruction buffer and instruction scheduler. To accurately study MeMCISA performance in real-world database systems, a software-hardware co-designed framework integrating reconfigurable MeMCISA prototype is created that can support end-to-end simulations for database workloads starting from raw software codes. Based on this framework, we evaluate MeMCISA performance with standalone database queries as well as complex database workloads from representative benchmarks including UniBench, neural collaborative filtering (NCF), and ResNet-18. Simulation results demonstrate that MeMCISA achieves up to 41.84 × ~ 1767.70 × in speed compared to general-purpose processors (CPUs/GPUs).},
booktitle = {Proceedings of the 2024 57th IEEE/ACM International Symposium on Microarchitecture},
pages = {1678–1692},
numpages = {15},
location = {Austin, TX, USA},
series = {MICRO '24}
}

