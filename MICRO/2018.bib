@inproceedings{10.1109/MICRO.2018.00010,
author = {Mukkara, Anurag and Beckmann, Nathan and Abeydeera, Maleen and Ma, Xiaosong and Sanchez, Daniel},
title = {Exploiting locality in graph analytics through hardware-accelerated traversal scheduling},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00010},
doi = {10.1109/MICRO.2018.00010},
abstract = {Graph processing is increasingly bottlenecked by main memory accesses. On-chip caches are of little help because the irregular structure of graphs causes seemingly random memory references. However, most real-world graphs offer significant potential locality---it is just hard to predict ahead of time. In practice, graphs have well-connected regions where relatively few vertices share edges with many common neighbors. If these vertices were processed together, graph processing would enjoy significant data reuse. Hence, a graph's traversal schedule largely determines its locality.This paper explores online traversal scheduling strategies that exploit the community structure of real-world graphs to improve locality. Software graph processing frameworks use simple, locality-oblivious scheduling because, on general-purpose cores, the benefits of locality-aware scheduling are outweighed by its overheads. Software frameworks rely on offline preprocessing to improve locality. Unfortunately, preprocessing is so expensive that its costs often negate any benefits from improved locality. Recent graph processing accelerators have inherited this design. Our insight is that this misses an opportunity: Hardware acceleration allows for more sophisticated, online locality-aware scheduling than can be realized in software, letting systems significantly improve locality without any preprocessing.To exploit this insight, we present bounded depth-first scheduling (BDFS), a simple online locality-aware scheduling strategy. BDFS restricts each core to explore one small, connected region of the graph at a time, improving locality on graphs with good community structure. We then present HATS, a hardware-accelerated traversal scheduler that adds just 0.4\% area and 0.2\% power over general-purpose cores.We evaluate BDFS and HATS on several algorithms using large real-world graphs. On a simulated 16-core system, BDFS reduces main memory accesses by up to 2.4x and by 30\% on average. However, BDFS is too expensive in software and degrades performance by 21\% on average. HATS eliminates these overheads, allowing BDFS to improve performance by 83\% on average (up to 3.1x) over a locality-oblivious software implementation and by 31\% on average (up to 2.1x) over specialized prefetchers.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1–14},
numpages = {14},
keywords = {scheduling, prefetching, multicore, locality, graph analytics, caches},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00011,
author = {Zhou, Xuda and Du, Zidong and Guo, Qi and Liu, Shaoli and Liu, Chengsi and Wang, Chao and Zhou, Xuehai and Li, Ling and Chen, Tianshi and Chen, Yunji},
title = {Cambricon-s: addressing irregularity in sparse neural networks through a cooperative software/hardware approach},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00011},
doi = {10.1109/MICRO.2018.00011},
abstract = {Neural networks have become the dominant algorithms rapidly as they achieve state-of-the-art performance in a broad range of applications such as image recognition, speech recognition and natural language processing. However, neural networks keep moving towards deeper and larger architectures, posing a great challenge to the huge amount of data and computations. Although sparsity has emerged as an effective solution for reducing the intensity of computation and memory accesses directly, irregularity caused by sparsity (including sparse synapses and neurons) prevents accelerators from completely leveraging the benefits; it also introduces costly indexing module in accelerators.In this paper, we propose a cooperative software/hardware approach to address the irregularity of sparse neural networks efficiently. Initially, we observe the local convergence, namely larger weights tend to gather into small clusters during training. Based on that key observation, we propose a software-based coarse-grained pruning technique to reduce the irregularity of sparse synapses drastically. The coarse-grained pruning technique, together with local quantization, significantly reduces the size of indexes and improves the network compression ratio. We further design a hardware accelerator, Cambricon-S, to address the remaining irregularity of sparse synapses and neurons efficiently. The novel accelerator features a selector module to filter unnecessary synapses and neurons. Compared with a state-of-the-art sparse neural network accelerator, our accelerator is 1.71x and 1.37x better in terms of performance and energy efficiency, respectively.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {15–28},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00012,
author = {Zhuo, Youwei and Cheng, Jinglei and Luo, Qinyi and Zhai, Jidong and Wang, Yanzhi and Luan, Zhongzhi and Qian, Xuehai},
title = {CSE: parallel finite state machines with convergence set enumeration},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00012},
doi = {10.1109/MICRO.2018.00012},
abstract = {Finite State Machine (FSM) is known to be "embarrassingly sequential" because the next state depends on the current state and input symbol. Enumerative FSM breaks the data dependencies by cutting the input symbols into segments and processing all segments in parallel. With unknown starting state (except the first segment), each segment needs to calculate the state transitions, i.e., state→state, for all states, each one is called an enumeration path. The current software and hardware implementations suffer from two drawbacks: 1) large amount of state→state computation overhead for the enumeration paths; and 2) the optimizations are restricted by the need to correctly performing state→state and only achieve limited improvements.This paper proposes CSE, a &lt;u&gt;C&lt;/u&gt;onvergence &lt;u&gt;S&lt;/u&gt;et based &lt;u&gt;E&lt;/u&gt;numeration based parallel FSM. Unlike prior approaches, CSE is based on a novel computation primitive set(N)→set(M), which maps N states to M states without giving the specific state→state mappings (which state is mapped to which). The set(N)→set(M) has two key properties: 1) if M is equal to 1, i.e., all N states are mapped to the same state, the state→state for all the N states are computed; 2) using one-hot encoding, the hardware implementation cost of state→state is the same as set(N)→set(M).The convergence property ensures that M is always less than N. The key idea of CSE is to partition the original all S states into n state sets CS1,CS2,...,CSn, i.e., convergence sets. Using set(N)→set(M) to process each CSi, if the states converge to a single state, then we have successfully computed the enumeration path for each state in CSi; otherwise, we may need to re-execute the stage when the outcome of the previous stage falls in CSi. CSE is realized by two techniques: convergence set prediction, which generates the convergence sets with random input based profiling that maximizes the probability of each CSi converging to one state; global re-execution algorithm, which ensures the correctness by re-executing the non-converging stages with known input state.Essentially, CSE reformulates the enumeration paths as set-based rather than singleton-based. We evaluate CSE with 13 benchmarks. It achieved on average 2.0x/2.4x and maximum 8.6x/2.7x speedup compared to Lookback Enumeration (LBE) and Parallel Automata Processor (PAP), respectively.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {29–41},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00013,
author = {Voitsechov, Dani and Port, Oron and Etsion, Yoav},
title = {Inter-thread communication in multithreaded, reconfigurable coarse-grain arrays},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00013},
doi = {10.1109/MICRO.2018.00013},
abstract = {Traditional von Neumann GPGPUs only allow threads to communicate through memory on a group-to-group basis. In this model, a group of producer threads writes intermediate values to memory, which are read by a group of consumer threads after a barrier synchronization. To alleviate the memory bandwidth imposed by this method of communication, GPGPUs provide a small scratchpad memory that prevents intermediate values from overloading DRAM bandwidth.In this paper we introduce direct inter-thread communications for massively multithreaded CGRAs, where intermediate values are communicated directly through the compute fabric on a point-to-point basis. This method avoids the need to write values to memory, eliminates the need for a dedicated scratchpad, and avoids workgroup global barriers. We introduce our proposed extensions to the programming model (CUDA) and execution model, as well as the hardware primitives that facilitate the communication. Our simulations of Rodinia benchmarks running on the new system show that direct inter-thread communication provides an average speedup of 2.8x (10.3x max) and reduces system power by an average of 5x (22x max), when compared to an equivalent Nvidia GPGPU.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {42–54},
numpages = {13},
keywords = {reconfigurable -architectures, non-von neumann-architectures, inter-thread communication, dataflow, SIMD, MPI, GPGPU, CGRA},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00014,
author = {Chen, Tao and Srinath, Shreesha and Batten, Christopher and Suh, G. Edward},
title = {An architectural framework for accelerating dynamic parallel algorithms on reconfigurable hardware},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00014},
doi = {10.1109/MICRO.2018.00014},
abstract = {In this paper, we propose ParallelXL, an architectural framework for building application-specific parallel accelerators with low manual effort. The framework introduces a task-based computation model with explicit continuation passing to support dynamic parallelism in addition to static parallelism. In contrast, today's high-level design frameworks for accelerators focus on static data-level or thread-level parallelism that can be identified and scheduled at design time. To realize the new computation model, we develop an accelerator architecture that efficiently handles dynamic task generation and scheduling as well as load balancing through work stealing. The architecture is general enough to support many dynamic parallel constructs such as fork-join, data-dependent task spawning, and arbitrary nesting and recursion of tasks, as well as static parallel patterns. We also introduce a design methodology that includes an architectural template that allows easily creating parallel accelerators from high-level descriptions. The proposed framework is studied through an FPGA prototype as well as detailed simulations. Evaluation results show that the framework can generate high-performance accelerators targeting FPGAs for a wide range of parallel algorithms and achieve an average of 4.0x speedup over an eight-core out-of-order processor (24.1x over a single core), while being 11.8x more energy efficient.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {55–67},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00015,
author = {Zhang, Sizhuo and Wright, Andrew and Bourgeat, Thomas and Arvind},
title = {Composable building blocks to open up processor design},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00015},
doi = {10.1109/MICRO.2018.00015},
abstract = {We present a framework called Composable Modular Design (CMD) to facilitate the design of out-of-order (OOO) processors. In CMD, (1) The interface methods of modules provide instantaneous access and perform atomic updates to the state elements inside the module; (2) Every interface method is guarded, i.e., it cannot be applied unless it is ready; and (3) Modules are composed together by atomic rules which call interface methods of different modules. A rule either successfully updates the state of all the called modules or it does nothing. CMD designs are compiled into RTL which can be run on FPGAs or synthesized using standard ASIC design flows.The atomicity properties of interfaces in CMD ensures composability when selected modules are refined selectively. We show the efficacy of CMD by building a parameterized out-of-order RISC-V processor which boots Linux and runs on FPGAs at 25 MHz to 40 MHz. We also synthesized several variants of it in a 32 nm technology to run at 1 GHz to 1.1 GHz. Performance evaluation shows that our processor beats in-order processors in terms of IPC but will require more architectural work to compete with wider superscalar commercial ARM processors. Modules designed under the CMD framework (e.g., ROB, reservation stations, load store unit) can be used and refined by other implementations. We believe that this realistic framework can revolutionize architectural research and practice as the library of reusable components grows.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {68–81},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00016,
author = {Ando, Hideki},
title = {Performance improvement by prioritizing the issue of the instructions in unconfident branch slices},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00016},
doi = {10.1109/MICRO.2018.00016},
abstract = {Single-thread performance has hardly improved for more than a decade. One of the largest problems for performance improvements is branch misprediction. There are two approaches to reduce the penalty caused by this. One is to reduce the frequency of misprediction, and the other is to reduce the cycles consumed because of misprediction. Improving branch predictors is the former approach, and many studies on this topic have been done for several decades. However, the latter approach has been rarely studied. The present paper hence explores the latter approach.The cycles consumed because of misprediction are divided into the following two parts. The first part is the state recovery penalty, which consists of cycles consumed for rolling back the processor state. The second part is the misspeculation penalty, which are cycles consumed during useless speculative execution from the fetch of a mispredicted branch until the completion of the branch execution. We focus on reducing the misspeculation penalty. For this, we propose a scheme called PUBS, which allows the instructions in unconfident branch slices to be issued with highest priority from the issue queue (IQ). Here, a branch slice is a set consisting of a branch and the instructions this branch directly or indirectly depends on, and we call the branch slice unconfident if the associated branch prediction cannot be sufficiently trusted. By issuing instructions in unconfident branch slices as early as possible, the wait cycles of these instructions in the IQ are minimized and thus the misspeculation penalty is minimized. Our evaluation results using SPEC2006 benchmark programs show that the PUBS scheme improves the performance of the programs with difficult branch prediction by 7.8\% on average (a maximum of 19.2\%) using only 4.0KB hardware cost.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {82–94},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00017,
author = {Ros, Alberto and Kaxiras, Stefanos},
title = {The superfluous load queue},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00017},
doi = {10.1109/MICRO.2018.00017},
abstract = {In an out-of-order core, the load queue (LQ), the store queue (SQ), and the store buffer (SB) are responsible for ensuring: i) correct forwarding of stores to loads and ii) correct ordering among loads (with respect to external stores). The first requirement safeguards the sequential semantics of program execution and applies to both serial and parallel code; the second requirement safeguards the semantics of coherence and consistency (e.g., TSO). In particular, loads search the SQ/SB for the latest value that may have been produced by a store, and stores and invalidations search the LQ to find speculative loads in case they violate uniprocessor or multiprocessor ordering. To meet timing constraints the LQ and SQ/SB system is composed of CAM structures that are frequently searched. This results in high complexity, cost, and significant difficulty to scale, but is the current state of the art.Prior research demonstrated the feasibility of a non-associative LQ by replaying loads at commit. There is a steep cost however: a significant increase in L1 accesses and contention for L1 ports. This is because prior work assumes Sequential Consistency and completely ignores the existence of a SB in the system.In contrast, we intentionally delay stores in the SB to achieve a total management of stores and loads in a core, while still supporting TSO. Our main result is that we eliminate the LQ without burdening the L1 with extra accesses. Store forwarding is achieved by delaying our own stores until speculatively issued loads are validated on commit, entirely in-core; TSO load→load ordering is preserved by delaying remote external stores in their SB until our own speculative reordered loads commit. While the latter is inspired by recent work on non-speculative load reordering, our contribution here is to show that this can be accomplished without having a load queue. Eliminating the LQ results in both energy savings and performance improvement from the elimination of LQ-induced stalls.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {95–107},
numpages = {13},
keywords = {total store order, out-of-order execution, memory consistency},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00018,
author = {Adileh, Almutaz and Lilja, David J. and Eeckhout, Lieven},
title = {Architectural support for probabilistic branches},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00018},
doi = {10.1109/MICRO.2018.00018},
abstract = {A plethora of research efforts have focused on fine-tuning branch predictors to increasingly higher levels of accuracy. However, several important optimization, financial, and statistical data analysis algorithms rely on probabilistic computation. These applications draw random values from a distribution and steer control flow based on those values. Such probabilistic branches are challenging to predict because of their inherent probabilistic nature. As a result, probabilistic codes significantly suffer from branch mispredictions.This paper proposes Probabilistic Branch Support (PBS), a hardware/software cooperative technique that leverages the observation that the outcome of probabilistic branches needs to be correct only in a statistical sense. PBS stores the outcome and the probabilistic values that lead to the outcome of the current execution to direct the next execution of the probabilistic branch, thereby completely removing the penalty for mispredicted probabilistic branches. PBS relies on marking probabilistic branches in software for hardware to exploit. Our evaluation shows that PBS improves MPKI by 45\% on average (and up to 99\%) and IPC by 6.7\% (up to 17\%) over the TAGE-SC-L predictor. PBS requires 193 bytes of hardware overhead and introduces statistically negligible algorithmic inaccuracy.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {108–120},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00019,
author = {Irie, Hidetsugu and Koizumi, Toru and Fukuda, Akifumi and Akaki, Seiya and Nakae, Satoshi and Bessho, Yutaro and Shioya, Ryota and Notsu, Takahiro and Yoda, Katsuhiro and Ishihara, Teruo and Sakai, Shuichi},
title = {STRAIGHT: hazardless processor architecture without register renaming},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00019},
doi = {10.1109/MICRO.2018.00019},
abstract = {The single-thread performance of a processor improves the capability of the entire system by reducing the critical path latency of programs. Typically, conventional superscalar processors improve this performance by introducing out-of-order (OoO) execution with register renaming. However, it is also known to increase the complexity and affect the power efficiency. This paper realizes a novel computer architecture called "STRAIGHT" to resolve this dilemma. The key feature is a unique instruction format in which the source operand is given based on the distance from the producer instruction. By leveraging this format, register renaming is completely removed from the pipeline. This paper presents the practical Instruction Set Architecture (ISA) design, the novel efficient OoO microarchitecture, and the compilation algorithm for the STRAIGHT machine code. Because the ISA has sequential execution semantics, as in general CPUs, and is provided with a compiler, programming for the architecture is as easy as that of conventional CPUs. A compiler, an assembler, a linker, and a cycle-accurate simulator are developed to measure the performance. Moreover, an RTL description of STRAIGHT is developed to estimate the power reduction. The evaluation using standard benchmarks shows that the performance of STRAIGHT is 18.8\% better than the conventional superscalar processor of the same issue-width and instruction window size. This improvement is achieved by STRAIGHT's rapid miss-recovery. Compilation technology for resolving the possible overhead of the ISA is also revealed. The RTL power analysis shows that the architecture reduces the power consumption by removing the power for renaming. The revealed performance and efficiencies support that STRAIGHT is a novel viable alternative for designing general purpose OoO processors.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {121–133},
numpages = {13},
keywords = {register renaming, power efficiency, out-of-order execution, microprocessor, instruction-level-parallelism, computer architecture, compiler},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00020,
author = {Mahmoud, Mostafa and Siu, Kevin and Moshovos, Andreas},
title = {Diffy: a d\'{e}j\`{a} vu-free differential deep neural network accelerator},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00020},
doi = {10.1109/MICRO.2018.00020},
abstract = {We show that Deep Convolutional Neural Network (CNN) implementations of computational imaging tasks exhibit spatially correlated values. We exploit this correlation to reduce the amount of computation, communication, and storage needed to execute such CNNs by introducing Diffy, a hardware accelerator that performs Differential Convolution. Diffy stores, communicates, and processes the bulk of the activation values as deltas. Experiments show that, over five state-of-the-art CNN models and for HD resolution inputs, Diffy boosts the average performance by 7.1x over a baseline value-agnostic accelerator [1] and by 1.41x over a state-of-the-art accelerator that processes only the effectual content of the raw activation values [2]. Further, Diffy is respectively 1.83x and 1.36x more energy efficient when considering only the on-chip energy. However, Diffy requires 55\% less on-chip storage and 2.5x less off-chip bandwidth compared to storing the raw values using profiled per-layer precisions [3]. Compared to using dynamic per group precisions [4], Diffy requires 32\% less storage and 1.43x less off-chip memory bandwidth. More importantly, Diffy provides the performance necessary to achieve real-time processing of HD resolution images with practical configurations. Finally, Diffy is robust and can serve as a general CNN accelerator as it improves performance even for image classification models.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {134–147},
numpages = {14},
keywords = {neural networks, differential convolution, deep learning, computational imaging, accelerator},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00021,
author = {Kwon, Youngeun and Rhu, Minsoo},
title = {Beyond the memory wall: a case for memory-centric HPC system for deep learning},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00021},
doi = {10.1109/MICRO.2018.00021},
abstract = {As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied. We propose a memory-centric deep learning system that can transparently expand the memory capacity available to the accelerators while also providing fast inter-device communication for parallel training. Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion. Compared to conventional systems, our proposal achieves an average 2.8x speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {148–161},
numpages = {14},
keywords = {system architecture, machine learning, HPC},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00022,
author = {Zhang, Xingyao and Xie, Chenhao and Wang, Jing and Zhang, Weidong and Fu, Xin},
title = {Towards memory friendly long-short term memory networks (LSTMs) on mobile GPUs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00022},
doi = {10.1109/MICRO.2018.00022},
abstract = {Intelligent Personal Assistants (IPAs) with the capability of natural language processing (NLP) are increasingly popular in today's mobile devices. Recurrent neural networks (RNNs), especially one of their forms - Long-Short Term Memory networks (LSTMs), are becoming the core machine learning technique applied in the NLP-based IPAs. With the continuously improved performance of mobile GPUs, local processing has become a promising solution to the large data transmission and privacy issues induced by the cloud-centric computations of IPAs. However, LSTMs exhibit quite inefficient memory access pattern when executed on mobile GPUs due to the redundant data movements and limited off-chip bandwidth. In this study, we aim to explore the memory friendly LSTM on mobile GPUs by hierarchically reducing the off-chip memory accesses. To address the redundant data movements, we propose inter-cell level optimizations that intelligently parallelize the originally sequentially executed LSTM cells (basic units in RNNs, corresponding to neurons in CNNs) to improve the data locality across cells with negligible accuracy loss. To relax the pressure on limited off-chip memory bandwidth, we propose intra-cell level optimizations that dynamically skip the loads and computations of rows in the weight matrices with trivial contribution to the outputs. We also introduce a light-weighted module to the GPUs architecture for the runtime row skipping in weight matrices. Moreover, our techniques are equipped with thresholds which provide a unique tunning space for performance-accuracy trade-offs directly guided by the user preferences. The experimental results show our optimizations achieves substantial improvements on both performance and power with user-imperceptible accuracy loss. And our optimizations exhibit the strong scalability with the increasing input data set. Our user study also shows that our designed system delivers the excellent user experience.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {162–174},
numpages = {13},
keywords = {mobile and embedded architectures, approximate computing, GPGPU/GPU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00023,
author = {Li, Youjie and Park, Jongse and Alian, Mohammad and Yuan, Yifan and Qu, Zheng and Pan, Peitian and Wang, Ren and Schwing, Alexander Gerhard and Esmaeilzadeh, Hadi and Kim, Nam Sung},
title = {A network-centric hardware/algorithm co-design to accelerate distributed training of deep neural networks},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00023},
doi = {10.1109/MICRO.2018.00023},
abstract = {Training real-world Deep Neural Networks (DNNs) can take an eon (i.e., weeks or months) without leveraging distributed systems. Even distributed training takes inordinate time, of which a large fraction is spent in communicating weights and gradients over the network. State-of-the-art distributed training algorithms use a hierarchy of worker-aggregator nodes. The aggregators repeatedly receive gradient updates from their allocated group of the workers, and send back the updated weights. This paper sets out to reduce this significant communication cost by embedding data compression accelerators in the Network Interface Cards (NICs). To maximize the benefits of in-network acceleration, the proposed solution, named INCEPTIONN (In-Network Computing to Exchange and Process Training Information Of Neural Networks), uniquely combines hardware and algorithmic innovations by exploiting the following three observations. (1) Gradients are significantly more tolerant to precision loss than weights and as such lend themselves better to aggressive compression without the need for the complex mechanisms to avert any loss. (2) The existing training algorithms only communicate gradients in one leg of the communication, which reduces the opportunities for in-network acceleration of compression. (3) The aggregators can become a bottleneck with compression as they need to compress/decompress multiple streams from their allocated worker group.To this end, we first propose a lightweight and hardware-friendly lossy-compression algorithm for floating-point gradients, which exploits their unique value characteristics. This compression not only enables significantly reducing the gradient communication with practically no loss of accuracy, but also comes with low complexity for direct implementation as a hardware block in the NIC. To maximize the opportunities for compression and avoid the bottleneck at aggregators, we also propose an aggregator-free training algorithm that exchanges gradients in both legs of communication in the group, while the workers collectively perform the aggregation in a distributed manner. Without changing the mathematics of training, this algorithm leverages the associative property of the aggregation operator and enables our in-network accelerators to (1) apply compression for all communications, and (2) prevent the aggregator nodes from becoming bottlenecks. Our experiments demonstrate that INCEPTIONN reduces the communication time by 70.9~80.7\% and offers 2.2~3.1x speedup over the conventional training system, while achieving the same level of accuracy.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {175–188},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00024,
author = {Deng, Chunhua and Liao, Siyu and Xie, Yi and Parhi, Keshab K. and Qian, Xuehai and Yuan, Bo},
title = {PermDNN: efficient compressed DNN architecture with permuted diagonal matrices},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00024},
doi = {10.1109/MICRO.2018.00024},
abstract = {Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity.To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x ~ 4.8x higher throughout, 5.9x ~ 8.5x better area efficiency and 2.8x ~ 4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {189–202},
numpages = {14},
keywords = {model compression, deep learning, VLSI},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00025,
author = {Tsai, Po-An and Gan, Yee Ling and Sanchez, Daniel},
title = {Rethinking the memory hierarchy for modern languages},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00025},
doi = {10.1109/MICRO.2018.00025},
abstract = {We present Hotpads, a new memory hierarchy designed from the ground up for modern, memory-safe languages like Java, Go, and Rust. Memory-safe languages hide the memory layout from the programmer. This prevents memory corruption bugs and enables automatic memory management.Hotpads extends the same insight to the memory hierarchy: it hides the memory layout from software and takes control over it, dispensing with the conventional flat address space abstraction. This avoids the need for associative caches. Instead, Hotpads moves objects across a hierarchy of directly addressed memories. It rewrites pointers to avoid most associative lookups, provides hardware support for memory allocation, and unifies hierarchical garbage collection and data placement. As a result, Hotpads improves memory performance and efficiency substantially, and unlocks many new optimizations.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {203–216},
numpages = {14},
keywords = {scratchpad, memory-safe languages, memory hierarchy, managed languages, garbage collection, cache},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00026,
author = {Jeffrey, Mark C. and Ying, Victor A. and Subramanian, Suvinay and Lee, Hyun Ryong and Emer, Joel and Sanchez, Daniel},
title = {Harmonizing speculative and non-speculative execution in architectures for ordered parallelism},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00026},
doi = {10.1109/MICRO.2018.00026},
abstract = {Multicore systems should support both speculative and non-speculative parallelism. Speculative parallelism is easy to use and is crucial to scale many challenging applications, while non-speculative parallelism is more efficient and allows parallel irrevocable actions (e.g., parallel I/O). Unfortunately, prior techniques are far from this goal. Hardware transactional memory (HTM) systems support speculative (transactional) and non-speculative (non-transactional) work, but lack coordination mechanisms between the two, and are limited to unordered parallelism. Prior work has extended HTMs to avoid the limitations of speculative execution, e.g., through escape actions and open-nested transactions. But these mechanisms are incompatible with systems that exploit ordered parallelism, which parallelize a broader range of applications and are easier to use.We contribute two techniques that enable seamlessly composing and coordinating speculative and non-speculative work in the context of ordered parallelism: (i) a task-based execution model that efficiently coordinates concurrent speculative and non-speculative ordered tasks, allowing them to create tasks of either kind and to operate on shared data; and (ii) a safe way for speculative tasks to invoke software-managed speculative actions that avoid hardware version management and conflict detection. These contributions improve efficiency and enable new capabilities. Across several benchmarks, they allow the system to dynamically choose whether to execute tasks speculatively or non-speculatively, avoid needless conflicts among speculative tasks, and allow speculative tasks to safely invoke irrevocable actions.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {217–230},
numpages = {14},
keywords = {transactional memory, thread-level speculation, synchronization, speculative parallelism, speculative forwarding, ordered parallelism, multicore, fine-grain parallelism},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00027,
author = {Silvestro, Sam and Liu, Hongyu and Zhang, Tong and Jung, Changhee and Lee, Dongyoon and Liu, Tongping},
title = {Sampler: PMU-based sampling to detect memory errors latent in production software},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00027},
doi = {10.1109/MICRO.2018.00027},
abstract = {Deployed software is still faced with numerous in-production memory errors. They can significantly affect system reliability and security, causing application crashes, erratic execution behavior, or security attacks. Unfortunately, existing tools cannot be deployed in the production environment, since they either impose significant performance/memory overhead, or can only detect partial errors. This paper presents Sampler, a library that employs the combination of hardware-based SAMPLing and novel heap allocator design to efficiently identify a range of memory ERrors, including buffer overflows, use-after-frees, invalid frees, and double-frees. Due to the stringent Quality of Service (QoS) requirement of production services, Sampler proposes to trade detection effectiveness for performance on each execution. Rather than inspecting every memory access, Sampler proposes the use of the Performance Monitoring Unit (PMU) hardware to sample memory accesses, and only checks the validity of sampled accesses. At the same time, Sampler proposes a novel dynamic allocator supporting fast metadata lookup, and a solution to prevent false alarms potentially caused by sampling. The sampling-based approach, although it may lead to reduced effectiveness on each execution, is suitable for in-production software, since software is generally employed by a large number of individuals, and may be executed many times or over a long period of time. By randomizing the start of the sampling, different executions may sample different sequences of memory accesses, working together to enable effective detection. Experimental results demonstrate that Sampler detects all known memory bugs inside real applications, without any false positive. Sampler only imposes negligible performance overhead (2.4\% on average). Sampler is the first work that simultaneously satisfies efficiency, preciseness, completeness, accuracy, and transparency, making it a practical tool for in-production deployment.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {231–244},
numpages = {14},
keywords = {vulnerability detection, sampling, PMU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00028,
author = {Margerm, Steven and Sharifian, Amirali and Guha, Apala and Shriraman, Arrvindh and Pokam, Gilles},
title = {TAPAS: generating parallel accelerators from parallel programs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00028},
doi = {10.1109/MICRO.2018.00028},
abstract = {High-level-synthesis (HLS) tools generate accelerators from software programs to ease the task of building hardware. Unfortunately, current HLS tools have limited support for concurrency, which impacts the speedup achievable with the generated accelerator. Current approaches only target fixed static patterns (e.g., pipeline, data-parallel kernels). This constraints the ability of software programmers to express concurrency. Moreover, the generated accelerator loses a key benefit of parallel hardware, dynamic asynchrony, and the potential to hide long latency and cache misses.We have developed TAPAS, an HLS toolchain for generating parallel accelerators from programs with dynamic parallelism. TAPAS is built on top of Tapir [22], [39], which embeds fork-join parallelism into the compiler's intermediate-representation. TAPAS leverages the compiler IR to identify parallelism and synthesizes the hardware logic. TAPAS provides first-class architecture support for spawning, coordinating and synchronizing tasks during accelerator execution. We demonstrate TAPAS can generate accelerators for concurrent programs with heterogeneous, nested and recursive parallelism. Our evaluation on Intel-Altera DE1-SoC and Arria-10 boards demonstrates that TAPAS generated accelerators achieve 20X the power efficiency of an Intel Xeon, while maintaining comparable performance. We also show that TAPAS enables lightweight tasks that can be spawned in ≃10 cycles and enables accelerators to exploit available fine-grain parallelism. TAPAS is a complete HLS toolchain for synthesizing parallel programs to accelerators and is open-sourced.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {245–257},
numpages = {13},
keywords = {power efficiency, high-level synthesis, hardware accelerator, dynamic parallelism, cilk, chisel, TAPAS, LLVM, HLS, FPGA},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00029,
author = {Liu, Qingrui and Izraelevitz, Joseph and Lee, Se Kwon and Scott, Michael L. and Noh, Sam H. and Jung, Changhee},
title = {iDO: compiler-directed failure atomicity for nonvolatile memory},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00029},
doi = {10.1109/MICRO.2018.00029},
abstract = {This paper presents iDO, a compiler-directed approach to failure atomicity with nonvolatile memory. Unlike most prior work, which instruments each store of persistent data for redo or undo logging, the iDO compiler identifies idempotent instruction sequences, whose re-execution is guaranteed to be side-effect-free, thereby eliminating the need to log every persistent store. Using an extension of prior work on JUSTDO logging, the compiler then arranges, during recovery from failure, to back up each thread to the beginning of the current idempotent region and re-execute to the end of the current failure-atomic section. This extension transforms JUSTDO logging from a technique of value only on hypothetical future machines with nonvolatile caches into a technique that also significantly outperforms state-of-the art lock-based persistence mechanisms on current hardware during normal execution, while preserving very fast recovery times.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {258–270},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00030,
author = {Bharadwaj, Srikant and Cox, Guilherme and Krishna, Tushar and Bhattacharjee, Abhishek},
title = {Scalable distributed last-level TLBs using low-latency interconnects},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00030},
doi = {10.1109/MICRO.2018.00030},
abstract = {Recent studies have shown the potential of last-level TLBs shared by multiple cores in tackling memory translation performance challenges posed by "big data" workloads. A key stumbling block hindering their effectiveness, however, is their high access time. We present a design methodology to reduce these high access times so as to realize high-performance and scalable shared L2 TLBs. As a first step, we study the benefits of replacing monolithic shared TLBs with a distributed set of small TLB slices. While this approach does reduce TLB lookup latency, it increases interconnect delays in accessing remote slices. Therefore, as a second step, we devise a lightweight single-cycle interconnect among the TLB slices by tailoring wires and switches to the unique communication characteristics of memory translation requests and responses. Our approach, which we dub Nocstar (&lt;u&gt;NOC&lt;/u&gt;s for &lt;u&gt;s&lt;/u&gt;calable &lt;u&gt;T&lt;/u&gt;LB &lt;u&gt;a&lt;/u&gt;rchitecture), combines the high hit rates of shared TLBs with low access times of private L2 TLBs, enabling significant system performance benefits.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {271–284},
numpages = {14},
keywords = {virtual memory, network-on-chip, caches, TLB},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00031,
author = {Lin, Ben (Ching-Pei) and Healy, Michael B. and Miftakhutdinov, Rustam and Emma, Philip G. and Patt, Yale},
title = {Duplicon cache: mitigating off-chip memory bank and bank group conflicts via data duplication},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00031},
doi = {10.1109/MICRO.2018.00031},
abstract = {Bank and bank group conflicts are major performance bottlenecks for memory intensive workloads. Idealized experiments show removing bank and bank group conflicts collectively can improve performance by up to 37.5\% and by 22.5\% on average for our mix of multi-programmed memory intensive workloads. We propose the Duplicon Cache to mitigate bank and bank group conflict penalties by duplicating select lines of data to an alternate bank group, giving the memory controller the freedom to source the data from the bank group which avoids conflicts. The Duplicon Cache is entirely implemented in the memory controller and does not require changes to commodity memory. We identify and address the main challenges associated with duplication: 1) tracking duplicated data efficiently, 2) identifying which data to duplicate, and 3) replacing stale duplicated data while protecting useful ones. Our evaluations show the Duplicon Cache configured with 128MB of storage (out of 16GB of main memory) improves performance by 8.3\% while reducing energy by 5.6\%.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {285–297},
numpages = {13},
keywords = {usefulness tracking, set-associative cache, sectored cache, probabilistic replacement, duplication, demand activates filtering, bank group conflicts, bank conflicts},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00032,
author = {Wang, Yaohua and Tavakkol, Arash and Orosa, Lois and Ghose, Saugata and Ghiasi, Nika Mansouri and Patel, Minesh and Kim, Jeremie S. and Hassan, Hasan and Sadrosadati, Mohammad and Mutlu, Onur},
title = {Reducing DRAM latency via charge-level-aware look-ahead partial restoration},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00032},
doi = {10.1109/MICRO.2018.00032},
abstract = {Long DRAM access latency is a major bottleneck for system performance. In order to access data in DRAM, a memory controller (1) activates (i.e., opens) a row of DRAM cells in a cell array, (2) restores the charge in the activated cells back to their full level, (3) performs read and write operations to the activated row, and (4) precharges the cell array to prepare for the next activation. The restoration operation is responsible for a large portion (up to 43.6\%) of the total DRAM access latency.We find two frequent cases where the restoration operations performed by DRAM do not need to fully restore the charge level of the activated DRAM cells, which we can exploit to reduce the restoration latency. First, DRAM rows are periodically refreshed (i.e., brought back to full charge) to avoid data loss due to charge leakage from the cell. The charge level of a DRAM row that will be refreshed soon needs to be only partially restored, providing just enough charge so that the refresh can correctly detect the cells' data values. Second, the charge level of a DRAM row that will be activated again soon can be only partially restored, providing just enough charge for the activation to correctly detect the data value. However, partial restoration needs to be done carefully: for a row that will be activated again soon, restoring to only the minimum possible charge level can undermine the benefits of complementary mechanisms that reduce the activation time of highly-charged rows.To enable effective latency reduction for both activation and restoration, we propose charge-level-aware look-ahead partial restoration (CAL). CAL consists of two key components. First, CAL accurately predicts the next access time, which is the time between the current restoration operation and the next activation of the same row. Second, CAL uses the predicted next access time and the next refresh time to reduce the restoration time, ensuring that the amount of partial charge restoration is enough to maintain the benefits of reducing the activation time of a highly-charged row. We implement CAL fully in the memory controller, without any changes to the DRAM module. Across a wide variety of applications, we find that CAL improves the average performance of an 8-core system by 14.7\%, and reduces average DRAM energy consumption by 11.3\%.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {298–311},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00033,
author = {Nguyen, Tri M. and Fuchs, Adi and Wentzlaff, David},
title = {CABLE: a CAche-based link encoder for bandwidth-starved manycores},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00033},
doi = {10.1109/MICRO.2018.00033},
abstract = {Off-chip bandwidth is a scarce resource in modern processors, and it is expected to become even more limited on a per-core basis as we move into the era of high-throughput and massively-parallel computation. One promising approach to overcome limited bandwidth is off-chip link compression. Unfortunately, previously proposed latency-driven compression schemes are not a good fit for latency-tolerant manycore systems, and they often do not have the dictionary capacity to accommodate more than a few concurrent threads. In this work, we present CABLE, a novel CAche-Based Link Encoder that enables point-to-point link compression between coherent caches, re-purposing the data already stored in the caches as a massive and scalable dictionary for data compression. We show the broad applicability of CABLE by applying it to two critical off-chip links: (1) the memory link interface to off-chip memory, and (2) the cache-coherent link between processors in a multi-chip system. We have implemented CABLE's search pipeline hardware in Verilog using the OpenPiton framework to show its feasibility. Evaluating with SPEC2006, we find that CABLE increases effective off-chip bandwidth by 7.2X and system throughput by 3.78X on average, 83\% and 258\% better than CPACK, respectively.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {312–325},
numpages = {14},
keywords = {parallel processing, data compression, cache memory},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00034,
author = {Hong, Seokin and Nair, Prashant J. and Abali, Bulent and Buyuktosunoglu, Alper and Kim, Kyu-Hyoun and Healy, Michael B.},
title = {Attach\'{e}: towards ideal memory compression by mitigating metadata bandwidth overheads},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00034},
doi = {10.1109/MICRO.2018.00034},
abstract = {Memory systems are becoming bandwidth constrained and data compression is seen as a simple technique to increase their effective bandwidth. However, data compression requires accessing Metadata which incurs additional bandwidth overheads. Even after using a Metadata-Cache, the bandwidth overheads of Metadata can reduce the benefits of compression.This paper proposes Attach\'{e}, a framework that reduces the overheads of Metadata accesses. The Attach\'{e} framework consists of two components. The first component, called the Blended Metadata Engine (BLEM), enables data and its Metadata to be accessed together. BLEM incurs additional Metadata accesses only 0.003\% times and removes almost all Metadata bandwidth overheads. The second component, called the Compression Predictor (COPR), predicts if the memory block is compressed. The COPR predictor uses a fine-grained line-level predictor, a coarse-grained page-level predictor, and a global indicator. This enables Attach\'{e} to predict the compressibility of the memory block before sending a memory read request. We implement Attach\'{e} on a memory system that uses Sub-Ranking. On average, Attach\'{e} achieves 15.3\% speedup (ideal 17\%) and saves 22\% energy consumption (ideal 23\%) when compared to a baseline system that does not employ data compression. Attach\'{e} is completely hardware-based and uses only 368KB of SRAM.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {326–338},
numpages = {13},
keywords = {sub-ranking, metadata, memory systems, data compression, bandwidth},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00035,
author = {Young, Vinson and Jaleel, Aamer and Bolotin, Evgeny and Ebrahimi, Eiman and Nellans, David and Villa, Oreste},
title = {Combining HW/SW mechanisms to improve NUMA performance of multi-GPU systems},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00035},
doi = {10.1109/MICRO.2018.00035},
abstract = {Historically, improvement in GPU performance has been tightly coupled with transistor scaling. As Moore's Law slows down, performance of single GPUs may ultimately plateau. To continue GPU performance scaling, multiple GPUs can be connected using system-level interconnects. However, limited inter-GPU interconnect bandwidth (e.g., 64GB/s) can hurt multi-GPU performance when there are frequent remote GPU memory accesses. Traditional GPUs rely on page migration to service the memory accesses from local memory instead. Page migration fails when the page is simultaneously shared between multiple GPUs in the system. As such, recent proposals enhance the software runtime system to replicate read-only shared pages in local memory. Unfortunately, such practice fails when there are frequent remote memory accesses to read-write shared pages. To address this problem, recent proposals cache remote shared data in the GPU last-level-cache (LLC). Unfortunately, remote data caching also fails when the shared-data working-set exceeds the available GPU LLC size.This paper conducts a combined performance analysis of state-of-the-art software and hardware mechanisms to improve NUMA performance of multi-GPU systems. Our evaluations on a 4-node multi-GPU system reveal that the combination of work scheduling, page placement, page migration, page replication, and caching remote data still incurs a 47\% slowdown relative to an ideal NUMA-GPU system. This is because the shared memory footprint tends to be significantly larger than the GPU LLC size and can not be replicated by software because the shared footprint has read-write property. Thus, we show that existing NUMA-aware software solutions require hardware support to address the NUMA bandwidth bottleneck. We propose &lt;u&gt;Ca&lt;/u&gt;ching &lt;u&gt;R&lt;/u&gt;emote Data in &lt;u&gt;V&lt;/u&gt;ideo M&lt;u&gt;e&lt;/u&gt;mory (CARVE), a hardware mechanism that stores recently accessed remote shared data in a dedicated region of the GPU memory. CARVE outperforms state-of-the-art NUMA mechanisms and is within 6\% the performance of an ideal NUMA-GPU system. A design space analysis on supporting cache coherence is also investigated. Overall, we show that dedicating only 3\% of GPU memory eliminates NUMA bandwidth bottlenecks while incurring negligible performance overheads due to the reduced GPU memory capacity.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {339–351},
numpages = {13},
keywords = {page-replication, page-migration, memory, coherence, NUMA, Multi-GPU, HBM, GPU, DRAM-Cache},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00036,
author = {Shin, Seunghee and LeBeane, Michael and Solihin, Yan and Basu, Arkaprava},
title = {Neighborhood-aware address translation for irregular GPU applications},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00036},
doi = {10.1109/MICRO.2018.00036},
abstract = {Recent studies on commercial hardware demonstrated that irregular GPU workloads could bottleneck on virtual-to-physical address translations. GPU's single-instruction-multiple-thread (SIMT) execution can generate many concurrent memory accesses, all of which require address translation before accesses can complete. Unfortunately, many of these address translation requests often miss in the TLB, generating many concurrent page table walks. In this work, we investigate how to reduce address translation overheads for such applications.We observe that many of these concurrent page walk requests, while irregular from the perspective of a single GPU wavefront, still fall on neighboring virtual page addresses. The address mappings for these neighboring pages are typically stored in the same 64-byte cache line. Since cache lines are the smallest granularity of memory access, the page table walker implicitly reads address mappings (i.e., page table entries or PTEs) of many neighboring pages during the page walk of a single virtual address (VA). However, in the conventional hardware, mappings not associated with the original request are simply discarded. In this work, we propose mechanisms to coalesce the address translation needs of all pending page table walks in the same neighborhood that happen to have their address mappings fall on the same cache line. This is almost free; the page table walker (PTW) already reads a full cache line containing address mappings of all pages in the same neighborhood. We find this simple scheme can reduce the number of accesses to the in-memory page table by 37\% on average. This speeds up a set of GPU workloads by an average of 1.7X.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {352–363},
numpages = {12},
keywords = {virtual address, computer architecture, GPU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00037,
author = {Oh, Yunho and Yoon, Myung Kuk and Song, William J. and Ro, Won Woo},
title = {FineReg: fine-grained register file management for augmenting GPU throughput},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00037},
doi = {10.1109/MICRO.2018.00037},
abstract = {Graphics processing units (GPUs) include a large amount of hardware resources for parallel thread executions. However, the resources are not fully utilized during runtime, and observed throughput often falls far below the peak performance. A major cause is that GPUs cannot deploy enough number of warps at runtime. The limited size of register file constrains the number of cooperative thread arrays (CTAs) as one CTA takes up a few tens of kilobytes of registers. We observe that the actual working set size of a CTA is much smaller in general, and therefore there is room for additional CTAs to run. In this paper, we propose a novel GPU architecture called FineReg that improves overall throughput by increasing the number of concurrent CTAs. In particular, FineReg splits the monolithic register file into two regions, one for active CTAs and another for pending CTAs. Using FineReg, the GPU begins normal executions by allocating all registers required by active CTAs. If all warps of a CTA become stalled, FineReg moves the live registers (i.e., working set) of CTA to the pending-CTA region and launches an additional CTA by assigning registers to the newly activated CTA. If the registers of either active or pending-CTA region are used up, FineReg stops introducing additional CTAs and simply performs context switching between active and pending CTAs. Thus, FineReg increases the number of concurrent CTAs by reducing the effective size of per-CTA registers. Experiment results show that FineReg achieves 32.8\% of performance improvement over a conventional GPU architecture.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {364–376},
numpages = {13},
keywords = {thread-level parallelism, register file, GPU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00038,
author = {Khorasani, Farzad and Esfeden, Hodjat Asghari and Abu-Ghazaleh, Nael and Sarkar, Vivek},
title = {In-register parameter caching for dynamic neural nets with virtual persistent processor specialization},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00038},
doi = {10.1109/MICRO.2018.00038},
abstract = {Dynamic neural networks enable higher representation flexibility compared to networks with a fixed architecture and are extensively deployed in problems dealing with varying input-induced network structure, such as those in Natural Language Processing. One of the optimizations used in training networks is persistency of recurrent weights on the chip. In dynamic nets, a possibly-inhomogeneous computation graph for every input prevents caching recurrent weights in GPU registers. Therefore, existing solutions suffer from excessive recurring off-chip memory loads as well as compounded kernel launch overheads and underutilization of GPU SMs.In this paper, we present a software system that enables persistency of weight matrices during the training of dynamic neural networks on the GPU. Before the training begins, our approach named Virtual Persistent Processor Specialization (VPPS) specializes a forward-backward propagation kernel that contains in-register caching and operation routines. VPPS virtualizes persistent kernel CTAs as CISC-like vector processors that can be guided to execute supplied instructions. VPPS greatly reduces the overall amount of off-chip loads by caching weight matrices on the chip, while simultaneously, provides maximum portability as it does not make any assumptions about the shape of the given computation graphs hence fulfilling dynamic net requirements. We implemented our solution on DyNet and abstracted away its design complexities by providing simple function calls to the user. Our experiments on a Volta micro-architecture shows that, unlike the most competitive solutions, VPPS shows excellent performance even in small batch sizes and delivers up to 6x speedup on training dynamic nets.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {377–389},
numpages = {13},
keywords = {specialization, register, persistent, neural network, dynamic neural network, deep learning, GPU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00039,
author = {Zou, An and Leng, Jingwen and He, Xin and Zu, Yazhou and Gill, Christopher D. and Reddi, Vijay Janapa and Zhang, Xuan},
title = {Voltage-stacked GPUs: a control theory driven cross-layer solution for practical voltage stacking in GPUs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00039},
doi = {10.1109/MICRO.2018.00039},
abstract = {More than 20\% of the available energy is lost in "the last centimeter" from the PCB board to the microprocessor chip due to inherent inefficiencies of power delivery subsystems (PDSs) in today's computing systems. By series-stacking multiple voltage domains to eliminate explicit voltage conversion and reduce loss along the power delivery path, voltage stacking (VS) is a novel configuration that can improve power delivery efficiency (PDE). However, VS suffers from aggravated levels of supply noise caused by current imbalance between the stacking layers, preventing its practical adoption in mainstream computing systems. Throughput-centric manycore architectures such as GPUs intrinsically exhibit more balanced workloads, yet suffer from lower PDE, making them ideal platforms to implement voltage stacking. In this paper, we present a cross-layer approach to practical voltage stacking implementation in GPUs. It combines circuit-level voltage regulation using distributed charge-recycling integrated voltage regulators (CR-IVRs) with architecture-level voltage smoothing guided by control theory. Our proposed voltage-stacked GPUs can eliminate 61.5\% of total PDS energy loss and achieve 92.3\% system-level power delivery efficiency, a 12.3\% improvement over the conventional single-layer based PDS. Compared to the circuit-only solution, the cross-layer approach significantly reduces the implementation cost of voltage stacking (88\% reduction in area overhead) without compromising supply reliability under worst-case scenarios and across a wide range of real-world benchmarks. In addition, we demonstrate that the cross-layer solution not only complements on-chip CR-IVRs to transparently manage current imbalance and restore stable layer voltages, but also serves as a seamless interface to accommodate higher-level power optimization techniques, traditionally thought to be incompatible with a VS configuration.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {390–402},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00040,
author = {Ye, Mao and Hughes, Clayton and Awad, Amro},
title = {Osiris: a low-cost mechanism to enable restoration of secure non-volatile memories},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00040},
doi = {10.1109/MICRO.2018.00040},
abstract = {With Non-Volatile Memories (NVMs) beginning to enter the mainstream computing market, it is time to consider how to secure NVM-equipped computing systems. Recent Melt-down and Spectre attacks are evidence that security must be intrinsic to computing systems and not added as an afterthought. Processor vendors are taking the first steps and are beginning to build security primitives into commodity processors. One security primitive that is associated with the use of emerging NVMs is memory encryption. Memory encryption, while necessary, is very challenging when used with NVMs because it exacerbates the write endurance problem.Secure architectures use cryptographic metadata that must be persisted and restored to allow secure recovery of data in the event of power-loss. Specifically, encryption counters must be persistent to enable secure and functional recovery of an interrupted system. However, the cost of ensuring and maintaining persistence for these counters can be significant. In this paper, we propose a novel scheme to maintain encryption counters without the need for frequent updates. Our new memory controller design, Osiris, repurposes memory Error-Correction Codes (ECCs) to enable fast restoration and recovery of encryption counters. To evaluate our design, we use Gem5 to run eight memory-intensive workloads selected from SPEC2006 and U.S. Department of Energy (DoE) proxy applications. Compared to a write-through counter-cache scheme, on average, Osiris can reduce 48.7\% of the memory writes (increase lifetime by 1.95x), and reduce the performance overhead from 51.5\% (for write-through) to only 5.8\%. Furthermore, without the need for backup battery or extra power-supply hold-up time, Osiris performs better than a battery-backed write-back (5.8\% vs. 6.6\% overhead) and has less write-traffic (2.6\% vs. 5.9\% overhead).},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {403–415},
numpages = {13},
keywords = {secure architecture, non-volatile memory, computer architecture, ECC},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00041,
author = {Saileshwar, Gururaj and Nair, Prashant J. and Ramrakhyani, Prakash and Elsasser, Wendy and Joao, Jose A. and Qureshi, Moinuddin K.},
title = {Morphable counters: enabling compact integrity trees for low-overhead secure memories},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00041},
doi = {10.1109/MICRO.2018.00041},
abstract = {Securing off-chip main memory is essential for protection from adversaries with physical access to systems. However, current secure-memory designs incur considerable performance overheads - a major cause being the multiple memory accesses required for traversing an integrity-tree, that provides protection against man-in-the-middle attacks or replay attacks.In this paper, we provide a scalable solution to this problem by proposing a compact integrity tree design that requires fewer memory accesses for its traversal. We enable this by proposing new storage-efficient representations for the counters used for encryption and integrity-tree in secure memories. Our Morphable Counters are more cacheable on-chip, as they provide more counters per cacheline than existing split counters. Additionally, they incur lower overheads due to counter-overflows, by dynamically switching between counter representations based on usage pattern. We show that using Morphable Counters enables a 128-ary integrity-tree, that can improve performance by 6.3\% on average (up to 28.3\%) and reduce system energy-delay product by 8.8\% on average, compared to an aggressive baseline using split counters with a 64-ary integrity-tree. These benefits come without any additional storage or reduction in security and are derived from our compact counter representation, that reduces the integrity-tree size for a 16GB memory from 4MB in the baseline to 1MB. Compared to recently proposed VAULT [1], our design provides a speedup of 13.5\% on average (up to 47.4\%).},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {416–427},
numpages = {12},
keywords = {split counters, replay attack, merkle tree, memory security, encryption, compression, MAC, Intel SGX},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00042,
author = {Yan, Mengjia and Choi, Jiho and Skarlatos, Dimitrios and Morrison, Adam and Fletcher, Christopher W. and Torrellas, Josep},
title = {InvisiSpec: making speculative execution invisible in the cache hierarchy},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00042},
doi = {10.1109/MICRO.2018.00042},
abstract = {Hardware speculation offers a major surface for micro-architectural covert and side channel attacks. Unfortunately, defending against speculative execution attacks is challenging. The reason is that speculations destined to be squashed execute incorrect instructions, outside the scope of what programmers and compilers reason about. Further, any change to micro-architectural state made by speculative execution can leak information.In this paper, we propose InvisiSpec, a novel strategy to defend against hardware speculation attacks in multiprocessors by making speculation invisible in the data cache hierarchy. InvisiSpec blocks micro-architectural covert and side channels through the multiprocessor data cache hierarchy due to speculative loads. In InvisiSpec, unsafe speculative loads read data into a speculative buffer, without modifying the cache hierarchy. When the loads become safe, InvisiSpec makes them visible to the rest of the system. InvisiSpec identifies loads that might have violated memory consistency and, at this time, forces them to perform a validation step. We propose two InvisiSpec designs: one to defend against Spectre-like attacks and another to defend against futuristic attacks, where any speculative load may pose a threat. Our simulations with 23 SPEC and 10 PARSEC workloads show that InvisiSpec is effective. Under TSO, using fences to defend against Spectre attacks slows down execution by 74\% relative to a conventional, insecure processor; InvisiSpec reduces the execution slowdown to only 21\%. Using fences to defend against futuristic attacks slows down execution by 208\%; InvisiSpec reduces the slowdown to 72\%.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {428–441},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00043,
author = {Zuo, Pengfei and Hua, Yu and Zhao, Ming and Zhou, Wen and Guo, Yuncheng},
title = {Improving the performance and endurance of encrypted non-volatile main memory through deduplicating writes},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00043},
doi = {10.1109/MICRO.2018.00043},
abstract = {Non-volatile memory (NVM) technologies are considered as promising candidates of the next-generation main memory. However, the non-volatility of NVMs leads to new security vulnerabilities. For example, it is not difficult to access sensitive data stored on stolen NVMs. Memory encryption can be employed to mitigate the security vulnerabilities, but it increases the number of bits written to NVMs due to the diffusion property and thereby aggravates the NVM wear-out induced by writes. To address these security and endurance challenges, this paper proposes DeWrite, a secure and deduplication-aware scheme to enhance the performance and endurance of encrypted NVMs based on a new in-line deduplication technique and the synergistic integrations of deduplication and memory encryption. Specifically, it performs low-latency in-line deduplication to exploit the abundant cache-line-level duplications leveraging the intrinsic read/write asymmetry of NVMs and light-weight hashing. It also opportunistically parallelizes the operations of deduplication and encryption and allows them to co-locate the metadata for high time and space efficiency. DeWrite was implemented on the gem5 with NVMain and evaluated using 20 applications from SPEC CPU2006 and PARSEC. Extensive experimental results demonstrate that DeWrite reduces on average 54\% writes to encrypted NVMs, and speeds up memory writes and reads of encrypted NVMs by 4.2X and 3.1X, respectively. Meanwhile, DeWrite improves the system IPC by 82\% and reduces 40\% of energy consumption on average.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {442–454},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00044,
author = {Kim, Joonsung and Park, Pyeongsu and Ahn, Jaehyung and Kim, Jihun and Kim, Jong and Kim, Jangwoo},
title = {SSDcheck: timely and accurate prediction of irregular behaviors in black-box SSDs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00044},
doi = {10.1109/MICRO.2018.00044},
abstract = {Modern servers are actively deploying Solid-State Drives (SSDs). However, rather than just a fast storage device, SSDs are complex devices designed for device-specific goals (e.g., latency, throughput, endurance, cost) with their internal mechanisms undisclosed to users as the proprietary asset, which leads to unpredictable, irregular inter/intra-SSD access latencies. This unpredictable irregular access latency has been a fundamental challenge to server architects aiming to satisfy critical quality-of-service requirements and/or achieve the full performance potential of commodity SSDs.In this paper, we propose SSDcheck, a novel SSD performance model to accurately predict the latency of next access to commodity black-box SSDs. First, after analyzing a wide spectrum of real-world SSDs, we identify key performance-critical features (e.g., garbage collection, write buffering) required to construct a general SSD performance model. Next, SSDcheck runs diagnosis code snippets to extract static feature parameters (e.g., size, threshold) from the target SSD, and constructs its performance model. Finally, during runtime, SSDcheck dynamically manages the performance model to predict the latency of the next access.Our evaluations show that SSDcheck achieves up to 98.96\% and 79.96\% on-average prediction accuracy for normal-latency and high-latency predictions, respectively. Next, we show the effectiveness of SSDcheck by implementing a new volume manager improving the throughput by up to 4.29X with the tail latency reduction down to 6.53\%, and a new I/O request handler improving the throughput by up to 44.0\% with the tail latency reduction down to 26.9\%. We then show how to further improve the results of scheduling with the help of an emerging NonVolatile Memory (e.g., PCM). SSDcheck does not require any hardware modifications, which can be harmlessly disabled for any SSDs uncovered by the performance model.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {455–468},
numpages = {14},
keywords = {storage system, performance modeling, SSD},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00045,
author = {Gouk, Donghyun and Kwon, Miryeong and Zhang, Jie and Koh, Sungjoon and Choi, Wonil and Kim, Nam Sung and Kandemir, Mahmut and Jung, Myoungsoo},
title = {Amber: enabling precise full-system simulation with detailed modeling of all SSD resources},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00045},
doi = {10.1109/MICRO.2018.00045},
abstract = {SSDs become a major storage component in modern memory hierarchies, and SSD research demands exploring future simulation-based studies by integrating SSD subsystems into a full-system environment. However, several challenges exist to model SSDs under a full-system simulations; SSDs are composed upon their own complete system and architecture, which employ all necessary hardware, such as CPUs, DRAM and interconnect network. Employing the hardware components, SSDs also require to have multiple device controllers, internal caches and software modules that respect a wide spectrum of storage interfaces and protocols. These SSD hardware and software are all necessary to incarnate storage subsystems under full-system environment, which can operate in parallel with the host system.In this work, we introduce a new SSD simulation framework, SimpleSSD 2.0, namely Amber, that models embedded CPU cores, DRAMs, and various flash technologies (within an SSD), and operate under the full system simulation environment by enabling a data transfer emulation. Amber also includes full firmware stack, including DRAM cache logic, flash firmware, such as FTL and HIL, and obey diverse standard protocols by revising the host DMA engines and system buses of a popular full system simulator's all functional and timing CPU models (gem5). The proposed simulator can capture the details of dynamic performance and power of embedded cores, DRAMs, firmware and flash under the executions of various OS systems and hardware platforms. Using Amber, we characterize several system-level challenges by simulating different types of full-systems, such as mobile devices and general-purpose computers, and offer comprehensive analyses by comparing passive storage and active storage architectures.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {469–481},
numpages = {13},
keywords = {storage, solid state drive, non-volatile memory, memory system, full-system simulator, flash memory},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00046,
author = {Choi, Wonil and Jung, Myoungsoo and Kandemir, Mahmut},
title = {Invalid data-aware coding to enhance the read performance of high-density flash memories},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00046},
doi = {10.1109/MICRO.2018.00046},
abstract = {High bit-density flash memories such as Multi-Level Cell (MLC) and Triple-Level Cell (TLC) flash have become a norm, since doubling the cell bit-density can increase the storage capacity by 2X using the same number of cells. However, these high bit-density flash memories suffer from the read variation problem - e.g., the three kinds of bits (i.e., lower, middle, and upper bits) in a TLC cell have different read latencies; reading an upper bit takes a longer time than reading a middle bit, and reading a lower bit takes the minimum time. In this paper, we note that, in the conventional coding, the long read latencies of the middle and upper bits are not reduced even after the lower bit value is invalidated (i.e., no longer used). Motivated by this problem with the traditional coding, we propose a new coding technique, called Invalid Data-Aware (IDA) coding, which reduces the upper and middle bit read latencies close to the lower bit read latency when the lower bit becomes invalid. The main strategy the IDA coding employs is to merge the duplicated voltage states coming from the bit invalidation and reduce the number of (read) trials to identify the voltage state of a cell. To hide the performance and reliability degradation caused by the application of the IDA coding, we also propose to implement it as a part of the data refresh function, which is a fundamental operation in modern SSDs to keep its data safer and longer. With an extensive analysis of a TLC-based SSD using a variety of read-intensive workloads, we report that our IDA coding improves the read response times by 28\%, on average; it is also quite effective in devices with different bit densities and timing parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {482–493},
numpages = {12},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00047,
author = {Hu, Xing and Ogleari, Matheus and Zhao, Jishen and Li, Shuangchen and Basak, Abanti and Xie, Yuan},
title = {Persistence parallelism optimization: a holistic approach from memory bus to RDMA network},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00047},
doi = {10.1109/MICRO.2018.00047},
abstract = {Emerging non-volatile memories (NVM), such as phase change memory (PCM) and Resistive RAM (ReRAM), incorporate the features of fast byte-addressability and data persistence, which are beneficial for data services such as file systems and databases. To support data persistence, a persistent memory system requires ordering for write requests. The data path of a persistent request consists of three segments: through the cache hierarchy to the memory controller, through the bus from the memory controller to memory devices, and through the network from a remote node to a local node. Previous work contributes significantly to improve the persistence parallelism in the first segment of the data path. However, we observe that the memory bus and the Remote Direct Memory Access (RDMA) network remain severely under-utilized because the persistence parallelism in these two segments is not fully leveraged during ordering.In this paper, we propose a novel architecture to further improve the persistence parallelism in the memory bus and the RDMA network. First, we utilize inter-thread persistence parallelism for barrier epoch management with better bank-level parallelism (BLP). Second, we enable intra-thread persistence parallelism for remote requests through RDMA network with buffered strict persistence. With these features, the architecture efficiently supports persistence through all three segments of the write datapath. Experimental results show that for local applications, the proposed mechanism can achieve 1.3X performance improvement, compared to the original buffered persistence work. In addition, it can achieve 1.93X performance improvement for remote applications serviced through the RDMA network.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {494–506},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00048,
author = {Nguyen, Tri M. and Wentzlaff, David},
title = {PiCL: a software-transparent, persistent cache log for nonvolatile main memory},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00048},
doi = {10.1109/MICRO.2018.00048},
abstract = {Software-transparent crash consistency is a promising direction to immediately reap the benefits of nonvolatile main memory (NVMM) without encumbering programmers with error-prone transactional semantics. Unfortunately, proposed hardware write-ahead logging (WAL) schemes have high performance overhead, particularly for multi-core systems with many threads and big on-chip caches and NVMs with low random-access performance. This paper proposes PiCL, a new WAL checkpointing mechanism that provides a low overhead, software-transparent crash consistency solution for NVMM. PiCL introduces multi-undo logging, cache-driven logging, and asynchronous cache-scan to reduce random accesses and enable good row locality at the NVM. The key idea is that: by relaxing the durability timing of checkpoints, crash consistency can be provided with less than 1\% performance overhead where 1.5X to 5.0X slowdown was typical with prior work. To demonstrate the feasibility of software-transparent crash consistency, we fully implemented PiCL as an FPGA prototype in Verilog using the OpenPiton framework.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {507–519},
numpages = {13},
keywords = {parallel processing, nonvolatile memory, computer crashes, checkpointing, cache memory},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00049,
author = {Jeong, Jungi and Park, Chang Hyun and Huh, Jaehyuk and Maeng, Seungryoul},
title = {Efficient hardware-assisted logging with asynchronous and direct-update for persistent memory},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00049},
doi = {10.1109/MICRO.2018.00049},
abstract = {Supporting atomic durability in emerging persistent memory requires data consistency across potential system failures. For atomic durability support in the non-volatile memory, the traditional write-ahead log (WAL) technique has been employed to guarantee the persistency of logs before actual data updates. Based on the WAL mechanism, recent studies proposed HW-assisted logging techniques with undo, redo, or undo+redo principles. The HW log manager allows the overlapping of log writing and transaction execution, as long as the atomicity invariant can be satisfied. Although the efficiency of both log and data writes must be optimized, the prior work exhibit trade-offs in performance under various access patterns. The undo approach experiences performance degradation due to synchronous in-place data updates since the log contains only the old values. On the other hand, the undo+redo approach stores both old and new values, and does not require synchronous in-place data updates. However, the larger log size increases the amount of log writes. The prior redo approach demands extra NVM read bandwidth for indirectly updating in-place data from the new values in logs. To overcome the limitations of the previous approaches, this paper proposes a novel redo-based logging (ReDU), which performs direct and asynchronous in-place data update to NVM. ReDU exploits a small region of DRAM as a write-cache to remove NVM writes from the critical path. The experimental results show that the proposed logging mechanism provides the best performance under a variety of write patterns, showing 8.6\%, 14.2\%, and 23.6\% better performance compared to the previous undo, redo, and undo+redo approaches, respectively.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {520–532},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00050,
author = {Kotra, Jagadish B. and Zhang, Haibo and Alameldeen, Alaa R. and Wilkerson, Chris and Kandemir, Mahmut T.},
title = {CHAMELEON: a dynamically reconfigurable heterogeneous memory system},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00050},
doi = {10.1109/MICRO.2018.00050},
abstract = {Modern computing systems and applications have growing demand for memories with higher bandwidth. This demand can be alleviated using fast, large on-die or die-stacked memories. They are typically used with traditional DRAM as part of a heterogeneous memory system and used either as a DRAM cache or as a hardware- or OS-managed part of memory (PoM). Caches adapt rapidly to application needs and typically provide higher performance but reduce the total OS-visible memory capacity. PoM architectures increase the total OS-visible memory capacity but exhibit additional overheads due to swapping large blocks of data between fast and slow memory.In this paper, we propose Chameleon, a hybrid architecture that bridges the gap between cache and PoM architectures. When applications need a large memory, Chameleon uses both fast and slow memories as PoM, maximizing the available space for the application. When the application's footprint is smaller than the total physical memory capacity, Chameleon opportunistically uses free space in the system as a hardware-managed cache. Chameleon is a hardware-software co-designed system where the OS notifies the hardware of pages that are allocated or freed, and hardware decides on switching memory regions between PoM- and cache-modes dynamically. Based on our evaluation of multi-programmed workloads on a system with 4GB fast memory and 20GB slow memory, Chameleon improves the average performance by 11.6\% over PoM and 24.2\% over a latency-optimized cache.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {533–545},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00051,
author = {Choukse, Esha and Erez, Mattan and Alameldeen, Alaa R.},
title = {Compresso: pragmatic main memory compression},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00051},
doi = {10.1109/MICRO.2018.00051},
abstract = {Today, larger memory capacity and higher memory bandwidth are required for better performance and energy efficiency for many important client and datacenter applications. Hardware memory compression provides a promising direction to achieve this without increasing system cost. Unfortunately, current memory compression solutions face two significant challenges. First, keeping memory compressed requires additional memory accesses, sometimes on the critical path, which can cause performance overheads. Second, they require changing the operating system to take advantage of the increased capacity, and to handle incompressible data, which delays deployment. We propose Compresso, a hardware memory compression architecture that minimizes memory overheads due to compression, with no changes to the OS. We identify new data-movement trade-offs and propose optimizations that reduce additional memory movement to improve system efficiency. We propose a holistic evaluation for compressed systems. Our results show that Compresso achieves a 1.85x compression for main memory on average, with a 24\% speedup over a competitive hardware compressed system for single-core systems and 27\% for multi-core systems. As compared to competitive compressed systems, Compresso not only reduces performance overhead of compression, but also increases performance gain from higher memory capacity.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {546–558},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00052,
author = {Shahab, Amna and Zhu, Mingcan and Margaritov, Artemiy and Grot, Boris},
title = {Farewell my shared LLC! a case for private die-stacked DRAM caches for servers},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00052},
doi = {10.1109/MICRO.2018.00052},
abstract = {The slowdown in technology scaling mandates rethinking of conventional CPU architectures in a quest for higher performance and new capabilities. This work takes a step in this direction by questioning the value of on-chip shared last-level caches (LLCs) in server processors and argues for a better alternative. Shared LLCs have a number of limitations, including on-chip area constraints that limit storage capacity, long planar interconnect spans that increase access latency, and contention for the shared cache capacity that hurts performance under workload colocation.To overcome these limitations, we propose a Die-&lt;u&gt;S&lt;/u&gt;tacked Pr&lt;u&gt;i&lt;/u&gt;vate &lt;u&gt;L&lt;/u&gt;LC &lt;u&gt;O&lt;/u&gt;rganization (SILO), which combines conventional on-chip private L1 (and optionally, L2) caches with a per-core private LLC in die-stacked DRAM. By stacking LLC slices directly above each core, SILO avoids long planar wire spans. The use of private caches inherently avoids inter-core cache contention. Last but not the least, engineering the DRAM for latency affords low access delays while still providing over 100MB of capacity per core in today's technology. Evaluation results show that SILO outperforms state-of-the-art conventional cache architectures on a range of scale-out and traditional workloads while delivering strong performance isolation under colocation.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {559–572},
numpages = {14},
keywords = {server workloads, private cache, last-level cache, die-stacking, DRAM},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00053,
author = {Hadjilambrou, Zacharias and Das, Shidhartha and Antoniades, Marco A. and Sazeides, Yiannakis},
title = {Leveraging CPU electromagnetic emanations for voltage noise characterization},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00053},
doi = {10.1109/MICRO.2018.00053},
abstract = {Worst-case dI/dt voltage noise is typically characterized post-silicon using direct voltage measurements through either on-package measurement points or on-chip dedicated circuitry. These approaches consume expensive pad resources or suffer from design-time and run-time overheads. This work proposes an alternative non-intrusive, zero-overhead approach for post-silicon dI/dt voltage noise generation based on sensing CPU electromagnetic emanations using an antenna and a spectrum analyzer. The approach is based on the observation that high amplitude electromagnetic emanations are correlated with high voltage noise. We leverage this observation to automatically generate voltage noise (dI/dt) stress tests with a genetic-algorithm that is driven by electromagnetic signal amplitude and to obtain the first-order resonance-frequency of the Power-Delivery LC-tank network. The generality of the approach is established by successfully applying it to three different CPUs: two ARM multi-core mobile CPU clusters hosted on a big.LITTLE configuration and an x86-64 AMD desktop CPU. The efficacy of the proposed methodology is validated through VMIN and direct voltage noise measurements.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {573–585},
numpages = {13},
keywords = {voltage-noise, virus, stress-tests, resonance-frequency, power-delivery-network, methodology, measurement, genetic-algorithms, electromagnetic-emanations, dI/dt, GA, EM},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00054,
author = {Jang, Hanhwi and Jo, Jae-Eon and Lee, Jaewon and Kim, Jangwoo},
title = {RpStacks-MT: a high-throughput design evaluation methodology for multi-core processors},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00054},
doi = {10.1109/MICRO.2018.00054},
abstract = {Computer architects put significant efforts on the design space exploration of a new processor, as it determines the overall characteristics (e.g., performance, power, cost) of the final product. To thoroughly explore the space and achieve the best results, they need high design evaluation throughput - the ability to quickly assess a large number of designs with minimal costs. Unfortunately, the existing simulators and performance models are either too slow or too inaccurate to meet this demand. As a result, architects often sacrifice the design space coverage to end up with a sub-optimal product.To address this challenge, we propose RpStacks-MT, a methodology to evaluate multi-core processor designs with high throughput. First, we propose a graph-based multi-core performance model, which overcomes the limitations of the existing models to accurately describe a multi-core processor's key performance behaviors. Second, we propose a reuse distance-based memory system model and a dynamic scheduling reconstruction method, which help our graph model to quickly track the performance changes from processor design changes. Lastly, we combine these models with a state of the art design exploration idea to evaluate multiple processor designs in an efficient way. Our evaluations show that RpStacks-MT achieves extremely high design evaluation throughput - 88X higher versus a conventional cycle-level simulator and 18X higher versus an accelerated simulator (on average, for evaluating 10,000 designs) - while maintaining simulator-level accuracy.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {586–599},
numpages = {14},
keywords = {simulation, performance analysis, design space exploration},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00055,
author = {Miguel, Joshua San and Ganesan, Karthik and Badr, Mario and Xia, Chunqiu and Li, Rose and Hsiao, Hsuan and Jerger, Natalie Enright},
title = {The EH model: early design space exploration of intermittent processor architectures},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00055},
doi = {10.1109/MICRO.2018.00055},
abstract = {Energy-harvesting devices---which operate solely on energy collected from their environment---have brought forth a new paradigm of intermittent computing. These devices succumb to frequent power outages that would cause conventional systems to be stuck in a perpetual loop of restarting computation and never making progress. Ensuring forward progress in an intermittent execution model requires saving state in nonvolatile memory (backup) and potentially re-executing from the last saved state upon a power loss (restore). The interplay between spending energy on useful processing and spending energy on these necessary overheads yield unexpected trade-offs. To facilitate early design space exploration, the field of intermittent computing requires better models for 1) generalizing and reasoning about these trade-offs and 2) helping architects and programmers in making early-stage design decisions.We propose the EH model, which characterizes an intermittent system's ability to maximize how much of its available energy is spent on useful processor execution. The model parametrizes the energy costs associated with intermittent execution to allow an intuitive understanding of how forward progress can change. We use the EH model to explore how forward progress is impacted with the frequency of backups and the energy cost of backups and restores. We validate the EH model with hardware measurements on an MSP430 and characterize its parameters via simulation. We also demonstrate how architects and programmers can use the model to explore the design space of intermittent processors, derive insights, and model new optimizations that are unique to intermittent processor architectures.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {600–612},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: mining big performance data from hardware counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina"24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3\% to 7.7\% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {performance counters, data mining, computer architecture, big data},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00057,
author = {Cho, Shenghsun and Suresh, Amoghavarsha and Palit, Tapti and Ferdman, Michael and Honarmand, Nima},
title = {Taming the killer microsecond},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00057},
doi = {10.1109/MICRO.2018.00057},
abstract = {Modern applications require access to vast datasets at low latencies. Emerging memory technologies can enable faster access to significantly larger volumes of data than what is possible today. However, these memory technologies have a significant caveat: their random access latency falls in a range that cannot be effectively hidden using current hardware and software latency-hiding techniques---namely, the microsecond range. Finding the root cause of this "Killer Microsecond" problem, is the subject of this work. Our goal is to answer the critical question of why existing hardware and software cannot hide microsecond-level latencies, and whether drastic changes to existing platforms are necessary to utilize microsecond-latency devices effectively.We use an FPGA-based microsecond-latency device emulator, a carefully-crafted microbenchmark, and three open-source data-intensive applications to show that existing systems are indeed incapable of effectively hiding such latencies. However, after uncovering the root causes of the problem, we show that simple changes to existing systems are sufficient to support microsecond-latency devices. In particular, we show that by replacing on-demand memory accesses with prefetch requests followed by fast user-mode context switches (to increase access-level parallelism) and enlarging hardware queues that track in-flight accesses (to accommodate many parallel accesses), conventional architectures can effectively hide microsecond-level latencies, and approach the performance of DRAM-based implementations of the same applications. In other words, we show that successful usage of microsecond-level devices is not predicated on drastically new hardware and software architectures.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {627–640},
numpages = {14},
keywords = {killer microseconds, emerging storage, data-intensive applications, FPGA},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00058,
author = {Tsai, Po-An and Chen, Changping and Sanchez, Daniel},
title = {Adaptive scheduling for systems with asymmetric memory hierarchies},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00058},
doi = {10.1109/MICRO.2018.00058},
abstract = {Conventional multicores rely on deep cache hierarchies to reduce data movement. Recent advances in die stacking have enabled near-data processing (NDP) systems that reduce data movement by placing cores close to memory. NDP cores enjoy cheaper memory accesses and are more area-constrained, so they use shallow cache hierarchies instead. Since neither shallow nor deep hierarchies work well for all applications, prior work has proposed systems that incorporate both. These asymmetric memory hierarchies can be highly beneficial, but they require scheduling computation to the right hierarchy.We present AMS, an adaptive scheduler that automatically finds high-quality thread-to-hierarchy mappings. AMS monitors threads, accurately models their performance under different hierarchies and core types, and adapts algorithms first proposed for cache partitioning to produce high-quality schedules. AMS is cheap enough to use online, so it adapts to program phases, and performs within 1\% of an exhaustive-search scheduler. As a result, AMS outperforms asymmetry-oblivious schedulers by up to 37\% and by 18\% on average.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {641–654},
numpages = {14},
keywords = {scheduling, near-data processing, cache hierarchies, asymmetric systems, analytical performance modeling},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00059,
author = {Liu, Jiawen and Zhao, Hengyu and Ogleari, Matheus Almeida and Li, Dong and Zhao, Jishen},
title = {Processing-in-memory for energy-efficient neural network training: a heterogeneous approach},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00059},
doi = {10.1109/MICRO.2018.00059},
abstract = {Neural networks (NNs) have been adopted in a wide range of application domains, such as image classification, speech recognition, object detection, and computer vision. However, training NNs - especially deep neural networks (DNNs) - can be energy and time consuming, because of frequent data movement between processor and memory. Furthermore, training involves massive fine-grained operations with various computation and memory access characteristics. Exploiting high parallelism with such diverse operations is challenging. To address these challenges, we propose a software/hardware co-design of heterogeneous processing-in-memory (PIM) system. Our hardware design incorporates hundreds of fix-function arithmetic units and ARM-based programmable cores on the logic layer of a 3D die-stacked memory to form a heterogeneous PIM architecture attached to CPU. Our software design offers a programming model and a runtime system that program, offload, and schedule various NN training operations across compute resources provided by CPU and heterogeneous PIM. By extending the OpenCL programming model and employing a hardware heterogeneity-aware runtime system, we enable high program portability and easy program maintenance across various heterogeneous hardware, optimize system energy efficiency, and improve hardware utilization.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {655–668},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00060,
author = {Mao, Haiyu and Song, Mingcong and Li, Tao and Dai, Yuting and Shu, Jiwu},
title = {LerGAN: a zero-free, low data movement and PIM-based GAN architecture},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00060},
doi = {10.1109/MICRO.2018.00060},
abstract = {As a powerful unsupervised learning method, Generative Adversarial Network (GAN) plays an important role in many domains such as video prediction and autonomous driving. It is one of the ten breakthrough technologies in 2018 reported in MIT Technology Review. However, training a GAN imposes three more challenges: (1) intensive communication caused by complex train phases of GAN, (2) much more ineffectual computations caused by special convolutions, and (3) more frequent off-chip memory accesses for exchanging inter-mediate data between the generator and the discriminator.In this paper, we propose LerGAN1, a PIM-based GAN accelerator to address the challenges of training GAN. We first propose a zero-free data reshaping scheme for ReRAM-based PIM, which removes the zero-related computations. We then propose a 3D-connected PIM, which can reconfigure connections inside PIM dynamically according to dataflows of propagation and updating. Our proposed techniques reduce data movement to a great extent, avoiding I/O to become a bottleneck of training GANs. Finally, we propose LerGAN based on these two techniques, providing different levels of accelerating GAN for programmers. Experiments shows that LerGAN achieves 47.2x, 21.42x and 7.46x speedup over FPGA-based GAN accelerator, GPU platform, and ReRAM-based neural network accelerator respectively. Moreover, LerGAN achieves 9.75x, 7.68x energy saving on average over GPU platform, ReRAM-based neural network accelerator respectively, and has 1.04x energy consuming over FPGA-based GAN accelerator.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {669–681},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00061,
author = {Hong, Byungchul and Ro, Yeonju and Kim, John},
title = {Multi-dimensional parallel training of winograd layer on memory-centric architecture},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00061},
doi = {10.1109/MICRO.2018.00061},
abstract = {Accelerating neural network training is critical in exploring design space of neural networks. Data parallelism is commonly used to accelerate training for Convolutional Neural Networks (CNN) where input batch is distributed across the multiple workers; however, the communication time of weight gradients can limit scalability for moderate batch size. In this work, we propose multi-dimensional parallel training (MPT) of convolution layers by exploiting both data parallelism and intra-tile parallelism available in Winograd transformed convolution. Workers are organized across two dimensions - one dimension exploiting intra-tile parallelism while the other dimension exploits data parallelism. MPT reduces the amount of communication necessary for weight gradients since weight gradients are only communicated within the data parallelism dimension. However, Winograd transform fundamentally requires more data accesses and the proposed MPT architecture also introduces a new type of communication which we refer to as tile transfer - gather/scatter of Winograd domain feature maps (tiles). We propose a scalable near-data processing (NDP) architecture to minimize the cost of data accesses through 3D stacked memory while leveraging a memory-centric network organization to provide high-connectivity between the workers to accelerate tile transfer. To minimize tile gathering communication overhead, we exploit prediction of activation of spatial domain neurons in order to remove the communication of tiles that are transformed to non-activated neurons. We also propose dynamic clustering of the memory-centric network architecture that reconfigures the interconnect topology between the workers for each convolution layer to balance the communication required for weight gradients and tile transfer. Our evaluations show that the proposed MPT with NDP architecture accelerates training by up to 2.7x, 21x compared to data parallel training on the NDP architecture and a multi-GPU system, respectively.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {682–695},
numpages = {14},
keywords = {winograd transform, near-data processing, memory-centric network, convolutional neural network},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00062,
author = {Li, Shuangchen and Glova, Alvin Oliver and Hu, Xing and Gu, Peng and Niu, Dimin and Malladi, Krishna T. and Zheng, Hongzhong and Brennan, Bob and Xie, Yuan},
title = {SCOPE: a stochastic computing engine for DRAM-based in-situ accelerator},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00062},
doi = {10.1109/MICRO.2018.00062},
abstract = {Memory-centric architecture, which bridges the gap between compute and memory, is considered as a promising solution to tackle the memory wall and the power wall. Such architecture integrates the computing logic and the memory resources close to each other, in order to embrace large internal memory bandwidth and reduce the data movement overhead. The closer the compute and memory resources are located, the greater these benefits become. DRAM-based in-situ accelerators [1] tightly couple processing units to every memory bitline, achieving the maximum benefits among various memory-centric architectures. However, the processing units in such architectures are typically limited to simple functions like AND/OR due to strict area and power overhead constraints in DRAMs, making it difficult to accomplish complex tasks while providing high performance.In this paper, we address the challenge by applying stochastic computing arithmetic to the DRAM-based in-situ accelerator, targeting at the acceleration of error-tolerant applications such as deep learning. In stochastic computing, binary numbers are converted into stochastic bitstreams, which turns integer multiplications into simple bitwise AND operations, but at the expense of larger memory capacity/bandwidth demands. Stochastic computing is a perfect match for the DRAM-based in-situ accelerators because it addresses the in-situ accelerator's low performance problem by simplifying the operations, while leveraging the in-situ accelerator's advantage of large memory capacity/bandwidth. To further boost the performance and compensate for the numerical precision loss, we propose a novel Hierarchical and Hybrid Deterministic (H2D) stochastic computing arithmetic. Finally, we consider quantized deep neural network inference and training applications as a case study. The proposed architecture provides 2.3x improvement in performance per unit area compared with the binary arithmetic baseline, and 3.8x improvement over GPU. The proposed H2D arithmetic contributes 11x performance boost and 60\% numerical precision improvement.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {696–709},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00063,
author = {Zhang, Da and Sridharan, Vilas and Jian, Xun},
title = {Exploring and optimizing chipkill-correct for persistent memory based on high-density NVRAMs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00063},
doi = {10.1109/MICRO.2018.00063},
abstract = {Emerging high-density non-volatile random access memories (NVRAMs) can significantly enhance server main memory by providing both higher memory density and fast persistent memory. An unique design requirement for server main memory is strong reliability because uncorrectable errors can cause a system crash or permanent data loss. Traditional dynamic random access memory (DRAM) subsystems have used chipkill-correct to provide this reliability, while storage systems provide similar protection using very long ECC words (VLEWs).This paper presents an efficient chipkill-correct scheme for persistent memory based on high-density NVRAMs. For efficiency, the scheme decouples error correction at boot time from error correction at runtime. At boot time, when bit error rates are higher, the scheme uses VLEWs to efficiently ensure reliable data survival for a week to a year without refresh by correcting a large number of bit errors at low storage cost. At runtime, when bit error rates are lower, it reuses each memory block's chip failure protection bits to opportunistically correct bit errors at high performance. The proposal incurs a total storage cost of 27\%. Compared to a bit error correction scheme, the proposal adds chip failure protection at no additional storage cost and at 2\% average performance overhead.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {710–723},
numpages = {14},
keywords = {persistent memory systems, microarchitecture, ECC},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00064,
author = {Salami, Behzad and Unsal, Osman S. and Kestelman, Adrian Cristal},
title = {Comprehensive evaluation of supply voltage underscaling in FPGA on-chip memories},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00064},
doi = {10.1109/MICRO.2018.00064},
abstract = {In this work, we evaluate aggressive undervolting, i.e., voltage scaling below the nominal level to reduce the energy consumption of Field Programmable Gate Arrays (FPGAs). Usually, voltage guardbands are added by chip vendors to ensure the worst-case process and environmental scenarios. Through experimenting on several FPGA architectures, we measure this voltage guardband to be on average 39\% of the nominal level, which in turn, delivers more than an order of magnitude power savings. However, further undervolting below the voltage guardband may cause reliability issues as the result of the circuit delay increase, i.e., start to appear faults. We extensively characterize the behavior of these faults in terms of the rate, location, type, as well as sensitivity to environmental temperature, with a concentration of on-chip memories, or Block RAMs (BRAMs). Finally, we evaluate a typical FPGA-based Neural Network (NN) accelerator under low-voltage BRAM operations. In consequence, the substantial NN energy savings come with the cost of NN accuracy loss. To attain power savings without NN accuracy loss, we propose a novel technique that relies on the deterministic behavior of undervolting faults and can limit the accuracy loss to 0.1\% without any timing-slack overhead.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {724–736},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00065,
author = {Ozer, Emre and Venu, Balaji and Iturbe, Xabier and Das, Shidhartha and Lyberis, Spyros and Biggs, John and Harrod, Peter and Penton, John},
title = {Error correlation prediction in lockstep processors for safety-critical systems},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00065},
doi = {10.1109/MICRO.2018.00065},
abstract = {This paper presents a new phenomenon called error correlation prediction for lockstep processors. Lockstep processors run the same copy of a program, and their outputs are compared at every cycle to detect divergence, and have been popular in safety-critical systems. When the lockstep error checker detects an error, it alerts the safety-critical system by putting the lockstep processor in a safe state in order to prevent hazards. This is done by running the online diagnostics to identify the cause of the error because the lockstep processor has no knowledge of whether the error is caused by a transient or permanent fault. The online diagnostics can be avoided if the error is caused by a transient fault, and the lockstep processor can recover from it. If, however, it is caused by a permanent fault, having prior knowledge about error's likely location(s) within the CPU speeds up the diagnostics process. We discover that the error's type and likely location(s) inside CPUs from which the fault may have originated can be predicted by analyzing the output signals of the CPU(s) when the error is detected. We design a simple static predictor exploiting this phenomenon and show that system availability can be increased by 42--64\% with an overhead of less than 2\% in silicon area and power.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {737–748},
numpages = {12},
keywords = {redundant execution, lockstepping, fault tolerance and functional safety},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00066,
author = {Nie, Bin and Yang, Lishan and Jog, Adwait and Smirni, Evgenia},
title = {Fault site pruning for practical reliability analysis of GPGPU applications},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00066},
doi = {10.1109/MICRO.2018.00066},
abstract = {Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU applications is the purpose of this study. To this end, it is imperative to explore the range of application output by injecting faults at all the potential fault sites. This problem is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space - in the order of billions even for some simple applications.In this paper, we present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience can be practical. The key insight behind our proposed methodology stems from the fact that GPGPU applications spawn a lot of threads, however, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by a careful analysis of faults across threads and instructions. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be first classified based on the number of the dynamic instructions they execute. We achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying and analyzing: a) the dynamic instruction commonalities (and differences) across code blocks within this representative set of threads, b) a subset of loop iterations within the representative threads, and c) a subset of destination register bit positions. The above steps result in a tremendous reduction of fault sites by up to seven orders of magnitude. Yet, this reduced fault site space accurately captures the error resilience profile of GPGPU applications.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {749–761},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00067,
author = {Sullivan, Michael B. and Hari, Siva Kumar Sastry and Zimmer, Brian and Tsai, Timothy and Keckler, Stephen W.},
title = {SwapCodes: error codes for hardware-software cooperative GPU pipeline error detection},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00067},
doi = {10.1109/MICRO.2018.00067},
abstract = {Intra-thread instruction duplication offers straightforward and effective pipeline error detection for data-intensive processors. However, software-enforced instruction duplication uses explicit checking instructions, roughly doubles program register usage, and doubles the arithmetic operation count per thread, potentially leading to severe slowdowns. This paper investigates SwapCodes, a family of software-hardware cooperative mechanisms to accelerate intra-thread duplication in GPUs. SwapCodes leverages the register file ECC hardware to detect pipeline errors without sacrificing the ability of ECC to detect and correct storage errors. By implicitly checking for pipeline errors on each register read, SwapCodes avoids the overheads of instruction checking without adding new hardware error checkers or buffers. We describe a family of SwapCodes implementations that successively eliminate the sources of inefficiency in intra-thread duplication with different complexities and error detection and correction trade-offs. We apply SwapCodes to protect a GPU-based processor against pipeline errors, and demonstrate that it is able to detect more than 99.3\% of pipeline errors while improving performance and system efficiency relative to software-enforced duplication---the most performant SwapCodes organization incurs just 15\% average slowdown over the un-duplicated program.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {762–774},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00068,
author = {Qureshi, Moinuddin K.},
title = {CEASER: mitigating conflict-based cache attacks via encrypted-address and remapping},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00068},
doi = {10.1109/MICRO.2018.00068},
abstract = {Modern processors share the last-level cache between all the cores to efficiently utilize the cache space. Unfortunately, such sharing makes the cache vulnerable to attacks whereby an adversary can infer the access pattern of a co-running application by carefully orchestrating evictions using cache conflicts. Conflict-based attacks can be mitigated by randomizing the location of the lines in the cache. Unfortunately, prior proposals for randomized mapping require storage-intensive tables and are effective only if the OS can classify the applications into protected and unprotected groups. The goal of this paper is to mitigate conflict-based attacks while incurring negligible storage and performance overheads, and without relying on OS support.This paper provides the key insight that randomized mapping can be accomplished efficiently by accessing the cache with an encrypted address, as encryption would cause the lines that map to the same set of a conventional cache to get scattered to different sets. This paper proposes CEASE, a design that uses Low-Latency Block-Cipher (LLBC) to translate the physical line-address into an encrypted line-address, and accesses the cache with this encrypted line-address. We analyze efficient designs for LLBC that can perform encryption and decryption within two cycles. We also propose CEASER, a design that periodically changes the encryption key and performs dynamic-remapping to improve robustness. CEASER provides strong security (tolerates 100+ years of attack), has low performance overhead (1\% slowdown), requires a storage overhead of less than 24 bytes for the newly added structures, and does not need any OS support.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {775–787},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00069,
author = {Manerkar, Yatin A. and Lustig, Daniel and Martonosi, Margaret and Gupta, Aarti},
title = {PipeProof: automated memory consistency proofs for microarchitectural specifications},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00069},
doi = {10.1109/MICRO.2018.00069},
abstract = {Memory consistency models (MCMs) specify rules which constrain the values that can be returned by load instructions in parallel programs. To ensure that parallel programs run correctly, verification of hardware MCM implementations would ideally be complete; i.e. verified as being correct across all possible executions of all possible programs. However, no existing automated approach is capable of such complete verification.To help fill this verification gap, we present PipeProof, a methodology and tool for complete MCM verification of an axiomatic microarchitectural (hardware-level) ordering specification against an axiomatic ISA-level MCM specification. PipeProof can automatically prove a microarchitecture correct in all cases, or return an indication (often a counterexample) that the microarchitecture could not be verified. To accomplish unbounded verification, PipeProof introduces the novel Transitive Chain Abstraction to represent microarchitectural executions of an arbitrary number of instructions using only a small, finite number of instructions. With the help of this abstraction, PipeProof proves microarchitectural correctness using an automatic abstraction refinement approach. PipeProof's implementation also includes algorithmic optimizations which improve runtime by greatly reducing the number of cases considered. As a proof-of-concept study, we present results for modelling and proving correct simple microarchitectures implementing the SC and TSO MCMs. PipeProof verifies both case studies in under an hour, showing that it is indeed possible to automate microarchitectural MCM correctness proofs.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {788–801},
numpages = {14},
keywords = {memory consistency models, happens-before graphs, formal verification, automated verification, abstraction refinement},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00070,
author = {Alian, Mohammad and Min, Seung Won and Asgharimoghaddam, Hadi and Dhar, Ashutosh and Wang, Dong Kai and Roewer, Thomas and McPadden, Adam and O'Halloran, Oliver and Chen, Deming and Xiong, Jinjun and Kim, Daehoon and Hwu, Wen-mei and Kim, Nam Sung},
title = {Application-transparent near-memory processing architecture with memory channel network},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00070},
doi = {10.1109/MICRO.2018.00070},
abstract = {The physical memory capacity of servers is expected to increase drastically with the deployment of the forthcoming non-volatile memory technologies. This is a welcomed improvement for the emerging data-intensive applications. For such servers to be cost-effective, nonetheless, we must cost-effectively increase compute throughput and memory bandwidth commensurate with the increase in memory capacity without compromising the application readiness. Tackling this challenge, we present Memory Channel Network (MCN) architecture in this paper. Specifically, first, we propose an MCN DIMM, an extension of a buffered DIMM where a small but capable processor called MCN processor is integrated with a buffer device on the DIMM for near-memory processing. Second, we implement device drivers to give the host and MCN processors in a server an illusion that they are independent heterogeneous nodes connected through an Ethernet link. These allow the host and MCN processors in a server to run a given data-intensive application together based on popular distributed computing frameworks such as MPI and Spark without any change in the host processor hardware and its application software, while offering the benefits of high-bandwidth and low-latency communication between the host and MCN processors over the memory channels. As such, MCN can serve as an application-transparent framework which can seamlessly unify the near-memory processing within a server and the distributed computing across such servers for data-intensive applications. Our simulation running the full software stack shows that a server with 8 MCN DIMMs offers 4.56 x higher throughput and consume 47.5\% less energy than a cluster with 9 conventional nodes connected through Ethernet links, as it facilitates up to 8.17 x higher aggregate DRAM bandwidth utilization. Lastly, we demonstrate the feasibility of MCN with an IBM POWER8 system and an experimental buffered DIMM.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {802–814},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00071,
author = {Zhang, Rui and Deutschbein, Calvin and Huang, Peng and Sturton, Cynthia},
title = {End-to-end automated exploit generation for validating the security of processor designs},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00071},
doi = {10.1109/MICRO.2018.00071},
abstract = {This paper presents Coppelia, an end-to-end tool that, given a processor design and a set of security-critical invariants, automatically generates complete, replayable exploit programs to help designers find, contextualize, and assess the security threat of hardware vulnerabilities. In Coppelia, we develop a hardware-oriented backward symbolic execution engine with a new cycle stitching method and fast validation technique, along with several optimizations for exploit generation. We then add program stubs to complete the exploit. We evaluate Coppelia on three CPUs of different architectures. Coppelia is able to find and generate exploits for 29 of 31 known vulnerabilities in these CPUs, including 11 vulnerabilities that commercial and academic model checking tools can not find. All of the generated exploits are successfully replayable on an FPGA board. Moreover, Coppelia finds 4 new vulnerabilities along with exploits in these CPUs. We also use Coppelia to verify whether a security patch indeed fixed a vulnerability, and to refine a set of assertions.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {815–827},
numpages = {13},
keywords = {symbolic execution, processor security, exploit generation},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00072,
author = {Ding, Yongshan and Holmes, Adam and Javadi-Abhari, Ali and Franklin, Diana and Martonosi, Margaret and Chong, Frederic T.},
title = {Magic-state functional units: mapping and scheduling multi-level distillation circuits for fault-tolerant quantum architectures},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00072},
doi = {10.1109/MICRO.2018.00072},
abstract = {Quantum computers have recently made great strides and are on a long-term path towards useful fault-tolerant computation. A dominant overhead in fault-tolerant quantum computation is the production of high-fidelity encoded qubits, called magic states, which enable reliable error-corrected computation. We present the first detailed designs of hardware functional units that implement space-time optimized magic-state factories for surface code error-corrected machines.Interactions among distant qubits require surface code braids (physical pathways on chip) which must be routed. Magic-state factories are circuits comprised of a complex set of braids that is more difficult to route than quantum circuits considered in previous work [1]. This paper explores the impact of scheduling techniques, such as gate reordering and qubit renaming, and we propose two novel mapping techniques: braid repulsion and dipole moment braid rotation. We combine these techniques with graph partitioning and community detection algorithms, and further introduce a stitching algorithm for mapping subgraphs onto a physical machine. Our results show a factor of 5.64 reduction in space-time volume compared to the best-known previous designs for magic-state factories.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {828–840},
numpages = {13},
keywords = {surface code, quantum error correction, quantum computing, magic state distillation},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00073,
author = {George, Sumitha and Liao, Minli Julie and Jiang, Huaipan and Kotra, Jagadish B. and Kandemir, Mahmut T. and Sampson, Jack and Narayanan, Vijaykrishnan},
title = {MDACache: caching for multi-dimensional-access memories},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00073},
doi = {10.1109/MICRO.2018.00073},
abstract = {For several emerging memory technologies, a natural formulation of memory arrays (cross-point) provides nearly symmetric access costs along multiple (e.g., both row and column) dimensions in contrast to the row-oriented nature of most DRAM and SRAM implementations, producing a Multi-Dimensional-Access (MDA) memory. While MDA memories can directly support applications with both row and column preferences, most modern processors do not directly access either the rows or columns of memories: memory accesses proceed through a cache hierarchy that abstracts many of the physical features that supply the aforementioned symmetry. To reap the full benefits of MDA memories, a co-design approach must occur across software memory layout, the mapping between the physical and logical organization of the memory arrays, and the cache hierarchy itself in order to efficiently express, convey, and exploit multidimensional access patterns.In this paper, we describe a taxonomy for different ways of connecting row and column preferences at the application level to an MDA memory through an MDA cache hierarchy and explore specific implementations for the most plausible design points. We extend vectorization support at the compiler level to provide the necessary information to extract preferences and provide compatible memory layouts, and evaluate the tradeoffs among multiple cache designs for the MDA memory systems. Our results indicate that both logically 2-D caching using physically 1-D SRAM structures and on-chip physically 2-D caches can both provide significant improvements in performance over a traditional cache system interfacing with an MDA memory, reducing execution time by 72\% and 65\%, respectively. We then explore the sensitivity of these benefits as a function of the working-set to cache capacity ratio as well as to MDA technology assumptions.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {841–854},
numpages = {14},
keywords = {symmetric memories, row-column vectorization, crosspoint memory, cache design, 2D memory access},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00074,
author = {Samajdar, Ananda and Mannan, Parth and Garg, Kartikay and Krishna, Tushar},
title = {GeneSys: enabling continuous learning through neural network evolution in hardware},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00074},
doi = {10.1109/MICRO.2018.00074},
abstract = {Modern deep learning systems rely on (a) a hand-tuned neural network topology, (b) massive amounts of labelled training data, and (c) extensive training over large-scale compute resources to build a system that can perform efficient image classification or speech recognition. Unfortunately, we are still far away from implementing adaptive general purpose intelligent systems which would need to learn autonomously in unknown environments and may not have access to some or any of these three components. Reinforcement learning and evolutionary algorithm (EA) based methods circumvent this problem by continuously interacting with the environment and updating the models based on obtained rewards. However, deploying these algorithms on ubiquitous autonomous agents at the edge (robots/drones) demands extremely high energy-efficiency due to (i) tight power and energy budgets, (ii) continuous / lifelong interaction with the environment, (iii) intermittent or no connectivity to the cloud to offloadheavy-weight processing.To address this need, we present Genesys, a HW-SW prototype of a EA-based learning system, that comprises of a closed loop learning engine called EvE and an inference engine called ADAM. EvE can evolve the topology and weights of neural networks completely in hardware for the task at hand, without requiring hand-optimization or backpropogation training. ADAM continuously interacts with the environment and is optimized for efficiently running the irregular neural networks generated by EvE. Genesys identifies and leverages multiple unique avenues of parallelism unique to EAs that we term "gene"-level parallelism, and "population"-level parallelism. We ran Genesys with a suite of environments from OpenAI gym and observed 2--5 orders of magnitude higher energy-efficiency over state-of-the-art embedded and desktop CPU and GPU systems.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {855–866},
numpages = {12},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00075,
author = {Rengasamy, Prasanna Venkatesh and Zhang, Haibo and Zhao, Shulin and Nachiappan, Nachiappan Chidambaram and Sivasubramaniam, Anand and Kandemir, Mahmut T and Das, Chita R},
title = {CritICs critiquing criticality in mobile apps},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00075},
doi = {10.1109/MICRO.2018.00075},
abstract = {In this paper, we conduct a systematic analysis to show that existing CPU optimizations targeting scientific/server workloads are not always well suited for mobile apps. In particular, we observe that the well-known and very important concept of identifying and accelerating individual critical instructions in workloads such as SPEC, are not as effective for mobile apps. Several differences in mobile app characteristics including (i) dependencies between critical instructions interspersed with non-critical instructions in the dependence chain, (ii) temporal proximity of the critical instructions in the dynamic stream, and (iii) the bottleneck shifting to the front from the rear of the datapath pipeline, are key contributors to the ineffectiveness of traditional criticality based optimizations. Instead, we propose the concept of Critical Instruction Chains (CritICs) - which are short, critical and self contained sequences of instructions, for aggregate level optimization. With motivating results, we show that an offline profiler/analysis framework can easily identify these CritICs, and we propose a very simple software mechanism in the compiler that exploits ARM's 16-bit ISA format to nearly double the fetch bandwidth of these instructions. We have implemented this entire framework - both profiler and compiler passes, and evaluated its effectiveness for 10 popular apps from the Play Store. Experimental evaluations show that our approach is much more effective than two previously studied criticality optimizations, yielding a speedup of 12.65\%, and energy savings of 15\% in the CPU (translating to a system wide energy savings of 4.6\%), requiring very little additional hardware support.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {867–880},
numpages = {14},
keywords = {mobile, energy, criticality, CPU},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00076,
author = {Dey, Moumita and Nazari, Alireza and Zajic, Alenka and Prvulovic, Milos},
title = {TEMProf: memory profiling via EM-emanation in iot and hand-held devices},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00076},
doi = {10.1109/MICRO.2018.00076},
abstract = {This paper presents EMProf, a new method for profiling the performance impact of the memory subsystem without any support on, or interference with, the profiled system. Rather than rely on hardware support and/or software instrumentation on the profiled system, EMProf analyzes the system's EM emanations to identify processor stalls that are associated with last-level cache (LLC) misses. This enables EMProf to accurately pinpoint LLC misses in the execution timeline and to measure the cost (stall time) of each miss. Since EMProf has zero "observer effect", so it can be used to profile applications that adjust their activity to their performance. It has no overhead on target machine, so it can be used for profiling embedded, hand-held, and IoT devices which usually have limited support for collecting, and limited resources for storing, the profiling data. Finally, since EMProf can profile the system as-is, its profiling of boot code and other hard-to-profile software components is as accurate as its profiling of application code. To illustrate the effectiveness of EMProf, we first validate its results using micro-benchmarks with known memory behavior, and also on SPEC benchmarks running a cycle-accurate simulator that can provide detailed ground-truth data about LLC misses and processor stalls. We then demonstrate the effectiveness of EMProf on real systems, including profiling of boot activity, show how its results can be attributed to the specific parts of the application code when that code is available, and provide additional insight on the statistics reported by EMProf and how they are affected by the EM signal bandwidth provided to EMProf.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {881–893},
numpages = {13},
keywords = {system profiling, memory profiling, main memory, last-level cache, cycle-accurate simulation, code attribution, EM side-channel, EM emanation},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00077,
author = {Boroujerdian, Behzad and Genc, Hasan and Krishnan, Srivatsan and Cui, Wenzhi and Faust, Aleksandra and Reddi, Vijay Janapa},
title = {MAVBench: micro aerial vehicle benchmarking},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00077},
doi = {10.1109/MICRO.2018.00077},
abstract = {Unmanned Aerial Vehicles (UAVs) are getting closer to becoming ubiquitous in everyday life. Among them, Micro Aerial Vehicles (MAVs) have seen an outburst of attention recently, specifically in the area with a demand for autonomy. A key challenge standing in the way of making MAVs autonomous is that researchers lack the comprehensive understanding of how performance, power, and computational bottlenecks affect MAV applications. MAVs must operate under a stringent power budget, which severely limits their flight endurance time. As such, there is a need for new tools, benchmarks, and methodologies to foster the systematic development of autonomous MAVs. In this paper, we introduce the "MAVBench" framework which consists of a closed-loop simulator and an end-to-end application benchmark suite. A closed-loop simulation platform is needed to probe and understand the intra-system (application data flow) and inter-system (system and environment) interactions in MAV applications to pinpoint bottlenecks and identify opportunities for hardware and software co-design and optimization. In addition to the simulator, MAVBench provides a benchmark suite, the first of its kind, consisting of a variety of MAV applications designed to enable computer architects to perform characterization and develop future aerial computing systems. Using our open source, end-to-end experimental platform, we uncover a hidden, and thus far unexpected compute to total system energy relationship in MAVs. Furthermore, we explore the role of compute by presenting three case studies targeting performance, energy and reliability. These studies confirm that an efficient system design can improve MAV's battery consumption by up to 1.8X.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {894–907},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00078,
author = {Liu, Hongyuan and Ibrahim, Mohamed and Kayiran, Onur and Pai, Sreepathi and Jog, Adwait},
title = {Architectural support for efficient large-scale automata processing},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00078},
doi = {10.1109/MICRO.2018.00078},
abstract = {The Automata Processor (AP) accelerates applications from domains ranging from machine learning to genomics. However, as a spatial architecture, it is unable to handle larger automata programs without repeated reconfiguration and re-execution. To achieve high throughput, this paper proposes for the first time architectural support for AP to efficiently execute large-scale applications. We find that a large number of existing and new Non-deterministic Finite Automata (NFA) based applications have states that are never enabled but are still configured on the AP chips leading to their underutilization. With the help of careful characterization and profiling-based mechanisms, we predict which states are never enabled and hence need not be configured on AP. Furthermore, we develop SparseAP, a new execution mode for AP to efficiently handle the mis-predicted NFA states. Our detailed simulations across 26 applications from various domains show that our newly proposed execution model for AP can obtain 2.1 x geometric mean speedup (up to 47 x) over the baseline AP execution.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {908–920},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00079,
author = {Angstadt, Kevin and Subramaniyan, Arun and Sadredini, Elaheh and Rahimi, Reza and Skadron, Kevin and Weimer, Westley and Das, Reetuparna},
title = {ASPEN: a scalable in-SRAM architecture for pushdown automata},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00079},
doi = {10.1109/MICRO.2018.00079},
abstract = {Many applications process some form of tree-structured or recursively-nested data, such as parsing XML or JSON web content as well as various data mining tasks. Typical CPU processing solutions are hindered by branch misprediction penalties while attempting to reconstruct nested structures and also by irregular memory access patterns. Recent work has demonstrated improved performance for many data processing applications through memory-centric automata processing engines. Unfortunately, these architectures do not support a computational model rich enough for tasks such as XML parsing.In this paper, we present ASPEN, a general-purpose, scalable, and reconfigurable memory-centric architecture for processing of tree-like data. We take inspiration from previous automata processing architectures, but support the richer deterministic pushdown automata computational model. We propose a custom datapath capable of performing the state matching, stack manipulation, and transition routing operations of pushdown automata, all efficiently stored and computed in memory arrays. Further, we present compilation algorithms for transforming large classes of existing grammars to pushdown automata executable on ASPEN, and demonstrate their effectiveness on four different languages: Cool (object oriented programming), DOT (graph visualization), JSON, and XML.Finally, we present an empirical evaluation of two application scenarios for ASPEN: XML parsing, and frequent subtree mining. The proposed architecture achieves an average 704.5 ns per KB parsing XML compared to 9983 ns per KB in a state-of-the-art XML parser across 23 benchmarks. We also demonstrate a 37.2x and 6x better end-to-end speedup over CPU and GPU implementations of subtree mining.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {921–932},
numpages = {12},
keywords = {pushdown automata, emerging technologies (memory and computing), accelerators},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00080,
author = {Hegde, Kartik and Agrawal, Rohit and Yao, Yulun and Fletcher, Christopher W.},
title = {Morph: flexible acceleration for 3D CNN-based video understanding},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00080},
doi = {10.1109/MICRO.2018.00080},
abstract = {The past several years have seen both an explosion in the use of Convolutional Neural Networks (CNNs) and the design of accelerators to make CNN inference practical. In the architecture community, the lion share of effort has targeted CNN inference for image recognition. The closely related problem of video recognition has received far less attention as an accelerator target. This is surprising, as video recognition is more computationally intensive than image recognition, and video traffic is predicted to be the majority of internet traffic in the coming years.This paper fills the gap between algorithmic and hardware advances for video recognition by providing a design space exploration and flexible architecture for accelerating 3D Convolutional Neural Networks (3D CNNs)---the core kernel in modern video understanding. When compared to (2D) CNNs used for image recognition, efficiently accelerating 3D CNNs poses a significant engineering challenge due to their large (and variable over time) memory footprint and higher dimensionality.To address these challenges, we design a novel accelerator, called Morph, that can adaptively support different spatial and temporal tiling strategies depending on the needs of each layer of each target 3D CNN. We codesign a software infrastructure alongside the Morph hardware to find good-fit parameters to control the hardware. Evaluated on state-of-the-art 3D CNNs, Morph achieves up to 3.4x (2.5x average) reduction in energy consumption and improves performance/watt by up to 5.1 x (4x average) compared to a baseline 3D CNN accelerator, with an area overhead of 5\%. Morph further achieves a 15.9x average energy reduction on 3D CNNs when compared to Eyeriss.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {933–946},
numpages = {14},
keywords = {video recognition, hardware/-software codesign, hardware acceleration, dataflow, 3D convolutional neural networks},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00081,
author = {Trippel, Caroline and Lustig, Daniel and Martonosi, Margaret},
title = {CheckMate: automated synthesis of hardware exploits and security litmus tests},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00081},
doi = {10.1109/MICRO.2018.00081},
abstract = {Recent research has uncovered a broad class of security vulnerabilities in which confidential data is leaked through programmer-observable microarchitectural state. In this paper, we present CheckMate, a rigorous approach and automated tool for determining if a microarchitecture is susceptible to specified classes of security exploits, and for synthesizing proof-of-concept exploit code when it is. Our approach adopts "microarchitecturally happens-before" (μhb) graphs which prior work designed to capture the subtle orderings and interleavings of hardware execution events when programs run on a microarchitecture. CheckMate extends μhb graphs to facilitate modeling of security exploit scenarios and hardware execution patterns indicative of classes of exploits. Furthermore, it leverages relational model finding techniques to enable automated exploit program synthesis from microarchitecture and exploit pattern specifications.As a case study, we use CheckMate to evaluate the susceptibility of a speculative out-of-order processor to Flush+Reload cache side-channel attacks. The automatically synthesized results are programs representative of Meltdown and Spectre attacks. We then evaluate the same processor on its susceptibility to a different timing side-channel attack: Prime+Probe. Here, Check-Mate synthesized new exploits that are similar to Meltdown and Spectre in that they leverage speculative execution, but unique in that they exploit distinct microarchitectural behaviors---speculative cache line invalidations rather than speculative cache pollution---to form a side-channel. Most importantly, our results validate the CheckMate approach to formal hardware security verification and the ability of the CheckMate tool to detect real-world vulnerabilities.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {947–960},
numpages = {14},
keywords = {side-channel attacks, relational model finding, hardware security, exploit synthesis, automated verification},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00082,
author = {Zhang, Xian and Sun, Guangyu and Xie, Peichen and Zhang, Chao and Liu, Yannan and Wei, Lingxiao and Xu, Qiang and Xue, Chun Jason},
title = {Shadow block: accelerating ORAM accesses with data duplication},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00082},
doi = {10.1109/MICRO.2018.00082},
abstract = {Oblivious RAM (ORAM) is a cryptographic primitive designed to hide memory access patterns. To achieve this objective, the intended data block is loaded and evicted back together with other data blocks and dummy blocks in each ORAM access. To further protect the timing pattern, extra dummy ORAM accesses are triggered periodically. Such designs lead to huge memory access overheads. Many techniques have been proposed to mitigate this problem by reducing the total number of ORAM accesses and the number of blocks per access. However, the impact of the access order of intended data block in an ORAM access is not addressed yet. In this work, we argue that higher performance can be achieved by advancing the access to the intended data block in ORAM accesses.However, changing the access order of blocks directly compromises the ORAM security. To solve this problem, we propose a duplication method to advance the access to the intended data blocks without compromising the ORAM security. The method leverages dummy blocks to store extra copies of data blocks, to facilitate early access of intended data blocks. These dummy blocks with valid data duplications are called Shadow blocks in this work. We further introduce two data duplication techniques, called RD-Dup and HD-Dup, to reorder the data block access for different purposes. In addition, we propose ORAM space partitioning to make RD-Dup and HD-Dup cooperate with each other efficiently. Compared with state-of-the-art ORAMs, our design can achieve a 32\% reduction in system execution time on average, with negligible hardware overheads.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {961–973},
numpages = {13},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1109/MICRO.2018.00083,
author = {Kiriansky, Vladimir and Lebedev, Ilia and Amarasinghe, Saman and Devadas, Srinivas and Emer, Joel},
title = {DAWG: a defense against cache timing attacks in speculative execution processors},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00083},
doi = {10.1109/MICRO.2018.00083},
abstract = {Software side channel attacks have become a serious concern with the recent rash of attacks on speculative processor architectures. Most attacks that have been demonstrated exploit the cache tag state as their exfiltration channel. While many existing defense mechanisms that can be implemented solely in software have been proposed, these mechanisms appear to patch specific attacks, and can be circumvented. In this paper, we propose minimal modifications to hardware to defend against a broad class of attacks, including those based on speculation, with the goal of eliminating the entire attack surface associated with the cache state covert channel.We propose DAWG, Dynamically Allocated Way Guard, a generic mechanism for secure way partitioning of set associative structures including memory caches. DAWG endows a set associative structure with a notion of protection domains to provide strong isolation. When applied to a cache, unlike existing quality of service mechanisms such as Intel's Cache Allocation Technology (CAT), DAWG fully isolates hits, misses, and metadata updates across protection domains. We describe how DAWG can be implemented on a processor with minimal modifications to modern operating systems. We describe a non-interference property that is orthogonal to speculative execution and therefore argue that existing attacks such as Spectre Variant 1 and 2 will not work on a system equipped with DAWG. Finally, we evaluate the performance impact of DAWG on the cache subsystem.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {974–987},
numpages = {14},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

